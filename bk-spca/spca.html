<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<meta name="generator" content="http://www.nongnu.org/elyxer/"/>
<meta name="create-date" content="2017-12-09"/>
<link rel="stylesheet" href="lyx.css" type="text/css" media="all"/>
<title>Scientific Programming and Computer Architecture</title>
</head>
<body>
<div id="globalWrapper">
<div class="Unindented">

</div>
<div class="Indented">

</div>
<div class="Indented">
<div class="left">
<b><span class="large">Scientific Programming and Computer Architecture</span></b>
</div>

</div>
<div class="Indented">
<p><br/>
</p>

</div>
<div class="Indented">

</div>
<div class="Indented">
<div class="left">
<b>Scientific and Engineering Computation</b>
</div>

</div>
<div class="Indented">
<div class="left">
William Gropp and Ewing Lusk, editors; Janusz Kowalik, founding editor
</div>

</div>
<div class="Indented">
<div class="left">
A complete list of books published in the Scientific and Engineering Computation series appears at the back of this book.
</div>

</div>
<div class="Indented">
<p><br/>
</p>

</div>
<div class="Indented">

</div>
<div class="Indented">
<div class="left">
<b><span class="large">Scientific Programming and Computer Architecture</span></b>
</div>

</div>
<div class="Indented">
<div class="vspace" style="height: 1cm;">

</div>

</div>
<div class="Indented">
<div class="left">
<span class="large">Divakar Viswanath</span>
</div>

</div>
<div class="Indented">
<div class="vfill"> </div>
</div>
<div class="Indented">
<div class="left">
The MIT Press<br/>
Cambridge, Massachusetts<br/>
London, England
</div>

</div>
<div class="Indented">
<p><br/>
</p>

</div>
<div class="Indented">

</div>
<div class="Indented">
<div class="left">
<span class="footnotesize">© 2017 Massachusetts Institute of Technology</span>
</div>

</div>
<div class="Indented">
<div class="left">
<span class="footnotesize">All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.</span>
</div>

</div>
<div class="Indented">
<div class="left">
<span class="footnotesize">This book was set in LyX by the author. Printed and bound in the United States of America.</span>
</div>

</div>
<div class="Indented">
<div class="vspace" style="height: 1.5cm;">

</div>

</div>
<div class="Indented">
<div class="left">
<span class="footnotesize">Library of Congress Cataloging-in-Publication Data<br/>
<span class="formula"> </span><br/>
Names: Viswanath, Divakar, author.</span><span class="default"> <br/>
</span><span class="footnotesize">Title: Scientific programming and computer architecture / Divakar Viswanath. <br/>
Description: Cambridge, MA : The MIT Press, [2017] | Series: Scientific and <br/>
<span class="hspace" style="width: 0.4cm;"></span>engineering computation | Includes bibliographical references and index. <br/>
Identifiers: LCCN 2016043792 | ISBN 9780262036290 (hardcover : alk. paper) <br/>
Subjects: LCSH: Computer programming. | Computer architecture. | Software <br/>
<span class="hspace" style="width: 0.4cm;"></span>engineering. | C (Computer program language)<br/>
Classification: LCC QA76.6 .V573 2017 | DDC 005.1--dc23 LC record <br/>
<span class="hspace" style="width: 0.4cm;"></span>available at https://lccn.loc.gov/2016043792</span>
</div>

</div>
<div class="Indented">
<div class="vspace" style="height: 1cm;">

</div>

</div>
<div class="Indented">
<div class="left">
<span class="footnotesize">10 9 8 7 6 5 4 3 2 1</span>
</div>

</div>
<div class="Indented">
<p><br/>
</p>

</div>
<div class="Indented">

</div>
<div class="Indented">
<div class="vfill"> </div>
</div>
<div class="Indented">
<div class="center">
To all my teachers, with thanks.
</div>

</div>
<div class="Indented">
<div class="vfill"> </div>
</div>
<div class="Indented">
<p><br/>
</p>

</div>
<div class="Indented">

</div>
<div class="Indented">
<span class="formula"> </span>
</div>
<div class="Indented">
<p><br/>
</p>

</div>
<div class="Indented">

</div>
<div class="fulltoc">
<div class="tocheader">
Table of Contents
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Chapter--1">Chapter: Preface</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-1">Chapter 1: C/C++: Review </a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-1.1">Section 1.1: An example: The Aitken transformation</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-1.1.1">Subsection 1.1.1: Leibniz series and the logarithmic series</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.1.2">Subsection 1.1.2: Modular organization of sources</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-1.2">Section 1.2: C review</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2.1">Subsection 1.2.1: Header files</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2.2">Subsection 1.2.2: Arrays and pointers</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2.3">Subsection 1.2.3: The Aitken iteration using arrays and pointers</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2.4">Subsection 1.2.4: Declarations and definitions</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2.5">Subsection 1.2.5: Function calls and the compilation process</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-1.3">Section 1.3: C++ review</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-1.3.1">Subsection 1.3.1: The <tt>Vector</tt> class</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.3.2">Subsection 1.3.2: Aitken transformation in C++</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-1.4">Section 1.4: A little Fortran</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-1.5">Section 1.5: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-2">Chapter 2: C/C++: Libraries and Makefiles</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-2.1">Section 2.1: Mixed-language programming</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-2.1.1">Subsection 2.1.1: Transmutation of names from source to object files</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.1.2">Subsection 2.1.2: Linking Fortran programs with C and C++</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-2.2">Section 2.2: Using BLAS and LAPACK libraries</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-2.2.1">Subsection 2.2.1: Arrays, matrices, and leading dimensions</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.2.2">Subsection 2.2.2: BLAS and LAPACK</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.2.3">Subsection 2.2.3: C++ class interface to BLAS/LAPACK</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-2.3">Section 2.3: Building programs using GNU Make</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.1">Subsection 2.3.1: The <tt>utils/ </tt>folder</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.2">Subsection 2.3.2: Targets, prerequisites, and dependency graphs</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.3">Subsection 2.3.3: Make variables in <tt>makevars.mk</tt></a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.4">Subsection 2.3.4: Pattern rules in <tt>makevars.mk</tt></a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.5">Subsection 2.3.5: Phony targets in <tt>makevars.mk</tt></a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.6">Subsection 2.3.6: Recursive <tt>make</tt> and <tt>.d</tt> files<tt></tt></a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.7">Subsection 2.3.7: Beyond recursive <tt>make</tt></a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.3.8">Subsection 2.3.8: Building your own library</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-2.4">Section 2.4: The Fast Fourier Transform </a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-2.4.1">Subsection 2.4.1: The FFT algorithm in outline</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.4.2">Subsection 2.4.2: FFT using MKL</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.4.3">Subsection 2.4.3: FFT using FFTW</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.4.4">Subsection 2.4.4: Cycles and histograms</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.4.5">Subsection 2.4.5: Optimality of FFT implementations</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-2.5">Section 2.5: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-3">Chapter 3: The Processor</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-3.1">Section 3.1: Overview of the x86 architecture</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-3.1.1">Subsection 3.1.1: 64-bit x86 architecture</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.1.2">Subsection 3.1.2: 64-bit x86 assembly programming</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.1.3">Subsection 3.1.3: The Time Stamp Counter</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.1.4">Subsection 3.1.4: Cache parameters and the CPUID instruction</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-3.2">Section 3.2: Compiler optimizations</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.1">Subsection 3.2.1: Preliminaries</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.2">Subsection 3.2.2: Loop unrolling</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.3">Subsection 3.2.3: Loop fusion</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.4">Subsection 3.2.4: Unroll and jam</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.5">Subsection 3.2.5: Loop interchange</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.6">Subsection 3.2.6: C++ overhead</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2.7">Subsection 3.2.7: A little compiler theory</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-3.3">Section 3.3: Optimizing for the instruction pipeline</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-3.3.1">Subsection 3.3.1: Instruction pipelines</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.3.2">Subsection 3.3.2: Chipsets </a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.3.3">Subsection 3.3.3: Peak floating point performance</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.3.4">Subsection 3.3.4: Microkernel for matrix multiplication</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-3.4">Section 3.4: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-4">Chapter 4: Memory</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-4.1">Section 4.1: DRAM and cache memory</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1.1">Subsection 4.1.1: DRAM memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1.2">Subsection 4.1.2: Cache memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1.3">Subsection 4.1.3: Physical memory and virtual memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1.4">Subsection 4.1.4: Latency to DRAM memory: First attempts</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1.5">Subsection 4.1.5: Latency to DRAM</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-4.2">Section 4.2: Optimizing memory access</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-4.2.1">Subsection 4.2.1: Bandwidth to DRAM</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.2.2">Subsection 4.2.2: Matrix transpose</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.2.3">Subsection 4.2.3: Optimized matrix multiplication</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-4.3">Section 4.3: Reading from and writing to disk</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-4.3.1">Subsection 4.3.1: C versus C++</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.3.2">Subsection 4.3.2: Latency to disk</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.3.3">Subsection 4.3.3: Bandwidth to disk</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-4.4">Section 4.4: Page tables and virtual memory</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-4.4.1">Subsection 4.4.1: Partitioning the virtual address space</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.4.2">Subsection 4.4.2: Physical address space and page tables</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-4.5">Section 4.5: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-5">Chapter 5: Threads and Shared Memory</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-5.1">Section 5.1: Introduction to OpenMP</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-5.1.1">Subsection 5.1.1: OpenMP syntax</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.1.2">Subsection 5.1.2: Shared variables and OpenMP’s memory model</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.1.3">Subsection 5.1.3: Overheads of OpenMP constructs</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-5.2">Section 5.2: Optimizing OpenMP programs</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-5.2.1">Subsection 5.2.1: Near memory and far memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.2.2">Subsection 5.2.2: Bandwidth to DRAM memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.2.3">Subsection 5.2.3: Matrix transpose</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.2.4">Subsection 5.2.4: Fast Fourier transform</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-5.3">Section 5.3: Introduction to Pthreads</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-5.3.1">Subsection 5.3.1: Pthreads</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.3.2">Subsection 5.3.2: Overhead of thread creation</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.3.3">Subsection 5.3.3: Parallel regions using Pthreads</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-5.4">Section 5.4: Program memory</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-5.4.1">Subsection 5.4.1: An easy system call</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.4.2">Subsection 5.4.2: Stacks</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.4.3">Subsection 5.4.3: Segmentation faults and memory errors</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-5.5">Section 5.5: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-6">Chapter 6: Special Topic: Networks and Message Passing</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-6.1">Section 6.1: MPI: Getting started</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-6.1.1">Subsection 6.1.1: Initializing MPI</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.1.2">Subsection 6.1.2: Unsafe communication in MPI</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-6.2">Section 6.2: High-performance network architecture</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-6.2.1">Subsection 6.2.1: Fat-tree network</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.2.2">Subsection 6.2.2: Infiniband network architecture</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-6.3">Section 6.3: MPI examples</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3.1">Subsection 6.3.1: Variants of MPI send and receive</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3.2">Subsection 6.3.2: Jacobi iteration</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3.3">Subsection 6.3.3: Matrix transpose</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3.4">Subsection 6.3.4: Collective communication</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3.5">Subsection 6.3.5: Parallel I/O in MPI</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-6.4">Section 6.4: The Internet</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4.1">Subsection 6.4.1: IP addresses</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4.2">Subsection 6.4.2: Send and receive</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4.3">Subsection 6.4.3: Server</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4.4">Subsection 6.4.4: Client</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4.5">Subsection 6.4.5: Internet latency</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4.6">Subsection 6.4.6: Internet bandwidth</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-6.5">Section 6.5: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-7">Chapter 7: Special Topic: The Xeon Phi Coprocessor</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-7.1">Section 7.1: Xeon Phi architecture </a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-7.1.1">Subsection 7.1.1: Peak floating point bandwidth</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-7.1.2">Subsection 7.1.2: A simple Phi program</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-7.1.3">Subsection 7.1.3: Xeon Phi memory system</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-7.2">Section 7.2: Offload</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-7.2.1">Subsection 7.2.1: Initializing to use the MIC device</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-7.2.2">Subsection 7.2.2: The <tt>target(mic)</tt><span class="default"> declaration specification</span></a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-7.2.3">Subsection 7.2.3: Summing the Leibniz series</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-7.2.4">Subsection 7.2.4: Offload bandwidth</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-7.3">Section 7.3: Two examples: FFT and matrix multiplication</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-7.3.1">Subsection 7.3.1: FFT</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-7.3.2">Subsection 7.3.2: Matrix multiplication</a>
</div>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-8">Chapter 8: Special Topic: Graphics Coprocessor</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-8.1">Section 8.1: Graphics coprocessor architecture </a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-8.1.1">Subsection 8.1.1: Graphics processor capability</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-8.1.2">Subsection 8.1.2: Host and device memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-8.1.3">Subsection 8.1.3: Timing CUDA kernels</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-8.1.4">Subsection 8.1.4: Warps and thread blocks</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-8.2">Section 8.2: Introduction to CUDA</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-8.2.1">Subsection 8.2.1: Summing the Leibniz series</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-8.2.2">Subsection 8.2.2: CUDA compilation</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-8.3">Section 8.3: Two examples</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-8.3.1">Subsection 8.3.1: Bandwidth to memory</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-8.3.2">Subsection 8.3.2: Matrix multiplication</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-8.4">Section 8.4: References</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Chapter-9">Chapter 9: Machines Used, Plotting, Python, GIT, Cscope, and gcc</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-9.1">Section 9.1: Machines used</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-9.2">Section 9.2: Plotting in C/C++ and other preliminaries</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-9.3">Section 9.3: C/C++ versus Python versus MATLAB </a>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-9.4">Section 9.4: GIT</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-9.5">Section 9.5: Cscope</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-9.6">Section 9.6: Compiling with gcc/g++</a>
</div>
</div>
</div>

</div>
<div class="Unindented">

</div>
<div class="Indented">
<div class="vspace" style="height: 1cm;">

</div>

</div>
<div class="Indented">
<div class="Doublebox" style="width: 75%;">
The website <a class="FlexURL" href="https://github.com/divakarvi/bk-spca">https://github.com/divakarvi/bk-spca</a> has all the programs discussed in this book.
</div>

</div>
<div class="Indented">

</div>
<div class="Indented">

</div>
<h1 class="Chapter-">
<a class="toc" name="toc-Chapter--1"></a>Preface
</h1>
<div class="Unindented">
It is a common experience that minor changes to C/C++ programs can make a big difference to their speed. Although all programmers who opt for C/C++ do so at least partly, and much of the time mainly, because programs in these languages can be fast, writing fast programs in these languages is not so straightforward. Well-optimized programs in C/C++ can be even 10 or more times faster than programs that are not well optimized.
</div>
<div class="Indented">
At the heart of this book is the following question: what makes computer programs fast or slow? Programming languages provide a level of abstraction that makes computers look simpler than they are. As soon as we ask this question about program speed, we have to get behind the abstractions and understand how a computer really works and how programming constructs map to different parts of the computer’s architecture. Although there is much that can be understood, the modern computer is such a complicated device that this basic question cannot be answered perfectly.
</div>
<div class="Indented">
Writing fast programs is the major theme of this book, but it is not the only theme. The other theme is modularity of programs. Structuring programs so that their structure explains what they do is a valuable principle. Computer programs are organized into a series of functions to serve the purpose of that principle. Yet when computer programs become large, merely dividing a program into functions becomes highly inadequate. It becomes necessary to organize the computer program into distinct sources and the sources into a source tree. The entire source tree can be made available as a library. We pay heed to program structure throughout this book.
</div>
<div class="Indented">
Most books on computer programming are written at the same level of abstraction as the programming language they utilize or explain. If we want to understand program speed, we have to understand the different parts of a computer, and such an approach is not feasible. It is inevitable that choices have to be made regarding the type of computer system that is studied.
</div>
<div class="Indented">
The big choices in this book are to opt for the x86 line of computers backed by Intel and AMD corporations, and for the Linux operating system. Nearly 100% of the computers in use today as servers, desktops, or laptops are x86 based, and the x86 line has been dominant for more than 30 years. So a great deal is not lost. The choice of the operating system does not have such a great impact on program speed. We pick Linux because it is the preferred platform for scientific computing and because it is open source. Because it is open source, we can peer into its inner workings when necessary to understand program speed.
</div>
<div class="Indented">
A computer program is mathematical logic in action. The picture of the computer that emerges from this book shows how layered, and therefore complex, that logic can be. There is of course the design of the program that we ourselves write. But that is only a small part of the overall design. There are other computer programs, the biggest of which is the operating system kernel (the Linux kernel for our purposes), which handle the program we write and make it run on the computer. These systems programs make the computer a more tractable device, hiding the complexity of many of its parts, such as the hard disk or the network interface. The hardware, which includes the processor architecture and memory system, is itself designed using complex logic similar in kind to the computer programs we ourselves write but vastly different in degree of complexity. 
</div>
<div class="Indented">
The conventional approach to high-performance computing revolves around general principles, such as Amdahl’s laws, weak and strong scaling, data and functional parallelism, SIMD/MIMD programming, load balancing, and blocking. The approach taken in this book is diametrically opposite. Our focus really is on understanding the computer as a whole, especially as viewed through programs. We dig into linkers, compilers, operating systems, and computer architecture to understand how the different parts of the computer interact with programs. The general principles are useful in getting a sense of the average program. However, in writing any particular program, the general principles are never so straightforward to apply. Understanding the parts of a computer can be far more nettlesome and also fascinating. Once that is done, the general principles, insofar as they are useful in actual programming, become self-evident. 
</div>
<div class="Indented">
The main principles of design that will concern us and that have an impact on program speed have not changed for decades. The approach we adopt is to begin with specific programs that are generally quite simple in what they do. We move up to general principles gradually using these specific programs as a vehicle. There are two advantages to this approach. First, it makes the discussion vivid, more organized, and far less reliant on rules of thumb that may appear arbitrary. Second, it lends useful context to the principles that inform the writing of well-optimized computer programs.
</div>
<div class="Indented">
The context is essential. There are no general principles of the range, precision, and completeness of Schrödinger’s equation in this area. Such principles as there are do not go far beyond common sense.  Take, for example, the idea of load balancing. It says that if a task is to be divided between equally capable workers, we are better off dividing the task equally, rather than burdening a single worker with an excessive share of the work. There can be absolutely no doubt that this principle has been known for millennia. Without the appropriate context, the principle is quite sterile. Indeed, to call it a principle seems an exaggeration.
</div>
<h? class="Subsubsection">
<b><u>Overview of chapters</u></b>
</h?>
<div class="Unindented">
Chapter <a class="Reference" href="#chap:C/C++:-Review">1↓</a> begins with a rapid review of C/C++. The reader is assumed to have an undergraduate knowledge of C/C++ programming. Our review emphasizes a few parts of these languages that students typically don’t learn in introductory classes. The C programming language is the most fundamental of all languages, to the extent that one can no longer speak of a computer without C. The C language is close to the machine and provides only a basic, although highly valuable, layer of abstraction. The C++ language is a colossal extension of C that includes many mechanisms for representing abstract concepts to bring the program closer to the problem domain. The idiom of C++ we use is close to C. However, we discuss more C++ than we actually need to dispel myths about C++ being slow. 
</div>
<div class="Indented">
Libraries and makefiles are the basis of modular programming in C/C++. In chapter <a class="Reference" href="#chap:C/C++:-Libraries-and">2↓</a>, we explain how libraries and linkers work. Although we do not recommend Fortran, it is not uncommon to have to use old Fortran 77 programs in scientific programming. This chapter explains how to call Fortran programs from C/C++. Knowledge of the GNU <tt>make</tt> utility explained in this chapter is far more valuable than complex C++ syntax. There is no modular programming in C/C++ without <tt>make</tt>. Yet it is a neglected topic.
</div>
<div class="Indented">
A significant part of the answer to what makes programs fast or slow is contained in chapter <a class="Reference" href="#chap:The-processor">3↓</a>. Compilers convert C/C++ programs into a stream of machine instructions, native to the processor. A small subset of x86 assembly language is introduced and used to understand how structuring loops in different ways leads to different instruction streams.
</div>
<div class="Indented">
Although part of chapter <a class="Reference" href="#chap:The-processor">3↓</a> is about optimizing for the processor pipeline, which has to be programmed in assembly language, in general we do not recommend programming in assembly. The small part of x86 assembly language introduced in that chapter is mainly used to understand how loop-nests map to machine instructions. There can be no rational account of program speed without a discussion of machine instructions. The discussion of assembly language is like a ladder that helps us understand how to structure loops and loop-nests, one of the most important tasks in optimizing a program. As in Wittgenstein’s famous metaphor, the ladder can be thrown away once the understanding is gained.
</div>
<div class="Indented">
Even skilled programmers are often unaware of registers. In chapter <a class="Reference" href="#chap:The-processor">3↓</a>, we not only talk about registers but also introduce techniques for optimizing for the instruction pipeline. This type of optimization can improve program speeds considerably---by as much as a factor of 3 to 15 in the example discussed in chapter <a class="Reference" href="#chap:The-processor">3↓</a>. However, the reader may wonder whether these techniques may become obsolete quickly because of rapid changes in hardware. The answer is an emphatic no. Although hardware changes rapidly, the design principles do not change so fast. In addition, the value of optimizing for the instruction pipeline has increased rapidly over time. Such optimization implied a speedup by a factor of <span class="formula">3</span> in 2010, whereas the speedup is much greater and nearly a factor of <span class="formula">15</span> in 2015. Yet the programming methodology is unchanged.
</div>
<div class="Indented">
Although optimizing for the instruction pipeline is a very difficult skill, it can be a valuable one. It can speed up C/C++ programs by a factor of <span class="formula">10</span>, or even a factor of <span class="formula">100</span> if the C/C++ coding is naive to begin with. As an argument for the value of instruction pipeline optimizations, we mention that although discussion of new algorithms overwhelmingly dominates the scientific literature, algorithms that speed up any nontrivial computation by a factor of <span class="formula">10</span> or even a factor of <span class="formula">2</span> are rare.
</div>
<div class="Indented">
Chapters <a class="Reference" href="#chap:Memory">4↓</a> and <a class="Reference" href="#chap:Threads-and-shared">5↓</a> have wider applicability than chapter <a class="Reference" href="#chap:The-processor">3↓</a>. The sort of loop optimizations discussed in chapter <a class="Reference" href="#chap:The-processor">3↓</a> apply mainly when data is regular, as in image processing or the solution of differential equations. However, memory optimizations, discussed in chapter <a class="Reference" href="#chap:Memory">4↓</a>, are useful in all kinds of programming. For simplicity, our examples use regular data, but the same principles apply even when dynamic data structures such as linked lists or graphs are used. 
</div>
<div class="Indented">
Much of program optimization is optimization of memory access, and many of the principles are the same for single- and multithreaded programs. Memory usually refers to Dynamic Random Access Memory (DRAM), which can be 10s, or even 100s, of gigabytes in extent. Faster and smaller caches are maintained to speed access to DRAM by nearly a factor of <span class="formula">100</span>. In addition, memory is organized into pages to give an independent view of memory to disparate processes. Memory accesses may be optimized for both caching and the paging system.
</div>
<div class="Indented">
Perhaps an even more important point that arises in chapter <a class="Reference" href="#chap:Memory">4↓</a>, as a natural consequence of the discussion of instruction level parallelism in chapter <a class="Reference" href="#chap:The-processor">3↓</a>, is the role of parallelism in the memory system. Programs that allow the processor to parallelize multiple memory accesses will be much faster. Thus, for example, there can be a huge difference in speed between a program that uses linked lists, which disrupt parallelism, and another program that accomplishes the same task by accessing an array in sequence. 
</div>
<div class="Indented">
The clock speed of processors stopped improving around 2005. Much of the growth in processing power since then is from putting more and more processor cores inside the same computer. All these processor cores share the same memory. Programming with threads, the topic of chapter <a class="Reference" href="#chap:Threads-and-shared">5↓</a>, derives its importance from this well-established trend. Many threaded programs can be written quite easily. Yet there are always subtleties under the surface whenever different threads or processes share memory. 
</div>
<div class="Indented">
Many of the subtleties of threaded programming are related to the memory system. Threaded programming is impossible without coherent caches, and any programmer who writes threaded programs in C/C++ without understanding as much will be befuddled sooner rather than later. Even the simplest threaded programs for adding a sequence of numbers, using OpenMP or Pthreads, rely on the memory model in ways that are often not appreciated. 
</div>
<div class="Indented">
In addition to the memory system, threaded programs interact with the operating system. The simple act of creating threads can involve an overhead that swamps the benefits of parallelizing. When is it advantageous to invoke multiple threads? Why is it not a good idea to change the number of threads between OpenMP parallel regions? Why should threaded programs avoid abusing the stack? These questions are answered in a rational manner once the role of the operating system is understood.
</div>
<div class="Indented">
Much of a C/C++ programmer’s time is spent dealing with segmentation faults. The precise manner in which segmentation faults and other memory errors arise from inside the operating system is explained in chapter <a class="Reference" href="#chap:Threads-and-shared">5↓</a>. Chapter <a class="Reference" href="#chap:Threads-and-shared">5↓</a> concludes what may be seen as the core of this book. Topics covered up to this point are relevant to many kinds of programming, well beyond the scientific world.
</div>
<div class="Indented">
The Top 500 list (see <a class="FlexURL" href="http://www.top500.org/">http://www.top500.org/</a>), which uses a linear algebra problem to benchmark and then rank the fastest computers in the world, has provided powerful impetus to scientific computing. For more than two decades, the most powerful computers in the world have been clusters where many computers are tightly connected using a high-performance network. In such clusters, concurrent processes communicate over the network by sending and receiving messages. Message passing is the topic of chapter <a class="Reference" href="#chap:Networks-and-message">6↓</a>. 
</div>
<div class="Indented">
An example of the outcome of our choice to focus on computer architecture instead of general principles may be found in this chapter. The general principle is to overlap processor and network activity. Few can contest the utility of such a principle or argue that there is anything in it beyond ordinary common sense. However, our discussion of the matrix transpose in chapter <a class="Reference" href="#chap:Networks-and-message">6↓</a> shows that it requires deep knowledge of network architecture to in fact overlap processor and network activity, although the general principle is quite obvious. It is precisely that type of knowledge that gets lost when we do not look past general abstractions and examine details of systems software and computer architecture.
</div>
<div class="Indented">
When a large group of computers is tightly coupled, message passing is the preferred paradigm. Because there is no truly credible challenge to message passing in the context of supercomputing, the largest physics computations are likely to continue to rely on message passing. However, a single computer today can tackle complex 3D problems with more than a billion grid points. The vast majority of scientific projects can be effectively implemented on a single node. When a single computer is so powerful, the additional difficulty of resorting to message passing between multiple computers, which can be considerable, becomes less attractive. 
</div>
<div class="Indented">
Market forces that are propelling the Internet are powerful, indeed amazingly powerful, and should not be ignored by the scientific programmer. The pertinence of the Internet is more obvious in newer areas such as genomics and data science than in classical physics. Chapter <a class="Reference" href="#chap:Networks-and-message">6↓</a> includes a section about the Internet. As in other parts of the book, here too we look behind the abstractions to understand the plumbing of the Internet.
</div>
<div class="Indented">
The final two chapters, chapters <a class="Reference" href="#chap:The-Xeon-Phi">7↓</a> and <a class="Reference" href="#chap:Graphics-co-processor-programmin">8↓</a>, are about coprocessors. In 2007, the NVIDIA corporation showed how its graphics processors can be used to speed up a range of scientific problems. The excitement that resulted was undoubtedly justified. The graphics coprocessors hint at the possibilities for architectural design that may be available beyond the x86 line. Intel, which could not afford to ignore this competition, has introduced its own coprocessor. It must be said that excitement about coprocessors has not been matched by utilization. When the coprocessor competes with the processor to execute the same tasks, there is a major disruption of modularity at the hardware level. The resulting heterogeneity makes it difficult to break tasks into subtasks in a uniform manner. Heterogeneity overly complicates programming models, which are already quite complicated.
</div>
<div class="Indented">
The brief appendix may be the right place to begin reading this book. The appendix begins with a table of machines used to time programs. The rest of the book makes frequent references to that table. 
</div>
<div class="Indented">
Although interpreted languages such as Python or Matlab are easier to use, the resulting programs will be much slower. How much slower does not seem to be widely understood, and the appendix dispels a few myths. This author has heard estimates of the slow-down factor ranging from a few percent to a factor of <span class="formula">2</span> to a better informed guess of a factor of <span class="formula">10</span>. In fact the interpreted languages can be several hundred times slower for even fairly simple programs that run on a single core. As the complexity of the program and the hardware platform increases, the slow-down penalty can get much worse. Even for moderately complex programs, the slow-down can be by a factor of <span class="formula">10<sup>4</sup></span>, as we note in the appendix. If the effort for mastering C/C++ is much greater, so is the reward.
</div>
<div class="Indented">
The entire source code corresponding to this book, which runs to more than 15,000 lines of C/C++, is available at <a class="FlexURL" href="https://github.com/divakarvi/bk-spca">https://github.com/divakarvi/bk-spca</a>. The appendix briefly introduces two tools---GIT and the <tt>cscope</tt> utility---essential for downloading and working with the code. Both GIT and <tt>cscope</tt> are of great value in programming in general. Even in the era of Internet search, <tt>cscope</tt>, which has been around since the 1970s, is an excellent option for browsing and searching source trees.
</div>
<div class="Indented">
The examples in this book rely on Intel’s <tt>icc/icpc</tt> compiler. However, except for chapter <a class="Reference" href="#chap:The-Xeon-Phi">7↓</a>, the widely used, easily available, and open source <tt>gcc/g++</tt> compiler may be substituted with little trouble. The few nuances that arise are described in the appendix. 
</div>
<h? class="Subsubsection">
<b><u>Acknowledgments</u></b>
</h?>
<div class="Unindented">
Most of all, I thank my undergraduate teachers at the Indian Institute of Technology, Bombay. From them I began to learn much of what is found in this book.
</div>
<div class="Indented">
I taught a graduate class based on this book on four occasions at the University of Michigan. The class typically covered about a third or more of the material in the book, with greater emphasis on the beginning sections in each chapter. I thank all the hundred or so students who took that class. Thanks in particular to Zhongming Qu, who penetrated the material quite deeply. I am especially grateful to Zhongming for helping me understand Makefiles much better.
</div>
<div class="Indented">
I was privileged to have access to superbly maintained systems at the University of Michigan. I thank Bennet Fauber, Brock Palen, Andy Caird, Neil Tweedy, Charles Antonelli, Reed Hoyer, and Rusty Dekema for their help. I am especially grateful to Seth Meyer for showing me how to build and load the Linux operating system. All who work with computers are aware of the peculiar difficulty of getting started. In the memorable words of Kernighan and Ritchie, it is a big hurdle, and once crossed &ldquo;everything else is comparatively easy.&rdquo; In addition to showing me how to get started with Linux, Seth also freely shared his deep knowledge of the Internet with me. 
</div>
<div class="Indented">
The last three chapters of this book were written using systems deployed at the Texas Advanced Computing Center (TACC), with access obtained through XSEDE. I am thankful to XSEDE as well as TACC for the wonderful support they offered. At TACC, I am especially grateful to Chris Hempel for being so accommodating. The technical help desk at TACC answered numerous questions with unfailing promptness and helped me in many ways. At the risk of omission, I thank Doug James, Lars Koesterke, Hang Liu, Si Liu, Robert McLay, Cyrus Proctor, and Frank Willmore.
</div>
<div class="Indented">
I thank Paul Besl, Tim Prince, and Mike Tucker of Intel Corporation for being gracious and helpful when difficulties arose with the Westmere microarchitecture. I also thank Intel’s Russian team for resolving these technical difficulties and Tim Prince for his expert comments.
</div>
<div class="Indented">
I thank my colleagues Danny Forger and Hans Johnston for offering advice and much needed support.
</div>
<div class="Indented">
I wrote this book using LyX, relying on Inkscape to produce figures. The html version of the book was produced using eLyXer. I thank all the people responsible for these wonderful open source tools. Technical information accessed via the Internet was invaluable, and I have acknowledged this help wherever possible.
</div>
<div class="Indented">
Finally, I am grateful to Dr. John Sarno for curing my chronic back pain through his books and to Dr. Howard Schubiner for a helpful consultation.
</div>
<div class="Indented">

</div>
<div class="Indented">

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-1">1</a> C/C++: Review <a class="Label" name="chap:C/C++:-Review"> </a>
</h1>
<div class="Unindented">
A computer program is a sequence of instructions to a machine. In this chapter and the next, we emphasize that it is a sequence of instructions that <i>builds on</i> other computer programs and that in turn can be <i>built on</i>. Codes that exist in isolation are often limited to quite trivial tasks and can hardly be considered computer programs.
</div>
<div class="Indented">
This chapter is a review of C/C++. The C programming language is the most fundamental of all programming languages. Computing machines come in great variety and are put together using many parts. The computer’s parts, consisting of the processor at the center, and with memory, hard disk, network interfaces, graphics devices, and other peripherals connected to it, are very different from each other. It would be an almost impossible task for any single programmer to deliver instructions to such a complicated machine. The C programming language is a major part of the setup to give the programmer a uniform view of computing machines. No modern computing device can exist and be useful without C.
</div>
<div class="Indented">
Much of the time when programs are written, the programmer is not at all aware of the many parts of the computer. Indeed, the programmer may not even be aware that there is a processor. It is more natural to think in terms of the abstractions of the programming language. This is in general a good thing because the purpose of programming languages is to set up abstractions that hide the complicated parts of the computer. In addition, programs written in this way can be moved from computer to computer easily.
</div>
<div class="Indented">
In this book, our goal is to understand what makes programs fast or slow. As soon as we set ourselves this goal, we have no choice but to peer behind the abstractions of programming and understand how those abstractions are realized through the many parts that constitute a computer. The C programming language is the most natural vehicle in moving toward this goal. In fact, it is the only vehicle that is really appropriate. 
</div>
<div class="Indented">
The C programming language is close to the machine. There are high-level languages, a notable and outstanding example being Python, which bring programming much closer to the problem domain. Concepts and ideas intrinsic to the problem domain are expressed far more easily in these programming languages. Programs that would take days to write in C can be written in hours, or even minutes, in Python. 
</div>
<div class="Indented">
Although these high-level languages are much easier on the programmer, programs in these languages run slowly. As shown in the appendix, these high-level programs can be more than  100 times slower, or even worse. In fact, even C programs written without a knowledge of computer architecture can be several times slower than C programs written with that knowledge. The C++ language is a compromise. It strives to combine the speed of C with the abstraction facilities of higher level languages. It can be quite useful, although it lacks the simplicity and elegance of C. The C++ idiom we use is close to C. Some of the facilities of C++, mainly the facility of defining classes, are adopted to make C a little easier and a little more presentable.
</div>
<div class="Indented">
The review of C/C++ in this chapter attempts to bring out certain features that people often do not learn from a single course or two. Beginning programmers often tend to think of a C/C++ program as a single <tt>.c</tt> or <tt>.cpp</tt> source file. Modular organization of sources is far superior. Modular organization is essential for writing programs whose structure reflects and explains how they work as well as the underlying concepts.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:review-example">1.1↓</a> sets up an example to exhibit the modular organization of sources in C/C++. The example is a technique called the Aitken iteration, which can transform certain sequences to hasten their convergence. In sections <a class="Reference" href="#sec:review-C">1.2↓</a> and <a class="Reference" href="#sec:review-cpp">1.3↓</a>, we review some features of C and C++ using this example. The concluding section <a class="Reference" href="#sec:review-Fortran">1.4↓</a> introduces a little Fortran. For reasons explained at length later, we do not recommend programming in Fortran. However, a lot of scientific programs are written in Fortran, mainly Fortran 77. A scientific programmer needs to know just enough Fortran to be able to use these old Fortran codes. 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-1.1">1.1</a> An example: The Aitken transformation<a class="Label" name="sec:review-example"> </a>
</h2>
<div class="Unindented">
The Aitken transformation maps a sequence of numbers to another sequence of numbers. It serves as the vehicle to introduce aspects of C, C++, and Fortran in this chapter. It is also interesting in its own right. 
</div>
<div class="Indented">
The Aitken transformation is given by the following formula: 
</div>
<div class="Indented">
<div class="formula">
<a class="eqnumber" name="eq:Aitken3">(1.1) </a><i>t</i><sub><i>n</i> − 1</sub> = <i>s</i><sub><i>n</i> − 1</sub> − <span class="fraction"><span class="ignored">(</span><span class="numerator">(<i>s</i><sub><i>n</i></sub> − <i>s</i><sub><i>n</i> − 1</sub>)<sup>2</sup></span><span class="ignored">)/(</span><span class="denominator"><i>s</i><sub><i>n</i> + 1</sub> − 2<i>s</i><sub><i>n</i></sub> + <i>s</i><sub><i>n</i> − 1</sub></span><span class="ignored">)</span></span>.
</div>
It transforms a sequence <span class="formula"><i>s</i><sub>0</sub>, <i>s</i><sub>1</sub>, …, <i>s</i><sub><i>N</i></sub></span> into a new sequence <span class="formula"><i>t</i><sub>0</sub>, <i>t</i><sub>1</sub>, …, <i>t</i><sub><i>N</i> − 2</sub></span>, which has two fewer terms.<span class="FootOuter"><span class="SupFootMarker"> [1] </span><span class="HoverFoot"><span class="SupFootMarker"> [1] </span>Sequence extrapolation and Aitken iteration are treated in <span class="bibcites">[<a class="bibliocite" name="cite-8" href="#biblio-8"><span class="bib-index">8</span></a>, <a class="bibliocite" name="cite-3" href="#biblio-3"><span class="bib-index">3</span></a>]</span>.</span></span> The idea behind the Aitken transformation is as follows. If the <span class="formula"><i>s</i><sub><i>n</i></sub></span> sequence is of the form <span class="formula"><i>s</i><sub><i>n</i></sub> = <i>S</i> + <i>a</i><i>λ</i><sup><i>n</i></sup></span>, all terms in the <span class="formula"><i>t</i><sub><i>n</i></sub></span> sequence are equal to <span class="formula"><i>S</i></span>. It is useful for speeding up the convergence of a number of sequences, even those that do not directly fit the <span class="formula"><i>S</i> + <i>a</i><i>λ</i><sup><i>n</i></sup></span> pattern. Section <a class="Reference" href="#sub:review-example-Leibniz-series-and">1.1.1↓</a> illustrates the dramatic power of the Aitken iteration on a couple of examples---the Leibniz series and the logarithmic series. To be sure, these examples are chosen carefully. 
</div>
<div class="Indented">
This section begins to make the point that it is generally advantageous to split a program into multiple sources. We could use a single source file to code the Aitken iteration and apply it to the Leibniz series as well as the logarithmic series. Such a program would work just as well to begin with. A few days later, we may want to apply the Aitken iteration to another example. If we also throw that example into the same source file, the source file will become a little more unwieldy. A few months later, we may want to use the Aitken iteration as part of a large project. If we insist on using a single source file, there are two equally unpleasant alternatives: copy the whole Aitken program into the large project or copy the large project into the Aitken program. There is a heavy price to pay for avoiding modular organization of programs. Section <a class="Reference" href="#sub:review-example-Modular-organization-of">1.1.2↓</a> gives a preliminary discussion of the modular organization of program sources using the Aitken iteration as an example. Later sections build on this preliminary discussion.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.1.1">1.1.1</a> Leibniz series and the logarithmic series<a class="Label" name="sub:review-example-Leibniz-series-and"> </a>
</h3>
<div class="Unindented">
The Leibniz series<span class="FootOuter"><span class="SupFootMarker"> [2] </span><span class="HoverFoot"><span class="SupFootMarker"> [2] </span>The Leibniz series was discovered by Madhava (c. 1350-c. 1410), well before Leibniz (1646-1716). Madhava reported it in the following passage: &ldquo;Multiply the diameter by 4. Subtract from it and add alternately the quotients obtained by dividing four times the diameter by the odd integers 3, 5, etc.&rdquo; (<span class="bibcites">[<a class="bibliocite" name="cite-30" href="#biblio-30"><span class="bib-index">30</span></a>]</span>). </span></span> is given by<div class="formula">
<i>π</i> = 4 − <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">3</span><span class="ignored">)</span></span> + <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">5</span><span class="ignored">)</span></span> − <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">7</span><span class="ignored">)</span></span> + <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">9</span><span class="ignored">)</span></span> − <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">11</span><span class="ignored">)</span></span> + ⋯
</div>
This series, whose terms alternate in sign and diminish in magnitude monotonically, will be a recurring example. So we will begin by looking at it carefully. Let <span class="formula"><i>S</i><sub><i>n</i></sub></span> be the sum of the first <span class="formula"><i>n</i></span> terms. As shown in figure <a class="Reference" href="#fig:Convergence-of-Leibniz">1.1↓</a>, the partial sums <span class="formula"><i>S</i><sub><i>n</i></sub></span> are alternately above and below <span class="formula"><i>π</i></span>. Further, the convergence is slow. In fact, <span class="formula"><span class="bigsymbol">|</span><i>π</i> − <i>S</i><sub><i>n</i></sub><span class="bigsymbol">|</span> &gt; 2 ⁄ (2<i>n</i> + 1)</span>, which implies that the first million terms of the Leibniz series can give only slightly more than six digits of <span class="formula"><i>π</i></span> after the decimal point.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:Convergence-of-Leibniz"> </a><div class="figure">
<div class="PlainVisible">
<div class="center">
<img class="embedded" src="FIGS/chapter1/LeibnizCvg.png" alt="figure FIGS/chapter1/LeibnizCvg.png" style="width: 4in; max-width: 288px; height: auto; max-height: 216px;"/>

</div>
<br/>
<div class="center">
<div class="caption">
Figure 1.1 Convergence of the Leibniz series to <span class="formula"><i>π</i></span> (dashed line).
</div>

</div>

</div>

</div>

</div>

</div>
<div class="Indented">
If we look at figure <a class="Reference" href="#fig:Convergence-of-Leibniz">1.1↑</a>, we may notice that although the convergence to the limit is slow, the partial sums seem to follow a certain trend as they approach <span class="formula"><i>π</i></span>. The partial sums are alternately above and below, and it seems as if we can fit a smooth curve through the iterates. The Aitken iteration guesses this trend quite well and speeds up the convergence of the Leibniz series.
</div>
<div class="Indented">
Table <a class="Reference" href="#table:AitkenLeibniz">1.1↓</a> shows Aitken’s transformation <a class="Reference" href="#eq:Aitken3">(1.1)↑</a> applied repeatedly to the first <span class="formula">13</span> partial sums of the Leibniz series. After each application, we have a sequence with two fewer numbers, and at the end of the sixth application of the Aitken transformation, we have just one number that equals <span class="formula"><i>π</i></span> to 10 digits of accuracy. Because none of the <span class="formula">13</span> partial sums gives even the first digit after the decimal point, it seems astonishing that an answer with 10 digits of accuracy can be produced from those numbers.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="table:AitkenLeibniz"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula">4.0000000000</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1666666667</span> 
</td>
<td align="center" valign="top">
<span class="formula">⋯</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1415926540</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1415926536</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.6666666667</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1333333333</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
<span class="formula">3.1415926535</span> 
</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.4666666667</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1452380952</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
<span class="formula">3.1415926536</span> 
</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.8952380952</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1396825397</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.3396825397</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1427128427</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.9760461760</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1408813409</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.2837384837</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1420718171</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.0170718171</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1412548236</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.2523659347</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1418396189</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.0418396189</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1414067185</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.2323158094</span> 
</td>
<td align="center" valign="top">
<span class="formula">3.1417360993</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.0584027659</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.2184027659</span> 
</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>

</table>

</div>
<div class="caption">
Table 1.1 The first column lists the first <span class="formula">13</span> partial sums of the Leibniz series. Every other column is gotten by applying the Aitken transformation <a class="Reference" href="#eq:Aitken3">(1.1)↑</a> to the previous column. The number at the upper right corner is <span class="formula"><i>π</i></span> correct to 10 digits.
</div>

</div>

</div>

</div>
<div class="Indented">
Computing the digits of <span class="formula"><i>π</i></span> is a mathematical sport of unending interest. Even Isaac Newton had a weakness for it.<span class="FootOuter"><span class="SupFootMarker"> [3] </span><span class="HoverFoot"><span class="SupFootMarker"> [3] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-12" href="#biblio-12"><span class="bib-index">12</span></a>]</span> (p. 112).</span></span> The Aitken iteration, although impressive, is far from being the best method for computing <span class="formula"><i>π</i></span>.<span class="FootOuter"><span class="SupFootMarker"> [4] </span><span class="HoverFoot"><span class="SupFootMarker"> [4] </span>For the mathematics of approximating <span class="formula"><i>π</i>, </span> see <span class="bibcites">[<a class="bibliocite" name="cite-9" href="#biblio-9"><span class="bib-index">9</span></a>]</span>.</span></span>
</div>
<div class="Indented">
The logarithmic series<span class="FootOuter"><span class="SupFootMarker"> [5] </span><span class="HoverFoot"><span class="SupFootMarker"> [5] </span>The logarithmic series was discovered by Nicholas Mercator (c. 1620-1687). See <span class="bibcites">[<a class="bibliocite" name="cite-11" href="#biblio-11"><span class="bib-index">11</span></a>]</span>.</span></span> <span class="formula">log(1 + <i>x</i>) = <i>x</i> − <i>x</i><sup>2</sup> ⁄ 2 + <i>x</i><sup>3</sup> ⁄ 3 − <i>x</i><sup>4</sup> ⁄ 4 + ⋯</span> diverges for <span class="formula">|<i>x</i>| &gt; 1</span>.  However, as shown in table <a class="Reference" href="#table:LogSeries">1.2↓</a>, the Aitken transformations of the first <span class="formula">13</span> partial sums recover the value of <span class="formula">log(1 + <i>x</i>)</span> for <span class="formula"><i>x</i> = 1.25</span>. If <span class="formula"><i>x</i></span> were larger, just the first <span class="formula">13</span> partial sums would not be enough to produce <span class="formula">10</span> digits of accuracy after the decimal point.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="table:LogSeries"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>x</i></span>
</td>
<td align="center" valign="top">
Partial Sum
</td>
<td align="center" valign="top">
Extrapolate
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">0.00</span>
</td>
<td align="center" valign="top">
<span class="formula">0.0000000000</span>
</td>
<td align="center" valign="top">
<span class="formula">0.0000000000</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">0.20</span>
</td>
<td align="center" valign="top">
<span class="formula">0.1823215568</span>
</td>
<td align="center" valign="top">
<span class="formula">0.1823215568</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">0.40</span>
</td>
<td align="center" valign="top">
<span class="formula">0.3364723763</span>
</td>
<td align="center" valign="top">
<span class="formula">0.3364722366</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">0.60</span>
</td>
<td align="center" valign="top">
<span class="formula">0.4700395318</span>
</td>
<td align="center" valign="top">
<span class="formula">0.4700036292</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">0.80</span>
</td>
<td align="center" valign="top">
<span class="formula">0.5895867562</span>
</td>
<td align="center" valign="top">
<span class="formula">0.5877866649</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1.00</span>
</td>
<td align="center" valign="top">
<span class="formula">0.7301337551</span>
</td>
<td align="center" valign="top">
<span class="formula">0.6931471806</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1.25</span>
</td>
<td align="center" valign="top">
<span class="formula">1.5615505069</span>
</td>
<td align="center" valign="top">
<span class="formula">0.8109302162</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 1.2 The partial sum column is the sum of the first <span class="formula">13</span> terms of the Taylor series of <span class="formula">log(1 + <i>x</i>)</span>. Each number in the last column is produced by applying the Aitken transformation <span class="formula">6</span> times to the first <span class="formula">13</span> partial sums, which leaves us with just one number. The last column shows that number, which gives <span class="formula">log(1 + <i>x</i>)</span>, with all the digits shown being correct.
</div>

</div>

</div>

</div>
<div class="Indented">
Here we have our first programming problem. There is a simple iteration <a class="Reference" href="#eq:Aitken3">(1.1↑)</a> to begin with. The problem is to code it in C/C++ and apply it to the Leibniz series and the logarithmic series. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.1.2">1.1.2</a> Modular organization of sources<a class="Label" name="sub:review-example-Modular-organization-of"> </a>
</h3>
<div class="Unindented">
Before we delve into the syntax of C/C++, let us look at how to structure a program for the Aitken iteration. In particular, we look at how to split the program between source files. 
</div>
<div class="Indented">
It is not uncommon to introduce computer programming using programs that reside in a single source file.<span class="FootOuter"><span class="SupFootMarker"> [6] </span><span class="HoverFoot"><span class="SupFootMarker"> [6] </span>Even programs limited to a single source file rely on the runtime library. Strictly speaking, there is no such thing as a program that entirely resides in a single source file.</span></span> A computer program as a single source file is a bad idea to allow into one’s head. It is a bad idea that can grow and grow. This author has heard of Fortran programs longer than 100,000 lines in a single file. Even the simple Aitken example shows why a single monolithic source is a bad idea.
</div>
<div class="Indented">
The Aitken iteration, as we have discussed it so far, consists of the iteration <a class="Reference" href="#eq:Aitken3">(1.1↑)</a> and its application to the Leibniz and Mercator series. One way to write this program is to code a function for the Aitken iteration, another function to set up and apply it to the Leibniz series, and likewise yet another function to apply it to the logarithmic series. 
</div>
<div class="Indented">
Breaking up programs into functions is the first step. However, throwing all the three functions into the same source file would not be a good idea. There is a simple conceptual reason for this. The Aitken iteration is a general technique, and its applications to the Leibniz and logarithmic series are two specific examples. Coding all the functions in the same source file limits the usefulness of the Aitken iteration. 
</div>
<div class="Indented">
In later sections, we will implement the Aitken iteration and the two examples in three separate source files.  The first source file <tt>aitken.c</tt> codes the Aitken iteration in a form that would apply to any sequence. The specific applications are in the source files <tt>leibniz.c</tt> and <tt>logseries.c</tt>.
</div>
<div class="Indented">
If the program is separated into three sources in this manner, the question of how they may interface with each other and work together arises. The first part of the interfacing is a header file that we will call <tt>aitken.h</tt>. This header file gives a basic summary of what is found inside <tt>aitken.c</tt> but does not have the implementation of any function. The source files <tt>leibniz.c</tt> and <tt>logseries.c</tt> include this interface within their code. Thus, the sources will look as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">aitken.h
aitken.c
leibniz.c
logseries.c
</pre>
</div>

</div>
<div class="Indented">
with the header file <tt>aitken.h</tt> included in all the three <tt>.c</tt> source files.
</div>
<div class="Indented">
This manner of breaking up a program into multiple source files is deeply integrated into C/C++. A C/C++ compiler converts a source file into a sequence of machine instructions independently of all other sources. For this reason, a source file is sometimes called a compilation unit. In our Aitken example, three object files <tt>aitken.o</tt>, <tt>leibniz.o</tt>, and <tt>logseries.o</tt>, holding machine instructions corresponding to the respective source files, result from compilation. 
</div>
<div class="Indented">
The source <tt>leibniz.c</tt> will include calls to functions defined in <tt>aitken.c</tt>. The header <tt>aitken.h</tt>, which is included in <tt>leibniz.c</tt>, includes just enough information to partially set up this function call when the machine instructions of <tt>leibniz.o</tt> are being compiled. The crucial information that is missing is the address and definition of the function in <tt>aitken.c</tt> that is invoked in <tt>leibniz.c</tt>. That information is supplied only when the object files <tt>aitken.o</tt> and <tt>leibniz.o</tt> are combined into an executable, which we will call <tt>leibniz.exe,</tt><span class="FootOuter"><span class="SupFootMarker"> [7] </span><span class="HoverFoot"><span class="SupFootMarker"> [7] </span>Although we assume the GNU/Linux operating system, the executable files will use the <tt>.exe</tt> extension as in Windows for greater clarity.</span></span> by the linker. It is the linker’s job to eliminate unresolved references in the object files and put together an executable without any unresolved references. The executable is the program that ultimately runs on the computer.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:AitkenSchematic"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/aitken_modules.png" alt="figure FIGS/chapter1/aitken_modules.png" style="width: 4in; max-width: 321px; height: auto; max-height: 196px;"/>

</div>
<div class="caption">
Figure 1.2 The three program sources are <tt>aitken.c</tt>, <tt>leibniz.c</tt>, and <tt>logseries.c</tt>. They are combined to produce two different executables, for extrapolating the partial sums of the Leibniz series and the Taylor series of <span class="formula">log(1 + <i>x</i>)</span>, respectively. The <tt>.o</tt> object files are omitted from this figure.
</div>

</div>

</div>

</div>
<div class="Indented">
We will make the discussion of compilation and linking far more concrete in later sections. The overall picture is as follows: the program is separated into multiple source files, the source files are compiled separately into object files holding machine instructions, and the linker merges the object files and eliminates unresolved references to produce an executable. The overall picture for the Aitken program is shown in figure <a class="Reference" href="#fig:AitkenSchematic">1.2↑</a>.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Substitute <span class="formula"><i>s</i><sub><i>n</i></sub> = <i>S</i> + <i>a</i><i>λ</i><sup><i>n</i></sup></span> into the Aitken formula <a class="Reference" href="#eq:Aitken3">(1.1↑)</a> and verify that each new term <span class="formula"><i>t</i><sub><i>n</i></sub></span> is equal to <span class="formula"><i>S</i></span>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In <a class="Reference" href="#eq:Aitken3">(1.1)↑</a>, <span class="formula"><i>S</i> = <i>s</i><sub><i>n</i> − 1</sub></span> if <span class="formula"><i>s</i><sub><i>n</i> − 1</sub> = <i>s</i><sub><i>n</i> − 2</sub></span> and there is a divide by zero if <span class="formula"><i>s</i><sub><i>n</i> + 1</sub> − 2<i>s</i><sub><i>n</i></sub> + <i>s</i><sub><i>n</i> − 1</sub> = 0</span>. Interpret both conditions.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Prove that<div class="formula">
<span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator">1 + <i>x</i><sup>2</sup></span><span class="ignored">)</span></span> = 1 − <i>x</i><sup>2</sup> + <i>x</i><sup>4</sup> − ⋯ + ( − 1)<sup><i>n</i> − 1</sup><i>x</i><sup>2<i>n</i> − 2</sup> + <i>R</i><sub><i>n</i></sub>, 
</div>
 where <span class="formula"><i>R</i><sub><i>n</i></sub> = ( − 1)<sup><i>n</i></sup><i>x</i><sup>2<i>n</i></sup> ⁄ (1 + <i>x</i><sup>2</sup>)</span>. Integrate from <span class="formula">0</span> to <span class="formula">1</span>, while noting that <div class="formula">
<span class="limits"><sup class="limit">1</sup><span class="limit">⌠</span><span class="limit">⌡</span><sub class="limit">0</sub></span><span class="fraction"><span class="ignored">(</span><span class="numerator"><i>x</i><sup>2<i>n</i></sup></span><span class="ignored">)/(</span><span class="denominator">1 + <i>x</i><sup>2</sup></span><span class="ignored">)</span></span> <i>dx</i> &lt; <span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator">2<i>n</i> + 1</span><span class="ignored">)</span></span>, 
</div>
 to deduce the Leibniz series. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  If <span class="formula"><i>S</i><sub><i>n</i></sub></span> is the partial sum of the first <span class="formula"><i>n</i></span> terms of the Liebniz series for <span class="formula"><i>π</i></span> (not <span class="formula"><i>π</i> ⁄ 4</span>), prove that <span class="formula"><span class="bigsymbol">|</span><i>π</i> − <i>S</i><sub><i>n</i></sub><span class="bigsymbol">|</span> &gt; 2 ⁄ (2<i>n</i> + 1)</span>, showing that the Leibniz series converges slowly.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  To understand the Aitken iteration, it is helpful to look at singularities. Prove that the function <span class="formula">arctan<i>z</i></span> has singularities at <span class="formula"><i>z</i> = ±<i>i</i></span> in the complex plane. Determine the type of the singularities.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-1.2">1.2</a> C review<a class="Label" name="sec:review-C"> </a>
</h2>
<div class="Unindented">
The C programming language is concise. In this review, we go over a few features of C, emphasizing points that introductory classes often miss. Thus, we discuss header files, arrays, and pointers, with emphasis on the distinction between lvalues and rvalues, as well as the distinction between declarations and definitions. We go over the compilation and linking process while adding more detail to the picture in figure <a class="Reference" href="#fig:AitkenSchematic">1.2↑</a>. Although C is concise, it demands precision in thinking. The emphasis in this review is toward greater precision. Table <a class="Reference" href="#tab:Books-and-reference-CC">1.3↓</a> shows two of the best books to learn C and C++.<span class="FootOuter"><span class="SupFootMarker"> [8] </span><span class="HoverFoot"><span class="SupFootMarker"> [8] </span><span class="bibcites">[<a class="bibliocite" name="cite-1" href="#biblio-1"><span class="bib-index">1</span></a>]</span> is quite possibly the most influential book in computer science. There is no better place to begin to learn the art of programming.</span></span>
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:Books-and-reference-CC"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="right" valign="top">
C programming 
</td>
<td align="left" valign="top" style="width: 3.5in;">
B.W. Kernighan and D.M. Ritchie, The C programming language, 2nd ed.
</td>

</tr>
<tr>
<td align="right" valign="top">
C++ programming
</td>
<td align="left" valign="top" style="width: 3.5in;">
B. Stroustrup, The C++ programming language, 3rd ed.
</td>

</tr>

</table>
<div class="caption">
Table 1.3 Books on C and C++ written by their inventors.
</div>

</div>

</div>

</div>

</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.2.1">1.2.1</a> Header files
</h3>
<div class="Unindented">
A header file in C is an interface to one or more source files, with each source file a collection of function definitions. As an example of a simple header file, we look at <tt>aitken.h</tt>. This header file is an interface to the functions defined in <tt>aitken.c.</tt><tt><a class="Label" name="page:aitken-in-c-headerfile"> </a></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#ifndef aitkenJuly09DVsfe
<span class="number-left">2</span>#define aitkenJuly09DVsfe
<span class="number-left">3</span>void aitken(double *seq1, double *seq2, int len);
<span class="number-left">4</span>double aitkenExtrapolate(double *seq1, double* seq2, 
<span class="number-left">5</span>			 int len);
<span class="number-left">6</span>#endif
</pre>
</div>

</div>
<div class="Indented">
The two symbolic names <tt>aitken</tt> and <tt>aitkenExtrapolate</tt> are introduced in this header file. These are the names of two functions that are defined in <tt>aitken.c</tt>. For the two functions, the declarations in the header file specify the type of the arguments as well as the type of the value that is returned. Thus, this header file is saying that <tt>aitken()</tt> is a function that takes three arguments---the first two of type pointer to <tt>double</tt> and the last of type <tt>int</tt>---and returns nothing (<tt>void</tt>). The function <tt>aitkenExtrapolate()</tt> is stated to take three arguments of the same types, but it returns a <tt>double</tt>. 
</div>
<div class="Indented">
Both of these are examples of function declarations. The declarations specify the arguments the functions take in as well as the types of what they return. When a declaration is made, the function is stated to exist somewhere. However, a sequence of statements defining the functions is lacking. 
</div>
<div class="Indented">
The source files <tt>aitken.c</tt>, <tt>leibniz.c</tt>, and <tt>logseries.c</tt> include <tt>aitken.h</tt> within their text using the line <tt>#include "aitken.h"</tt>. The <tt>#include</tt> statement has the effect of splicing all of <tt>aitken.h</tt> into those source files. The source <tt>aitken.c</tt> includes the header <tt>aitken.h</tt> and goes on to define the functions declared in the header. The arguments and return type of a function at the point of definition must exactly match the declaration in the header. The sources <tt>leibniz.c</tt> and <tt>logseries.c</tt> include the header <tt>aitken.h</tt> with a different intent. Their intent is to obtain license to make calls to the functions <tt>aitken()</tt> and <tt>aitkenExtrapolate()</tt>.
</div>
<div class="Indented">
The header file acts as an interface, and it is natural, as well as typical, to include comments in it about the arguments to functions declared in it and about what those functions do. In this book, there are few comments in the code listings. The comments are instead found in the text. 
</div>
<div class="Indented">
The function <tt>aitken()</tt> (line 3) applies one step of the Aitken transformation <a class="Reference" href="#eq:Aitken3">(1.1↑)</a> to the array <tt>seq1[0..len-1]</tt> and leaves the result in the array <tt>seq2[0..len-3]</tt>. The function <tt>aitkenExtrapolate()</tt> (line 4) applies the Aitken transformation repeatedly to the array <tt>seq1[0..len-1]</tt> while using <tt>seq2[]</tt> as scratch space, until only one or two numbers are left. One of the remaining numbers is returned as the extrapolated value.
</div>
<div class="Indented">
At the beginning of <tt>aitken.h</tt> and at the end, three lines (1, 2, and 6) begin with the special character <tt>#</tt>. These lines are directives to the macro preprocessor, one of the initial phases of compilation. Sources are manipulated textually by the macro preprocessor.
</div>
<div class="Indented">
The source file <tt>leibniz.c</tt> includes <tt>aitken.h</tt> using a <tt>#include</tt> directive. When the compiler is applied to <tt>leibniz.c</tt>, the initial preprocessing stage handles the <tt>#include</tt> directive. When processing that directive, the file <tt>aitken.h</tt> is opened by the preprocessor. The first directive in that file (<tt>#ifndef</tt> on line 1) asks the preprocessor to check whether a name called <tt>aitkenJuly09DVsfe</tt> is already defined. If the name <tt>aitkenJuly09DVsfe</tt> is not defined, the second line of <tt>aitken.h</tt> (<tt>#define</tt> directive) asks the preprocessor to define such a name, which it does. The preprocessor splices in all the text between the second and last lines of <tt>aitken.h</tt> (lines 3 to 5) into the source file that issued the <tt>include "aitken.h"</tt> directive. The splicing into <tt>leibniz.c</tt> is done at the point where the <tt>#include</tt> directive was issued.
</div>
<div class="Indented">
However, if such a name is already defined (because the header file was included by an earlier directive), it skips over everything until it sees a <tt>#endif</tt> directive (line 6); in this case, it would be skipping the entire <tt>aitken.h</tt> file. 
</div>
<div class="Indented">
Using <tt>#ifndef</tt>,<tt> #define</tt>, and<tt> #endif</tt> directives ensures that if a directive to include the same header file is issued twice by the same source file, the second directive has no effect. That may not sound useful at first. However, in practice, a source file may include a header file that includes some other header file and so on. So two directives to include different header files may end up including the same header file twice. Such a thing is prevented by the combination of preprocessor directives we just discussed.
</div>
<div class="Indented">
The macro variable <tt>aitkenJuly09DVsfe</tt> is chosen to be complicated to avoid accidentally using the same name in two different header files. Such accidental reuse will mean that the two header files can never be simultaneously included in the same source. Such errors at the macro preprocessing stage can be a little bothersome.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.2.2">1.2.2</a> Arrays and pointers
</h3>
<div class="Unindented">
Arrays and pointers are the heart of the C language. An array is a sequence of locations in memory. A pointer-type variable is a variable that holds the address of a memory location. The word pointer can be used for either such a variable or an expression that evaluates to an address. The two concepts may appear different, but the C language blurs the distinction between them. In C, arrays and pointers are almost interchangeable. 
</div>
<div class="Indented">
There is a very good reason for blurring the distinction between arrays and pointers. Suppose we want to pass a long sequence, occupying a great deal of memory, as an argument to a function. It would be wasteful to allocate new memory and copy the sequence entry by entry at every function call. In C, arrays are passed as pointers.
</div>
<div class="Indented">
The key idea in almost identifying arrays with pointers is as follows. A sequence of data items in memory can be specified using three pieces of information: the address of the first item, the size in bytes of each item, and the number of entries in the sequence. In C, a pointer holding an address is specified as a pointer of a certain type. The size of each item is inferred from type information. For example, in an array of <tt>double</tt>s, the size of each item is 8 bytes and in an array of <tt>int</tt>s the size of each item is 4 bytes (on GNU/Linux). Thus, from merely knowing a pointer, we can infer the first two pieces of information: the address of the first item in memory and the size of each item in bytes. The last piece of information, namely, the length of the array, is often tagged along separately. Thus, arrays and pointers may be identified, and arrays may be passed to functions efficiently as pointers with the length of the array tagged along as an extra parameter.
</div>
<div class="Indented">
How this idea plays out in practice, we will examine presently.
</div>
<h? class="Subsubsection">
<b><u>Arrays, pointers, lvalues, and rvalues</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:MemoryInC"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/MemoryInC.png" alt="figure FIGS/chapter1/MemoryInC.png" style="width: 4in; max-width: 302px; height: auto; max-height: 60px;"/>

</div>
<div class="caption">
Figure 1.3 A schematic and simplified view of computer memory. The symbol <span class="formula"><i>x</i></span> is of integer type, whereas the symbol <span class="formula"><i>a</i></span> is a pointer. Addresses are shown on top.
</div>

</div>

</div>
The C language takes a certain view of computer memory, and arrays and pointers are both best understood in terms of computer memory. Figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a> is a schematic view of a portion of the computer memory (in hardware, this would be DRAM in most circumstances). Names that we introduce into our code---whether they correspond to variables of basic types such as <tt>char</tt> or <tt>int</tt> or <tt>double</tt>, or to arrays, or to pointers---will all ultimately correspond to locations in computer memory.<span class="FootOuter"><span class="SupFootMarker"> [9] </span><span class="HoverFoot"><span class="SupFootMarker"> [9] </span>In practice, as opposed to C semantics, program variables may be allocated using registers or DRAM. They may even be eliminated during compiler optimizations.</span></span> To introduce a name of a variable of basic type, we may use a definition such as <tt>int x;</tt> to introduce a name of an array, we may use a definition such as <tt>int list[3];</tt> to introduce a name of a pointer, we may use a definition such as <tt>int *a.</tt>
</div>
<div class="Indented">
In C semantics, an expression may evaluate to a value that is the name of a memory location. Such a value is called an <i>lvalue</i>. The &ldquo;l&rdquo; refers to the fact that such values may occur on the left-hand side of an assignment. In contrast, an expression may evaluate to a value that may be used to fill a memory location but is not necessarily the name of any location in memory. Such a value is called an <i>rvalue</i>. The &ldquo;r&rdquo; here refers to the possible occurrence of such a value on the right-hand side of an assignment statement. The concept of rvalues and lvalues is useful for understanding arrays and pointers.
</div>
<div class="Indented">
The distinction between lvalues and rvalues arises fundamentally because of the assignment statement. In an assignment, what occurs on the left is the name of a memory location or an lvalue. What occurs on the right is a value that is used to fill a memory location or an rvalue. The distinction is important in the context of pointers and arrays because, although a pointer is naturally thought of as an address and an address is nothing but a number naming a memory location, pointers themselves may be stored in pointer-type variables. 
</div>
<div class="Indented">
If the variable <tt>x</tt> is introduced using the definition <tt>int x</tt>, it is an lvalue because it is the name of a memory location as shown in figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a>. If the assignment statement <tt>x=37</tt> executes, the location is filled with <span class="formula">37</span> as shown in the figure. The variable <tt>x</tt> can be both an lvalue and an rvalue. In the statement <tt>x=x+7</tt>, the occurrence on the left is an lvalue and the occurrence on the right is an rvalue.
</div>
<div class="Indented">
Addresses are shown in figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a> in a slightly nonstandard way. At the top of the figure, the address of the memory location named by <tt>x</tt> is shown as <tt>A(0..3)</tt>. On a typical computer system today, <tt>A</tt> may be understood as a <span class="formula">64</span>-bit (<span class="formula">8</span>-byte) address. An <tt>int</tt> occupies four bytes in GNU/Linux, and the addresses of the four bytes named by the variable <tt>x</tt> are <span class="formula"><i>A</i> + 0, <i>A</i> + 1, <i>A</i> + 2, <i>A</i> + 3</span>. This is shown in the figure as <tt>A(0..3)</tt>. 
</div>
<div class="Indented">
The C language allows us to take the address of a variable (in general, the address of any lvalue). To get the address of the variable <tt>x</tt> of type <tt>int</tt>, we may use the syntax <tt>&amp;x</tt>. Although the <tt>int</tt> location is <span class="formula">4</span> bytes with addresses <span class="formula"><i>A</i>, <i>A</i> + 0, <i>A</i> + 1, <i>A</i> + 2</span>, the value of <tt>&amp;x</tt> is <span class="formula"><i>A</i></span>, which is the address of the first byte in the memory location named by <tt>x</tt>. Here <tt>&amp;x</tt> is an rvalue (its value being <span class="formula"><i>A</i></span>) but not an lvalue because it is not the name of any location in memory. In the same expression <tt>&amp;x</tt>, <tt>x</tt> is an lvalue.
</div>
<div class="Indented">
We may define a pointer using the syntax <tt>int *a</tt>. As shown in figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a>, <tt>a</tt> is the name of <span class="formula">8</span> bytes of memory and not just <span class="formula">4</span>, as in the case of <tt>x</tt>. This is because the pointer-type variables are meant to hold addresses, and as we have already said, an address is <span class="formula">8</span> bytes on most computers today. 
</div>
<div class="Indented">
If we now say <tt>a=&amp;x</tt>, the memory location named <tt>a</tt> gets filled with the address of <tt>x</tt>, which is <span class="formula"><i>A</i></span> in figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a>. If we were to say <tt>a=&amp;x+1</tt>, the location <tt>a</tt> would get filled with <span class="formula"><i>A</i> + 4</span> and not <span class="formula"><i>A</i> + 1</span>. That is because both <tt>&amp;x</tt> and <tt>a</tt> are of type pointer to <tt>int</tt>, and the C compiler knows that an <tt>int</tt> is <span class="formula">4</span> bytes and not just <span class="formula">1</span> byte. In the assignment <tt>a=&amp;x</tt>, <tt>a</tt> is an lvalue and <tt>&amp;x</tt> is an rvalue. Less obviously, <tt>x</tt> is an lvalue in the same expression. The distinction between names of memory locations (lvalues) and values that may be used to fill memory locations (rvalues) is valuable to keep in mind.
</div>
<div class="Indented">
The operator <tt>&amp;</tt> allows us to extract the address of an lvalue. Conversely, the operator <tt>*</tt> converts an address to the name of a memory location (an lvalue). So if we say <tt>a=&amp;x</tt> and then say <tt>*a=7</tt>, the effect is as follows. First, the location named <tt>a</tt> is filled with the address of <tt>x</tt>. Next, when we say <tt>*a=7</tt>, <tt>*a</tt> is the name of the location whose address is the value held in <tt>a</tt>. The lvalue in this assignment is <tt>*a</tt>, not <tt>a</tt>. Of course, the location named by <tt>*a</tt> is the same as the location named by <tt>x</tt>. So the effect of <tt>*a=7</tt> is to change the value of <tt>x</tt> to <span class="formula">7</span>.
</div>
<div class="Indented">
It is worth noting that the word pointer may refer to an lvalue or an rvalue. A variable, or any lvalue, that holds addresses may be called pointer. In addition, the address itself (an rvalue) may be called a pointer. The picture behind either usage is of an address pointing to a location.
</div>
<div class="Indented">
If <tt>e</tt> is an expression that evaluates to a pointer to a <tt>double</tt> (rvalue), the numerical value of <tt>e+1</tt> is 8 more than the value of <tt>e</tt>. That is because a <tt>double</tt> is 8 bytes. If <tt>e</tt> is a pointer to a more complex type such as a <tt>struct</tt>, the C compiler calculates the size of the <tt>struct</tt> in bytes and increments <tt>e</tt> by that amount when evaluating <tt>e+1</tt>. Thus, <tt>e+1</tt> advances the pointer by one item and <tt>e+27</tt> advances the pointer by <span class="formula">27</span> items, taking into account the data type that <tt>e</tt> points to. If <tt>p</tt> is a pointer-type variable, the assignment <tt>p=p+17</tt> advances the pointer by <span class="formula">17</span> items. Likewise, the assignment <tt>p=p-17</tt> moves the pointer backward by <span class="formula">17</span> items.
</div>
<div class="Indented">
An array may be introduced using a definition such as <tt>int list[3]</tt>. After this definition <tt>list</tt> is an rvalue but not an lvalue (it is not the name of any memory location). The (r)value of <tt>list</tt> is the address of the first of three consecutive memory locations of type <tt>int</tt> set aside by the definition. In figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a>, it is <span class="formula"><i>A</i> + 12</span>. If we use syntax such as <tt>list[1]=8</tt>, <tt>list[1]</tt> is an lvalue as shown in the figure. It is nothing but an abbreviation of <tt>*(list+1)</tt>. Similarly, <tt>list[0]</tt> and <tt>list[2]</tt> are also valid lvalues as shown in the figure. In fact, <tt>list[100]</tt> is also a syntactically valid lvalue. The only problem is that an assignment such as <tt>list[100]=2</tt> will most likely lead to a runtime error because only three <tt>int</tt> locations have been legally claimed. Such runtime errors are triggered by the operating system in a manner that is explained in a later chapter.
</div>
<div class="Indented">
We began by noting that a sequence may be specified using three pieces of information: a pointer to the first location, the size of each item, and the number of items. The definition <tt>int list[3]</tt> supplies all three pieces of information. The address of the first location is the value of <tt>list</tt>. The size of each item is 4 bytes (on GNU/Linux) because an <tt>int</tt> is four bytes. Finally, the number of items in the array is 3 as shown in figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a>. The first two pieces of information are contained in the type and value of <tt>list</tt>. If <tt>list</tt> is passed as an argument to a function, the length of the array, which is the third piece of information, must be supplied separately.
</div>
<div class="Indented">
Machine or assembly languages too access data items in a sequence using an address, the size of each item, and the index of the item, much as in C.
</div>
<h? class="Subsubsection">
<b><u>A C source with arrays and pointers</u></b>
</h?>
<div class="Unindented">
So far our discussion of arrays and pointers has been with reference to figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a>. We will now write a simple C program illustrating the discussion.
</div>
<div class="Indented">
Before looking at C source, a few comments about indentation are in order. For program source not meant for display in a book, we use eight-space indentation.<span class="FootOuter"><span class="SupFootMarker"> [10] </span><span class="HoverFoot"><span class="SupFootMarker"> [10] </span>The Linux kernel programming guidelines require eight-space indentation. See <tt>www.kernel.org/doc/Documentation/CodingStyle</tt>.  While noting that style is a matter of personal choice, Linus Torvalds, the creator of the Linux operating system, adds that using four-space indentation &ldquo;is akin to trying to define the value of PI to be 3.&rdquo; </span></span> Tab stops are separated by eight characters and terminal screens are conventionally <span class="formula">80</span> character wide. When the program code is lined up according to tab stops, the code is much easier to browse. The nesting level of loops becomes readily evident. The nesting level is an indication of the level of complexity of the code. Therefore, it is useful to be able to recognize the nesting level immediately. Too many levels of nesting often imply that the code is poorly structured. 
</div>
<div class="Indented">
Let us look at the following code, which uses five-space indentation. The suggestion of eight-space indentation assumes <span class="formula">80</span>-character-wide lines. Here the lines are about <span class="formula">50</span> characters, and the indentation has been scaled down proportionally.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#include &lt;stdio.h&gt;
<span class="number-left">2</span>int main()
<span class="number-left">3</span>{
<span class="number-left">4</span>     int x;
<span class="number-left">5</span>     int list[3];
<span class="number-left">6</span>     int *a; 
<span class="number-left">7</span>     a = &amp;x;
<span class="number-left">8</span>     list[1]=2;
<span class="number-left">9</span>     *a = 35+list[1];
<span class="number-left">10</span>     printf("%p %p %d\n", &amp;x, list, x); 
<span class="number-left">11</span>}
</pre>
</div>

</div>
<div class="Indented">
We are allowed to say <tt> list[1] = 2;</tt> (line 8), but  <tt>list = &amp;x;</tt> would have been illegal. The reason is that  <tt>list</tt> has a value but is not the name of any location. On one run, this code had the following output (line 10). 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">0x7fff3cb8e414 0x7fff3cb8e400 37 
</pre>
</div>

</div>
<div class="Indented">
The value of<tt> x</tt> is printed as <span class="formula">37</span>, as we may have expected. The address of  x and the value of  <tt>list</tt> are printed in hexadecimal as indicated by the  <tt>0x</tt> at front. Thus, each address is <span class="formula">48</span> bits long. Figure <a class="Reference" href="#fig:MemoryInC">1.3↑</a> may give the impression that each address is the address of a location in physical memory. In fact, the addresses that are printed out are virtual addresses, a concept we will discuss in later chapters.
</div>
<div class="Indented">
The <tt>printf() </tt>function used on line 10 is  part of the standard C library. Its declaration will be in the standard header file <tt>stdio.h</tt>, which is included on line 1. The C compiler knows where to look for this header file.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.2.3">1.2.3</a> The Aitken iteration using arrays and pointers<a class="Label" name="sub:review-C-the-Aitken-iteration"> </a>
</h3>
<div class="Unindented">
As already noted, arrays and pointers are almost equivalent in C. The principal advantage of thinking of arrays in this way arises in passing arrays as arguments to functions. Here we use the Aitken example to illustrate how arrays may be passed as pointers.
</div>
<div class="Indented">
The file <tt>aitken.c</tt> begins with two directives<a class="Label" name="assert-in-aitken.c"> </a>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;assert.h&gt;
#include "aitken.h"
</pre>
</div>

</div>
<div class="Indented">
the second of which includes the header file <tt>aitken.h</tt>. Including <tt>aitken.h</tt> allows the compiler to ensure that the definitions in <tt>aitken.c</tt> are consistent with the declarations in the header file. The first line includes the standard header <tt>assert.h</tt>. The job of finding that header file is left to the compiler. Including that header file allows us to use the <tt>assert</tt> statement to check a condition in the body of the code (see line 4 below).
</div>
<div class="Indented">
The function <tt>aitken()</tt> operates on arrays that are passed to it as pointers.<a class="Label" name="aitken-in-C"> </a> 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void aitken(double* seq1, double* seq2, int len){
<span class="number-left">2</span>     int i;
<span class="number-left">3</span>     double a, b, c;
<span class="number-left">4</span>     assert(len &gt; 2);
<span class="number-left">5</span>     for(i=0; i &lt; len-2; i++){
<span class="number-left">6</span>	 	a = seq1[i];
<span class="number-left">7</span>	 	b = seq1[i+1];
<span class="number-left">8</span>	 	c = seq1[i+2];
<span class="number-left">9</span>	 	seq2[i] =  a - (b-a)*(b-a)/(a-2*b+c);
<span class="number-left">10</span>     }
<span class="number-left">11</span>}
</pre>
</div>

</div>
<div class="Indented">
In a function that calls <tt>aitken()</tt> from some other source file, we may have declared two arrays using <tt>double s[100], t[100]</tt>. Those two arrays will correspond to two segments of memory each equal to <span class="formula">100</span> <tt>doubles</tt> (see figure <a class="Reference" href="#fig:chapter1-fn-call">1.4↓</a>). As we have noted, <tt>s[0]...s[99]</tt> and <tt>t[0]...t[99] </tt>are names of <tt>double </tt>locations, which make up those segments of memory. In contrast, <tt>s</tt> and <tt>t</tt> have values of type <tt>double *</tt> but are not names of any locations in memory. If a call is made as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">aitken(s, t, 100);
</pre>
</div>

</div>
<div class="Indented">
it has the following effect. The function parameters <tt>seq1</tt> and <tt>seq2 </tt>are names of locations in memory that can hold <tt>double *</tt>. The values of <tt>s</tt> and <tt>t</tt> are copied to the locations in memory whose names are <tt>seq1</tt> and <tt>seq2</tt>, respectively (see figure <a class="Reference" href="#fig:chapter1-fn-call">1.4↓</a>). The value of <tt>seq1+17</tt> is the same as the value of <tt>s+17</tt>. Thus, <tt>seq1[17]</tt>, which is exactly the same as <tt>*(seq1+17)</tt>, is another name for the memory location <tt>s[17]</tt>. Thus, we see that by indexing into <tt>seq1</tt> and <tt>seq2</tt> as <tt>seq1[0]...seq1[99]</tt> and <tt>seq2[0]...seq2[99]</tt>, we may refer to any entry in the arrays <tt>s</tt> and <tt>t</tt> defined by the caller (see figure <a class="Reference" href="#fig:chapter1-fn-call">1.4↓</a>).
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:chapter1-fn-call"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/fncall_array2pointer.png" alt="figure FIGS/chapter1/fncall_array2pointer.png" style="max-width: 319px; max-height: 147px;"/>

</div>
<div class="caption">
Figure 1.4 Passing arrays as pointers. In this picture, <tt>s</tt> and <tt>t</tt> are values but not names of any locations.
</div>

</div>

</div>

</div>
<div class="Indented">
There is a little catch here, however. What happens if we say <tt>seq1[100]</tt> or <tt>seq1[200]</tt>? We would be generating a name for a location in memory that was not legally claimed by the caller. That is likely to result in a run-time error. By just using the pointers <tt>seq1</tt> and <tt>seq2</tt>, there is no way we can tell how long the array is. Therefore, the length of the array is the third parameter, which is named <tt>len</tt>, in the function definition. The caller has to explicitly give the length of the array, as it does here by passing <tt>100</tt> as the third argument. 
</div>
<div class="Indented">
Line 9 of the listing corresponds directly to the Aitken transformation formula <a class="Reference" href="#eq:Aitken3">(1.1)↑</a>.
</div>
<div class="Indented">
The <tt>assert(len&gt;2)</tt> statement on line 4 works as follows. If the code is compiled with the option <tt>-DNDEBUG</tt>, it is as if that line were not there and no extra overhead is incurred. If that option is not used during compilation, the condition <tt>len&gt;2</tt> is checked during run-time. If it is violated, the program will abort and print a message indicating the name of the file and the line number of the assertion that turned out to be false. The <tt>assert</tt> statements are valuable aids to debugging and indirectly useful as comments. 
</div>
<div class="Indented">
The definition of <tt>aitkenExtrapolate()</tt>, which is also  in <tt>aitken.c</tt>, is shown below to give a complete account of the source <tt>aitken.c</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double aitkenExtrapolate(double *seq1, double* seq2,
<span class="number-left">2</span>			             int len){
<span class="number-left">3</span>     int n, i, j;
<span class="number-left">4</span>     n = len/2;
<span class="number-left">5</span>     if(len%2==0)
<span class="number-left">6</span>	 	n--;
<span class="number-left">7</span>     for(i=0; i &lt; n; i++){
<span class="number-left">8</span>	 	aitken(seq1, seq2, len-2*i);
<span class="number-left">9</span>	 	for(j=0; j &lt; len-2*(i+1); j++)
<span class="number-left">10</span>	 		seq1[j] = seq2[j];
<span class="number-left">11</span>     }
<span class="number-left">12</span>     return (len%2==0)?seq1[1]:seq1[0];
<span class="number-left">13</span>}
</pre>
</div>

</div>
<div class="Indented">
We assume familiarity with the level of C that occurs in this function, although reading programs such as this can be harder than writing them. Writing good for-loops is the heart of C programming, and there are two nested for-loops in this program. Line 12 uses a conditional expression that may be less familiar. The conditional expression <tt>(a&lt;b)?c:d</tt> tests the condition <tt>a&lt;b</tt>. If the condition is true, it evaluates to <tt>c</tt> but to <tt>d</tt> otherwise. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.2.4">1.2.4</a> Declarations and definitions
</h3>
<div class="Unindented">
Names introduced into a C program are for the most part names of either functions or variables. The names can be introduced as either declarations or definitions. 
</div>
<div class="Indented">
Suppose a variable name is introduced using <tt>int x</tt>. When the compiler encounters that statement, it sets aside a location in memory for an <tt>int</tt> and makes <tt>x</tt> the name of that memory location. This statement is a variable definition, not merely a declaration, because it sets aside memory for <tt>x</tt>.
</div>
<div class="Indented">
A declaration gives type information about a variable that is expected to be defined elsewhere. An example of a variable declaration is a statement such as <tt>extern int x</tt>. When the compiler encounters such a statement, it notes that <tt>x</tt> is a variable of type <tt>int</tt> that is expected to be defined in some other source file. It does not set aside any location in memory. If it later encounters a statement such as <tt>x=x+2</tt>, it does not complain. However, it cannot generate complete machine instructions to carry out that statement because it has no idea where <tt>x</tt> is defined and what address it corresponds to. That information has to be supplied by the linker later.
</div>
<div class="Indented">
Both the lines in the header file <tt>aitken.h </tt>are function declarations not definitions. When the compiler encounters a declaration such as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void aitken(double *seq1, double *seq2, int len);
</pre>
</div>

</div>
<div class="Indented">
it notes that <tt>aitken()</tt> is the name of a function that takes three arguments, the first two of which are of type <tt>double *</tt> and the last of which is an <tt>int</tt>, and returns nothing (<tt>void</tt>). We can omit <tt>seq1</tt>, <tt>seq2</tt>, and <tt>len</tt> from the declaration. Such an omission would make the declaration difficult to read and understand for us, but it makes no difference as far as the compiler is concerned. The compiler does nothing more than note the types of the arguments (or parameters). 
</div>
<div class="Indented">
When the compiler later encounters a function call such as <tt>aitken(s, t, n)</tt>, it first checks that <tt>s</tt> and <tt>t</tt> are of type <tt>double *</tt> and <tt>n</tt> is of type <tt>int</tt>. If the check succeeds, the compiler will generate instructions to set up the arguments and pass control to the function <tt>aitken()</tt>. If the function <tt>aitken() </tt>is defined in some other source file, which it may well be, the compiler has no idea where in memory the code for <tt>aitken()</tt> is located. So it cannot generate complete machine instructions for passing control. That job is the linker’s responsibility. 
</div>
<div class="Indented">
A function definition such as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void aitken(double *seq1, double *seq2, int len){
  ...
}
</pre>
</div>

</div>
<div class="Indented">
has a totally different effect. When the compiler encounters a function definition, it generates machine instructions for the body of the function (which is omitted here) and figures out where to place these instructions in memory. After that, <tt>aitken</tt> corresponds to the chunk of memory that contains machine instructions that implement the body of the function. Just as with variables, defining a function amounts to setting aside memory for it during compilation.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.2.5">1.2.5</a> Function calls and the compilation process
</h3>
<div class="Unindented">
Here we take a look at the mechanism of function calls and the compilation process. Much of the discussion is centered around the file <tt>leibniz.c</tt>, which uses the functions defined by <tt>aitken.c</tt> to extrapolate the partial sums of the Leibniz series and produces data corresponding to table <a class="Reference" href="#table:AitkenLeibniz">1.1↑</a>.
</div>
<div class="Indented">
The source file <tt>leibniz.c</tt> begins with two directives:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include "aitken.h"
#include &lt;stdio.h&gt;
</pre>
</div>

</div>
<div class="Indented">
The first directive includes <tt>aitken.h</tt> to interface to functions defined in <tt>aitken.c</tt>. The second directive includes <tt>stdio.h</tt> to interface to printing functions defined in the <tt>stdio</tt> (standard input/output) library.
</div>
<div class="Indented">
The <tt>leibniz()</tt> function, which generates partial sums of the Leibniz series, is defined below without comment.<a class="Label" name="leibniz-function-in-C"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">//partial sums of 4(1-1/3+1/5-1/7+1/9-...)
void leibniz(double* seq, int len){
     int i;
     for(i=0; i &lt; len; i++)
	  if(i==0)
	       seq[i] = 4.0;
	  else if(i%2==1)
	       seq[i] = seq[i-1] - 4.0/(2.0*i+1);
	  else
	       seq[i] = seq[i-1] + 4.0/(2.0*i+1);
}
</pre>
</div>

</div>
<div class="Indented">
The <tt>printseq()</tt> function prints a sequence using the <tt>printf()</tt> function defined in the <tt>stdio</tt> library. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void printseq(double* seq, int len){
     int i;
     printf("\n \n");
     for(i=0; i &lt; len; i++)
	 	printf("%-.10f\n",seq[i]); 
}
</pre>
</div>

</div>
<div class="Indented">
The logic used by the <tt>main()</tt> function for generating the data shown in table <a class="Reference" href="#table:AitkenLeibniz">1.1↑</a> is similar to that of <tt>aitkenExtrapolate()</tt>. In a C or C++ program, the function named <tt>main()</tt> is the first to gain control when a program is run.<a class="Label" name="main-function-leibniz"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int main(){
<span class="number-left">2</span>     const int len = 13;
<span class="number-left">3</span>     double seq1[len];
<span class="number-left">4</span>     double seq2[len];
<span class="number-left">5</span>     int n, i, j;
<span class="number-left">6</span>     leibniz(seq1, len);
<span class="number-left">7</span>     n = len/2;
<span class="number-left">8</span>     if(len%2==0)
<span class="number-left">9</span>	 	n--;
<span class="number-left">10</span>     for(i=0; i &lt; n; i++){
<span class="number-left">11</span>	 	printseq(seq1,len-2*i);
<span class="number-left">12</span>	 	aitken(seq1, seq2, len-2*i);
<span class="number-left">13</span>	 	for(j=0; j &lt; len-2*(i+1); j++)
<span class="number-left">14</span>	 		seq1[j] = seq2[j];
<span class="number-left">15</span>     }
<span class="number-left">16</span>     if(len%2==0)
<span class="number-left">17</span>	 	printseq(seq1, 2);
<span class="number-left">18</span>     else
<span class="number-left">19</span>	 	printseq(seq1, 1);
<span class="number-left">20</span>}
</pre>
</div>

</div>
<div class="Indented">
The <tt>main()</tt> function calls other functions on lines 6, 11, 12, 17, and 19. All calls except the one to <tt>aitken()</tt> on line 12 are to functions defined in the same source file. We have looked at the call to <tt>aitken()</tt> from the point of view of the callee in section <a class="Reference" href="#sub:review-C-the-Aitken-iteration">1.2.3↑</a>. Let us look at the function call <tt>leibniz(seq1, len)</tt> on line 6 from the point of view of the caller.
</div>
<div class="Indented">
Within  <tt>main()</tt>,  <tt>seq1</tt> is the name of an array of length <span class="formula">13</span> and  <tt>len</tt> is an  <tt>int</tt> (because it is declared  <tt>const</tt>, it must be treated as something that is merely an rvalue). When the function call  <tt>leibniz(seq1, len)</tt> is executed, the value of  <tt>seq1</tt> is copied to the location whose name is  <tt>seq</tt> within <tt>leibniz()</tt>(see the definition of <tt>leibniz()</tt> given above). In addition, the content of the location whose name is  <tt>len</tt> inside  <tt>main()</tt> is copied to the location whose name is  <tt>len</tt> inside  <tt>leibniz()</tt>. This is the call-by-value semantics of C.
</div>
<div class="Indented">
The function call  <tt>aitken(seq1, seq2, len)</tt> on line 12 invokes a function that is not defined in the <tt>leibniz.c</tt> compilation unit. To see what difference that makes, let us see what happens when we issue the command 
</div>
<blockquote class="Quote">
<tt>icc -c leibniz.c</tt>
</blockquote>
<div class="Unindented">
The  <tt>-c</tt> option tells the <tt>icc</tt> C compiler that it should only produce the object file and not the executable. The object file it produces will be called  <tt>leibniz.o</tt> (on Unix systems). To produce the object file, the compiler runs through the code and converts the  <tt>leibniz()</tt> function definition into a sequence of machine instructions. Next it converts  <tt>printseq()</tt> into a sequence of machine instructions before turning to <tt>main()</tt>. When it hits the function call  <tt>leibniz(seq1, len)</tt> on line 6 of the <tt>main()</tt> function, it copies the arguments <tt>seq1</tt> and <tt>len</tt> into a place where the <tt>leibniz()</tt> function can retrieve them. After that it simply generates a machine instruction to make the function call. That machine instruction will pass control to the  <tt>leibniz()</tt> function, which the compiler has already converted into a set of machine instructions.
</div>
<div class="Indented">
When the compiler sees the  <tt>aitken(seq1, seq2, len)</tt> call on line 12 of the <tt>main()</tt> function, the process is initially similar. Thanks to the included header  <tt>aitken.h</tt>, the compiler will have already seen the declaration of  <tt>aitken()</tt> to be  <tt>void aitken(double *seq1, double *seq2, int len)</tt>. So the compiler generates machine instructions to copy <tt>seq</tt>1, <tt>seq2</tt>, and <tt>len</tt> into a place where the definition of <tt>aitken()</tt> can find them. At this point, there is a problem: the compiler cannot generate machine instructions to pass control to <tt>aitken()</tt>. The compiler has no idea where the machine instructions for  <tt>aitken()</tt> are. The name  <tt>aitken()</tt> remains an unresolved reference in the object file <tt>leibniz.o</tt>. 
</div>
<div class="Indented">
Thus, the object file  <tt>leibniz.o</tt> by itself cannot be turned into an executable. To turn it into an executable, we must first compile  <tt>aitken.c</tt> as follows: 
</div>
<blockquote class="Quote">
<tt>icc -c aitken.c</tt> 
</blockquote>
<div class="Unindented">
The <tt>-c</tt> option tells the <tt>icc</tt> command to compile only. Without that option, the command will try to compile and then link and produce an executable with the default name <tt>a.out</tt>. With the compile only option, the command will generate an object file called <tt>aitken.o</tt> with machine instructions for the two functions defined in <tt>aitken.c</tt>. To generate the executable, we run the following command:
</div>
<blockquote class="Quote">
<tt>icc -o leibniz.exe leibniz.o aitken.o</tt>
</blockquote>
<div class="Unindented">
The <tt>-o</tt> option tells the <tt>icc</tt> command to leave its output in the file <tt>leibniz.exe</tt> instead of the default <tt>a.out</tt>. The C linker uses the definition of <tt>aitken()</tt> in <tt>aitken.o</tt> to eliminate the unresolved reference to <tt>aitken()</tt> in <tt>leibniz.o</tt>. The executable file <tt>leibniz.exe </tt>it generates is a sequence of machine instructions with no unresolved names. It is ready to be loaded and run.
</div>
<div class="Indented">
The extensions <tt>.cpp</tt>, <tt>.c</tt>, and <tt>.o</tt> correspond to C++ source files, C source files, and object files, respectively. In the command <tt>icc -c aitken.c</tt>, the file extension of <tt>aitken.c</tt> indicates to the <tt>icc</tt> command that it is operating on a C source file. So the command will invoke the C compiler. In the linking command, the filename extension <tt>.o</tt> indicates that<tt> leibniz.o</tt> and <tt>aitken.o</tt> are both object files. Therefore, <tt>icc</tt> invokes the C linker.
</div>
<div class="Indented">
We have used the <tt>icc</tt> compiler/linker from Intel . But the syntax is almost identical if we used GNU’s <tt>gcc</tt> or the Portland Group’s <tt>pgcc</tt>. Commands such as <tt>icc</tt>, <tt>pgcc</tt>, and <tt>gcc</tt> look at the file name extension to determine the type of the file. 
</div>
<div class="Indented">
We use Intel’s C/C++ compilers in this book because it is easier to link certain libraries that we will discuss later. In addition, the Intel compilers were the default standard on some supercomputing systems used in later chapters. GNU’s <tt>gcc/g++</tt> compiler is widely used, open source, and reputed to be of excellent quality.<span class="FootOuter"><span class="SupFootMarker"> [11] </span><span class="HoverFoot"><span class="SupFootMarker"> [11] </span>The Intel compiler appears to carry some loop optimizations better, but this is not a major point.</span></span> For work unrelated to this book, this author normally uses the GNU compilers.
</div>
<div class="Indented">
Conventionally, the executable files in Linux do not use the file name extension <tt>.exe</tt>. The <tt>.exe</tt> filename extension for executables is a Windows convention. We have adopted the Windows convention throughout this book as it makes for greater clarity.
</div>
<div class="Indented">
Going back to <tt>leibniz.exe</tt>, we may wonder how the name <tt>printf()</tt> in <tt>leibniz.o</tt> gets resolved. It is used in the definition of <tt>printseq()</tt>, and we understood that <tt>printf()</tt> is defined in the <tt>stdio</tt> library. However, the linking command gave no explicit instructions to fetch that library. It did not need to because the linker fetches and links a number of standard libraries by default and <tt>stdio</tt> is one of them.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Look up the meaning of the options  <tt>-c -o -I -L -lm -lmath</tt> in the C compiler’s user’s guide.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use a short C program and the  <tt>sizeof()</tt> facility to print the size in bytes of variables of  <tt>char</tt>, <tt>int</tt>,  <tt>long int</tt>,  <tt>double</tt>,  <tt>char *</tt>,  <tt>int *</tt>, and  <tt>double *</tt> types. Notice that all pointer are the same size (8 bytes).
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  If <tt>p</tt> and  <tt>q</tt> are pointers of type  <tt>char *</tt> and  <tt>double *</tt>, <tt>p=p+1</tt> moves  <tt>p</tt> by one byte while <tt>q=q+1</tt> moves <tt>q</tt> up by eight bytes, so that <tt>p</tt> or <tt>q</tt> point to the next  <tt>char</tt> or  <tt>double</tt>, respectively. Write a short C program to demonstrate this aspect of pointer arithmetic.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In C, a pointer can be an lvalue or an rvalue. Give an example of a pointer that is an rvalue but not an lvalue. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The list of numbers <span class="formula">1, 2, …, <i>n</i></span> can be rearranged in <span class="formula"><i>n</i>!</span> different orders. Write a function that takes an argument <span class="formula"><i>n</i></span> of the type <tt>int</tt> and prints the <span class="formula"><i>n</i>!</span> different permutations. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose <tt>a[]</tt> is an array of type <tt>double</tt> and size <span class="formula"><i>n</i></span>. Write a function that takes <tt>a</tt> and <tt>n</tt> as arguments and sorts the entries of the array. The basic sorting algorithms are bubble sort, quick sort, and heap sort.<span class="FootOuter"><span class="SupFootMarker"> [12] </span><span class="HoverFoot"><span class="SupFootMarker"> [12] </span>Many different sorting algorithms are derived and discussed in <span class="bibcites">[<a class="bibliocite" name="cite-7" href="#biblio-7"><span class="bib-index">7</span></a>]</span>. Most sorting problems are typically handled using library functions.</span></span>
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Rewrite <tt>aitken()</tt> to confirm to the declaration <div class="listing">
<pre class="listing">void aitken(double *seq, int len);
</pre>
</div>
so that it takes a single sequence and transforms it in place.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  What is special about the following C program?<span class="FootOuter"><span class="SupFootMarker"> [13] </span><span class="HoverFoot"><span class="SupFootMarker"> [13] </span><span class="bibcites">[<a class="bibliocite" name="cite-4" href="#biblio-4"><span class="bib-index">4</span></a>]</span> (p. 288) states that, &ldquo;once you understand this program you have understood the main idea&rdquo; behind Gödel’s proof of his incompleteness theorem. </span></span><div class="listing">
<pre class="listing">char *s="char *s=%c%s%c;%cmain(){printf(s,34,s,34,10,10)};%c";
main(){printf(s,34,s,34,10,10);}
</pre>
</div>
The program does not <tt>#include</tt> the header file <tt>stdio.h</tt>, which contains the declaration of <tt>printf()</tt>. Modify the program to <tt>#include</tt> that header file while serving the same purpose.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The following C function copies one array to another array, if the two arrays do not overlap.<div class="listing">
<pre class="listing">void arr_copy(int* in, int* out, int len)
{     
  int i;     
  for(i=0; i &lt; len; i++) 	
    out[i] = in[i]; 
}
</pre>
</div>
If <span class="formula">1, 2, 3, 4</span> is rotated left, it becomes <span class="formula">2, 3, 4, 1</span>. Write a C function for rotating an array left (in-place rotation) that does not use a loop but is allowed to make one call to <tt>arr_copy()</tt>. Can you write a similar function to rotate right?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose that <tt>double **x</tt> points to an array of pointers of length <tt>n</tt> and that each entry of the array points to an array of <tt>double</tt>s, whose length is also <tt>n</tt>. 
</div>
<ul>
<li class="nested">
<ul>
<li>
Interpret <tt>x</tt> as a square matrix and write a function <div class="listing">
<pre class="listing">void transpose(double **x, int n);
</pre>
</div>
which transposes the matrix.
</li>
<li>
Write a function that rotates the matrix columnwise and another function that rotates it rowwise.
</li>
<li>
Write a function that rotates each diagonal forward by one step. More precisely, an entry in position <span class="formula">(<i>i</i>, <i>j</i>)</span> with <span class="formula">0 ≤ <i>i</i>, <i>j</i> &lt; <i>n</i></span> must end up in the position <span class="formula">(<i>i</i> + 1<span class="mathrm"> mod</span><i>n</i>, <i>j</i> + 1<span class="mathrm"> mod</span><i>n</i>)</span>.
</li>

</ul>

</li>

</ul>
<h2 class="Section">
<a class="toc" name="toc-Section-1.3">1.3</a> C++ review<a class="Label" name="sec:review-cpp"> </a>
</h2>
<div class="Unindented">
The C language has a simple philosophy. Its aim is to offer a uniform view of the computer, especially computer memory, to the programmer. C has been so successful that nearly every object that can be called a computer, ranging from supercomputers to routers to embedded and mobile devices, is equipped with a C compiler or a cross-compiler. C is the best vehicle for highly optimized programs.
</div>
<div class="Indented">
Because C seeks to be close to the machine, it is a low-level language. There is often a considerable distance between concepts that are native to a problem domain and their expression as C programs. High-level languages provide constructs and syntax that bring the program much closer to ideas and concepts that are native to the problem domain.
</div>
<div class="Indented">
The C++ language is something of a compromise to provide the facilities of high-level languages without sacrificing the speed of C. Despite its name, it is not an incremental extension of C. It is a colossal expansion of C syntax. It does not have the seamless nature of truly high-level languages such as Python. Classes in Python use very little syntax and fit cleanly within the highly modular architecture of Python programs. However, languages such as Python are much slower than C++.
</div>
<div class="Indented">
Although the C++ language is a compromise, or perhaps because of being a compromise, it has found a great range of uses. On the one hand, C++ has all of C inside it. On the other hand, it provides many mechanisms for capturing concepts and ideas more precisely. Its downside is its complexity. Although clear and careful thinking are essential to all programming, a failure in this respect has particularly acute consequences in C++. 
</div>
<div class="Indented">
Because our focus is on program speed, the part of C++ we use is quite small. Narrowly defined and flat (as opposed to hierarchical) classes, references, occasional function name overloading, and the ability to define variables in the middle of programs is an almost complete list of the C++ features we use. The classes we define are no more than C <tt>struct</tt>s endowed with functions.  C features such as <tt>enum</tt>, <tt>struct</tt>, <tt>typedef</tt>, and <tt>static</tt> can be quite powerful for representing concepts and ideas when used judiciously.
</div>
<div class="Indented">
Classes in C++ are a mechanism for representing concepts and endowing them with functionality that makes them easy to use. Classes can be general or narrow. The <tt>Vector</tt> class studied in this section is an example of a  general class. It can be made even more general. The <tt>Vector</tt> class assumes that each entry of a vector is a <tt>double</tt>. Using templates, one may define a class that allows each entry to be a <tt>double</tt> or a <tt>float</tt> or an <tt>int</tt> or even some user defined type or class.
</div>
<div class="Indented">
The <tt>Vector</tt> class is the only example in this book of a class that attempts to capture a general concept (in this instance, the concept of vectors in linear algebra). Every other class discussed in this book is narrowly defined. We begin our discussion of C++ with the <tt>Vector</tt> class for two reasons. It is a good vehicle for reviewing some of the features of C++. Indeed, we introduce more features of C++ than we need, but that too serves a purpose. C++ is sometimes believed to lead to slow programs. This example helps us explain thoroughly how that may come about. In later chapters, we illustrate the overhead of the <tt>Vector</tt> class and the related <tt>Matrix</tt> class, when used in inner-loops or for disk input/output. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.3.1">1.3.1</a> The <tt>Vector</tt> class<a class="Label" name="sec:chapter1-vector-class"> </a>
</h3>
<div class="Unindented">
The C++ language can be used in many different ways. Using general classes, one may make C++ look like easy to use languages such as Python or Matlab without incurring the enormous cost of such interpreted languages. Our interest is in fast programs and, even more so, in understanding what makes programs fast or slow. The C++ style we adopt is quite close to C.
</div>
<div class="Indented">
Nevertheless, we begin with a general type of class, namely, the <tt>Vector</tt> class. This class helps us review a few of the features of C++ and is used to implement the Aitken iteration later. In a later chapter, we criticize the use of this class and show it to be slow.
</div>
<h? class="Subsubsection">
<b><u>Header file with class definition</u></b>
</h?>
<div class="Unindented">
The<tt> Vector</tt> class is defined in the header file <tt>Vector.hh</tt>. The C++ class consists of data members and member functions. With respect to computer memory, a class object is a collection of data items. The data items could be of basic types such as <tt>double</tt> or <tt>char</tt>, or pointers. The data items may also be other class objects or C structures. The member functions provide various means to manipulate the class object or, equivalently, the package of data items that constitutes the class object.
</div>
<div class="Indented">
It is typical for header files to give only  part of the definition of the class. Many of the member functions are typically defined in a separate source. Here the entire class definition is in the header file <tt>Vector.hh</tt>, which makes the header file a bit long. We present the contents of the header file in stages, gradually unveiling features of C++. The skeleton of the header file is listed below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#ifndef MYVECTOR
<span class="number-left">2</span>#define MYVECTOR
<span class="number-left">3</span>#include &lt;cassert&gt;
<span class="number-left">4</span>#include &lt;cmath&gt;
<span class="number-left">5</span>#include &lt;cstdlib&gt;
<span class="number-left">6</span>#include &lt;cstring&gt;
<span class="number-left">7</span>#include &lt;iostream&gt;
<span class="number-left">8</span>#include &lt;fstream&gt;
<span class="number-left">9</span>​
<span class="number-left">10</span>using namespace std;
<span class="number-left">11</span>​
<span class="number-left">12</span>class Vector{
<span class="number-left">13</span>private:
<span class="number-left">14</span>     ...
<span class="number-left">15</span>public:
<span class="number-left">16</span>     ...
<span class="number-left">17</span>};
<span class="number-left">18</span>​
<span class="number-left">19</span>#endif
</pre>
</div>

</div>
<div class="Indented">
Lines 1, 2, and 19 ensure that the header file expands to the source code in between if and only if the macro variable <tt>MYVECTOR</tt> is defined. Here <tt>MYVECTOR</tt> is not good nomenclature, as it may be inadvertently reused by some other header, subverting our attempt to ensure conditional inclusion of header files. 
</div>
<div class="Indented">
It is typical for C/C++ sources to begin by including a number of header files to interface to other source files or libraries. C programs include the header <tt>assert.h</tt> to use the <tt>assert()</tt> macro (see section <a class="Reference" href="#assert-in-aitken.c">1.2.3↑</a>). C++ programs may include either <tt>assert.h</tt> or <tt>cassert</tt>, as on line 3, to emphasize that a C facility is being employed.
</div>
<div class="Indented">
The <tt>Vector</tt> class uses other C facilities as well. It uses the <tt>fabs()</tt> function from <tt>cmath</tt> (line 4) in one of its member functions to compute the norm of the vector. C facilities to allocate and release memory are defined in <tt>stdlib</tt> (line 5). 
</div>
<div class="Indented">
C++ facilities for input/output from terminals and files are in <tt>iostream</tt> (line 7) and <tt>fstream</tt> (line 8), respectively. Member functions that input/output vectors to/from files use these facilities. 
</div>
<div class="Indented">
To output a variable <tt>x</tt> to standard output (typically the terminal), we may say 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">cout&lt;&lt;x&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
Here <tt>cout</tt> is the name of standard output declared in <tt>iostream</tt>. The end of line character <tt>endl</tt> is also declared in <tt>iostream</tt>. In C, input/output syntax is sensitive to the type of data items being handled. The abstraction features of C++ are used by the <tt>iostream</tt> library to provide a uniform interface for input/output regardless of the type of the variable. Even class objects may be input/output in this manner if the operators <tt>&lt;&lt;</tt> (for output) and <tt>&gt;&gt;</tt> (for input) are overloaded suitably.
</div>
<div class="Indented">
To output to a file, the syntax looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">ofstream ofile("tmp.txt");
ofile&lt;&lt;x&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
Here <tt>ofile</tt> is defined as an <tt>ofstream</tt> object. At the point of definition, it is tied to the file <tt>tmp.txt</tt>. 
</div>
<div class="Indented">
To input <tt>x</tt> from standard input, we may say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">cin&gt;&gt;x;
</pre>
</div>

</div>
<div class="Indented">
To input from a file, we may say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">ifstream ifile("tmp.txt")
ifile&gt;&gt;x;
</pre>
</div>

</div>
<div class="Indented">
This will work regardless of whether <tt>x</tt> is a <tt>double</tt> or an <tt>int</tt> or a <tt>long</tt> or a <tt>char</tt>.
</div>
<div class="Indented">
The class names <tt>ofstream</tt> and <tt>ifstream</tt> as well as class object names <tt>cout</tt> and <tt>cin</tt> are defined in the namespace <tt>std</tt>. In general, we should say <tt>std::cout</tt> and <tt>std::ofstream</tt> because these names do not exist outside the namespace. However, the <tt>using namespace</tt> declaration on line 10 brings in all the names in <tt>std</tt> into scope. It allows us to say <tt>cout</tt> instead of <tt>std::cout</tt>.
</div>
<div class="Indented">
It is often not a good idea to bring in the entire <tt>std</tt> namespace, especially within header files. The C++ standard library is vast. Bringing in the entire <tt>std</tt> namespace, as we do on line 10, pollutes the namespace considerably. For example, a programmer may define a function called <tt>copy()</tt> to copy <tt>double</tt> arrays in a program-specific manner and conflict with the <tt>std</tt> namespace. There are functions with common names such as <tt>copy()</tt> and <tt>sort()</tt> in the standard library.
</div>
<div class="Indented">
The listing shows the outline of the definition of the class <tt>Vector</tt> (lines 12 through 17). There is a private section (line 13) in the class definition and a public section (line 15). The class definition must end with a semicolon as on line 17. Omitting the semicolon at the end of class definitions is a common novice error.
</div>
<h? class="Subsubsection">
<b><u>Private section of the <tt>Vector</tt> class</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:VectorSchematic"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/VectorSchematic.png" alt="figure FIGS/chapter1/VectorSchematic.png" style="width: 4in; max-width: 229px; height: auto; max-height: 98px;"/>

</div>
<div class="caption">
Figure 1.5 Schematic picture of a <tt>Vector</tt> of length 2, which ‘‘owns’’ its data. 
</div>

</div>

</div>

</div>
<div class="Indented">
A class is a collection of data members and member functions. In the <tt>Vector</tt> class all the data members are in the private section.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class Vector{
private:
     long int size;
     double *data;
     int owner;
public:
     ...
};
</pre>
</div>

</div>
<div class="Indented">
The class member <tt>size</tt> is the length of the vector and <tt>data</tt> is a pointer to the contents of the vector; so <tt>data[i]</tt> is in effect a name for the <tt>i</tt>th entry of the vector. The <tt>owner</tt> field will be either 0 or 1---its meaning is explained later.
</div>
<div class="Indented">
If <tt>v</tt> is a variable of type <tt>Vector</tt>, then it is a name for a segment of memory that includes a location of type <tt>int</tt>, a location of type <tt>double *</tt>, and a location of type <tt>int</tt>. A schematic view of a <tt>Vector</tt> of length <span class="formula">2</span> is shown in figure <a class="Reference" href="#fig:VectorSchematic">1.5↑</a>. If <tt>v</tt> is the name of an object of class <tt>Vector</tt>, <tt>v.size</tt>, <tt>v.data</tt>, and <tt>v.owner</tt> are names for members of <tt>v</tt> as shown in the figure. One may say that the class object <tt>v</tt> is a package of the data items <tt>v.size</tt>, <tt>v.data</tt>, and <tt>v.owner</tt>.
</div>
<div class="Indented">
If a data member or a member function is in the private section, its access is restricted. Because the data items <tt>v.size</tt>, <tt>v.data</tt>, and <tt>v.owner</tt> are in the private section, only member functions are allowed to access them.<span class="FootOuter"><span class="SupFootMarker"> [14] </span><span class="HoverFoot"><span class="SupFootMarker"> [14] </span>This statement is not exactly true. C++ has a notion of <tt>friend</tt>s, which too get access to the private section of a class. </span></span> Whatever functionality we want must be defined through the member functions. For example, suppose we want to input a vector from a file. We are not allowed to directly set the size of the <tt>Vector</tt> object during input using syntax such as <tt>v.size=100</tt>. Instead, we may define a member function <tt>input()</tt>, which takes care of reading data and setting <tt>v.size</tt> appropriately.
</div>
<h? class="Subsubsection">
<b><u>Member functions of the <tt>Vector</tt> class</u></b>
</h?>
<div class="Unindented">
All the member functions of the <tt>Vector</tt> class are in the public section. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>class Vector{
<span class="number-left">2</span>private:
<span class="number-left">3</span>     ...
<span class="number-left">4</span>public:
<span class="number-left">5</span>     Vector(){...}
<span class="number-left">6</span>     Vector(long int  n){...}
<span class="number-left">7</span>     Vector(const Vector&amp; v){...} 
<span class="number-left">8</span>     ~Vector(){...}
<span class="number-left">9</span>     void shadow(double *dataptr, long int len)
<span class="number-left">10</span>     {...}
<span class="number-left">11</span>     void shadow(const Vector&amp; v){...}
<span class="number-left">12</span>     void shadow(const Vector&amp; v,  long int i,  
<span class="number-left">13</span>		 long int len){...}
<span class="number-left">14</span>     long int getSize() const{...}
<span class="number-left">15</span>     double * getRawData() const{...}
<span class="number-left">16</span>     double&amp; operator()(long int i){...}
<span class="number-left">17</span>     const double&amp; operator()(long int i) const{.}
<span class="number-left">18</span>     Vector&amp; operator=(const Vector&amp; v){...}
<span class="number-left">19</span>     void add(const Vector&amp; v){...}
<span class="number-left">20</span>     void sub(const Vector&amp; v){...}
<span class="number-left">21</span>     void mul(const Vector&amp; v){...}
<span class="number-left">22</span>     void div(const Vector&amp; v){...}
<span class="number-left">23</span>     void scale(const double x){...}
<span class="number-left">24</span>     void add_constant(const double x){...}
<span class="number-left">25</span>     double norm() const{...}
<span class="number-left">26</span>     void output(const char* fname)const{...}
<span class="number-left">27</span>     void input(const char* fname){...}
<span class="number-left">28</span>};
</pre>
</div>

</div>
<div class="Indented">
There is a basic difference between data members and member functions. Suppose <tt>v</tt> is a <tt>Vector</tt> class object. Then <tt>v.size</tt>, <tt>v.data</tt>, and <tt>v.owner</tt> refer to the data items packaged inside <tt>v</tt>. In contrast, <tt>v.norm()</tt> applies the member function <tt>norm()</tt> (line 25) with the class object <tt>v</tt> as its target. 
</div>
<div class="Indented">
The member functions exist within the namespace defined by the class. Each class defines a namespace. The <tt>Vector</tt> class defines the eponymous namespace <tt>Vector::</tt>. If we want to refer to a member function such as <tt>norm()</tt> outside the scope of the class (the scope here is from lines 1 through 28), the name must be given as <tt>Vector::norm()</tt>.
</div>
<div class="Indented">
For example, we may define the member function <tt>norm()</tt> in a compilation unit and outside the scope of the class <tt>Vector</tt> as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double Vector::norm() const{
     ...
}
</pre>
</div>

</div>
<div class="Indented">
We have chosen to define all the member functions in <tt>Vector.hh</tt>  and within the scope of the <tt>Vector</tt> class.
</div>
<div class="Indented">
Before delving into the member functions, we give an example of how <tt>Vector</tt> objects are used. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include "Vector.hh"
​
int main(){
     Vector v(20);	
     for(int i=0; i &lt; 20; i++)
          v(i) = i;
     Vector w1, w2;
     w1.shadow(v, 0, 10);
     w2.shadow(v, 10, 10);
     w1.add(w2);
     w1.output("w1.txt");
}
</pre>
</div>

</div>
<div class="Indented">
The definition <tt>Vector v(20)</tt> invokes the class constructor on line 6. This constructor will make <tt>v.data</tt> point to <tt><span class="formula">20</span></tt> <tt>double</tt>s, set <tt>v.size to <span class="formula">20</span></tt>, and set <tt>v.owner=1</tt>, making <tt>v</tt> the owner of its data. 
</div>
<div class="Indented">
The body of the for-loop has <tt>v(i)=i</tt>. When we say <tt>f(x)</tt> in C/C++, <tt>f()</tt> is a function being applied to <tt>x</tt>. Here we say <tt>v(i)</tt>, but <tt>v</tt> is a class object not a function being applied to <tt>i</tt>. However, thanks to operator overloading, the compiler interprets <tt>v(i)</tt> as <tt>v.operator()(i)</tt>---in words, the member function <tt>operator()</tt> is applied to the target <tt>v</tt> with <tt>i</tt> as the sole argument. The member function defined on line 16 is called. This member function will look at <tt>v.data</tt>, access the <span class="formula"><i>i</i></span>th entry, and return a reference to it. Thus, when we say <tt>v(i)=i</tt>, the <span class="formula"><i>i</i></span>th entry of the <tt>Vector</tt> object <tt>v</tt> gets set to <span class="formula"><i>i</i></span>. At the end of the for-loop, the entries of <tt>v</tt> are <span class="formula">0, ..., 19</span>.
</div>
<div class="Indented">
At the definition <tt>Vector w1, w2;</tt>, the <tt>Vector</tt> objects <tt>w1</tt> and <tt>w2</tt> are created using the empty constructor (line 5). This empty constructor sets <tt>w1.size</tt> to <span class="formula">0</span>, <tt>w1.data</tt> to <tt>NULL</tt>, and <tt>w1.owner</tt> to <span class="formula">0</span>, and likewise for <tt>w2</tt>. Neither <tt>w1</tt> nor <tt>w2</tt> owns any data. To begin with, they are vectors of size <span class="formula">0</span>.
</div>
<div class="Indented">
When we say <tt>w1.shadow(v, 0, 10)</tt>, the member function <tt>shadow()</tt> defined on line 12 gets invoked. Its effect is to set <tt>w1.data</tt> to <tt>v.data</tt> and <tt>w1.size</tt> to <span class="formula">10</span>. However, <tt>w1.owner</tt> remains <span class="formula">0</span> because the data is owned by <tt>v</tt>. The <tt>Vector</tt> object <tt>w1</tt> is a shadow of the first <span class="formula">10</span> entries of <tt>v</tt>. Likewise, after <tt>w2.shadow(v, 10, 10)</tt>, <tt>w2.data</tt> is set to <tt>v.data+10</tt>, and <tt>w2</tt> becomes a shadow of entries <span class="formula">10</span> through <span class="formula">19</span> of <tt>v</tt>.
</div>
<div class="Indented">
When we say <tt>w1.add(w2)</tt>, the member function <tt>add()</tt> (line 19) is invoked with <tt>w1</tt> as the target and <tt>w2</tt> as its argument. This member function verifies that its target and its argument are vectors of the same size and adds <tt>w2</tt> to <tt>w1</tt>, entry by entry.
</div>
<div class="Indented">
The final line <tt>w1.output(&ldquo;w1.txt&rdquo;)</tt> outputs <tt>w1</tt> to the file <tt>w1.txt</tt> via the member function <tt>output()</tt> (line 26). Thus, the numbers <span class="formula">10, 12, ..., 28</span> will be output to <tt>w1.txt</tt>.
</div>
<div class="Indented">
The program is not done yet, however. When the class objects <tt>v</tt>, <tt>w1</tt>, and <tt>w2</tt> go out of scope at the end of <tt>main()</tt>, the compiler inserts calls to the destructor (line 8). The destructor is called thrice. When destroying <tt>v</tt>, the destructor notes that <tt>v</tt> is the owner of its data and releases the memory that <tt>data</tt> points to. There is nothing to be done to destroy <tt>w1</tt> and <tt>w2</tt> because they do not own their data. The destructor returns as soon as it notes that they are not owners.
</div>
<h? class="Subsubsection">
<b><u>References</u></b>
</h?>
<div class="Unindented">
We will step through a few C++ concepts using the public member functions of the class <tt>Vector</tt> as illustrations. The first of these is references.
</div>
<div class="Indented">
A reference in C++ is another name for an object that is already in existence. For example, we may say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double &amp;x = y;
</pre>
</div>

</div>
<div class="Indented">
and <tt>x</tt> becomes another name for <tt>y</tt>, which is assumed to be of type <tt>double</tt>. If we say <tt>x=2</tt> and then print <tt>y</tt>, its value will be <tt><span class="formula">2</span>.</tt>
</div>
<div class="Indented">
The <tt>&amp;</tt> character is used to take the address of locations. It is also used for references as illustrated. Overloading the <tt>&amp;</tt> operator in this way may create some confusion initially, but in fact references are really just a shorthand for the use of pointers.
</div>
<div class="Indented">
We may use references in passing arguments to a function. For example, if we define a function as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double f(double &amp;x){
     ...
}
</pre>
</div>

</div>
<div class="Indented">
we may call it as <tt>f(z)</tt>, assuming <tt>z</tt> is a variable of type <tt>double</tt> and <tt>x</tt> will become a reference to <tt>z</tt>. The call <tt>f(2.0)</tt> is not legal, however, because <span class="formula">2.0</span> is a value and not the name of a <tt>double</tt> location. 
</div>
<div class="Indented">
References are commonly used to pass class objects as arguments to functions. The <tt>Vector</tt> class holds just three data items (<tt>size</tt>, <tt>data</tt>, and <tt>owner</tt>), but in general a lot of data can be packaged into a single class. When a class object is passed as an argument to a function, do we really want a fresh copy to be made of all the data inside that object? Typically, the answer is no. First, the expense of having to copy all that data may be undesirable. Second, even if there is only a small amount of data, the semantics could be incorrect. For <tt>Vector</tt> objects, item-by-item copying results in multiple objects incorrectly owning the same data. 
</div>
<div class="Indented">
A declaration of a member function can specify that some of the arguments are references. The declaration <tt>void add(const Vector&amp; v)</tt> (line 19) specifies that the argument named <tt>v</tt> is passed by reference (the <tt>const</tt> qualifier is discussed later). The crucial symbol here is the <tt>&amp;</tt>, which precedes <tt>v</tt> in the declaration. If arguments are passed as references, we do not have to worry about the correct semantics for copying. 
</div>
<div class="Indented">
Functions may also return a reference as does the member function <tt>operator()</tt> (line 16) . This function is defined as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">     double&amp; operator()(long int i){
         assert(i &lt; size);
	     return(data[i]);
     }
</pre>
</div>

</div>
<div class="Indented">
in <tt>Vector.hh</tt> within the scope of the class <tt>Vector</tt>. Here <tt>size</tt> and <tt>data</tt> are data members of the target. So if <tt>v</tt> is a <tt>Vector </tt>object and we say <tt>v(6)=17</tt>, this member function is called with <tt>v</tt> as the target and with argument <tt>i</tt> equal to <span class="formula">6</span>. Assuming the vector is of size <span class="formula">6</span> or greater, the member function returns <tt>data[i]</tt>, which is the same as <tt>v.data[i]</tt> by reference. Whatever is returned is another name for the <span class="formula">6</span>th entry of the <tt>Vector</tt> object <tt>v</tt>. Thus, saying <tt>v(6)=17</tt> has the effect of setting that entry to <span class="formula">17</span>.
</div>
<h? class="Subsubsection">
<b><u>Operator and function name overloading</u></b>
</h?>
<div class="Unindented">
We have already seen how the function call operator is overloaded. Below is another example. In this example, the assignment operator is overloaded. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">     Vector&amp; operator=(const Vector&amp; v){
          assert(size==v.size);
          memcpy((char *)data, (char *)v.data, 
	              size*sizeof(double));
          return(*this);
  }
</pre>
</div>

</div>
<div class="Indented">
The assignment operator uses <tt>memcpy()</tt> to copy the data in <tt>v</tt> to its target.<span class="FootOuter"><span class="SupFootMarker"> [15] </span><span class="HoverFoot"><span class="SupFootMarker"> [15] </span>Strictly speaking, the overloaded <tt>=</tt> operator should check that <tt>data</tt> and <tt>v.data</tt> do not overlap.</span></span> If we say <tt>w=v</tt>, with both <tt>v</tt> and <tt>w</tt> being <tt>Vector</tt> objects, the member function <tt>operator=()</tt> is invoked with <tt>w</tt> as the target and <tt>v</tt> as its argument. As before, <tt>size</tt> and <tt>data</tt> refer to the corresponding items of the target.
</div>
<div class="Indented">
A new bit of syntax here is the keyword <tt>this</tt>. Within a member function, <tt>this</tt> is a pointer to the target. Therefore, <tt>*this</tt> is the target. The overloaded assignment operator returns a reference to the target.
</div>
<div class="Indented">
If <tt>u</tt>, <tt>v</tt>, and <tt>w</tt> are <tt>Vector</tt> objects, and we say <tt>u=v=w</tt>, the compiler interprets that as <tt>u=(v=w)</tt>. In C/C++, the assignment operator is right associative. The result of <tt>v=w</tt> is a reference to <tt>v</tt>, which is the argument in the next call to the overloaded assignment operator that assigns to <tt>u</tt>.
</div>
<div class="Indented">
C++ also allows function name overloading. For instance, we can define a function as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void add(double *v, const double *w, 
         const int len){...}
</pre>
</div>

</div>
<div class="Indented">
which adds the array <tt>w</tt> to the array <tt>v</tt>. We can define another function 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void add(const double *u, const double *w, double *v, 
	     const int len){...}
</pre>
</div>

</div>
<div class="Indented">
which leaves the sum of <tt>u</tt> and <tt>w</tt> in <tt>v</tt>. In C, the two definitions would conflict because they are both trying to bind a definition to the same name <tt>add</tt>. However, in C++, the compiler allows both the definitions because they can be disambiguated using the number and types of the arguments. The compiler internally generates mangled names to keep the definitions separate in the object file, so as not to confuse the linker. 
</div>
<h? class="Subsubsection">
<b><u>Function call inlining</u></b>
</h?>
<div class="Unindented">
The member functions of the class <tt>Vector</tt> are defined in the header file <tt>Vector.hh</tt> within the scope of the class definition. If we wanted to define the overloaded function call operator elsewhere, we can give its definition as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double&amp; Vector::operator()(long int i){
     assert(i &lt; size);
     return(data[i]);
}
</pre>
</div>

</div>
<div class="Indented">
In fact, it is better to define the overloaded function call operator within the scope of the class as we did. Every source that uses the <tt>Vector</tt> class will include the header <tt>Vector.hh</tt>. Thus, every compilation unit will see the complete definition of the function call operator. If we say <tt>v(i) = 7.0</tt>, with <tt>v</tt> being a <tt>Vector</tt> object, the compiler does not actually generate a function call for <tt>v(i)</tt>. Instead it scans the definition of the overloaded function call operator and splices in the body of the function at the point of call. The overhead of making a function call is eliminated. This is called function call inlining.
</div>
<div class="Indented">
The function call overhead in modern processors is quite small and is typically less than <span class="formula">10</span> cycles.<span class="FootOuter"><span class="SupFootMarker"> [16] </span><span class="HoverFoot"><span class="SupFootMarker"> [16] </span>The function call overhead depends on the number of arguments, types of the arguments, number of registers used by the caller, context of the function call, and other factors.</span></span> Much of the time it is nothing to worry about. However, if a member function compiles to only a few machine instructions and is called frequently, one must ensure that the member function is inlined. 
</div>
<div class="Indented">
In C++, one can explicitly ask the compiler to inline functions that are not member functions. For instance, if a function with the declaration <tt>void add(Vector u, Vector v)</tt> is instead declared <tt>inline void add(Vector u, Vector v)</tt> and defined within the header file, the compiler will try to and almost certainly succeed in inlining that function call. The compiler may refuse or fail to inline functions that are long and complicated without warning.
</div>
<div class="Indented">
If several source files are compiled and linked simultaneously with interprocedural optimization turned on (<tt>-ipo</tt> option in <tt>icc</tt> and <tt>icpc</tt>), the compiler will again try to inline certain function calls. Normally, we will avoid interprocedural optimization, preferring to build object files for each source file or compilation unit separately. 
</div>
<div class="Indented">
Inlining is not quite the panacea it is sometimes believed to be. Suppose we need to replace each entry of an array by the cumulative sum of the preceding entries. We can write the loop in C using pointers and pointer arithmetic or we can use a class such as the <tt>Vector</tt> class. A well-written C-style loop will  lead to faster and more compact assembly code for reasons explained in a later chapter (see section <a class="Reference" href="#subsec:proc-compileropt-C++-overhead">3.2.6↓</a>). Inlining occasionally leads to worse code as we illustrate later in this chapter.
</div>
<h? class="Subsubsection">
<b><u>Constructors and destructors</u></b>
</h?>
<div class="Unindented">
Constructors and destructors are central to the class mechanism in C++. The three constructors of the <tt>Vector</tt> class are defined below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">     //empty constructor
     Vector(){
          size = 0;
          data = NULL;
          owner = 0;
     }
	
     //only constructor to allocate space for data
     Vector(long int  n){
          size = n;
          data = (double *)malloc(sizeof(double)*n);
          owner = 1;
     }
  
  
     //*this becomes shadow of Vector v 
     //(copy constructor)
     Vector(const Vector&amp; v){
          size = v.size;
          data = v.data;
          owner = 0;
     }  
</pre>
</div>

</div>
<div class="Indented">
The empty constructor and the copy constructor do not allocate space. The constructor in the middle allocates space using <tt>malloc()</tt>, which is in <tt>cstdlib</tt>. These member functions are understood to be constructors because they have the same name as the class. Constructors are not allowed to return anything.
</div>
<div class="Indented">
Constructors are called implicitly at the point of definition of class objects. It is essential to understand when constructors are called. Suppose we define a vector object as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector v;
</pre>
</div>

</div>
<div class="Indented">
the empty constructor is called to initialize the object <tt>v</tt>. If we define an array of vectors
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector v[100];
</pre>
</div>

</div>
<div class="Indented">
the empty constructor is called for each object in the array. It is illegal to define an array of objects if the class is not equipped with an empty constructor. 
</div>
<div class="Indented">
If we define a vector object as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector v(27)
</pre>
</div>

</div>
<div class="Indented">
the constructor in the middle is called. The compiler notes that the object <tt>v</tt> is being built with the single argument <tt>27</tt>, which is a constant of type <tt>int</tt>. The constructor that calls <tt>malloc()</tt> takes <tt>long int</tt> as an argument. In C/C++, an <tt>int</tt> is automatically promoted to a <tt>long int</tt> if necessary.
</div>
<div class="Indented">
The constructor uses <tt>malloc()</tt> to claim space for this <tt>Vector</tt> object. If we make the call
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">malloc(1000*1000)
</pre>
</div>

</div>
<div class="Indented">
the function returns a pointer to <span class="formula">10<sup>6</sup></span> bytes of memory. The pointer is of type <tt>void *</tt>. 
</div>
<div class="Indented">
In the usage
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">data = (double *)malloc(sizeof(double)*n);
</pre>
</div>

</div>
<div class="Indented">
<tt>void *</tt> is <i>cast</i> to <tt>double *</tt>. Type casts are used to convert values of one type to another type. For example, we may say <tt>(int)1.4142</tt> to convert the double value <tt>1.4142</tt> to an <tt>int</tt> (it will be truncated). The cast is needed here because <tt>data</tt> is of type <tt>double *</tt> while <tt>malloc()</tt> returns <tt>void *</tt>. 
</div>
<div class="Indented">
What happens during <tt>malloc()</tt>? The short answer, which will be elaborated later, is that the function call first goes to the C library. The C library may in turn call the operating system if it is not able to come up with the memory by itself. The operating system typically allocates an area in virtual memory to the calling process. No region in physical memory is set aside. Physical memory is set aside only when the process first tries to access the memory it has claimed for itself. There is a page fault during first access, and the page fault handler sets aside physical memory.
</div>
<div class="Indented">
Suppose we need an array of <tt>Vector</tt> objects of length <span class="formula">100</span> with each vector of size <span class="formula">1000</span>. The definition 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector v[100]
</pre>
</div>

</div>
<div class="Indented">
will not do. The <span class="formula">100</span> <tt>Vector </tt>objects it creates all use the empty constructor and therefore have length <span class="formula">0</span> and do not own any data. We need the following code:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector *v[100];
for(int i=0; i &lt; 100; i++)
     v[i] = new Vector(1000);
</pre>
</div>

</div>
<div class="Indented">
What we get here is an array of pointers to <tt>Vector</tt> objects and not an array of <tt>Vector</tt> objects. The for-loop initializes each pointer in the array to point to a <tt>Vector</tt> object of size <span class="formula">1000</span>. In C++, the usage
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">new Vector(1000)
</pre>
</div>

</div>
<div class="Indented">
calls a constructor explicitly and returns a pointer to the object that was created.
</div>
<div class="Indented">
The <tt>new</tt> operator may also be used to allocate arrays. For example, we can say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">data = new double[size]
</pre>
</div>

</div>
<div class="Indented">
to make <tt>data</tt> point to an array of <tt>double</tt>s of length <tt>size</tt>. Thus, <tt>new</tt> can be an alternative to <tt>malloc()</tt>. But <tt>malloc()</tt> cannot create class objects, although <tt>new</tt> can.
</div>
<div class="Indented">
We come at last to the copy constructor. The copy constructor makes its target a shadow of its argument. If we define <tt>v</tt> as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector v(w)
</pre>
</div>

</div>
<div class="Indented">
where <tt>w</tt> is an object of type <tt>Vector</tt>, the copy constructor is invoked with <tt>w</tt> as its argument. A standard situation where the copy constructor is invoked is as follows. Suppose we define a function
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void add(Vector v1, Vector v2)
</pre>
</div>

</div>
<div class="Indented">
and call it <tt>add(v, w)</tt>. By default, arguments are passed by copying (pass by value) in C/C++. At the point of call, <tt>v</tt> is copied to <tt>v1</tt> and <tt>w</tt> is copied to <tt>v2</tt>. In C++, copying class objects means calling the copy constructor. Here, class objects <tt>v1</tt> and <tt>v2</tt> are created using copy constructors. 
</div>
<div class="Indented">
For the sake of completeness, we give the definition of one of the member functions named <tt>shadow()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">     //makes *this shadow v(i:i+len-1)
     void shadow(const Vector&amp; v,  long int i,  
                 long int len){ 
          assert(!owner);
          assert(i+len&lt;=v.size);
          size = len;
          data = v.data + i;
          owner = 0;
     }
</pre>
</div>

</div>
<div class="Indented">
The target becomes a shadow of <tt>v</tt> of length <tt>len</tt> beginning at the <span class="formula"><i>i</i></span>th entry of <tt>v</tt>.
</div>
<div class="Indented">
The sole destructor of the class is defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">     ~Vector(){
     	if(owner!=0)
     		free(data);
     }
</pre>
</div>

</div>
<div class="Indented">
This member function is understood to be a destructor because its name is the class name prefixed with a tilde. Destructors are not allowed to return anything nor can they take any arguments. Destructors are called automatically when the class object goes out of scope. The destructor of the <tt>Vector</tt> class frees its data if it is an owner.
</div>
<div class="Indented">
The function <tt>free()</tt> is defined in the C library, like <tt>malloc()</tt>. The space allocated by <tt>malloc()</tt> was stored in <tt>data</tt>. It is released by <tt>free()</tt>. 
</div>
<div class="Indented">
If a class object is allocated using <tt>new</tt>, as in 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector *vptr = new Vector(1000);
</pre>
</div>

</div>
<div class="Indented">
where <tt>vptr</tt> is a pointer to <tt>Vector</tt>, it must be released using the <tt>delete</tt> operator:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">delete vptr;
</pre>
</div>

</div>
<div class="Indented">
The class destructor is called when <tt>vptr</tt> is deleted. If an array is allocated using <tt>new[]</tt>, as in 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double *data = new double[size];
</pre>
</div>

</div>
<div class="Indented">
it must be released using <tt>delete[]</tt>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">delete[] data;
</pre>
</div>

</div>
<h? class="Subsubsection">
<b><u>The <tt></tt></u></b>const<span class="default"> qualifier</span>
</h?>
<div class="Unindented">
The following definition is contained in the first few lines of <tt>Vector.hh</tt> (but omitted from earlier listings): 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">const double PI = 3.1415926535897932384e+00;
</pre>
</div>

</div>
<div class="Indented">
Because the definition of <tt>PI</tt> is qualified using <tt>const</tt>, any attempt to change the value of <tt>PI</tt> is illegal. If a user attempts to change the value of <tt>PI</tt>, the compiler will catch the error. 
</div>
<div class="Indented">
The <tt>Vector</tt> class is equipped with member functions <tt>output()</tt> and <tt>input()</tt> to facilitate output to files and input from files. Definition of the member function <tt>input()</tt> begins as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void input(const char* fname){...}
</pre>
</div>

</div>
<div class="Indented">
Here the argument <tt>fname</tt> (file name) is a pointer to a <tt>char</tt> (character). In C and C++, a <tt>char</tt> is a single byte with ascii encoding. Strings are arrays of characters terminated by <tt>’\0’</tt> or equivalently the byte of value <tt>0</tt>. If a string is passed as a pointer to a <tt>char</tt>, its length need not be passed explicitly. The convention for terminating strings determines its length. 
</div>
<div class="Indented">
A pointer holds an address which points to some data. Prefixing the <tt>const</tt> qualifier to the declaration or definition of a pointer implies that the data does not change, but the pointer may change. So it is illegal to say <tt>fname[0]=’M’</tt> within the body of the function, but it is legal to say <tt>char c; fname = &amp;c;.</tt> 
</div>
<div class="Indented">
Suppose we want to initialize a vector of length <span class="formula">20</span> using data stored in a file named <tt>init.dat</tt> in the current directory. The data in the file must be a sequence of <span class="formula">20</span> (or more---the extra values are ignored) values separated by whitespace. The following code does that:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>char fname[9]={’i’,’n’,’i’,’t’,’.’,’d’,’a’,’t’,’\0’};
<span class="number-left">2</span>Vector v(20);
<span class="number-left">3</span>v.input(fname);
</pre>
</div>

</div>
<div class="Indented">
In line 1, the number <span class="formula">9</span>, which gives the length of the array, can be omitted because the compiler can use the initializing sequence to determine the length. The usage below is much more convenient.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector v(20);
v.input("init.dat");
</pre>
</div>

</div>
<div class="Indented">
We are allowed to pass <tt>&ldquo;init.dat&rdquo;</tt> as an argument explicitly because the first argument of the member function <tt>input()</tt> is of the type <tt>const char *</tt> and not just <tt>char *</tt>. 
</div>
<div class="Indented">
Use of <tt>const</tt> qualified references is illustrated by the member function <tt>add()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">     void add(const Vector&amp; v){...}
</pre>
</div>

</div>
<div class="Indented">
Here the <tt>const</tt> qualifier indicates that <tt>add()</tt> will read entries of <tt>v</tt> but not change them. The target will be changed when <tt>v</tt> is added to it. The usage of the <tt>const</tt> qualifier in the member function <tt>norm()</tt> is different.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">    double norm()const{...}
</pre>
</div>

</div>
<div class="Indented">
This function returns the <span class="formula">∞</span>-norm of its target vector (largest magnitude of an entry of the vector). Therefore, if it is called  <tt>v.norm()</tt>, the returned value is the <span class="formula">∞</span>-norm of <tt>v</tt>. The <tt>const</tt> qualifier specifies that the member function will not change its target. Any attempt to change the entries of <tt>v</tt> inside the definition of <tt>norm()</tt> is illegal. 
</div>
<div class="Indented">
The <tt>const</tt> protections can be easily broken using shadows. For example, inside the definition of the member function <tt>add()</tt>, we can say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector w(v);
</pre>
</div>

</div>
<div class="Indented">
and go on to modify the entries of <tt>v</tt>. 
</div>
<h? class="Subsubsection">
<b><u>Default arguments</u></b>
</h?>
<div class="Unindented">
Unlike C, C++ allows default arguments. Suppose the function 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double f(double x, int flag){
     ...
}
</pre>
</div>

</div>
<div class="Indented">
is defined in the source file <tt>xyz.cpp</tt>. In the header file <tt>xyz.hh</tt>, we may declare it as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double f(double x, int flag = 0);
</pre>
</div>

</div>
<div class="Indented">
Then we are allowed to call the function as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">f(1.4142, 1);
</pre>
</div>

</div>
<div class="Indented">
or
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">f(1.4144);
</pre>
</div>

</div>
<div class="Indented">
In the latter case, the compiler supplies the second argument (<tt>flag</tt>) as <tt>0</tt>. Only trailing arguments can be assigned default values. 
</div>
<h? class="Subsubsection">
<b><u>The <tt></tt></u></b>-&gt;<span class="default"> operator</span>
</h?>
<div class="Unindented">
Suppose we define an array of pointers to <tt>Vector</tt> objects as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Vector *v[100];
</pre>
</div>

</div>
<div class="Indented">
Each entry may be made to point to a <tt>Vector</tt> object as explained already. Suppose all those <tt>Vector</tt> objects are initialized in some way and we want to print the norm of each <tt>Vector</tt> object. The following syntax will not do:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">for(int i=0; i &lt; 100; i++)
     cout&lt;&lt;v[i].norm()&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
The problem here is that <tt>v[i]</tt> is a pointer to a <tt>Vector</tt> and not a <tt>Vector</tt>. The correct syntax is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">for(int i=0; i &lt; 100; i++)
     cout&lt;&lt;(*v[i]).norm()&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
The parentheses in <tt>(*v[i]).norm()</tt> are needed. The <tt>*</tt> dereferencing operator has lower precedence than the <tt>.</tt> selection operator. This usage occurs often enough that there is a special operator <tt>-&gt;</tt> that combines dereferencing and selection. It may be used as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">for(int i=0; i &lt; 100; i++)
     cout&lt;&lt;v[i]-&gt;norm()&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
We will look at another example of the element selection through pointer operator <tt>-&gt;</tt>. Consider the C <tt>struct</tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">struct node{
     double val;
     struct node *next;
};
</pre>
</div>

</div>
<div class="Indented">
A <tt>struct</tt> is a package of data items. Here the two data items of <tt>struct node</tt> are <tt>val</tt> and a pointer to the next node. The pointer <tt>next</tt> is a link from one node to the next. A series of nodes may be chained together to form a linked list. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:chapter1-llist"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/llist.png" alt="figure FIGS/chapter1/llist.png" style="max-width: 319px; max-height: 82px;"/>

</div>
<div class="caption">
Figure 1.6 Schematic view of a linked list. 
</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:chapter1-llist">1.6↑</a> shows a linked list with each node pointing to the next. The variable <tt>llist</tt> points to the beginning of the list. The <tt>next</tt> pointer of the last node is set to <tt>NULL</tt>. 
</div>
<div class="Indented">
The following function finds the length of a linked list:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int llen(struct node *llist){
	int len = 0;
	while(llist != NULL){
		llist = llist-&gt;next;
		len++;
	}
	return len;
}
</pre>
</div>

</div>
<div class="Indented">
The element selection through pointer operator <tt>-&gt;</tt> occurs frequently in the context of linked lists, trees, and graphs.
</div>
<h? class="Subsubsection">
<b><u>Abstraction features of C++</u></b>
</h?>
<div class="Unindented">
The C++ language has a number of powerful features to bring programming closer to ideas and concepts intrinsic to the problem domain. Classes that arise later are all narrowly defined, provide a single service, and do not use these features. The <tt>Vector</tt> class defined above attempts to capture the abstract notion of vectors. C++ has a number of abstraction features beyond the basic class mechanism we have studied.
</div>
<div class="Indented">
C++ supports object-oriented programming through inheritance of classes, multiple inheritance, and virtual and pure virtual functions. On many occasions, a single problem morphs into multiple instances, each of which has its own distinctive features while sharing a great deal in common with other instances. Object-oriented programming is the best way to tackle such problems. 
</div>
<div class="Indented">
An example of a problem well suited to object-oriented programming is the implementation of file systems. A file system creates the concept of file, which may be read, written, opened, and closed in standard ways. File systems vary in the way they cache data, store meta information, and other respects. Yet they have a great deal in common. In addition, each file system is ultimately realized on disparate storage devices.
</div>
<div class="Indented">
Mapping of file systems to devices too can be organized using object-oriented techniques. Another example is the implementation of a graphics library for plotting that supports a variety of backends.
</div>
<div class="Indented">
Object-oriented programming can be done in plain C as well using function pointers. However, it becomes far more laborious. Object-oriented techniques do not really come up in this book, although we do use templates on one occasion in chapter <a class="Reference" href="#chap:Graphics-co-processor-programmin">8↓</a>. Therefore, we include the following basic example of a templated function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>template&lt;class Num&gt; Num sum(Num *arr, int len){
<span class="number-left">2</span>	Num ans;
<span class="number-left">3</span>	ans = 0;
<span class="number-left">4</span>	for(int i=0; i &lt; len; i++)
<span class="number-left">5</span>		ans += arr[i];
<span class="number-left">6</span>	return ans;
<span class="number-left">7</span>}
</pre>
</div>

</div>
<div class="Indented">
In this function, <tt>Num</tt> is a generic class or type. It can stand for <tt>int</tt> or <tt>double</tt> or some user-defined class that overloads the operator <tt>+=</tt> (line 5) and therefore supports addition. If the template is instantiated by setting <tt>Num</tt> to a user-defined class, it must also support the empty constructor (line 2) as well as assignment to zero (line 3). 
</div>
<div class="Indented">
A templated function may be invoked as shown below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	int x[5] = {1, 2, 3, 4, 5};
<span class="number-left">2</span>	cout&lt;&lt;"1 + ... + 5 = "&lt;&lt;sum(x, 5)&lt;&lt;endl;
<span class="number-left">3</span>	double y[5] = {6, 7, 8, 9, 10};
<span class="number-left">4</span>	cout&lt;&lt;"6 + ... + 10 = "&lt;&lt;sum(y, 5)&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
A templated function is called just like an ordinary function as evident from the invocations of <tt>sum()</tt> on lines 2 and 4. Because <tt>x[]</tt> is an array of <tt>int</tt>s, the C++ compiler replaces the generic type <tt>Num</tt> by <tt>int</tt> to generate code for a new function. On line 2, <tt>sum(x,5)</tt> invokes that function. However, <tt>sum(y,5)</tt> on line 4 invokes a quite different function generated by replacing <tt>Num</tt> by <tt>double</tt>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-1.3.2">1.3.2</a> Aitken transformation in C++
</h3>
<div class="Unindented">
The source files <tt>Aitken.cpp</tt>, <tt>Leibniz.cpp</tt>, and <tt>Logseries.cpp</tt> contain the C++ implementation of the Aitken transformation and its application to the Leibniz and log series. The listing below is of the C++ header file <tt>Aitken.hh</tt>. <a class="Label" name="Aitken.hh"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#ifndef AitkenAugust09DVjli
<span class="number-left">2</span>#define AitkenAugust09DVjli
<span class="number-left">3</span>#include "Vector.hh"
<span class="number-left">4</span>void Aitken(const Vector&amp; seq1, Vector&amp; seq2);
<span class="number-left">5</span>void Aitken(Vector&amp; seq);
<span class="number-left">6</span>double AitkenE(const Vector&amp; seq, int printflag=0);
<span class="number-left">7</span>#endif
</pre>
</div>

</div>
<div class="Indented">
This header file declares three functions, two of which have the same name. The function name <tt>Aitken</tt> is overloaded (lines 4 and 5). We can make the function call <tt>Aitken(v,w)</tt>, where <tt>v</tt> and <tt>w</tt> are both <tt>Vectors</tt>. After the call, the <tt>const</tt> in the first declaration guarantees that <tt>v</tt> is not changed. The use of <tt>const</tt> simplifies the documentation of this function. The other way to make the function call takes the form <tt>Aitken(v)</tt>. In this later usage, the transformed sequence overwrites the original data, and the last two entries are set to <span class="formula">0</span>. We can get away with using the same name for two different functions because the compiler can tell them apart by looking at the list of arguments. 
</div>
<div class="Indented">
Although there is just one declaration for <tt>AitkenE()</tt> (line 6), it can be called with one or two arguments. If we call it in the form <tt>AitkenE(v)</tt>, the compiler interprets the call as <tt>Aitken(v, 0)</tt>. The declaration of <tt>AitkenE()</tt> (line 13) gives its second parameter as <tt>int printflag=0</tt>, thus indicating to the compiler that the second argument assumes the default value of <span class="formula">0</span> if it is omitted. We can also call it in the form<tt> AitkenE(v,1)</tt>, where the value of the second argument is given explicitly as <span class="formula">1</span>. Usually it is best to use the default argument facility only for the last argument.
</div>
<div class="Indented">
The function <tt>AitkenE()</tt> transforms the sequence repeatedly until the sequence has only one or two numbers. The function <tt>Aitken()</tt> listed below performs a single transformation of the sequence. <a class="Label" name="Aitken-C++"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void Aitken(Vector&amp; seq1){
<span class="number-left">2</span>	int len = seq1.getSize();
<span class="number-left">3</span>	for(int i=0; i &lt; len-2; i++){
<span class="number-left">4</span>		double a = seq1(i);
<span class="number-left">5</span>		double b = seq1(i+1);
<span class="number-left">6</span>		double c = seq1(i+2);
<span class="number-left">7</span>		seq1(i) = a - (b-a)*(b-a)/(a-2*b+c);
<span class="number-left">8</span>	}
<span class="number-left">9</span>	seq1(len-2) = 0;//invalid entries
<span class="number-left">10</span>	seq1(len-1) = 0;
<span class="number-left">11</span>}
</pre>
</div>

</div>
<div class="Indented">
Entries of the <tt>Vector</tt> object <tt>seq1</tt> are accessed on lines 4, 5, 6, 9, and 10. Each one of these accesses is resolved using the overloaded function call operator and is not a simple array look-up. 
</div>
<div class="Indented">
The function <tt>AitkenE()</tt> listed below is defined in the source file <tt>Aitken.cpp</tt>. It uses the shadowing capability of the <tt>Vector</tt> class. It repeatedly transforms the sequence, and each transformation is effected using a call to <tt>Aitken()</tt> on line 13.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double AitkenE(const Vector&amp; seq, int printflag){
<span class="number-left">2</span>	int len = seq.getSize();
<span class="number-left">3</span>	Vector myseq(len);
<span class="number-left">4</span>	myseq = seq;
<span class="number-left">5</span>	Vector v;
<span class="number-left">6</span>	v.shadow(seq);
<span class="number-left">7</span>	int n = len/2;
<span class="number-left">8</span>	if(len%2==0)
<span class="number-left">9</span>		n--;
<span class="number-left">10</span>	if(printflag==1)
<span class="number-left">11</span>		printseq(v);//defined in Aitken.cpp
<span class="number-left">12</span>	for(int i=0; i &lt; n; i++){
<span class="number-left">13</span>		Aitken(v);//defined in Aitken.cpp
<span class="number-left">14</span>		v.shadow(v, 0, v.getSize()-2);
<span class="number-left">15</span>		if(printflag==1)
<span class="number-left">16</span>			printseq(v);
<span class="number-left">17</span>	}
<span class="number-left">18</span>	int indx = v.getSize()-1; //can be 0 or 1
<span class="number-left">19</span>	return v(indx);
<span class="number-left">20</span>}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>AitkenE()</tt> uses shadows instead of creating new <tt>Vector</tt>s of different lengths repeatedly. After each invocation of <tt>Aitken(v)</tt> on line 13, <tt>v</tt> is made a shadow of itself, with the last two entries dropped, on line 14. 
</div>
<div class="Indented">
Object files are produced from <tt>Aitken.cpp</tt>, <tt>Leibniz.cpp</tt>, and <tt>Logseries.cpp</tt> using the <tt>icpc</tt> compiler with the <tt>-c</tt> option. The linking command builds the executable.<a class="Label" name="linking-cpp"> </a> 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc -c Aitken.o
icpc -c Leibniz.o
icpc -o Leibniz.exe Leibniz.o Aitken.o
</pre>
</div>

</div>
<div class="Indented">
The executable  <tt>Logseries.exe</tt> is built similarly. Instead of <tt>icpc</tt>, we may use <tt>g++</tt> or <tt>pgCC</tt>, and the compilation and linking syntax shown here does not change.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the <tt>sizeof()</tt> facility to determine the size of a <tt>Vector</tt> object. Does the reported size agree with your expectation?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Copy the file Vector.hh to VectorEX.hh and modify it as follows. Insert a line right at the beginning of the first three constructors that makes them print ‘‘empty constructor” or ‘‘constructor to acquire space’’ or ‘‘copy constructor’’ in a single line. Similarly, add a line right at the top of the destructor that prints ‘‘destructor.’’ When the following code is run <div class="listing">
<pre class="listing">#include "VectorEX.hh"
void donthg(Vector v1, Vector&amp; v2){
}
int main(){
	Vector u(20), v(10), w(v), ww;   
	donthg(v, w);   
	donthg(u, v); 
}
</pre>
</div>
it produces exactly 12 lines of output. Explain each line carefully. Exactly which object and place in the code does each line correspond to? 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why the default copy constructor, which copies <tt>size</tt>, <tt>data</tt>, and <tt>owner</tt> fields, is semantically incorrect for the <tt>Vector</tt> class.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Consider the following C++ program.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  <div class="listing">
<pre class="listing">#include &lt;iostream&gt;
using namespace std;
​
int main(){
  cout&lt;&lt;"Hello World!"&lt;&lt;endl;
}
</pre>
</div>
It prints &ldquo;Hello World!&rdquo; as you may expect. Modify the program so that it prints<div class="listing">
<pre class="listing">Big Bang
Hello World!
Apocalypse
</pre>
</div>
You are not allowed to modify <tt>main()</tt>.<span class="FootOuter"><span class="SupFootMarker"> [17] </span><span class="HoverFoot"><span class="SupFootMarker"> [17] </span>This problem is based on <span class="bibcites">[<a class="bibliocite" name="cite-2" href="#biblio-2"><span class="bib-index">2</span></a>]</span>.</span></span>
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Consider the <span class="formula">100</span> numbers <span class="formula"><span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root">1</span><span class="ignored">)</span></span>, <span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root">2</span><span class="ignored">)</span></span>, …, <span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root">100</span><span class="ignored">)</span></span></span>. Write a C/C++ program that partitions the numbers into two sets such that the difference of the sums of the two sets is as small in magnitude as possible. Does your program work for <span class="formula">1000</span> numbers or for <span class="formula">10<sup>6</sup></span> numbers?<span class="FootOuter"><span class="SupFootMarker"> [18] </span><span class="HoverFoot"><span class="SupFootMarker"> [18] </span>This is the Floyd partitioning problem. See <span class="bibcites">[<a class="bibliocite" name="cite-6" href="#biblio-6"><span class="bib-index">6</span></a>]</span>. The Karmarkar-Karp algorithm for partitioning gives an interesting approach to this problem.</span></span> 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a C/C++ program that will open a file and print the last <span class="formula"><i>n</i></span> lines of the file. The name of the file and <span class="formula"><i>n</i></span> are inputs to the program.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Let <span class="formula"><i>p</i>(<i>z</i>)</span> be a polynomial of <span class="formula"><i>n</i></span>th degree with complex coefficients. The fundamental theorem of algebra states that <span class="formula"><i>p</i>(<i>z</i>) = 0</span> has <span class="formula"><i>n</i></span> complex roots. The Newton iteration for finding the roots is <span class="formula"><i>z</i><sub><i>n</i> + 1</sub> = <i>z</i><sub><i>n</i></sub> − <i>p</i>(<i>z</i><sub><i>n</i></sub>) ⁄ <i>p</i>’(<i>z</i><sub><i>n</i></sub>)</span>. The iteration converges to different roots depending on the initial iterate <span class="formula"><i>z</i><sub>0</sub></span>. Write a C/C++ program that takes a polynomial <span class="formula"><i>p</i>(<i>z</i>)</span> and <span class="formula"><i>z</i><sub>0</sub></span> as inputs and determines which root the Newton iteration converges to. Color the complex plane depending on which root the Newton iteration converges to.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In sexagesimal, a non-negative integer is written in the form <span class="formula"><span class="limits"><span class="limit">∑</span></span><span class="scripts"><sup class="script"><i>n</i> − 1</sup><sub class="script"><i>i</i> = 0</sub></span><i>a</i><sub><i>i</i></sub> × 60<sup><i>i</i></sup></span>, where <div class="formula">
<i>a</i><sub><i>i</i></sub> ∈ <span class="bigsymbol">{</span>0, 1, …, 59<span class="bigsymbol">}</span>.
</div>
There are exactly <span class="formula">144</span> sexagesimal numbers with leading digit <span class="formula"><i>a</i><sub><i>n</i> − 1</sub> = 1</span>, <span class="formula"><i>n</i> ≤ 6</span>, and a terminating sexagesimal expansion for their reciprocals. Determine all <span class="formula">144</span> such numbers and their sexagesimal reciprocals. For <span class="formula"><i>n</i> = 6, 7, …, 20</span>, determine the number of sexagesimal numbers with <span class="formula"><i>a</i><sub><i>n</i> − 1</sub> ≠ 0</span> and a terminating reciprocal.<span class="FootOuter"><span class="SupFootMarker"> [19] </span><span class="HoverFoot"><span class="SupFootMarker"> [19] </span>See chapter 11 of <span class="bibcites">[<a class="bibliocite" name="cite-6" href="#biblio-6"><span class="bib-index">6</span></a>]</span>; originally published as Ancient Babylonian Algorithms in Communications of the ACM, vol. 15 (1972), with errata in vol. 19 (1976). Inakibit-Anu, priest of Anu and Antub in Uruk around 300 BC, determined 105 of the 144 sexagesimal numbers with terminating reciprocals. Inakibit-Anu’s table gave the reciprocal of <span class="formula">60<sup>2</sup> + 45</span> as <span class="formula">59 × 60<sup> − 3</sup> + 15 × 60<sup> − 4</sup> + 33 × 60<sup> − 5</sup> + 20 × 60<sup> − 6</sup></span>.</span></span> 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  C <tt>struct</tt>s are like C++ classes with public data members but no function members. In the following <tt>struct</tt> <div class="listing">
<pre class="listing">struct node{
  double x;
  struct node *next;
}
</pre>
</div>
each node points to the next to form a linked list (see figure <a class="Reference" href="#fig:chapter1-llist">1.6↑</a>). The <tt>next</tt> field of the last node is assumed to be <tt>NULL</tt>. Write a function that takes a pointer to the first node in a linked list and reverses the linked list.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The definition of <tt>Aitken()</tt> given here overwrites the input sequence. Give an implementation that does not overwrite the input sequence and that corresponds to the declaration on line 6 of <tt>Aitken.hh</tt> listed on page <a class="Reference" href="#Aitken.hh">1↑</a>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Time C++ (page <a class="Reference" href="#Aitken-C++">1↑</a>) and C (page <a class="Reference" href="#aitken-in-C">1↑</a>) implementations of the Aitken transformation and compare.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-1.4">1.4</a> A little Fortran<a class="Label" name="sec:review-Fortran"> </a>
</h2>
<div class="Unindented">
In this section, we show a bit of Fortran syntax. The syntax is deliberately Fortran 77 and not the newer varieties. When the need to use old Fortran codes arises, it is often Fortran of this variety. We do not recommend programming in Fortran.<span class="FootOuter"><span class="SupFootMarker"> [20] </span><span class="HoverFoot"><span class="SupFootMarker"> [20] </span><span class="bibcites">[<a class="bibliocite" name="cite-5" href="#biblio-5"><span class="bib-index">5</span></a>]</span> has noted that the inadequacy of Fortran for systems programming became apparent as soon as Ken Thompson attempted to write Unix in Fortran in 1969. Fortran was an amazing advance in compiler theory and technology for its time but  has been forgotten in the world of systems programming for more than four decades. </span></span> The language is rigid and does not allow for dynamic data structures such as linked lists, trees, and graphs in its 77 version. Such data structures are increasingly used in scientific computing and are indispensable to computer science. 
</div>
<div class="Indented">
The core data structure in Fortran is the array. In our opinion, Fortran does not do a good job here. In Fortran, the array is thought of as a variable name, the length of the sequence, and the type of each item, which determines the size of each item in bytes. The variable name is actually a pointer (an address), but only covertly and not explicitly as in C. Fortran does not allow pointers in any generality. The Fortran array is an abstraction that strives to be close to what happens on the machine. However, it is an awkward abstraction because the notion of pointers is not thrown away but adopted covertly in a highly restricted form. 
</div>
<div class="Indented">
Unlike C/C++, the Fortran language does not provide access to machine capabilities. For some of the more sophisticated optimizations, the Fortran language is inadequate.
</div>
<div class="Indented">
There is a belief that Fortran is faster than C/C++. This belief is a complete myth, being no more than an indication of the proficiency of those who believe in it, and will be completely dispelled later.
</div>
<div class="Indented">
Part of the Fortran code for applying the Aitken iteration to the logarithmic series follow. The listing of the function <tt>partialsum(x,n)</tt> follows.<span class="FootOuter"><span class="SupFootMarker"> [21] </span><span class="HoverFoot"><span class="SupFootMarker"> [21] </span>Seasoned Fortran programmers would use <tt>implicit none</tt><span class="default"> in the function definitions to preclude the compiler from assuming undeclared function parameters to be of type <tt>integer</tt>.</span></span></span> It returns the partial sum of the first <span class="formula"><i>n</i></span> terms of the Taylor series of <span class="formula">log(1 + <i>x</i>)</span>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">      double precision function partialSum(x, n)
      double precision x
      integer n
      double precision prod
      integer i
      partialSum = 0
      prod = 1.0D0
      do 10 i=1,n,1
         prod = prod*x
         if(mod(i,2).eq.0) then
            partialSum = partialSum - prod/i
         else
            partialSum = partialSum + prod/i
         endif
 10   continue
      return
      end
</pre>
</div>

</div>
<div class="Indented">
The <tt>main </tt>program listed below prints the <span class="formula"><i>n</i></span>th partial sum, the extrapolated value using the first <span class="formula"><i>n</i></span> partial sums, and the true value of <span class="formula">log(1 + <i>x</i>)</span> for a few values of <span class="formula"><i>x</i></span>. It corresponds to the data shown in table <a class="Reference" href="#table:LogSeries">1.2↑</a>. It calls the function <tt>extrapolateSum()</tt> in addition to <tt>partialSum()</tt>, but the definition of the former is not listed.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">      program main
      double precision xlist(11), seq1(13), seq2(11)
      double precision val1, val2, val3
      double precision partialSum, extrapolateSum
      integer n, i
      xlist(11) = 1.25D0
      do 30 i=0,9,1
         xlist(i+1) = i*1.0D0/9
 30   continue
      
      n = 13
      do 40 i=1,11,1
         val1 = partialSum(xlist(i),n)
         val2 = extrapolateSum(xlist(i),n, seq1, seq2)
         val3 = log(1+xlist(i))
         write (6, 100) val1, val2, val3
 100     format(F14.10, F14.10, F14.10)
 40   continue
      stop
      end
</pre>
</div>

</div>
<div class="Indented">
There are huge differences between C and Fortran in the way function calls work. Let us look at the first function call that occurs here:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">val1 = partialSum(xlist(i),n)
</pre>
</div>

</div>
<div class="Indented">
Here <tt>xlist</tt> is a name for an array of <tt>double</tt>s and <tt>xlist(i)</tt> is a name for the <tt>i</tt>th location in that array. Similarly, <tt>n</tt> and <tt>val1</tt> are names for locations that are big enough to hold an <tt>int</tt> and a <tt>double</tt>, respectively. 
</div>
<div class="Indented">
So far, nothing is really different from C. The difference is in the way the arguments of <tt>partialSum()</tt> are set up. The function <tt>partialSum()</tt> has two arguments that are called <tt>x</tt> and <tt>n</tt>. If we were in the C world, new locations would be created, and <tt>x</tt> and <tt>n</tt> would become names for those two new locations. In Fortran, nothing of that sort happens. Instead, <tt>x</tt> and <tt>n</tt> merely become names for the locations named by <tt>xlist(i)</tt> and <tt>n</tt> in the main program. So if we change <tt>x</tt> inside <tt>partialSum()</tt>, that will change <tt>xlist(i)</tt> inside the main program. This awkward semantics is a result of using pointers implicitly to represent arrays efficiently but not allowing pointers into the language.<span class="FootOuter"><span class="SupFootMarker"> [22] </span><span class="HoverFoot"><span class="SupFootMarker"> [22] </span>Languages such as Python and Java also use pointers implicitly. However, both languages have a more abstract as well as more consistent view of objects, and the awkwardness associated with Fortran does not arise. In Python, every variable is really a pointer to an object, and not to a memory location as in Fortran. So if we say <span class="formula"><i>x</i> = 7</span>, the variable <span class="formula"><i>x</i></span> points to the object <span class="formula">7</span>. If <span class="formula"><i>x</i></span> is passed as an argument to a function <span class="formula"><i>f</i>(<i>y</i>)</span>, <span class="formula"><i>y</i></span> begins to point to the object <span class="formula">7</span>. The crucial difference from Fortran is as follows: suppose we say <span class="formula"><i>y</i> = 5</span> in the body of the function then <span class="formula"><i>y</i></span> begins to point to the object <span class="formula">5</span>, and at the point of call, <span class="formula"><i>x</i></span> <i>continues</i> to point to the object <span class="formula">7</span>. Analogously to Fortran, if <span class="formula"><i>x</i></span> is a pointer to a large object such as a Numpy array, one may change its entries using syntax such as <span class="formula"><i>y</i>[0] =  − 1</span>.</span></span> To reconcile these opposing tendencies, Fortran passes even <tt>int</tt> and <tt>double</tt> arguments by reference.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-1.5">1.5</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-1"><span class="bib-index">1</span></a>] </span> <span class="bib-authors">B. Kernighan, D. Ritchie</span>: <i><span class="bib-title">The C Programming Language</span></i>. <span class="bib-publisher">Prentice-Hall</span>, <span class="bib-year">1988</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-2"><span class="bib-index">2</span></a>] </span> <span class="bib-authors">B. Stroustrup</span>: <i><span class="bib-title">The C++ Programming Language</span></i>. <span class="bib-publisher">Addison-Wesley</span>, <span class="bib-year">1997</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-3"><span class="bib-index">3</span></a>] </span> <span class="bib-authors">C. Brezinski, M. Redivo Zaglia</span>: <i><span class="bib-title">Extrapolation Methods: Theory and Practice</span></i>. <span class="bib-publisher">North Holland</span>, <span class="bib-year">1991</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-4"><span class="bib-index">4</span></a>] </span> <span class="bib-authors">D. Kozen</span>: <i><span class="bib-title">Automata and Computability</span></i>. <span class="bib-publisher">Springer-Verlag</span>, <span class="bib-year">1997</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-5"><span class="bib-index">5</span></a>] </span> <span class="bib-authors">D. Ritchie</span>: <i><span class="bib-title">The development of the C language</span></i> in <i><span class="bib-booktitle">History of Programming Languages II</span></i> (<span class="bib-editor">T.J. Bergin and R.G. Gibson</span>, ed.). <span class="bib-publisher">Addison-Wesley</span>, <span class="bib-year">1996</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-6"><span class="bib-index">6</span></a>] </span> <span class="bib-authors">D.E. Knuth</span>: <i><span class="bib-title">Selected Papers on Computer Science</span></i>. <span class="bib-publisher">Cambridge University Press</span>, <span class="bib-year">1996</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-7"><span class="bib-index">7</span></a>] </span> <span class="bib-authors">D.E. Knuth</span>: <i><span class="bib-title">The Art of Computer Programming</span></i>. <span class="bib-publisher">Addison-Wesley</span>, <span class="bib-year">1998</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-8"><span class="bib-index">8</span></a>] </span> <span class="bib-authors">G.A. Baker, P. Graves-Morris</span>: <i><span class="bib-title">Padé Approximants</span></i>. <span class="bib-publisher">Cambridge University Press</span>, <span class="bib-year">1996</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-9"><span class="bib-index">9</span></a>] </span> <span class="bib-authors">J.M. Borwein, P.B. Borwein</span>: <i><span class="bib-title">Pi and the AGM: A Study in Analytic Number Theory and Computational Complexity</span></i>. <span class="bib-publisher">Wiley-Interscience</span>, <span class="bib-year">1998</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-10"><span class="bib-index">10</span></a>] </span> <span class="bib-authors">K.V. Sarma</span>: <i><span class="bib-title">A History of the Kerala School of Hindu Astronomy</span></i>. <span class="bib-publisher">Vishveshvaranand Institute</span>, <span class="bib-year">1992</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-11"><span class="bib-index">11</span></a>] </span> <span class="bib-authors">Morris Kline</span>: <i><span class="bib-title">Mathematical Thought from Ancient to Modern Times</span></i>. <span class="bib-publisher">Oxford University Press</span>, <span class="bib-year">1990</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-12"><span class="bib-index">12</span></a>] </span> <span class="bib-authors">R.S. Westfall</span>: <i><span class="bib-title">Never at Rest: A Biography of Isaac Newton</span></i>. <span class="bib-publisher">Cambridge University Press</span>, <span class="bib-year">1980</span>.
</p>

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-2">2</a> C/C++: Libraries and Makefiles<a class="Label" name="chap:C/C++:-Libraries-and"> </a>
</h1>
<div class="Unindented">
Splitting a program into several source and header files, as in the previous chapter, is essential but not sufficiently powerful in itself to capture the conceptual relationships of many programs. When the interdependence between the modules is complex, it is no longer adequate to put all the source files in a single directory. The source files must be organized into directories and subdirectories to bring greater order and clarity. 
</div>
<div class="Indented">
There are two powerful ideas for bringing greater modularity into C/C++ programs, and both of them will be introduced in this chapter. The first idea is to combine object files into libraries, and the second idea is to organize program sources into a source tree.
</div>
<div class="Indented">
In outline, a C/C++ program is built as follows. There are program sources to begin with. These are turned into object files, which mainly consist of machine instructions, by the compiler. The linker eliminates unresolved external references and merges the object files to produce an executable. The two ideas for bringing greater modularity occur at different points in this process.
</div>
<div class="Indented">
The organization of sources into a tree precedes both compilation and linking. The solution to most problems naturally breaks up into several components. For example, an image-processing program may be broken up into modules for handling different image formats, modules for displaying images, modules for image transformations, modules for image enhancements, modules for combining images, and so on. If the sources for each of these functions is put in separate directories, the directories become modules, and the sources are now submodules within these directories. Although an overly deep hierarchy can cause complications and must be used only for truly complex programs, one can easily imagine directories within directories so that the source files are grouped into modules, and these modules are grouped into higher level modules, and so on in a tree-like hierarchy.
</div>
<div class="Indented">
In contrast, libraries follow compilation and precede linking. The linking model is always flat. It does not matter how or if the sources are arranged in a tree. The linker takes in a flat list of object files and smashes them together to form an executable, regardless of where the object files or their sources are located. A library is simply a group of object files that may be fed to a linker as a single unit.
</div>
<div class="Indented">
It is easy to see why libraries are so useful. An image-processing program may use linear algebra, Fourier analysis, and yet other tools. Although linear algebra and Fourier analysis are used, those functions are extraneous to image processing, and it is not right to include them within an image-processing source tree. Programs for optimization, solving differential equations, clustering, and other tasks have an equal right to use linear algebra and Fourier analysis. Thus, the most natural thing to do here is to combine all the linear algebra object files into a linear algebra library and all the Fourier analysis object files into a Fourier analysis library. Commercial vendors may sell libraries of object files without the sources to safeguard their profitability.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:libmake-mixedlang">2.1↓</a> exhibits an important feature of the translation of C/C++/Fortran sources to object files. In the previous chapter, we have looked at C/C++/Fortran sources. Several names appear within these sources. There are names of variables used to hold data and then there are names of functions. If a function or a variable defined in one source may be used by another, the name of that function or variable is retained in the object file, although in a transmuted form. An object file uses the same machine language (we will look at machine instructions only in the next chapter) and has nearly the same format, syntax, and semantics regardless of whether it is from a C, C++, or Fortran source. However, the transmutation of names from source to object file is different in C, C++, and Fortran. Once the convention for transmuting names is understood, it is easy to call Fortran programs from C/C++. The convention is the simplest and the most natural in C. In addition, C uses a relatively simple runtime library. It is typical to have to use C in the middle if one wants interoperability between other languages such as Java or Python.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:libmake-blas-lapack">2.2↓</a> is a brief introduction to the BLAS/LAPACK linear algebra libraries. There is no area of mathematics that is as ubiquitous in applications as linear algebra, and there are no scientific libraries as widely used as BLAS/LAPACK. Every scientific programmer needs some familiarity with these libraries. Optimizing or even implementing the BLAS/LAPACK libraries would be an impossible task for almost any programmer or team. There is much to be gained from using good libraries whenever they are available. Like their bricks and mortar counterparts, libraries are a powerful means to propagate knowledge.
</div>
<div class="Indented">
When a program is split into several source files and the source files are grouped into modules, compiling source files into object files and then generating executables become tedious, repetitive, and error-prone. Makefiles and the <tt>make</tt> utility bring organization and coherence to compilation and linking. Section <a class="Reference" href="#sec:libmake-gnumake">2.3↓</a> gives an account of GNU <tt>make</tt>. The importance of understanding <tt>make</tt> cannot be overstated. There is no such thing as modular programming in C/C++ without it. Although there are other ways to manage sources, the <tt>make</tt> utility is the oldest and most widely used.
</div>
<div class="Indented">
The final section of this chapter, section <a class="Reference" href="#sec:libmake-fft">2.4↓</a>, makes a transition to the rest of the book. In addition, it demonstrates the importance of well-optimized libraries. An implementation of the Fast Fourier Transform (FFT), one of the most important algorithms in science, which is coded expertly but without regard to computer architecture, is compared with libraries that are cognizant of computer architecture. The optimized libraries are found to be nearly an order of magnitude faster. Section <a class="Reference" href="#sec:libmake-fft">2.4↓</a> also shows how unpredictable compilers can be. A slight change degrades performance of one of the optimized libraries by nearly a factor of two.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-2.1">2.1</a> Mixed-language programming<a class="Label" name="sec:libmake-mixedlang"> </a>
</h2>
<div class="Unindented">
Compilers translate <tt>.c</tt> (C), <tt>.cpp</tt> (C++), and <tt>.f</tt> (Fortran) sources into <tt>.o</tt> object files. The <tt>.o</tt> object files are mainly a sequence of machine instructions. If the source file calls functions defined externally, which is the typical scenario, there will be unresolved names in the corresponding object file. In section <a class="Reference" href="#sub:libmake-mixed-transmutation">2.1.1↓</a>, we look at the map from sources to object files as a precursor to mixed-language programming.
</div>
<div class="Indented">
The manner in which for-loops and other constructs map to machine instructions is the topic of the next chapter. In this section, we only look at the map from globally visible names in the sources to names in the object file. The <tt>aitken.c</tt> source looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;assert.h&gt; 
#include "aitken.h" 
void aitken(const double* seq1, double* seq2, 
		int len){
...
}
​
double aitkenExtrapolate(double *seq1, double* seq2, 
			int len){
...
}
</pre>
</div>

</div>
<div class="Indented">
In this source, only the function names <tt>aitken</tt> and <tt>aitkenExtrapolate</tt> are globally visible. Names of arguments are local to the function body. Other variables defined inside the function body have only local scope as well. Names with local scope are not mapped to object files.
</div>
<div class="Indented">
The CPP and Fortran sources for the Aitken iteration define functions with similar names. The main point of section <a class="Reference" href="#sub:libmake-mixed-transmutation">2.1.1↓</a> is that the same name maps to a different name in the object file depending on whether the source is C, C++, or Fortran. Once this point is understood, making C, C++, and Fortran work together becomes much simpler. The mapping is the most straightforward in C, where names are unchanged from sources to object files. In Fortran, the map is a little more complicated. In C++, the mapping is a great deal more complicated.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:libmake-mixed-FCCPP">2.1.2↓</a>, we explain how to call Fortran from C or C++. Beyond the transmutation of names, which differs between the three languages, the additional issue of runtime libraries has to be dealt with. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.1.1">2.1.1</a> Transmutation of names from source to object files<a class="Label" name="sub:libmake-mixed-transmutation"> </a>
</h3>
<div class="Indented">
A <tt>.o</tt> object file is mostly a collection of machine instructions that translate the corresponding source into machine-intelligible language. If the source has a statement such as <tt>a=b+c</tt>, for example, the names <tt>a</tt>, <tt>b</tt>, <tt>c</tt> typically disappear from the object file. The compiler decides the memory locations or registers that these variable names map to. What is found in the object file is simply an add instruction of some type with operands being either memory locations or registers.
</div>
<div class="Indented">
Not all names present in the source disappear, however. Those names present in the source that survive in the object file are some of the most important. These are, typically, names of functions that may be called from other object files or names of functions defined in external object files that are called from this one. Variable names also may have global scope.
</div>
<div class="Indented">
The names present in the object file are needed to resolve undefined references during linking of object files. The names may not be exactly the same as in the original source. Compilers may alter names before mapping them from source to object files. In C, the names are not altered at all.  In C++, the names must necessarily be altered to support the overloading facility that allows the same name for multiple functions. In Fortran, too, the names are altered, although the only reason here seems to be to maintain compatibility with earlier conventions.
</div>
<div class="Indented">
The GNU/Linux command <tt>nm</tt><span class="FootOuter"><span class="SupFootMarker"> [23] </span><span class="HoverFoot"><span class="SupFootMarker"> [23] </span>The <tt>nm</tt> utility is part of GNU’s binutils package.</span></span> lists the names present in an object file. To examine the object file <tt>aitken.o</tt>, we use the command <tt>nm aitken.o</tt>. A part of the output of that command follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>                 U _intel_fast_memcpy
<span class="number-left">2</span>0000000000000000 T aitken
<span class="number-left">3</span>0000000000000070 T aitkenExtrapolate
</pre>
</div>

</div>
<div class="Indented">
The second and third lines verify that names are unchanged when a C source is transformed to an object file. This object file was produced using Intel’s <tt>icc</tt>. The first line refers to a function call inserted by the compiler that was not present in our source. That name is undefined, but the linker will supply the appropriate definition. Any compiler may insert function names during optimization.
</div>
<div class="Indented">
The function names are preceded by an address that is <span class="formula">16</span> hexadecimal digits long and by the letter <tt>T</tt>. The letter is <tt>T</tt> because both functions reside in the text area of the object code. The letter would be <tt>C</tt> for the name of a Fortran common block or a C global variable defined outside the scope of any function. The hexadecimal addresses indicate that code for <tt>aitken()</tt> begins at <span class="formula">0</span> and for <tt>aitkenExtrapolate()</tt> at <span class="formula">70</span> (hexadecimal). These addresses will be shifted by the linker when it merges several object files into a single executable.
</div>
<div class="Indented">
A partial listing of the output of <tt>nm leibniz.o</tt> is included to make one more point about the transmutation of names from C sources to object files. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">                 U __intel_new_proc_init_H
                 U _intel_fast_memcpy
                 U aitken
00000000000002e0 T leibniz
0000000000000000 T main
                 U printf
0000000000000280 T printseq
</pre>
</div>

</div>
<div class="Indented">
This object file two includes a couple of names introduced by <tt>icc</tt>. These appear at the top and are undefined. There are two more undefined names---<tt>aitken</tt> and <tt>printf</tt>. The <tt>leibniz.c</tt> source calls the function <tt>aitken()</tt>, which is externally defined. Within the object file <tt>leibniz.o</tt>, that name is undefined.
</div>
<div class="Indented">
At this point, we have a clear picture of what the linker does. The linking command 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc -o leibniz.exe aitken.o leibniz.o
</pre>
</div>

</div>
<div class="Indented">
concatenates the object files while resolving the undefined symbols. The symbol <tt>aitken,</tt> which is unresolved in <tt>leibniz.o</tt>, is resolved using the definition in the object file <tt>aitken.o</tt>. The linker uses a runtime library to resolve the undefined symbol <tt>printf</tt>. Likewise, the undefined symbols inserted by the compiler are resolved using internal libraries. 
</div>
<div class="Indented">
The C function names survive intact in the object code. The Fortran names change only slightly. The command <tt>nm logseriesf.o</tt> produces the following output (partial listing):
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>0000000000000000 T MAIN__
<span class="number-left">2</span>                 U __intel_new_proc_init_H
<span class="number-left">3</span>                 U aitkenextrapolate_
<span class="number-left">4</span>0000000000000280 T extrapolatesum_
<span class="number-left">5</span>                 U for_set_reentrancy
<span class="number-left">6</span>                 U for_stop_core
<span class="number-left">7</span>                 U for_write_seq_fmt
<span class="number-left">8</span>                 U for_write_seq_fmt_xmit
<span class="number-left">9</span>                 U log
<span class="number-left">10</span>0000000000000230 T partialsum_
</pre>
</div>

</div>
<div class="Indented">
The names <tt>extrapolatesum</tt> and <tt>partialsum</tt> have changed to <tt>extrapolatesum_</tt> (line 8) and <tt>partialsum_</tt> (line 15) in the object files. Fortran names are typically changed by appending an underscore at the end. The name<tt> aitkenextrapolate_</tt> (line 7) is undefined and has to be resolved using another object file. All the other undefined names are introduced by the Intel Fortran compiler and are resolved using standard libraries by the Fortran linker.
</div>
<div class="Indented">
Unlike in C or Fortran, the transmutation of names in C++ is quite extensive. The command <tt>nm Aitken.o</tt> produces output that is much more complicated. We show only part of the output. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">0000000000000090 T _Z6AitkenR6Vector
0000000000000350 T _Z6AitkenRK6VectorRS_
0000000000000110 T _Z7AitkenERK6Vectori
0000000000000000 r _Z7AitkenERK6Vectori$$LSDA
0000000000000030 T _Z8printseqRK6Vector
0000000000000000 W _ZN6VectorD1Ev
                 U _ZNSt8ios_base4InitC1Ev
                 U _ZNSt8ios_base4InitD1Ev
                 U _intel_fast_memcpy
                 U printf
</pre>
</div>

</div>
<div class="Indented">
None of the names in Aitken.cpp can be easily recognized here. The transmutation of names in C++ follows involved rules and is called name mangling. We can issue the command <tt>nm --demangle Aitken.o</tt> to get names in a form that is easily recognizable. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>0000000000000090 T Aitken(Vector&amp;)
<span class="number-left">2</span>0000000000000350 T Aitken(Vector const&amp;, Vector&amp;)
<span class="number-left">3</span>0000000000000110 T AitkenE(Vector const&amp;, int)
<span class="number-left">4</span>0000000000000000 r _Z7AitkenERK6Vectori$$LSDA
<span class="number-left">5</span>0000000000000030 T printseq(Vector const&amp;)
<span class="number-left">6</span>0000000000000000 W Vector::~Vector()
<span class="number-left">7</span>                 U std::ios_base::Init::Init()
<span class="number-left">8</span>                 U std::ios_base::Init::~Init()
<span class="number-left">9</span>                 U _intel_fast_memcpy
<span class="number-left">10</span>                 U printf
</pre>
</div>

</div>
<div class="Indented">
In the demangled listing, we can recognize not only the function names but also the types of the arguments. 
</div>
<div class="Indented">
Because of the overloading mechanism, several different C++ functions can have the same name. The names have to be mangled in the object code to distinguish between different functions with the same name. The C++ standard strongly recommends that each compiler use its own conventions for name mangling. If that suggestion is heeded, object code produced by one C++ compiler cannot be linked with the object code produced by another C++ compiler. However, the Intel C++ compiler uses the same name mangling as the defacto <tt>g++</tt> standard on Linux (and the same as Microsoft <tt>vc++</tt> on Windows). Therefore, in principle at least, C++ object files compiled using <tt>icpc</tt> and <tt>g++</tt> can be linked together. Linking <tt>g++</tt> object files using the <tt>icpc</tt> linker appears reasonably safe but not the other way around.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.1.2">2.1.2</a> Linking Fortran programs with C and C++<a class="Label" name="sub:libmake-mixed-FCCPP"> </a>
</h3>
<div class="Unindented">
In scientific computing, C and C++ functions may need to call Fortran routines. Scientific software from earlier generations tends to be in Fortran 77.
</div>
<div class="Indented">
To use Fortran functions within C or C++ programs, the naming used for the Fortran functions in C or C++ has to be cognizant of the way the names in the source files are altered in the object file. We want the names to agree in the object files because it is the object files and not the source files that get linked against each other. If the naming is right, the linker takes care of resolving the function calls. 
</div>
<div class="Indented">
Let us implement the repeated application of Aitken transformations to partial sums of the Leibniz series by mixing Fortran and C programs. Part of the output of <tt>nm aitkenf.o</tt> is given below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">                 U _intel_fast_memcpy
0000000000000000 T aitken_
0000000000000220 T aitkenextrapolate_
                 U for_write_seq_fmt
                 U for_write_seq_lis
0000000000000150 T printseq_
</pre>
</div>

</div>
<div class="Indented">
We will write a C program that calls the functions defined in <tt>aitkenf.o</tt> to extrapolate the Leibniz series to illustrate the nature of mixed-language programming.
</div>
<div class="Indented">
The C code includes the following declarations near its beginning.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">extern void aitken_(double *seq1, double *seq2, 
		    int *len);
extern void printseq_(double *seq, int *len);
extern double aitkenextrapolate_(double *seq1, 
			      double* seq2, int * len);
</pre>
</div>

</div>
<div class="Indented">
The <tt>extern</tt> keyword indicates that the three function names that are declared must be found in some other object file. The underscore is appended to the names to follow the convention of the Fortran compiler. This convention is common among Fortran compilers but may not be universal. The three arguments to <tt>aitken_</tt> have types <tt>double *</tt>, <tt>double *</tt>, and <tt>int *</tt>. The first few lines of the definition of <tt>aitken()</tt> in the Fortran source are as follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"> subroutine aitken(seq1, seq2, len)
      double precision seq1(*), seq2(*)
      integer len
</pre>
</div>

</div>
<div class="Indented">
The first argument of <tt>aitken()</tt> must be an array of double-precision numbers;  once the function is called, <tt>seq1</tt> becomes another name for that array. The first argument is nothing other than a pointer to a double, although in Fortran we simply think about it as an array not as a pointer. When calling the function from C, we have to drop that pretension and say explicitly that the first argument will be a pointer to <tt>double</tt>. The last argument to <tt>aitken()</tt> is an integer type in Fortran. Once the function is called, <tt>len</tt> becomes another name for that argument. When calling the function from C, we have to be explicit and specifically state that the last argument is of pointer to <tt>int</tt> type (and not of type <tt>int</tt>). Every use of <tt>len</tt> inside the Fortran code of <tt>aitken()</tt> will in effect dereference that pointer.
</div>
<div class="Indented">
The C function below calls the Fortran routines. The <tt>leibniz()</tt> function used to generate partial sums of the Leibniz series (see line 6 below) is in C. Its definition was given earlier (see page <a class="Reference" href="#leibniz-function-in-C">1↑</a>). 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int main(){
<span class="number-left">2</span>	const int len = 13;
<span class="number-left">3</span>	double seq1[len];
<span class="number-left">4</span>	double seq2[len];
<span class="number-left">5</span>	int n, i, j; int farg;
<span class="number-left">6</span>	leibniz(seq1, len);
<span class="number-left">7</span>	n = len/2;
<span class="number-left">8</span>	if(len%2==0)
<span class="number-left">9</span>		n--;
<span class="number-left">10</span>	for(i=0; i &lt; n; i++){
<span class="number-left">11</span>		farg = len-2*i; printseq_(seq1,&amp;farg);
<span class="number-left">12</span>		aitken_(seq1, seq2, &amp;farg);
<span class="number-left">13</span>		for(j=0; j &lt; len-2*(i+1); j++)
<span class="number-left">14</span>			seq1[j] = seq2[j];
<span class="number-left">15</span>  }
<span class="number-left">16</span>	if(len%2==0)
<span class="number-left">17</span>		{farg = 2; printseq_(seq1, &amp;farg);}
<span class="number-left">18</span>	else
<span class="number-left">19</span>		{farg = 1; printseq_(seq1, &amp;farg);}
<span class="number-left">20</span>}
</pre>
</div>

</div>
<div class="Indented">
Line by line, this is almost the same as the C program for extrapolating the Leibniz series using functions defined in C (see page <a class="Reference" href="#main-function-leibniz">1↑</a>). There are only a few differences. Using function names with the underscore appended is the most obvious one. Another difference is that we invoke <tt>printseq_</tt> as <tt>printseq_(seq1, &amp;farg)</tt> after taking care to store <tt>len-2*i</tt> in <tt>farg</tt> on line 11. Similarly, the call to <tt>aitken_()</tt> on line 12 gives the third argument as <tt>&amp;farg</tt>. A value such as <tt>len-2*i</tt> cannot be passed as an argument to a Fortran function. The value must be stored in a memory location, and the address of that memory location must be passed as a pointer. On lines 17 and 18, the second argument of <tt>printseq_</tt> is <tt>&amp;farg</tt> for the same reason. 
</div>
<div class="Indented">
To build the executable, we save the C source (comprised of the <tt>extern</tt> declarations, the definition of <tt>main()</tt> given above, and the definition of <tt>leibniz()</tt> on page <a class="Reference" href="#leibniz-function-in-C">1↑</a>) in the file <tt>leibnizFinC.c</tt>. The following three commands are issued using a makefile. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc -c leibnizFinC.c 
ifort -c  aitkenf.f 
icc -o leibnizFinC.exe leibnizFinC.o aitkenf.o 
</pre>
</div>

</div>
<div class="Indented">
The first two commands create the object files. The -c option tells <tt>icc</tt> to compile only. The third command attempts to build the executable using the object files. This command uses the <tt>icc</tt> linker and not the Fortran linker, which leads to a problem. We have seen already that the object code for <tt>aitkenf.o</tt> contains some undefined names that have to do with the workings of the <tt>ifort</tt> compiler. An <tt>ifort</tt> linker would automatically take care of resolving those names, but the <tt>icc</tt> linker does not. So we have to do something explicit to resolve the undefined names created by the <tt>ifort</tt> compiler. 
</div>
<div class="Indented">
To figure out what to do, we will build an executable using Fortran object files and the following command.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"> ifort -v -o leibnizf.exe leibnizf.o aitkenf.o
</pre>
</div>

</div>
<div class="Indented">
The <tt>-v</tt> option to the <tt>ifort</tt> linker asks the linker to be verbose and point out every step of what it does. The linker produces quite an eyeful. From that output, it appears that the Fortran runtime library is fetched by <tt>-lifcore</tt>. 
</div>
<div class="Indented">
We are now in a position to tackle the linking of <tt>aitkenf.o</tt> produced from Fortran source with <tt>leibnizFinC.o</tt> produced from a C source. We issue the command 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc -o leibnizFinC.exe leibnizFinC.o aitkenf.o \
       -lifcore
</pre>
</div>

</div>
<div class="Indented">
 Fortunately, the <tt>-lifcore</tt> option resolves all the undefined names and an executable is generated. With the GNU compilers, the <tt>-lgfortran</tt> option ensures that the Fortran runtime libraries are linked.
</div>
<div class="Indented">
Linking object files generated from Fortran with C++ object files presents a new issue. Suppose we copied the C source file <tt>leibnizFinC.c</tt> as follows: <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">cp leibnizFinC.c leibnizFinCPP.cpp 
</pre>
</div>

</div>
<div class="Indented">
We can try to build the executable as follows. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc -c leibnizFinCPP.cpp 
ifort  -c  aitkenf.f 
icpc -o leibnizFinCPP.exe leibnizFinCPP.o \
		aitkenf.o -lifcore
</pre>
</div>

</div>
<div class="Indented">
The linking will fail because of C++ name mangling. In <tt>leibnizFinCPP.cpp</tt>, we have the extern declaration
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">extern void aitken_(double * seq1, double * seq2, 
                          int * len); 
</pre>
</div>

</div>
<div class="Indented">
When it generates object code for that source file, the <tt>icpc</tt> compiler assumes that <tt>aitken_</tt> is externally defined and the name of a<i> C++ </i>function. So it mangles the name. Issuing the <tt>nm leibnizFinCPP.o</tt> command shows the mangled name to be as follows: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"> U _Z7aitken_PdS_Pi
</pre>
</div>

</div>
<div class="Indented">
Of course there is no such mangled name in the <tt>aitkenf.o</tt> file, and the linking has to fail. To get around this problem, we have to change all the three <tt>extern</tt> declarations in <tt>leibnizFinCPP.cpp</tt> slightly. For instance, the declaration of <tt>aitken_</tt> should be as follows: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">extern "C" void aitken_(double * seq1, double * seq2, 
                           int * len);
</pre>
</div>

</div>
<div class="Indented">
Here we are specifically telling the compiler that the name <tt>aitken_</tt> has C linkage. So the C++ compiler will not mangle that name.
</div>
<div class="Indented">
The C++ language is an extension of the C language. Every C program should be a valid C++ program. But name mangling becomes an issue if we want to call C programs from C++ programs. For instance, if <tt>aitken.h</tt> is the header file for <tt>aitken.c</tt>, any C source can include the header, and the linker will find the definitions of the names declared in the header in <tt>aitken.o</tt>. However, if a C++ source includes the header, the names in the header will get mangled by the C++ compiler. The linker will not be able to find the mangled names in <tt>aitken.o</tt>.
</div>
<div class="Indented">
There is a simple workaround that makes a C header file good for inclusion in both C and C++ sources. The workaround encloses the body of the header file within a few lines. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#ifdef __cplusplus 
extern "C" { 
#endif
...(declarations)...
#ifdef __cplusplus 
}
#endif
</pre>
</div>

</div>
<div class="Indented">
 As required by the standard, <tt>__cplusplus</tt> is defined in all C++ source files but not in C source files. If the header file is included in a C++ source file, all the declarations are enclosed in an <tt>extern "C"{}</tt> block and have C linkage. The C++ compiler will not mangle their names. If the header file is included in a C source file, the declarations are not enclosed in an <tt>extern &ldquo;C&rdquo;{}</tt> block. 
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The object file of the program source<div class="listing">
<pre class="listing">char *s="char *s=%c%s%c;%cmain(){printf(s,34,s,34,10,10)};%c";
main(){printf(s,34,s,34,10,10);}
</pre>
</div>
has a name that is undefined, a name in the data segment, and a name in the text segment. Which are these? Verify using <tt>nm</tt>. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Look up the <tt>nm</tt> man page and figure out the meaning of <tt>W</tt> and <tt>r</tt> designations attached to some symbols in object files compiled from C++.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the verbose option for the linking command and make a list of all the object files and libraries that are used to build the executables <tt>leibniz.exe</tt> and <tt>Leibniz.exe</tt>, respectively. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Following <tt>gcc</tt> and the GNU compiler collection, <tt>icc</tt> and <tt>icpc</tt> report all the header files that get included during compilation when invoked with the <tt>-M</tt> option. Make a list of all the header files that get included in <tt>leibniz.c</tt> and <tt>Leibniz.cpp</tt>, respectively.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The <tt>aitken.h</tt> header file (defined on page <a class="Reference" href="#page:aitken-in-c-headerfile">1↑</a>) is not suitable for inclusion in a C++ source. Modify it so that it may be used in either C or C++ source files.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose you are not allowed to modify it. Explain how you can still include it in a C++ source.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-2.2">2.2</a> Using BLAS and LAPACK libraries<a class="Label" name="sec:libmake-blas-lapack"> </a>
</h2>
<div class="Unindented">
The basic concepts of linear algebra are matrices and vectors. Many problems in science, such as the numerical solution of partial differential equations and numerical optimization, reduce to problems in numerical linear algebra. BLAS and LAPACK are widely used numerical linear algebra libraries.<span class="FootOuter"><span class="SupFootMarker"> [24] </span><span class="HoverFoot"><span class="SupFootMarker"> [24] </span>The original papers on BLAS are <span class="bibcites">[<a class="bibliocite" name="cite-14" href="#biblio-14"><span class="bib-index">14</span></a>, <a class="bibliocite" name="cite-18" href="#biblio-18"><span class="bib-index">18</span></a>, <a class="bibliocite" name="cite-17" href="#biblio-17"><span class="bib-index">17</span></a>]</span>. The basic reference for LAPACK is <span class="bibcites">[<a class="bibliocite" name="cite-15" href="#biblio-15"><span class="bib-index">15</span></a>]</span>. The Intel MKL manuals include documentation for BLAS and LAPACK. </span></span>
</div>
<div class="Indented">
The BLAS library is split into three levels. Functions for vector operations such as dot products are included in the first level, for matrix-vector operations in the second level, and for matrix-matrix operations such as matrix multiplication in the third level. The split into three levels is conceptual and reflects the historical order in which the interfaces for the BLAS functions were specified. Implementations of BLAS and LAPACK such as MKL and ACML, which are supported by Intel and AMD, respectively, bundle all three levels of BLAS as well as LAPACK into the same library. 
</div>
<div class="Indented">
The specifications of the BLAS functions have been frozen for nearly three decades. However, LAPACK evolves from time to time to include new algorithms. LAPACK is built on top of BLAS. Functions for solving systems of matrices, solving linear least squares problems, finding eigenvalues, and finding singular values are found in LAPACK. 
</div>
<div class="Indented">
Although the BLAS specifications have been frozen for decades, implementations of BLAS have to constantly respond to the rapid changes in computer architecture. A good implementation of matrix multiplication in 1990 looks nothing like a good implementation of matrix multiplication in 2015. In the intervening decades, computer architecture has advanced to include instruction pipelines, instruction-level parallelism, multiple levels of cache memory, expanded register sets, out-of-order execution, and multiple processing cores. Good implementations optimize BLAS for all these features of modern computers.
</div>
<div class="Indented">
The early specifications of BLAS were given in Fortran. However, Fortran does not provide adequate access to features of computer architecture. One cannot see the source code of commercial BLAS implementations in libraries such as MKL and ACML are coded. However, it is almost certain that BLAS functions are coded in C and in assembly language native to the computer architecture that is targeted. C functions can easily mimic the interfaces and calling conventions of Fortran subroutines. 
</div>
<div class="Indented">
Many LAPACK functions were coded in Fortran using BLAS years ago. The hope was that architecture-specific optimizations would be confined to BLAS as computers evolved. However, modern implementations of LAPACK functions such as LU and Cholesky factorizations use specialized algorithms that are a great deal more complicated than using the BLAS routines in a direct manner.<span class="FootOuter"><span class="SupFootMarker"> [25] </span><span class="HoverFoot"><span class="SupFootMarker"> [25] </span>For a general discussion of BLAS and LAPACK, see <span class="bibcites">[<a class="bibliocite" name="cite-21" href="#biblio-21"><span class="bib-index">21</span></a>]</span>. <span class="bibcites">[<a class="bibliocite" name="cite-25" href="#biblio-25"><span class="bib-index">25</span></a>]</span> gives a taste of optimizing LAPACK functions such as LU factorization. For a survey of blocking algorithms in matrix computations, see <span class="bibcites">[<a class="bibliocite" name="cite-16" href="#biblio-16"><span class="bib-index">16</span></a>]</span>. </span></span>
</div>
<div class="Indented">
In this chapter, our discussion is limited to programs that run on a single processor core, although the basic algorithms of dense linear algebra can be effectively adapted to many processor cores and large networks. We will discuss some of the aspects of concurrent programs as they relate to numerical linear algebra in later chapters. 
</div>
<div class="Indented">
Our introduction to BLAS/LAPACK in this section begins with a discussion of the representation of matrices using arrays (section <a class="Reference" href="#sub:libmake-blas-leadingdim">2.2.1↓</a>). One may think that the natural way to represent matrices is to use two-dimensional arrays. In fact, matrices are represented using one-dimensional arrays, as we explain. A brief introduction to BLAS/LAPACK functionality in section <a class="Reference" href="#sub:libmake-blas-BLAS-and-LAPACK">2.2.2↓</a> is followed by the discussion of a C++ class interface in section <a class="Reference" href="#sub:libmake-blas-++-class-interface">2.2.3↓</a>. This class is unlike the general <tt>Vector</tt> class of the previous chapter and more typical of the way we use C++. The class is specific, is narrowly defined, and does just one thing, which is to provide an easy interface to LAPACK’s linear solver.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.2.1">2.2.1</a> Arrays, matrices, and leading dimensions<a class="Label" name="sub:libmake-blas-leadingdim"> </a>
</h3>
<div class="Unindented">
We look at multidimensional arrays in C/C++ briefly, although it is nearly always better to work with one-dimensional arrays. A two-dimensional array in C can be defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double two_d[20][40];
</pre>
</div>

</div>
<div class="Indented">
Here <tt>two_d[][]</tt> can be thought of as a two-dimensional array with <span class="formula">20</span> rows and <span class="formula">40</span> columns. One-dimensional arrays are almost equivalent to pointers. However, two-dimensional arrays are not.
</div>
<div class="Indented">
The array subscripting operator <tt>[]</tt> has left to right associativity. Therefore, the compiler parses our definition as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double (two_d[20])[40];
</pre>
</div>

</div>
<div class="Indented">
In words, <tt>two_d</tt> is an array of size <span class="formula">20</span>, each entry of which is an array of <span class="formula">40</span> doubles. In memory, the <span class="formula">800</span> <tt>double</tt> locations are next to each other same as for a one-dimensional array of size <span class="formula">800</span>. However, it is illegal to say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double *p = two_d;
</pre>
</div>

</div>
<div class="Indented">
The value <tt>two_d</tt> is of type pointer to an array of <span class="formula">40</span> <tt>double</tt>s and not <tt>double *</tt>. The following usage would be legal.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double (*p)[40];
p = two_d;
</pre>
</div>

</div>
<div class="Indented">
Here <tt>p</tt>, like <tt>two_d</tt>, is a pointer to an array of <span class="formula">40</span> <tt>double</tt>s. The connection of multidimensional arrays in C to pointers is not straightforward, which is the principal reason to avoid multidimensional arrays in C/C++. Legitimate uses of multidimensional arrays are rare but do exist.
</div>
<div class="Indented">
Suppose we want a matrix of dimension <span class="formula">20 × 40</span>. We can simply say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double a[800];
</pre>
</div>

</div>
<div class="Indented">
It is a bad idea to allocate large data structures statically.<span class="FootOuter"><span class="SupFootMarker"> [26] </span><span class="HoverFoot"><span class="SupFootMarker"> [26] </span>Statically defined variables are allocated on the stack and the stack size is limited to the order of several MB in threaded programming. That is one half of the reason it is a bad idea to allocate large data structures on the stack. The other half  is that memory allocated on the stack is not released until program termination.</span></span> More generally, we can make room for an <span class="formula"><i>m</i> × <i>n</i></span> matrix as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double *a = (double *)malloc(m*n*sizeof(double));
</pre>
</div>

</div>
<div class="Indented">
We must remember to say <tt>free(a)</tt> when the memory is no longer needed. 
</div>
<div class="Indented">
Here we come to the distinction between column-major and row-major storage. In column-major storage, the <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of <tt>a[]</tt> is accessed as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">a[i+j*m]
</pre>
</div>

</div>
<div class="Indented">
Here <span class="formula">0 ≤ <i>i</i> &lt; <i>m</i></span> and <span class="formula">0 ≤ <i>j</i> &lt; <i>n</i></span>. The column-major format is used by Fortran, BLAS, and LAPACK. In row-major storage, the <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of <tt>a[]</tt> is accessed as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">a[i*n+j]
</pre>
</div>

</div>
<div class="Indented">
There is a natural way to approach the distinction between column-major format and row-major format. To access the entries of <tt>a[]</tt> in the order of storage, we may code as follows assuming column-major format.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">for(int j=0; j &lt; n; j++)
	for(int i=0; i &lt; m; i++)
		a[i + j*m] = ...
</pre>
</div>

</div>
<div class="Indented">
Here the inner-loop is accessing the entries of column <span class="formula"><i>j</i></span>. So columns are &ldquo;innermost&rdquo; in column-major format. Likewise rows are innermost in row-major storage. When a matrix (or a higher dimensional tensor) is stored in a one-dimensional array, we need two items of information to access entries of the array. First, we need the bounds of the index variables. Here the bounds are <span class="formula">0 ≤ <i>i</i> &lt; <i>m</i></span> and <span class="formula">0 ≤ <i>j</i> &lt; <i>n</i></span>. Second, we need the ordering of matrix (or tensor) indices from inner to outer. In column-major format, the row index is innermost (which corresponds to columns being innermost). 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:chapter1-column-major-lda"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/colmajor_lda.png" alt="figure FIGS/chapter1/colmajor_lda.png" style="max-width: 281px; max-height: 102px;"/>
<div class="caption">
Figure 2.1 Column-major storage of a matrix in a one-dimensional array. The shaded submatrix can be extracted by setting the leading dimension to <span class="formula">4</span>.
</div>

</div>

</div>

</div>
The crux of the matter is to think of row and column indices as loop variables in a loop-nest. Figure <a class="Reference" href="#fig:chapter1-column-major-lda">2.1↑</a> shows a matrix with the row index <span class="formula"><i>i</i></span> in the range <span class="formula">0 ≤ <i>i</i> &lt; 4</span> and the column index <span class="formula"><i>j</i></span> in the range <span class="formula">0 ≤ <i>j</i> &lt; 3</span> laid out in a one-dimensional array with columns innermost. 
</div>
<div class="Indented">
If we think of <span class="formula"><i>i</i></span> and <span class="formula"><i>j</i></span> as loop variables in a loop-nest, a way to extract submatrices suggests itself. Suppose we want to extract the top-left <span class="formula">2 × 2</span> matrix shaded in figure <a class="Reference" href="#fig:chapter1-column-major-lda">2.1↑</a>. We can take <span class="formula">0 ≤ <i>i</i> &lt; 2</span> and <span class="formula">0 ≤ <i>j</i> &lt; 2</span>. Assuming <tt>a</tt> is a pointer to the entry <tt>0</tt>, we may access the <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of the submatrix as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">a[i+j*4]
</pre>
</div>

</div>
<div class="Indented">
The leading dimension is <span class="formula">4</span> here and not <span class="formula">2</span> because the <span class="formula">2 × 2</span> matrix is embedded in a matrix whose columns have <span class="formula">4</span> entries. A submatrix is accessed using its dimensions, the pointer to its first entry, and the leading dimension. 
</div>
<div class="Indented">
The leading dimension is the number of entries in the innermost dimension of the array. It is probably more appropriate to call it the inner dimension, implying a connection to the innermost loop, but the current usage is well established. Suppose we want to extract the <span class="formula">2 × 2</span> submatrix of the matrix shown in figure <a class="Reference" href="#fig:chapter1-column-major-lda">2.1↑</a> whose top-left corner is <span class="formula">5</span>. We may do so by taking <tt>b=a+5</tt> to point to the first entry, which is <span class="formula">5</span>, and setting the leading dimension to <span class="formula">4</span>. The leading dimension is once again the number of entries in the innermost dimension of the enclosing matrix. In this example, the gap between two columns of the submatrix straddles both columns.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.2.2">2.2.2</a> BLAS and LAPACK<a class="Label" name="sub:libmake-blas-BLAS-and-LAPACK"> </a>
</h3>
<div class="Unindented">
BLAS and LAPACK functions, to which we now turn, typically have long argument lists. For example, the Fortran interface to the BLAS function for multiplying a matrix and a vector has the following declaration in C: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">extern "C" void dgemv_(char *,int *,int *,double *,double *,
                       int *,double *,int *,double *,double *, 
                       int *, int);
</pre>
</div>

</div>
<div class="Indented">
This function, which has <span class="formula">12</span> arguments in total, implements the operation <span class="formula"><i>y</i> ← <i>α</i><i>Ax</i> + <i>β</i><i>y</i></span> or <span class="formula"><i>y</i> ← <i>α</i><i>A</i><sup><i>T</i></sup><i>y</i> + <i>β</i><i>y</i></span>. The first argument, which is a character string, allows us to specify whether the matrix <span class="formula"><i>A</i></span> must be transposed or not. The next <span class="formula">10</span> arguments allow us to specify the entries and dimensions of the matrix <span class="formula"><i>A</i></span> and of the vectors <span class="formula"><i>x</i></span> and <span class="formula"><i>y</i></span>, as well as the scalars <span class="formula"><i>α</i></span> and <span class="formula"><i>β</i></span>. 
</div>
<div class="Indented">
The last argument to <tt>dgemv_() </tt>is the only one that is not a pointer. Because all arguments to Fortran subroutines are passed by reference, we may expect all arguments in the C declaration of a Fortran subroutine to be pointers. Why then is the last argument not a pointer? The answer lies in the differing conventions for character strings in C and in Fortran. In C, a character string may be passed as a pointer of type <tt>char *</tt>. Its length is determined by the convention that the terminating character is <tt>’\0’</tt>. There is no such convention in Fortran, and the lengths of character strings must be supplied explicitly. The last argument of type <tt>int</tt> must be the length of the first argument, which is a character string. This last argument is invisible in Fortran as the Fortran compiler takes care to supply it surreptitiously.<span class="FootOuter"><span class="SupFootMarker"> [27] </span><span class="HoverFoot"><span class="SupFootMarker"> [27] </span>It is typical for Fortran compilers to use an additional argument at the end to indicate the length of a character string, but this practice is not mandated by the Fortran standard.</span></span>
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:Convention-for-naming-BLapack"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
S
</td>
<td align="center" valign="top" style="width: 1in;">
single precision
</td>
<td align="center" valign="top">
GE
</td>
<td align="center" valign="top" style="width: 1in;">
general
</td>
<td align="left" valign="top">
TRF
</td>
<td align="center" valign="top" style="width: 1in;">
triangular factorization
</td>

</tr>
<tr>
<td align="center" valign="top">
D
</td>
<td align="center" valign="top" style="width: 1in;">
double precision
</td>
<td align="center" valign="top">
GT
</td>
<td align="center" valign="top" style="width: 1in;">
general triangular
</td>
<td align="left" valign="top">
TRS
</td>
<td align="center" valign="top" style="width: 1in;">
triangular solve
</td>

</tr>
<tr>
<td align="center" valign="top">
C
</td>
<td align="center" valign="top" style="width: 1in;">
complex single precision
</td>
<td align="center" valign="top">
PB
</td>
<td align="center" valign="top" style="width: 1in;">
positive definite banded
</td>
<td align="left" valign="top">
LSS
</td>
<td align="center" valign="top" style="width: 1in;">
least squares solver
</td>

</tr>
<tr>
<td align="center" valign="top">
Z
</td>
<td align="center" valign="top" style="width: 1in;">
complex double precision
</td>
<td align="center" valign="top">
SP
</td>
<td align="center" valign="top" style="width: 1in;">
symmetric packed storage
</td>
<td align="left" valign="top">
SVD
</td>
<td align="center" valign="top" style="width: 1in;">
singular values
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top" style="width: 1in;">

</td>
<td align="center" valign="top">
TB
</td>
<td align="center" valign="top" style="width: 1in;">
triangular banded
</td>
<td align="left" valign="top">
EV
</td>
<td align="center" valign="top" style="width: 1in;">
eigenvalues 
</td>

</tr>

</table>

</div>
<div class="caption">
Table 2.1 Convention for naming functions in BLAS and LAPACK. Function names are formed by adjoining codes in columns 1, 3, and 5. For instance, <tt>DGETRF()</tt> implements triangular factorization of general matrices with double-precision entries. Columns 3 and 5 list only a small subset of the possible codes. 
</div>

</div>

</div>

</div>
<div class="Indented">
The naming convention used by BLAS/LAPACK is described by table <a class="Reference" href="#tab:Convention-for-naming-BLapack">2.1↑</a>. According to that convention, <tt>DGETRF</tt> is the name of the LAPACK function for triangular factorization of general matrices with double-precision entries. As we already mentioned, many LAPACK functions were written in Fortran long ago. If we want to call an LAPACK Fortran routine named <tt>DGETRF</tt> from C, we have to be aware of the convention for altering names when Fortran source files are converted to object code. However, to save us the trouble, the header file <tt>mkl_lapack.h,</tt> which is included by the header file <tt>mkl.h</tt>, allows many possible names for the same function.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void DGETRF( MKL_INT *m, MKL_INT *n, double *a, MKL_INT *lda, MKL_INT *ipiv, MKL_INT *info );
void DGETRF_( MKL_INT *m, MKL_INT *n, double *a, MKL_INT *lda, MKL_INT *ipiv, MKL_INT *info );
void dgetrf( MKL_INT *m, MKL_INT *n, double *a, MKL_INT *lda, MKL_INT *ipiv, MKL_INT *info );
void dgetrf_( MKL_INT *m, MKL_INT *n, double *a, MKL_INT *lda, MKL_INT *ipiv, MKL_INT *info );
</pre>
</div>

</div>
<div class="Indented">
These four names are all bound to the same function in the object code of the library. So we may call the function by any one of the four names. <tt>MKL_INT</tt> is used as a synonym for <tt>int</tt> to facilitate porting the library to systems with different conventions for <tt>int</tt> and <tt>long int</tt>. The parameters <tt>m</tt> and <tt>n</tt> of <tt>dgetrf_</tt> are the number of rows and columns of the matrix whose <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry is <tt>a[i+j*lda]</tt>.
</div>
<div class="Indented">
Triangular factorizations are useful for solving matrix systems. The following is a <span class="formula">2 × 2</span> example:<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
0
</span>
<span class="arraycell align-c">
1
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
1
</span>
<span class="arraycell align-c">
0
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
5
</span>
<span class="arraycell align-c">
 − 3
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
10
</span>
<span class="arraycell align-c">
1
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span> = <span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
1
</span>
<span class="arraycell align-c">
0
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
1 ⁄ 2
</span>
<span class="arraycell align-c">
1
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
10
</span>
<span class="arraycell align-c">
1
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
0
</span>
<span class="arraycell align-c">
 − 3.5
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>.
</div>
The left-most matrix flips the rows to ensure that the subdiagonal entries of the lower triangular factor are at most <span class="formula">1</span> in magnitude. Such row pivoting promotes numerical stability.
</div>
<div class="Indented">
<tt>DGETRF</tt> carries out the triangular factorization in place, and when the function returns, the lower and upper triangular factors will be stored using the same array <tt>a</tt>. The unit diagonal of the lower triangular matrix is not stored. The row that ends up as the <tt>i</tt>th row after pivoting is <tt>ipiv[i]</tt> (<tt>ipiv</tt> must be allocated as an integer array of length <tt>m</tt> before the function is called). The last parameter <tt>info</tt> returns information about possible errors: if <tt>*info=0</tt>, the execution was successful; if <tt>*info=-i</tt>, the <tt>i</tt>th argument had an illegal value; and if <tt>*info=i</tt> with <span class="formula"><i>i</i> = 1, …, <i>m</i></span>, the input matrix is singular and the <span class="formula">(<i>i</i> − 1, <i>i</i> − 1)</span> entry of the upper triangular factor is zero. 
</div>
<div class="Indented">
After triangular factorization, matrix systems are solved using <tt>DGETRS</tt>. Here <tt>TRS</tt> is the code for triangular solve. Within MKL, there are four possible names for the same function as shown by declarations in <tt>mkl_lapack.h</tt>. One of these is <tt>dgetrs_()</tt>: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void dgetrs_( char *trans, MKL_INT *n, 
                 MKL_INT *nrhs, double *a, 
                 MKL_INT *lda, MKL_INT *ipiv, 
                 double *b, MKL_INT *ldb, 
                 MKL_INT *info );
</pre>
</div>

</div>
<div class="Indented">
Before this function is called, the matrix stored in the array <tt>a</tt> must have undergone triangular factorization. The first argument to this function is a character string named <tt>trans</tt>. This string allows us to ask for the solution of the transposed system. The system is not transposed if <tt>trans=’N’</tt> but is transposed if <tt>trans=’T’</tt>. When the Fortran function is called from C, we must ordinarily pass the length of the string explicitly as the last argument. However, MKL takes advantage of the way strings are handled by LAPACK’s Fortran routines and does not require us to pass that last argument. 
</div>
<div class="Indented">
Once <span class="formula"><i>A</i></span> has been factorized as <span class="formula"><i>PA</i> = <i>LU</i></span>, where <span class="formula"><i>P</i></span> is the pivoting matrix and <span class="formula"><i>L</i></span> and <span class="formula"><i>U</i></span> are lower and upper triangular factors, respectively, the solution of <span class="formula"><i>Ax</i> = <i>b</i></span> is calculated by solving <span class="formula"><i>Ly</i> = <i>P</i><sup> − 1</sup><i>b</i></span> for <span class="formula"><i>y</i></span> and then <span class="formula"><i>Ux</i> = <i>y</i></span> for <span class="formula"><i>x</i></span>. In a triangular system such as <div class="formula">
<span class="environment"><span class="arrayrow">
<span class="arraycell align-r">
<i>l</i><sub>11</sub><i>x</i><sub>1</sub>
</span>
<span class="arraycell align-c">
 = 
</span>
<span class="arraycell align-l">
<i>b</i><sub>1</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-l">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
<i>l</i><sub>21</sub><i>x</i><sub>1</sub> + <i>l</i><sub>22</sub><i>x</i><sub>2</sub>
</span>
<span class="arraycell align-c">
 = 
</span>
<span class="arraycell align-l">
<i>b</i><sub>2</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-l">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">

</span>
<span class="arraycell align-c">
⋮
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
<i>l</i><sub><i>n</i>1</sub><i>x</i><sub>1</sub> + ⋯ + <i>l</i><sub><i>nn</i></sub><i>x</i><sub><i>n</i></sub>
</span>
<span class="arraycell align-c">
 = 
</span>
<span class="arraycell align-l">
<i>b</i><sub><i>n</i></sub>
</span>

</span>
</span>
</div>
the first equation is solved for <span class="formula"><i>x</i><sub>1</sub></span>, the second equation for <span class="formula"><i>x</i><sub>2</sub></span>, and so on. The numerical stability of this obvious back substitution algorithm is quite subtle.<span class="FootOuter"><span class="SupFootMarker"> [28] </span><span class="HoverFoot"><span class="SupFootMarker"> [28] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-24" href="#biblio-24"><span class="bib-index">24</span></a>]</span>.</span></span>
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.2.3">2.2.3</a> C++ class interface to BLAS/LAPACK<a class="Label" name="sub:libmake-blas-++-class-interface"> </a>
</h3>
<div class="Unindented">
The <tt>Vector</tt> class of section <a class="Reference" href="#sec:chapter1-vector-class">1.3.1↑</a> is an attempt to capture the general concept of vectors. The <tt>LU_Solve</tt> class of this section is narrowly defined. It does just one thing, which is to provide an easy interface to LAPACK’s LU solver functions <tt>dgetrf()</tt> and <tt>dgetrs()</tt>.
</div>
<div class="Indented">
The class is defined (in the header file <tt>lusolve.hh</tt>) as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class LU_Solve{
private:
     int dim;
     double *A;
     int *ipiv;
public:
     LU_Solve(double *a, int dimi);
     ~LU_Solve();
     void factorize();
     void solve(double *v);
};
</pre>
</div>

</div>
<div class="Indented">
In the <tt>Vector</tt> class, the member functions were defined within the class definition itself. In the <tt>LU_Solve</tt> class, the member functions are declared as part of the class definition, but they are defined separately. The class constructor <tt>LU_Solve()</tt> takes the matrix to be solved as well as its dimension as arguments. The member function <tt>factorize()</tt> factorizes the matrix. Linear systems are solved using the member function <tt>solve()</tt>. The argument <tt>v</tt> is the right-hand side at entry to the member function. It is overwritten by the solution at exit. The <tt>factorize()</tt> function must be invoked immediately after a class object is defined, and it must be invoked just once. Once the matrix is factorized, any number of linear systems may be solved using <tt>LU_Solve::solve()</tt>.
</div>
<div class="Indented">
The BLAS/LAPACK functions for LU factorization are cumbersome to call directly. The <tt>LU_Solve</tt> class is a hassle-free interface. To solve two linear systems of dimension <span class="formula">1000</span>, we may code as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double *A = (double *)malloc(8l*1000*1000);
... initialize A ...
double *v = (double *)malloc(1l*2*1000);
double *w = v + 1000;
... initialize v and w ...
​
LU_Solve lu(A, 1000);
lu.factorize();
lu.solve(v);
lu.solve(w);
​
... report/graph v and w ...
​
free(v);
free(A);
</pre>
</div>

</div>
<div class="Indented">
Here <tt>lu.solve(v)</tt> overwrites <tt>v</tt> with the solution and likewise for <tt>w</tt>. 
</div>
<div class="Indented">
The class constructor is defined (in <tt>lusolve.cpp) </tt>as follows: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>LU_Solve::LU_Solve(double *a, int dimi)
<span class="number-left">2</span>     :dim(dimi)
<span class="number-left">3</span>{
<span class="number-left">4</span>     A = new double[dim*dim];
<span class="number-left">5</span>     ipiv = new int[dim];
<span class="number-left">6</span>     for(int i=0; i &lt; dim*dim; i++)
<span class="number-left">7</span>		A[i] = a[i];
<span class="number-left">8</span>}
</pre>
</div>

</div>
<div class="Indented">
There is a bit of new syntax here. On line 2, the colon introduces the member initializer list. The only entry of that list is <tt>dim(dimi)</tt>, where <tt>dim</tt> is a data item in the class object  being constructed and <tt>dimi</tt> is an argument to the constructor. Saying <tt>dim(dimi)</tt> is equivalent to calling the constructor of the class that <tt>dim</tt> belongs to with <tt>dimi</tt> as the argument. Because <tt>dim</tt> is an <tt>int</tt>, which is a basic type, it is equivalent to saying <tt>dim=dimi</tt> at the beginning of the function. On lines 4 and 5, <tt>A</tt> and <tt>ipiv</tt> are allocated using the <tt>new</tt> operator (instead of <tt>malloc()</tt>). The for-loop copies the input matrix to <tt>A[]</tt>. 
</div>
<div class="Indented">
The member functions of the class <tt>LU_Solve</tt> are declared within the namespace <tt>LU_Solve</tt> introduced by the class definition. When the member functions are defined externally, their names must be qualified using <tt>LU_Solve::</tt> as for the constructor above. The names of the other three member functions are similarly qualified in <tt>lusolve.cpp</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">​
LU_Solve::~LU_Solve(){
     delete[] A;
     delete[] ipiv;
}
​
void LU_Solve::factorize(){
     int m = dim;
     int n = dim;
     int lda = dim;
     int info;
     dgetrf_(&amp;m, &amp;n, A, &amp;lda, ipiv, &amp;info);
}
​
void LU_Solve::solve(double *v){
     char trans[3] = "N ";
     int nrhs = 1;
     int lda = dim;
     int ldv  = dim;
     int info;
     dgetrs_(trans, &amp;dim, &amp;nrhs, A, &amp;lda, ipiv, v, 
	     &amp;ldv, &amp;info);
}
</pre>
</div>

</div>
<div class="Indented">
The simplicity of the <tt>LU_Solve</tt> interface to LAPACK’s LU solver comes at the cost of lesser generality. The member function <tt>factorize()</tt> assumes the matrix to be square, with its leading dimension exactly equal to the size of its columns. Similarly, <tt>solve()</tt> assumes that the number of right-hand sides is <tt>nrhs=1</tt> to offer a simpler interface. 
</div>
<div class="Indented">
Some LAPACK routines, including triangular solve, are optimized for multiple right-hand sides in the MKL library. Making repeated calls instead of a single call with a suitable <tt>nrhs</tt> may degrade program speed considerably.
</div>
<div class="Indented">
Narrow classes such as <tt>LU_Solve</tt> are simple to code but can still be quite useful. All C++ classes defined in this book from here onward are of the same type.
</div>
<div class="Indented">
If the Intel compilers and the MKL library are used, one may include the header file <tt>mkl.h</tt>, which includes declarations of BLAS/LAPACK functions. One simply needs to pass the <tt>-mkl</tt> option to the compiler and it will look for the header file in the right place. Linking is equally easy. One needs to pass the <tt>-mkl=sequential</tt> option to the linker to link the sequential version of the MKL library. 
</div>
<div class="Indented">
If the GNU compilers are used, compiling and linking open source BLAS and LAPACK libraries can be equally easy. If the header file is put in a standard location such as <tt>/usr/include</tt>, there is no need to do anything special when the CBLAS header file <tt>cblas.h</tt> is included. One may need to explicitly declare the LAPACK functions with extern C linkage. Likewise, if BLAS functions are used, instead of CBLAS, they too may need to be declared explicitly. If the libraries are put in a standard place such as <tt>/usr/lib</tt>, it suffices to pass options  <tt>-lblas</tt> and <tt>-llapack</tt> to link the BLAS and LAPACK libraries. The online MKL link advisor may be consulted to link MKL libraries with <tt>gcc/g++</tt>.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Assume that a square matrix of dimension <tt>dim</tt> is stored in the array <tt>a[]</tt> with leading dimension equal to <tt>lda</tt>. Assume that <tt>dim</tt> is divisible by <span class="formula">4</span>. Write a function <div class="listing">
<pre class="listing">print_center(double *a, int dim)
</pre>
</div>
which prints the square matrix of dimension <tt>dim/2</tt> at the center of the matrix store in <tt>a[]</tt>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Assume that <tt>a[]</tt> is an array of dimension <span class="formula"><i>n</i><sub>1</sub><i>n</i><sub>2</sub><i>n</i><sub>3</sub></span>, which stores three-dimensional data indexed by <span class="formula">0 ≤ <i>i</i> &lt; <i>n</i><sub>1</sub></span>, <span class="formula">0 ≤ <i>j</i> &lt; <i>n</i><sub>2</sub></span>, and <span class="formula">0 ≤ <i>k</i> &lt; <i>n</i><sub>3</sub></span>. The three indices can be ordered from innermost to outermost in six different ways. For each ordering, calculate the location in <tt>a[]</tt> of the entry with index <span class="formula">(<i>i</i>, <i>j</i>, <i>k</i>)</span>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Assume that the array <tt>a[]</tt> of dimension <span class="formula"><i>n</i><sub>1</sub><i>n</i><sub>2</sub><i>n</i><sub>3</sub></span> stores three-dimensional data indexed using <span class="formula"><i>i</i>, <i>j</i>, <i>k</i></span> as in the previous exercise. Assume that <span class="formula"><i>i</i></span> is innermost and <span class="formula"><i>k</i></span> is outermost. Explain how to extract the submatrix with <span class="formula"><i>i</i><sub>0</sub> ≤ <i>i</i> &lt; <i>i</i><sub>1</sub></span>, <span class="formula"><i>j</i><sub>0</sub> ≤ <i>j</i> &lt; <i>j</i><sub>1</sub></span>, and <span class="formula"><i>k</i> = <i>k</i><sub>0</sub></span>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  If the array <tt>a[]</tt> is as in the previous exercise, explain how to extract a submatrix with <span class="formula"><i>i</i><sub>0</sub> ≤ <i>i</i> &lt; <i>i</i><sub>1</sub></span>, <span class="formula"><i>j</i> = <i>j</i><sub>0</sub></span>, and <span class="formula"><i>k</i><sub>0</sub> ≤ <i>k</i> &lt; <i>k</i><sub>1</sub></span>. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The <tt>LU_Solve</tt> class relies on its user to remember to factorize exactly once before attempting to solve linear systems. Add a data member <tt>state</tt> in addition to <tt>dim</tt>, <tt>A</tt>, and <tt>ipiv</tt> to the private section of the class and use it verify that the matrix is factorized exactly once before any call to <tt>solve()</tt>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The BLAS function <tt>dgemv()</tt> multiplies a matrix into a vector. Define a function <div class="listing">
<pre class="listing">void mult_mv(const double *A, int m, int n, double *x)
</pre>
</div>
which multiplies the <span class="formula"><i>m</i> × <i>n</i></span> matrix <tt>A</tt> into the vector <tt>x</tt>. There is no need for even a narrowly defined class to build a usable interface to <tt>dgemv()</tt>. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Program and test a narrowly defined class that interfaces to LAPACK’s least squares solver.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Program and test a narrowly defined class that interfaces to LAPACK’s tridiagonal solver. For multiple right-hand sides, calling the tridiagonal solver just once can be far more efficient than making multiple calls. Therefore, endow your class with the ability to solve multiple right-hand sides in a single function call. 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-2.3">2.3</a> Building programs using GNU Make<a class="Label" name="sec:libmake-gnumake"> </a>
</h2>
<div class="Indented">
The organization of source files into directories and subdirectories is the heart of modular programming. Typically, several source files cooperate to do a task, and yet more source files are involved in bigger tasks. A directory holds source files that are related or  perform similar tasks. Directories may be organized further into subdirectories in a source tree to reflect the structure of the program.
</div>
<div class="Indented">
The <tt>make</tt> utility provides a method for building a program from its source tree. Each source file must be turned into an object file, and the object files must be linked together to form executables. Compiling and linking become quite repetitive and error-prone if done from the command line. Makefiles offer a more systematic approach to building programs. 
</div>
<div class="Indented">
A build system such as <tt>make</tt> is essential to C/C++ programming. The Makefiles hold valuable information about the structure of the program as a whole, which is absent from the source files. In Python, the correspondence between modules and the directory hierarchy is wired into the language, but there is no such facility in C/C++. Modular programming aims to organize and conquer. There can be no modular programming without organization of program sources into a directory hierarchy. Well-thought-out source trees aid programming as much as structured definitions of functions and classes.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:libmake-make-source-tree"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/make_dir_structure.png" alt="figure FIGS/chapter1/make_dir_structure.png" style="max-width: 396px; max-height: 260px;"/>

</div>
<div class="caption">
Figure 2.2 Directories and files in the source tree for this book. The entire source tree is found at <a class="FlexURL" href="https://github.com/divakarvi/bk-spca">https://github.com/divakarvi/bk-spca</a>.
</div>

</div>

</div>
We begin our discussion of GNU <tt>make</tt> by looking at the source tree shown in figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>. The <tt>makevars.mk</tt> file at the root of the source tree defines <tt>make</tt> variables that are used in modules such as <tt>utils</tt> and <tt>linking</tt>. These modules correspond to directories. Some modules have several submodules. For example, the <tt>linking</tt> module has submodules <tt>aitken</tt>, <tt>easy</tt>, <tt>fft</tt>, <tt>lib</tt>, and <tt>linalg</tt>. 
</div>
<div class="Indented">
Much of the discussion in this chapter pertains to these submodules of <tt>linking</tt>. All the Aitken iteration programs are in the <tt>aitken</tt> submodule. The programs we used to illustrate use of the BLAS and LAPACK libraries are in <tt>linalg</tt>. We will discuss the use of shared and static libraries as well as the Fast Fourier Transform (FFT) later in this chapter. The programs and makefiles that will aid that discussion are in <tt>lib</tt> and <tt>fft</tt>. 
</div>
<div class="Indented">
The source tree shown in figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a> is partial. The module <tt>proc</tt>, which is at the root, corresponds to the next chapter. The next chapter deals with the x86 processor core, its instruction set, registers, and pipeline. The modules corresponding to the last two chapters about the Xeon Phi and GPU programming are show in the figure, but many others are omitted.
</div>
<div class="Indented">
The source files are the leaves of the source tree. The source tree as a whole includes <span class="formula">173</span> <tt>.cpp</tt> files, <span class="formula">65</span> <tt>.hh</tt> headers, <span class="formula">12</span> <tt>.c</tt> files, <span class="formula">6</span> <tt>.h</tt> files, <span class="formula">6</span> <tt>.cu</tt> (CUDA/GPU) files, and <span class="formula">14</span> <tt>.py</tt> (Python) files. Most of these are omitted in figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>. The only source files shown belong to the module <tt>utils</tt>, which is at the root of the source tree and the submodule <tt>aitken</tt> of the module <tt>linking</tt>. The modules <tt>utils</tt> and <tt>linking/aitken</tt> will be the basis of our discussion of makefiles.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:libmake-make-utils">2.3.1↓</a>, we begin by looking at part of the <tt>utils/</tt> directory and part of the <tt>linking/linalg</tt> directory shown in the source tree of figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>. A testing program in <tt>linking/linalg</tt> uses some utilities defined in <tt>utils/</tt>, thus providing an example of a program that depends on sources in distinct directories in the source tree. This example will be used to exhibit how the <tt>make</tt> utility may be used to build programs with sources scattered across the source tree. 
</div>
<div class="Indented">
The introduction to GNU <tt>make</tt> in sections <a class="Reference" href="#sub:libmake-make-begin">2.3.2↓</a> through <a class="Reference" href="#sub:libmake-make-recursive-end">2.3.6↓</a> emphasizes the two-pass structure of <tt>make</tt>, the use of pattern rules, and recursive <tt>make</tt>. Recursive <tt>make</tt> is the simplest method for handling a source tree with multiple directories. Although it has certain disadvantages, it is adequate for small projects, and even some large projects use recursive <tt>make</tt>. In section <a class="Reference" href="#sub:libmake-make-beyond-recursive">2.3.7↓</a>, we discuss some of the disadvantages of recursive <tt>make</tt> and how to overcome them.
</div>
<div class="Indented">
Finally, section <a class="Reference" href="#sub:libmake-make-library">2.3.8↓</a> has an importance that is far beyond its length. Here we discuss how static and shared libraries work, and how to find out exactly which library has been linked. Linking and loading problems that every C/C++ programmer will encounter are discussed along with suggestions of how to tackle them.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.1">2.3.1</a> The <tt>utils/ </tt>folder<a class="Label" name="sub:libmake-make-utils"> </a>
</h3>
<div class="Unindented">
A testing program in the source <tt>linking/linalg/test_lusolve.cpp</tt>, which uses functions defined in sources in a different folder in the source tree <tt>utils/</tt> (see the source tree in figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>), will be described here. Later in section <a class="Reference" href="#sub:libmake-make-beyond-recursive">2.3.7↓</a>, this program is used to illustrate how GNU’s <tt>make</tt> utility builds an executable from sources scattered in different parts of the source tree.
</div>
<div class="Indented">
The modules in <tt>utils/</tt> facilitate timing, generation of random numbers, gathering statistics, making tables, and manipulation of <tt>double</tt> arrays. All the modules in <tt>utils/</tt> are used extensively. The modules are used for testing, timing, and laying out data elsewhere in the source tree. The corresponding code is almost always omitted from the text. We avoid mentioning the modules in <tt>utils</tt> for the most part, but a brief discussion is given here. 
</div>
<div class="Indented">
In more complicated settings, there will be many dependencies between the directories and subdirectories of the source tree. In the source tree for this book, the directories are mostly self-contained. Most of the dependencies are on utilities in <tt>utils/</tt> and on modules for plotting and displaying data that are not shown in the source tree in figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>. 
</div>
<div class="Indented">
The header file <tt>utils.hh</tt> defines a macro called <tt>assrt()</tt>. This macro, which is used frequently, is similar to <tt>assert()</tt>, which is defined in the C standard header file <tt>assert.h</tt>. The only difference is that <tt>assrt()</tt> always checks its assertion, and not only if the preprocessor macro <tt>DEBUG</tt> is defined. We find little use for debuggers. The debugger is a blunt tool that works without an idea of the logical structure of the program. When programs are compiled in debug mode, the memory layout of their data can change. Memory errors may not be reproduced faithfully in debug mode.
</div>
<div class="Indented">
In addition to <tt>assrt()</tt>, <tt>utils.hh</tt> declares the following functions:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void array_abs(double *v, int n);
double array_max(double *v, int n);
void array_show(double *v, int n, const char* mesg);
void array_diff(double *restrict v, 
		double *restrict w, int n);
void array_copy(double *restrict v, 
		double *restrict w, int n);
void array_out(double *v, int m, int n, 
	       const char *fname);
void array_in(double *v, int size,  const char* fname);
</pre>
</div>

</div>
<div class="Indented">
These functions are defined in <tt>utils.cpp</tt>. Most of the declarations are self-explanatory. The function <tt>array_max()</tt> takes the absolute values of <span class="formula"><i>n</i></span> entries of <tt>v</tt> and returns the maximum. The function <tt>array_out()</tt> interprets <tt>v</tt> as an <span class="formula"><i>m</i> × <i>n</i></span> matrix in column-major order and outputs it to a file. These functions are used for testing and timing.
</div>
<div class="Indented">
Another function declared in <tt>utils.hh</tt> and defined in <tt>utils.cpp</tt> is the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void verify_dir(const char *dir);
</pre>
</div>

</div>
<div class="Indented">
This function uses Linux system calls to verify whether <tt>dir</tt> is already present, and if it is not, it creates such a directory. Linux system calls are declared in the header file <tt>unistd.h</tt>. A basic familiarity with system calls is of much value in programming.<span class="FootOuter"><span class="SupFootMarker"> [29] </span><span class="HoverFoot"><span class="SupFootMarker"> [29] </span>For a detailed, systematic, thorough, and readable account of Linux system calls, see <span class="bibcites">[<a class="bibliocite" name="cite-46" href="#biblio-46"><span class="bib-index">46</span></a>]</span>.</span></span>
</div>
<div class="Indented">
Programs must be tested as extensively as possible. Although details of testing are normally omitted, we give a single example here, partly to illustrate how some of the modules in <tt>utils/</tt> are used and partly to set up later discussion of recursive <tt>make</tt>.
</div>
<div class="Indented">
The <tt>LU_Solve</tt> class of section <a class="Reference" href="#sec:libmake-blas-lapack">2.2↑</a> is tested using the function <tt>testlu()</tt>. It is defined in a source file in <tt>linking/linalg</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void testlu(int n){
<span class="number-left">2</span>	assrt(n &gt; 0);
<span class="number-left">3</span>	double *A = new double[n*n];
<span class="number-left">4</span>	double *v = new double[n];
<span class="number-left">5</span>	for(int i = 0; i &lt; n; i++){
<span class="number-left">6</span>		v[i] = rand()*1.0/RAND_MAX-0.5;
<span class="number-left">7</span>		for(int j = 0; j &lt; n; j++)
<span class="number-left">8</span>			A[i+j*n] = rand()*1.0/RAND_MAX-0.5;
<span class="number-left">9</span>	}
<span class="number-left">10</span>	verify_dir("DBG/");
<span class="number-left">11</span>	array_out(A, n, n, "DBG/A.dat");
<span class="number-left">12</span>	
<span class="number-left">13</span>	LU_Solve lu(A, n);
<span class="number-left">14</span>	lu.factorize();
<span class="number-left">15</span>	array_out(v, n, 1, "DBG/b.dat");
<span class="number-left">16</span>	lu.solve(v);
<span class="number-left">17</span>	array_out(v, n, 1, "DBG/x.dat");
<span class="number-left">18</span>	
<span class="number-left">19</span>	system("test_lusolve.py DBG/A.dat"
<span class="number-left">20</span>	       " DBG/b.dat DBG/x.dat");
<span class="number-left">21</span>	
<span class="number-left">22</span>	delete[] v;
<span class="number-left">23</span>	delete[] A;
<span class="number-left">24</span>}
</pre>
</div>

</div>
<div class="Indented">
The first block of <tt>testlu()</tt> (lines 2 through 11) initializes the arrays <tt>A[]</tt> and <tt>v[]</tt> with a square matrix and a vector of dimension <span class="formula"><i>n</i></span>. Notice the use of <tt>assrt()</tt> on line 2. Line 10 creates the <tt>DBG/</tt> directory if it does not already exist. The matrix <tt>A[]</tt> is saved in <tt>DBG/A.txt</tt> using <tt>array_out()</tt> (line 11). On lines 10, 11, 15, and 17, <tt>testlu()</tt> calls functions that are defined in source file in an external module (<tt>utils/utils.cpp</tt>). This testing program is later used to illustrate how we may handle programs with sources scattered in multiple directories.
</div>
<div class="Indented">
The middle block (lines 13 through 17) solves the linear system <span class="formula"><i>Ax</i> = <i>b</i></span> and saves <span class="formula"><i>x</i></span> as well as <span class="formula"><i>b</i></span>. 
</div>
<div class="Indented">
Lines 19 and 20 invoke <tt>system()</tt>, which is a C library function.<span class="FootOuter"><span class="SupFootMarker"> [30] </span><span class="HoverFoot"><span class="SupFootMarker"> [30] </span>The C library function <tt>system()</tt> must never be invoked from a privileged process: <a class="FlexURL" href="http://linux.die.net/man/3/system">http://linux.die.net/man/3/system</a> explains that strange values for environment variables may be used to break the system.</span></span> This function forks a shell process and runs its argument as a shell command. The argument is a single string of type <tt>const char *</tt>, but it is broken across two lines. In C/C++, a string such as "one two" can be written as "one""two". Long strings can be conveniently split across lines.
</div>
<div class="Indented">
The shell command
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">test_lusolve.py DBG/A.dat DBG/b.dat DBG/x.dat
</pre>
</div>

</div>
<div class="Indented">
calls a Python script that looks at the output data and verifies that <span class="formula"><i>Ax</i></span> is indeed nearly equal to <span class="formula"><i>b</i></span>. It prints the relative error <span class="formula">||<i>b</i> − <i>Ax</i>|| ⁄ ||<i>b</i>||</span>.
</div>
<div class="Indented">
Of course, we could have called the Python script from the command line. But then <tt>testlu()</tt> is incomplete by itself, and we have to remember to do something more to complete the test. It is usually good practice to make dependencies explicit in the source and not rely on memory. The testing program calls <tt>testlu()</tt> multiple times.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int main(){
	testlu(10);
	testlu(100);
	testlu(1000);
}
</pre>
</div>

</div>
<div class="Indented">
Like <tt>testlu()</tt>, this <tt>main()</tt> function is also defined in <tt>test_lusolve.cpp</tt>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.2">2.3.2</a> Targets, prerequisites, and dependency graphs<a class="Label" name="sub:libmake-make-begin"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:libmake-dependency-graph"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/make_dependencies.png" alt="figure FIGS/chapter1/make_dependencies.png" style="max-width: 235px; max-height: 80px;"/>

</div>
<div class="caption">
Figure 2.3 Makefile dependency graph. 
</div>

</div>

</div>

</div>
<div class="Indented">
Dependencies are fundamental to <tt>make</tt>. Figure <a class="Reference" href="#fig:libmake-dependency-graph">2.3↑</a> shows that each object file depends on a single source and a header. Typically, the dependency is on multiple header files, unlike the simple situation shown in the figure. Each executable in turn depends on multiple object files. 
</div>
<div class="Indented">
The first purpose of a Makefile is to capture the dependency graph between headers, sources, object files, and executables. Each object in the dependency graph is typically a file as in figure <a class="Reference" href="#fig:libmake-dependency-graph">2.3↑</a>. All files that have incoming edges in the dependency graph are targets. The incoming edges indicate that a target file must be built using a set of some other files. Those other files are the prerequisites. The targets may reappear as prerequisites, as is the case for all the object files in figure <a class="Reference" href="#fig:libmake-dependency-graph">2.3↑</a>.
</div>
<div class="Indented">
Typically, <tt>make</tt> assumes that a target file may not exist. The target file is considered out of date if its time stamp (accessed using <tt>stat</tt> on GNU/Linux) is older than that of any of its prerequisites. If a target either does not exist or is out of date, <tt>make</tt> takes it upon itself to create a file corresponding to the target.
</div>
<div class="Indented">
The question arises of how <tt>make</tt> can create a new file corresponding to a target. To answer that question is the second purpose of a Makefile. The Makefile associates each target with a recipe, and the recipe is a shell command invoked by the <tt>make</tt> utility to build the target if the target is either absent or out of date.
</div>
<div class="Indented">
The executables <tt>leibniz.exe</tt> and <tt>logseries.exe</tt> are built using the following Makefile:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>leibniz.exe: leibniz.o aitken.o
<span class="number-left">2</span>	icc -o leibniz.exe leibniz.o aitken.o
<span class="number-left">3</span>​
<span class="number-left">4</span>logseries.exe: logseries.o aitken.o
<span class="number-left">5</span>	icc  -o logseries.exe logseries.o aitken.o -lm
<span class="number-left">6</span>​
<span class="number-left">7</span>aitken.o: aitken.c aitken.h
<span class="number-left">8</span>	icc -fPIC -c aitken.c
<span class="number-left">9</span>​
<span class="number-left">10</span>leibniz.o: leibniz.c aitken.h
<span class="number-left">11</span>	icc -c leibniz.c
<span class="number-left">12</span>​
<span class="number-left">13</span>logseries.o: logseries.c aitken.h
<span class="number-left">14</span>	icc -c logseries.c
</pre>
</div>

</div>
<div class="Indented">
This listing is a fraction of the Makefile in the <tt>linking/aitken</tt> directory in the source tree. Other parts of the Makefile are used to build executables from Fortran and C++ sources.
</div>
<div class="Indented">
This simple Makefile consists of two types of information. Lines 1, 4, 7, 10, and 13 are dependencies. In each dependency, the item before the colon is the target. Thus, the targets in lines 1, 4, 7, 10, and 13 are <tt>leibniz.exe</tt>, <tt>logseries.exe</tt>, <tt>aitken.o</tt>, <tt>leibniz.o</tt>, and <tt>logseries.o</tt>, respectively. The prerequisites follow the colon. In the rule on line 1, the prerequisites are <tt>leibniz.o</tt> and <tt>aitken.o</tt>.
</div>
<div class="Indented">
Together the five rules specify the dependency graph shown in figure <a class="Reference" href="#fig:libmake-dependency-graph">2.3↑</a>. For each executable, the dependency graph shows all the object files on which it depends. For each object file, the graph shows the source file and one of the header files on which the object file depends. When an executable is built, the flow of information is from source files to objects files and from object files to executables as shown in the figure. 
</div>
<div class="Indented">
The other type of information in the Makefile are the recipes. Lines 2, 5, 8, and 11 are recipes. Each recipe begins with a tab and corresponds to the target in the dependency above it. Beginning each recipe with a tab is a <i>major</i> aspect of <tt>make</tt> syntax. Thus, line 2 corresponds to the target <tt>leibniz.exe</tt>. Together the dependency and the recipe form a rule. For example, lines 1 and 2 form a rule.
</div>
<div class="Indented">
The <tt>make</tt> utility makes two passes. In the first pass, it consumes all the dependency rules and builds a dependency tree such as the one shown in figure <a class="Reference" href="#fig:libmake-dependency-graph">2.3↑</a>. In addition, targets are associated with recipes. In our example, there are five targets: <tt>leibniz.exe</tt>, <tt>logseries.exe</tt>, <tt>aitken.exe</tt>, <tt>leibniz.o</tt>, and <tt>logseries.o</tt>. These targets appear on lines 1, 4, 7, 10, and 13, respectively. The recipes that are bound to these targets in the first pass occur on the following lines.
</div>
<div class="Indented">
In the second pass, the <tt>make</tt> utility brings the target list given at its invocation up to date. For each target, it first makes sure that its prerequisites are up to date before bringing the target up to date. In general, this leads to a depth first traversal of part of the dependency graph. 
</div>
<div class="Indented">
Once the prerequisites are up to date, the <tt>make</tt> utility checks whether the target is older. If every target and prerequisite is assumed to be a file, <tt>make</tt> finds the date of each file using a GNU/Linux command called <tt>stat</tt> (or a system call of the same name) and takes that to be the date of the target or the prerequisite. A target is out of date or older if any of its prerequisites is newer. If the target is out of date or older, the corresponding recipe is invoked to bring it up to date (<tt>make</tt> does not check the date of the target after executing the recipe). This two-pass operation is the heart of how <tt>make</tt> works.
</div>
<div class="Indented">
Suppose we change to the <tt>linking/aitken</tt> directory and invoke the <tt>make</tt> utility at a shell prompt as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">make logseries.exe
</pre>
</div>

</div>
<div class="Indented">
During the first pass, the <tt>make</tt> utility notes that the target <tt>logseries.exe</tt> depends on <tt>logseries.o</tt> and <tt>aitken.o</tt>. These object files in turn depend on the corresponding sources and the header file <tt>aitken.h</tt>. If the object file <tt>logseries.o</tt> is <i>older</i> than either <tt>logseries.c</tt> or <tt>aitken.c</tt>, the target <tt>logseries.o</tt> is considered to be out of date during the second pass. Each file is stored with a time stamp that indicates the time at which it was last modified or changed (you can use the GNU/Linux utility <tt>stat</tt> to look at the time stamp of a file). If the target does not exist as a file, it is considered to be out of date.
</div>
<div class="Indented">
If the target <tt>logseries.o</tt> is out of date, the <tt>make</tt> utility will execute the corresponding recipe during the second pass. More specifically, the command 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc  -c  logseries.c
</pre>
</div>

</div>
<div class="Indented">
is issued to bring the target up to date. Here the <tt>-c</tt> option to the C compiler tells it to compile only. The target <tt>aitken.o</tt> is brought up to date in a similar manner by executing the recipe corresponding to it if it is out of date with respect to either of its prerequisites.
</div>
<div class="Indented">
Once the two object files <tt>logseries.o</tt> and <tt>aitken.o</tt> are ensured to be up to date, the second pass of the <tt>make</tt> utility checks whether the executable <tt>logseries.exe </tt>is out of date with respect to either of its prerequisites. The check is carried out by looking at the time stamps of the files as before. As noted already, a target such as <tt>logseries.exe</tt> is considered out of date if no file by that name exists. If the executable is out of date, the <tt>make</tt> utility issues the linking command
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc -o logseries.exe logseries.o aitken.o -lm
</pre>
</div>

</div>
<div class="Indented">
during the second pass. Here the <tt>-o</tt> option tells the <tt>icc</tt> linker to leave the output in <tt>logseries.exe</tt>. The <tt>-lm</tt> option at the end tells the <tt>icc</tt> linker to link the library <tt>libm.so</tt>. The shared library <tt>libm.so</tt> defines math function such as<tt> <span class="formula">log</span></tt>, <span class="formula">exp</span>, and the trigonometric functions. 
</div>
<div class="Indented">
The reader may notice the <tt>-fPIC</tt> option in the recipe for <tt>aitken.o</tt> (line 8). This object file will be included in a shared library later in this section. The <tt>fPIC</tt> option is needed for that purpose.
</div>
<div class="Indented">
The Makefile we have examined so far is quite simple. Yet it brings out the two passes in <tt>make</tt>’s operation, targets, prerequisites, recipes, and dependency graphs. We emphasize that nothing is done if a target is already up to date. For example, if we build <tt>leibniz.exe</tt> and then make a small change to <tt>aitken.c</tt>, the invocation <tt>make leibniz.exe</tt> will recognize that the prerequisite <tt>aitken.o</tt> is out of date and recompile <tt>aitken.c</tt>. However, <tt>leibniz.o</tt> is not out of date and the source file <tt>leibniz.c</tt> is not recompiled. The <tt>make</tt> utility uses the dependency graph to eliminate needless compilations. In a large source tree, the resulting saving can be considerable.
</div>
<div class="Indented">
In our listing, there is a single rule for each target that specifies all the prerequisites for that target. The prerequisites can be given separately, but for each target, there can be only one effective recipe. For example, we can delete lines 1 and 2 of the listing and replace them by the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">leibniz.exe: leibniz.o
leibniz.exe: aitken.o
leibniz.exe:
	icc -o leibniz.exe leibniz.o aitken.o
</pre>
</div>

</div>
<div class="Indented">
Here the two prerequisites are given in separate rules, and the recipe for the target <tt>leibniz.exe</tt> is given as part of a rule with no prerequisites. The three rules here can be given in any order. Because of the two-pass nature of <tt>make</tt>’s operation, the effect is the same. 
</div>
<div class="Indented">
We are also allowed to specify a dependency with multiple targets. The entire Makefile may be rewritten as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">aitken.o: aitken.c
	icc -c aitken.c
leibniz.o: leibniz.c
	icc -c leibniz.c
logseries.o: logseries.c
	icc -c logseries.c
leibniz.exe: leibniz.o
logseries.exe: logseries.o
leibniz.exe logseries.exe: aitken.o
	icc -o $@ $^ -lm
</pre>
</div>

</div>
<div class="Indented">
The last rule in this Makefile has two targets. The recipe for the last rule uses two automatic variables: <tt>$@</tt>, which expands to the target, and <tt>$^</tt>, which expands to the list of all prerequisites of the target. We will study automatic variables soon, but this is a little hint of what is to come. Thanks to automatic variables, we can use the same recipe for both the targets <tt>leibniz.exe</tt> and <tt>logseries.exe</tt>. 
</div>
<div class="Indented">
Many of the operations in building an executable are repetitive. In particular, executables are nearly always built by linking together all the object files in their prerequisite list along with libraries. The Makefile here fails to recognize that each <tt>.o</tt> object file is built from a <tt>.c</tt> source following the same pattern, which is to invoke <tt>icc</tt> with the <tt>-c</tt> compile only option. If pattern rules are defined appropriately, the entire Makefile can be reduced to two lines.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">leibniz.exe: leibniz.o aitken.o
logseries.exe: logseries.o aitken.o
</pre>
</div>

</div>
<div class="Indented">
With suitable pattern rules, <tt>make</tt> will automatically generate the dependency of <tt>.o</tt> object files on <tt>.c</tt> prerequisites, invoke the right compilation command to update object files, and invoke the right linking command to build the executable targets. Automatic variables, <tt>make</tt> variables, and pattern rules enable us to simplify repetitive tasks as we will now learn. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.3">2.3.3</a> Make variables in <tt>makevars.mk</tt>
</h3>
<div class="Unindented">
Almost all Makefiles have <tt>make</tt> variables. We use <tt>makevars.mk</tt> at the root of the source tree to show how <tt>make</tt> variables are used (see figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>).
</div>
<div class="Indented">
The <tt>makevars.mk</tt> file will serve us throughout this book. If has three sections. The first section defines variables.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#########
<span class="number-left">2</span>CPP 	 := icpc
<span class="number-left">3</span>CFLAGS   := -xHost -O3 -prec-div -no-ftz -restrict \
<span class="number-left">4</span> -Wshadow -MMD -MP
<span class="number-left">5</span>FFTWINC  := $(FFTW_INC)
<span class="number-left">6</span>MKLINC := -mkl
<span class="number-left">7</span>#########
<span class="number-left">8</span>MKLLIBS := -mkl=sequential
<span class="number-left">9</span>MKLTHRD := -mkl=parallel
<span class="number-left">10</span>FFTWLIBS  :=  $(FFTW_LINK)
</pre>
</div>

</div>
<div class="Indented">
In C/C++, a variable is a name for a location in memory. In <tt>make</tt>, a variable is a string. 
</div>
<div class="Indented">
The variable definitions from <tt>CPP</tt> to <tt>FFTWLIBS</tt> use <tt>:=</tt> and not <tt>=</tt> following the variable. The use of <tt>:=</tt> implies that the variables are evaluated immediately during the first pass. We do not discuss the other type of variable evaluation, which is called deferred evaluation.
</div>
<div class="Indented">
All characters in a line following the <tt>#</tt> character, including that character, are ignored. Lines 1 and 7 begin with the <tt>#</tt> character and are therefore comment lines.
</div>
<div class="Indented">
The variable <tt>CPP</tt> is set to <tt>icpc</tt> (line 2). It is the name of the C++ compiler used later in <tt>makevars.mk</tt>.
</div>
<div class="Indented">
The <tt>CFLAGS</tt> variable (lines 3 and 4) stands for the options passed to the C++ compiler. The definition of <tt>CFLAGS</tt> is split across two lines using the continuation character \. It merits careful scrutiny. The optimization level is <tt>-xHost -O3</tt>. The <tt>-xHost</tt> flag ensures that the compiler generates instructions corresponding to the highest capability of the machine. This flag is essential for our purposes.
</div>
<div class="Indented">
We do not bother with debug levels such as <tt>-g</tt> or <tt>-O0</tt>. The recommended optimization option in <tt>icpc</tt> is <tt>-fast</tt>. We do not use that option. It turns on <tt>-ipo</tt> or interprocedural optimization, which we do not want. Other dubious flags are also turned on by <tt>-fast</tt>.
</div>
<div class="Indented">
By default the <tt>icpc</tt> compiler may use a less precise but faster division for IEEE double-precision numbers, according to the compiler’s manual. It is unclear whether the faster division is ever really faster or whether the flag ever really has any effect. The <tt>-prec-div </tt>flag (line 3) forces conformance to IEEE arithmetic. The compiler manual states that the flush-to-zero optimization is used for really small numbers that almost underflow. This is another &ldquo;optimization&rdquo; of dubious value and uncertain meaning. It is turned off using <tt>-no-ftz</tt> (line 3). 
</div>
<div class="Indented">
The <tt>-restrict</tt> option (line 3) enables <tt>restrict</tt> qualified pointers, a C99 feature we find to be quite valuable in the next chapter and later. 
</div>
<div class="Indented">
C++ member functions may accidentally redefine a class variable, leading to runtime errors. For example, <tt>state</tt> could be a data member that keeps track of the state of the class object, and a member function, which wants to set it to <span class="formula">1</span>, may say <tt>int state=1</tt> instead of <tt>state=1</tt>. The <tt>-Wshadow</tt> option (line 4) tells the compiler to issue a warning when variables defined in an outer scope are redefined in an inner scope.
</div>
<div class="Indented">
The <tt>-MMD</tt> and <tt>-MP</tt> options (line 4) to the <tt>icpc</tt> compiler tell it to generate a <tt>.d</tt> file listing all dependencies of the source on header files. The way dependencies of C/C++ sources on header files is handled is discussed in section <a class="Reference" href="#sub:libmake-make-recursive-end">2.3.6↓</a>.
</div>
<div class="Indented">
A few of our programs make use of the FFTW library. On line 5, the <tt>make</tt> variable <tt>FFTWINC</tt> is set to <tt>$(FFTW_INC</tt>). The understanding is that <tt>FFTW_INC</tt> is defined in the shell environment before calling <tt>make</tt>. It can be set to whatever is needed to find the FFTW header files. If the installation is along standard lines, the header file will be in a standard place such as <tt>/usr/local/include/</tt>, where the compiler always looks. So the shell variable <tt>FFTW_INC</tt> can be even blank or undefined. If the header is not in a standard place, the shell variable (assuming the bash shell)<span class="FootOuter"><span class="SupFootMarker"> [31] </span><span class="HoverFoot"><span class="SupFootMarker"> [31] </span>We always assume the shell to be bash.</span></span> must be set as in 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">export FFTW_INC=-I &lt;dir-with-fftw-header&gt;
</pre>
</div>

</div>
<div class="Indented">
The <tt>-I</tt> option tells the compiler to look for headers at the directory that follows the option, in addition to the standard places. The directory is typically given as a full path.
</div>
<div class="Indented">
If <tt>FFTW_INC</tt> is defined as a shell variable, it may be evaluated using <tt>$(FFTW_INC)</tt> as if it were just another <tt>make</tt> variable. If it is in fact not defined in the shell, it evaluates to the empty string.
</div>
<div class="Indented">
Similarly, on line 10, the make variable <tt>FFTW_LIB</tt> is set by evaluating <tt>FFTW_LINK</tt>, which is presumed to be set in the shell. The shell variable <tt>FFTW_LINK</tt> can be as simple as <tt>-lfftw3</tt> if the installation is along standard lines, which leaves the libraries in standard places such as <tt>/usr/local/lib/</tt> or <tt>/usr/local/lib64</tt>. If the installation is not standard, the shell variable must be set as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">export FFTW_LINK=  -L &lt;dir-with-fftw-lib&gt; -lfftw3
</pre>
</div>

</div>
<div class="Indented">
The <tt>-L</tt> option tells the linker to look for libraries in an additional place, and <tt>-lfftw3</tt> tells it to look for the <tt>fftw3</tt> library.
</div>
<div class="Indented">
Handling the MKL library is easy if the Intel compilers are used. The option <tt>-mkl</tt> (line 6) to the compiler tells it to look for the MKL header files in the right places. Linking is equally easy. We may use <tt>-mkl=sequential</tt> or <tt>-mkl=parallel</tt> (lines 8 and 9).
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.4">2.3.4</a> Pattern rules in <tt>makevars.mk<a class="Label" name="sub:libmake-pattern-rules"> </a></tt>
</h3>
<div class="Unindented">
Makefile rules are made up of dependencies and recipes. The variables defined in the first section of <tt>makevars.mk</tt>, which we just discussed, are used to construct recipes. The recipes have a formulaic character. For example, if the target is an object file to be built from a C++ source, the recipe generally invokes the C++ compiler specified by <tt>CPP</tt> using the options listed in <tt>CFLAGS</tt>. Pattern rules take advantage of the repetitive nature of recipes to simplify their specification.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:make-automatic-variables"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="left" valign="top">
$@
</td>
<td align="left" valign="top">
Target 
</td>

</tr>
<tr>
<td align="left" valign="top">
<tt>$&lt;</tt>
</td>
<td align="left" valign="top">
The first prerequisite
</td>

</tr>
<tr>
<td align="left" valign="top">
<tt>$?</tt>
</td>
<td align="left" valign="top">
Prerequisites newer than the target
</td>

</tr>
<tr>
<td align="left" valign="top">
<tt>$^</tt>
</td>
<td align="left" valign="top">
All prerequisites with duplicates eliminated
</td>

</tr>
<tr>
<td align="left" valign="top">
<tt>$+</tt>
</td>
<td align="left" valign="top">
All prerequisites including duplicates
</td>

</tr>

</table>

</div>
<div class="caption">
Table 2.2 Automatic variables recognized by the <tt>make</tt> utility.
</div>

</div>

</div>

</div>
<div class="Indented">
Automatic variables, partially listed and explained in table <a class="Reference" href="#tab:make-automatic-variables">2.2↑</a>, are the basis of pattern rules. Automatic variables enable a recipe to parse and extract tokens from the dependency that precedes it (in the same rule). For example, in the rule
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">leibniz.exe: leibniz.o aitken.o
	icc -o $@ $^
</pre>
</div>

</div>
<div class="Indented">
the automatic variable <tt>$@</tt> evaluates to the target, which is <tt>leibniz.exe</tt> for this rule, and the automatic variable <tt>$^</tt> evaluates to <tt>leibniz.o aitken.o</tt>, which is the list of <i>all</i> prerequisites of the target. If a dependency <tt>leibniz.exe: xyz.o</tt> is given elsewhere in the Makefile, <tt>xyz.o</tt> will be in <tt>$^</tt> as well. The three most important automatic variables are <tt>$@</tt>, <tt>$^</tt>, and<tt> $&lt;</tt>. 
</div>
<div class="Indented">
Pattern rules form the second section of the <tt>makevars.mk</tt> file at the root of the source tree (see figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a> for the source tree).<a class="Label" name="libmake-pattern-rules"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>.SUFFIXES:
<span class="number-left">2</span>.SUFFIXES: .cpp .o .exe .s .d
<span class="number-left">3</span>%.o: %.cpp
<span class="number-left">4</span>	$(CPP)  $(CFLAGS)  -c $&lt;
<span class="number-left">5</span>%.s: %.cpp 
<span class="number-left">6</span>	$(CPP) $(CFLAGS) -fno-verbose-asm  -S $&lt; 
<span class="number-left">7</span>%.o: %.s 
<span class="number-left">8</span>	$(CPP) $(CFLAGS) -c $&lt; 
<span class="number-left">9</span>%.exe: %.o 
<span class="number-left">10</span>	$(CPP) -o $@ $(filter %.o,$^) $(LIBS) 
</pre>
</div>

</div>
<div class="Indented">
 Line 11 is a rule with <tt>.SUFFIXES</tt> as the target and an empty list of prerequisites. If has the effect of deleting many suffixes and pattern rules stored by GNU <tt>make</tt> by default. The rule on line 16 specifies the suffixes we want to use in pattern rules that are explicitly given later.
</div>
<div class="Indented">
The rule on lines 13 and 14 is the pattern rule for generating an object file from a C++ source with the file name extension <tt>.cpp</tt>. The pattern in the dependency is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">%.o: %.cpp
</pre>
</div>

</div>
<div class="Indented">
It matches a dependency such as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Aitken.o: Aitken.cpp Aitken.hh
</pre>
</div>

</div>
<div class="Indented">
The <tt>%</tt> in the target matches with <tt>Aitken</tt>. The <tt>make</tt> utility substitutes <tt>Aitken</tt> for <tt>%</tt> in <tt>%.cpp</tt>. It looks for <tt>Aitken.cpp</tt> in the prerequisite list to complete a match to the pattern rule. Because the prerequisite list has <tt>Aitken.cpp</tt> the match is complete.
</div>
<div class="Indented">
Even if the dependency of <tt>Aitken.o</tt> on <tt>Aitken.cpp</tt> is not explicitly given, there is still a pattern match if the current directory contains a file named <tt>Aitken.cpp</tt>. The <tt>make</tt> utility generates the dependency automatically. 
</div>
<div class="Indented">
If a rule with target <tt>Aitken.o</tt> has a recipe, that recipe is used to update <tt>Aitken.o</tt>. If no recipe is explicitly specified for a target, the recipe of the pattern rule that matches the target is used for updating it. In this instance, that rule would be 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	$(CPP) $(CFLAGS) -c $&lt;
</pre>
</div>

</div>
<div class="Indented">
We have gone over the definition of variables such as <tt>CPP</tt> (line 2) and <tt>CFLAGS</tt> (lines 3 and 4). When a variable is evaluated, the evaluation is specified as in <tt>$(CPP)</tt>. The difference in syntax between the point of definition of a variable and its point of use is one of the oddities of <tt>make</tt>. 
</div>
<div class="Indented">
The recipes are evaluated and applied only during the second pass; the variables are evaluated during the first pass. Therefore, some of the variables may be defined after the pattern rules. Here <tt>CPP</tt> and <tt>CFLAGS</tt> are defined before the recipe. However,<tt> </tt>a later makefile that includes <tt>makevars.mk</tt> may append additional options to <tt>CFLAGS</tt>, and those will be used by the recipe during the second pass. The <tt>-c</tt> option tells <tt>icpc</tt> to compile only (and not attempt to link against any libraries). The <tt>-c</tt> option could have been folded into <tt>CFLAGS</tt>, but it is perhaps a little clearer to make it explicit in the recipe. The recipe uses the automatic variable <tt>$&lt;</tt> to find the name of the source. It assumes that the source for generating the object file is given as the  first prerequisite.
</div>
<div class="Indented">
The recipe expands to 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc -O3 -prec-div -no-ftz -restrict -Wshadow -c $&lt;
</pre>
</div>

</div>
<div class="Indented">
after substituting for <tt>CPP</tt> and <tt>CFLAGS</tt>. A makefile that includes <tt>makevars.mk</tt> may extend <tt>CFLAGS</tt> as <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">CFLAGS := $(CFLAGS) $(MKLINC) $(FFTWINC) -openmp
</pre>
</div>

</div>
<div class="Indented">
The recipe will then include options for finding MKL and FFTW headers, as well as <tt>-openmp</tt>. OpenMP is the topic of a later chapter. We can influence the compilation command by appending to <tt>CFLAGS</tt> because assignments to variables are evaluated during the first pass, whereas recipes are evaluated during the second pass.
</div>
<div class="Indented">
Suppose there is no rule in the Makefile with <tt>Aitken.o</tt> as the target and we say <tt>make Aitken.o</tt> at the prompt. GNU <tt>make</tt> then notes that <tt>Aitken.o</tt> matches the target pattern <tt>%.o</tt> in a target rule. If the directory contains a file named <tt>Aitken.cpp</tt>, the pattern rule on lines 13 and 14 is considered a match. Its recipe will be used to build <tt>Aitken.o</tt>. Thus, pattern rules are capable of generating dependencies automatically.
</div>
<div class="Indented">
The pattern rule for generating <tt>%.s</tt> from<tt> %.cpp</tt> (lines 15 and 16) is used in chapter 3 to look at assembly code. The <tt>-fno-verbose-asm</tt> option leads to less cluttered assembly output. The <tt>-S</tt> option to the compiler tells it to generate the assembly code instead of the object code.
</div>
<div class="Indented">
Lines 19 and 20 define a pattern rule for building <tt>%.exe</tt> executables from <tt>%.o</tt> object files. The recipe for that rule uses a <tt>make</tt> construct that is new to us:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">$(filter %.o,$^)
</pre>
</div>

</div>
<div class="Indented">
GNU <tt>make</tt> has several built-in functions, and <tt>filter</tt> is one of them. As used here, it goes through the list of all prerequisites <tt>$^</tt> and selects only those that match the pattern <tt>%.o</tt>. Therefore, the recipe on line 20 has the effect of building the executable using all the object files, and only the object files, in the list of prerequisites. In general, the dependency list of a <tt>.exe</tt> target will contain all the object files needed to build it in addition to a few phony targets (see below), which are eliminated.
</div>
<div class="Indented">
The recipe on line 20 is
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	$(CPP) -o $@ $(filter %.o,$^) $(LIBS)
</pre>
</div>

</div>
<div class="Indented">
The list of libraries is passed to the linker using the variable <tt>LIBS</tt>. The evaluation of recipes is always deferred to the second pass, but variables such as <tt>LIBS</tt> are evaluated in the first pass, as explained already. Therefore, this variable does not need to be defined when the pattern rule is consumed during the first pass. If <tt>LIBS</tt> is defined as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">LIBS := $(FFTWLIBS) $(MKLLIBS) -openmp
</pre>
</div>

</div>
<div class="Indented">
during the first pass, the recipe for building a <tt>%.exe</tt> target that depends on the corresponding <tt>%.o</tt> object file will automatically link the MKL, FFTW, and OpenMP libraries (in that order). The order in which libraries are linked can be significant.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.5">2.3.5</a> Phony targets in <tt>makevars.mk</tt>
</h3>
<div class="Unindented">
We will discuss recursive make, useful for building programs with object files in several subdirectories, shortly. Recursive make relies on phony targets. Our first encounter with phony targets is in a simpler context. The third and last section of <tt>makevars.mk</tt> is listed below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>.PHONY: clean cleanxx
<span class="number-left">2</span>clean:
<span class="number-left">3</span>	rm *.o; rm *.exe; rm a.out;
<span class="number-left">4</span>cleanxx:
<span class="number-left">5</span>	rm *.o; rm *.a; rm *.so;  rm *.exe;  rm *.d
</pre>
</div>

</div>
<div class="Indented">
The rule with target <tt>.PHONY</tt> has <tt>clean</tt> and <tt>cleanxx</tt> as prerequisites (line 22). These are treated as phony targets. Ordinarily, <tt>make</tt> expects to find a file with the same name as a target and checks the latest modification time of that file to determine whether the target is out of date. For phony targets, <tt>make</tt> does not look for a file of the same name. Phony targets are always assumed to be out of date.
</div>
<div class="Indented">
In this example, saying <tt>make clean</tt> will remove all object files and executables in the current directory (the directory from which <tt>make</tt> is invoked). Making the target <tt>cleanxx</tt> removes certain other files in addition. The <tt>.d</tt> files (see below) used to capture dependencies of a C++ source on header files get removed with <tt>cleanx</tt>x. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.6">2.3.6</a> Recursive <tt>make</tt> and <tt>.d</tt> files<tt><a class="Label" name="sub:libmake-make-recursive-end"> </a></tt>
</h3>
<div class="Unindented">
The <tt>make</tt> utility, as we have discussed it so far, applies to programs all of whose source files are in a single directory. That assumption fails for even moderately large programs. When the source and object files required to build a single executable reside in several subdirectories, recursive make may be used to complete the build. 
</div>
<div class="Indented">
Recursive make is a straightforward concept. It consists of calling <tt>make</tt> within a recipe for a phony target. The <tt>linking/linalg/</tt> module defines interfaces to LAPACK’s LU factorization routines as discussed in a previous section. These interfaces reside in the source file <tt>lusolve.cpp</tt>. The source <tt>test_lusolve.cpp</tt> tests the interface. It uses a few utilities such as <tt>verify_dir() </tt>defined in <tt>utils/utils.cpp</tt>. Building the executable <tt>test_lusolve.exe</tt> offers a simple example of recursive make.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>include ../../makevars.mk
<span class="number-left">2</span>CFLAGS := $(CFLAGS) $(MKLINC)
<span class="number-left">3</span>LIBS := $(LIBS) $(MKLLIBS)
<span class="number-left">4</span>######
<span class="number-left">5</span>.PHONY: ../../utils objl
<span class="number-left">6</span>../../utils:
<span class="number-left">7</span>	make --directory=$@ objl
<span class="number-left">8</span>######
<span class="number-left">9</span>objl: lusolve.o
<span class="number-left">10</span>lusolve.o: lusolve.cpp
<span class="number-left">11</span>-include lusolve.d
<span class="number-left">12</span>test_lusolve.o: test_lusolve.cpp
<span class="number-left">13</span>-include test_lusolve.d
<span class="number-left">14</span>######
<span class="number-left">15</span>test_lusolve.exe: test_lusolve.o lusolve.o \
<span class="number-left">16</span>		../../utils ../../utils/utils.o
</pre>
</div>

</div>
<div class="Indented">
This Makefile is used as the basis for three somewhat distinct discussions.
</div>
<div class="Indented">
The first discussion is of recursive make. There are two targets declared as phony on line 5. The first of these, which is <tt>../../utils</tt>, enables recursive build using the Makefile in <tt>../../utils</tt>. The other phony target <tt>objl</tt> is the target in the present Makefile that builds all object files and libraries that may be useful externally. Indeed, we use <tt>objl</tt> generally in Makefiles as the name of the phony target that builds all object files and libraries in the same directory as the Makefile that may be useful externally.
</div>
<div class="Indented">
Thus, the recipe for the phony target <tt>../../utils </tt>(lines 6 and 7) is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	make --directory=$@ objl
</pre>
</div>

</div>
<div class="Indented">
The <tt>--directory</tt> option implies that a new shell process is forked with <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">../../utils
</pre>
</div>

</div>
<div class="Indented">
as the current directory and <tt>make objl</tt> is invoked within that directory (see table <a class="Reference" href="#tab:make-automatic-variables">2.2↑</a> for the meaning of <tt>$@</tt>). The Makefile in <tt>../../utils</tt> defines <tt>objl</tt> as a phony target. Its recipe updates all the object files and libraries that may be used externally, including <tt>utils.o</tt>, which will be used in this Makefile.
</div>
<div class="Indented">
The rule with the phony target <tt>objl</tt> (line 9) updates <tt>lusolve.o</tt>, which is the one object file in the <tt>linking/linalg</tt>/ folder that may be linked externally. Therefore, when other modules recursively call the Makefile in <tt>linking/linalg</tt> with <tt>objl</tt> as the target, the only object file that will be built is <tt>lusolve.o</tt>.
</div>
<div class="Indented">
Recursive invocation of <tt>make</tt> is a consequence of the dependency on lines 15 and 16. The executable <tt>test_lusolve.exe</tt> is built using three object files (lines 15 and 16). Two of these object files, <tt>test_lusolve.o</tt> and <tt>lusolve.o</tt>, are built in the current directory. The object file <tt>utils.o</tt>, however, resides in <tt>../../utils</tt>. To build that object file correctly, the phony target <tt>../../utils</tt> is listed as a prerequisite. Updating <tt>../../utils</tt> will lead to a recursive invocation of <tt>make</tt>, which builds <tt>utils.o</tt> in <tt>../utils.o</tt> (lines 6 and 7). The linking recipe defined in <tt>makevars.mk</tt> filters out the phony target.
</div>
<div class="Indented">
The second discussion is of <tt>.d</tt> files and the way dependencies on header files are handled.
</div>
<div class="Indented">
Line 10 gives the dependency of <tt>lusolve.o</tt> on the source <tt>lusolve.cpp</tt>. It is typical for each object file to depend primarily on one source file. However, the source file typically includes several header files that in turn include other header files. The source <tt>lusolve.cpp</tt> includes the following header files:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;cmath&gt;
#include &lt;iostream&gt;
#include &lt;mkl.h&gt;
#include "../utils/utils.hh"
#include "lusolve.hh"
</pre>
</div>

</div>
<div class="Indented">
These header files include yet others, and the command 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc -O3 -restrict -M -mkl -c lusolve.cpp
</pre>
</div>

</div>
<div class="Indented">
shows all the included files, which are <span class="formula">123</span> in number. This command uses the <tt>-mkl</tt> option supported by the <tt>icpc</tt> compiler to find the MKL header files. The <tt>-M</tt> option tells the compiler to give a list of all the included header files. If any of these header files is altered, the object file <tt>lusolve.o</tt> must be rebuilt, although <tt>lusolve.cpp</tt> is unchanged.
</div>
<div class="Indented">
Lines 11 and 13 are responsible for tracking the dependence of <tt>lusolve.o</tt> and <tt>test_lusolve.o</tt>, respectively, on header files. To understand how these lines work, we should go back to line 4 of <tt>makevars.mk</tt>, which included the <tt>-MMD</tt> and<tt> -MP</tt> options in <tt>CFLAGS</tt>. When <tt>icpc</tt> is invoked with <tt>-MMD -MP</tt>, it creates a <tt>.d</tt> file along with the object file. The <tt>.d</tt> file contains dependencies on the header files. For example, the generated <tt>lusolve.d</tt> contains the following lines:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">lusolve.o: lusolve.cpp ../utils/utils.hh lusolve.hh \
 /opt/caen/intel-12.1/mkl/include/mkl.h \
 [... mkl header files omitted ...]     \
 /opt/apps/intel-12.1/mkl/include/mkl_vsl_types.h
​
../utils/utils.hh:
	
lusolve.hh:
	
</pre>
</div>

</div>
<div class="Indented">
The first rule here, with <tt>lusolve.o</tt> as target, has generated the dependencies on all the MKL header files as well as <tt>lusolve.hh</tt> and <tt>../utils/utils.hh</tt>. The dependencies on system header files such as <tt>iostream</tt> are omitted if the flag is <tt>-MMD</tt> but included with <tt>-MD</tt>.<span class="FootOuter"><span class="SupFootMarker"> [32] </span><span class="HoverFoot"><span class="SupFootMarker"> [32] </span>I thank Zhongming Qu for showing me how to use the <tt>-MMD</tt> and <tt>-MP</tt> options to handle dependencies on header files.</span></span> 
</div>
<div class="Indented">
Line 11 of the Makefile reads
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">-include lusolve.d
</pre>
</div>

</div>
<div class="Indented">
The directive here is <tt>-include</tt> and not <tt>include</tt>. When the directive is <tt>include</tt> (as on line 1), GNU <tt>make</tt> signals an error if the file to be included is not found (or cannot be built using <tt>make</tt> rules). In contrast, <tt>-include</tt> moves forward silently if the file to be included is not found. If <tt>lusolve.o</tt> has been built, <tt>make</tt> will definitely find <tt>lusolve.d</tt> during its first pass because the compiler outputs the <tt>.d</tt> file along with the <tt>.o</tt> file. If the <tt>.d</tt> file is missing, the <tt>.o</tt> file must also be missing, and there is no need to track dependencies on the header files. 
</div>
<div class="Indented">
There can be a subtle problem with generating the <tt>.d</tt> files that list dependencies on header files during every compilation. Suppose <tt>lusolve.o</tt> is built correctly by the compiler, which also outputs a <tt>.d</tt> file as above, and then the implementation is changed. Suppose the header file <tt>../utils/utils.hh</tt> is deleted during the new implementation. If we try to rebuild <tt>lusolve.o</tt>, GNU <tt>make</tt> looks at the old <tt>.d</tt> file and tries to resolve a dependency of <tt>lusolve.o</tt> on <tt>../utils/utils.hh</tt>. There will be an error as the header file has since been removed. To handle this problem, the compiler generated <tt>.d</tt> files include <tt>make</tt> rules such as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">../utils/utils.hh: 
</pre>
</div>

</div>
<div class="Indented">
If the header file is not found, it is assumed to be built correctly by doing nothing.
</div>
<div class="Indented">
Although the <tt>-M</tt> and related options supported by <tt>icpc</tt> and <tt>gcc/g++</tt> make it relatively painless to handle dependencies of C/C++ sources on header files, it must be said that the use of <tt>.d</tt> files is far from a perfect solution. The <tt>.d</tt> files replicate information that is already present in the C/C++ sources and header files. Such replication of information or logic is usually not a good idea. Every time information or logic is replicated, it creates room for inconsistencies. 
</div>
<div class="Indented">
The third and final discussion reiterates points that have already arisen in a concrete way.
</div>
<div class="Indented">
The <tt>include</tt> directive on line 1 splices in all of <tt>makevars.mk</tt> at this point. This <tt>Makefile</tt> resides in <tt>linking/linalg</tt>, while <tt>makevars.mk</tt> is at the root of the source tree (see figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>). Therefore, line 1 refers to the file to be included as <tt>../../makevars.mk</tt>. Once this file is included, all the variables and pattern rules defined in it become available.
</div>
<div class="Indented">
We want the compilation command to look for MKL header files at the right places. The <tt>CFLAGS</tt> variable is modified to do that on line 2. It will be evaluated during the first pass and used when the recipe for generating <tt>%.o</tt> targets that depend on <tt>%.cpp</tt> sources is invoked during the second pass. That recipe of course is in <tt>makevars.mk</tt>, which is above the definition of <tt>CFLAGS</tt>. Similarly, line 3 sets the value of <tt>LIBS</tt> so that the MKL libraries are linked when the <tt>%.exe</tt> target that depends on <tt>%.o</tt> object file is built using the recipe in <tt>makevars.mk</tt>. 
</div>
<div class="Indented">
The dependencies on lines 10 and 12 may be omitted. GNU <tt>make</tt> will use the pattern rule for <tt>%.o</tt> targets depending on <tt>%.cpp</tt> sources to generate those rules automatically---if the corresponding source files <tt>lusolve.cpp</tt> and <tt>test_lusolve.cpp</tt> are in the directory. 
</div>
<div class="Indented">
Makefiles capture the dependence of object files on source files and headers perfectly. Such a perfect capture is possible because typically an object file depends only on the source file (of the same name but with a different extension) and the header files included from inside the source file. In contrast, no attempt is made to capture the dependencies between object files. Each object file is a collection of functions. Because functions may call one another, the dependency graph between the object files is typically more complicated. Circular dependencies between object files are common. Resolving these dependencies is left to the linker. 
</div>
<div class="Indented">
If suitable pattern rules have been defined, the dependencies of object files on sources is deduced automatically by <tt>make</tt>. In contrast, we almost always need to indicate which object files are used to build an executable explicitly. We usually include rules such as lines 10 and 12, which indicate the dependence of object files on sources, to make the structure of the program more explicit within the Makefiles.
</div>
<div class="Indented">
Makefiles bring order into the translation of sources to object files and the building of executables from the object files. They reflect the hierarchy of the source tree. Makefiles can get quite complicated when executables must be built on multiple platforms. If the source tree of figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a> must be built for Linux machines using GNU, Intel, or PGI compilers, or if they must be built for both Linux and Windows, the Makefiles get much more complicated. In such situations, it is common to write configure scripts that generate the Makefiles. Configure scripts are typically written as shell scripts, although Python is equally effective and far more pleasant to use.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.7">2.3.7</a> Beyond recursive <tt>make<a class="Label" name="sub:libmake-make-beyond-recursive"> </a></tt>
</h3>
<div class="Unindented">
Modularity and simplicity are two virtues of recursive make. However, there are several problems with it.<span class="FootOuter"><span class="SupFootMarker"> [33] </span><span class="HoverFoot"><span class="SupFootMarker"> [33] </span>See Peter Miller (Recursive Make considered harmful, 1997, <a class="FlexURL" href="http://aegis.sourceforge.net/auug97.pdf">http://aegis.sourceforge.net/auug97.pdf</a>).</span></span> It can be slow for large projects because a new shell process is created every time <tt>make</tt> is called recursively. It does not gel too well with parallel <tt>make</tt> using the <tt>-j</tt> option. It leads to needless compilation because all object files and libraries that may be externally needed are built during recursive <tt>make</tt> and not just those that are actually needed. It must be said that these deficiencies are not fatal. Recursive <tt>make</tt> is still used. 
</div>
<div class="Indented">
There appears to be a fundamental tension between the two-pass structure of the <tt>make</tt> utility and recursive invocation of <tt>make</tt>. The first pass is supposed to build a dependency graph, for example, of object files on sources and executables on object files. The second pass is supposed to invoke recipes to update targets that are out of date with respect to their prerequisites. The recursive invocation of <tt>make</tt> happens during the second pass. The dependency graph of object files and sources in the external module is built only when its Makefile is invoked recursively. The result is to splinter the dependency graph, leading to multiple first and second passes.
</div>
<div class="Indented">
It is possible to avoid recursive <tt>make</tt> entirely by building a single dependency graph for all the sources, object files, and executables in the project.<span class="FootOuter"><span class="SupFootMarker"> [34] </span><span class="HoverFoot"><span class="SupFootMarker"> [34] </span>One way to avoid recursive <tt>make</tt> was suggested by Peter Miller (Recursive Make considered harmful, 1997, <a class="FlexURL" href="http://aegis.sourceforge.net/auug97.pdf">http://aegis.sourceforge.net/auug97.pdf</a>) and developed further by Emile van Bergen (<a class="FlexURL" href="http://evbergen.home.xs4all.nl/nonrecursive-make.html">http://evbergen.home.xs4all.nl/nonrecursive-make.html</a>). This method relies on stack manipulation using <tt>make</tt> variables. Another method developed by Zhongming Qu is briefly described in the text. The heart of Qu’s idea is to use pattern rules specific to each subdirectory or submodule. </span></span> One way to do this is to include a <tt>rules.mk</tt> file in each subdirectory or submodule. The <tt>rules.mk </tt>file contains pattern rules specific to the subdirectory as well as dependencies of object files on sources present in the subdirectory. 
</div>
<div class="Indented">
In each <tt>rules.mk</tt>, it is assumed that a variable <tt>R</tt>, which expands to the full absolute path of the root of the project, is defined. A <tt>rules.mk</tt> file in <tt>linking/lingalg</tt> may look as follows: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>saved := $(D)
<span class="number-left">2</span>D := $(R)/linking/linalg #R defined externally
<span class="number-left">3</span>$(D)CFLAGS := $(CFLAGS) -mkl
<span class="number-left">4</span>​
<span class="number-left">5</span>$(D)/%.o: $(D)/%.cpp
<span class="number-left">6</span>	$(CPP) $($(@D)CFLAGS) -o $@ -c $&lt;
<span class="number-left">7</span>​
<span class="number-left">8</span>$(D)/lusolve.o: $(D)/lusolve.cpp
<span class="number-left">9</span>$(D)/test_lusolve.o: $(D)/test_lusolve.cpp
<span class="number-left">10</span>D := $(saved)
</pre>
</div>

</div>
<div class="Indented">
This <tt>rules.mk</tt> file defines the variable <tt>D</tt> (line 2) to be the absolute path to the directory that contains itself as well as the source files that it manages. The variable <tt>R</tt> is assumed to point to the root of the source tree. 
</div>
<div class="Indented">
On line 3, we define the variable <tt>$(D)CFLAGS</tt>. Here <tt>$(D)</tt> evaluates to the absolute path of the directory, and the variable name is in fact that absolute path name with <tt>CFLAGS</tt> appended to it. This variable holds compilation flags specific to the directory. In this example, the only change is to append the <tt>-mkl</tt> flag.
</div>
<div class="Indented">
On lines 5 and 6, the pattern rule for generating a <tt>.o</tt> object file from a <tt>.cpp</tt> source is made specific to the directory. In GNU make, <tt>@D</tt> is a variable that evaluates to the current directory. Therefore, the syntax
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">$($(@D)CFLAGS)
</pre>
</div>

</div>
<div class="Indented">
which occurs on line 6, evaluates the variable defined on line 3. GNU <tt>make</tt> always chooses the most specific pattern rule. On lines 5 and 6, the pattern rule is made specific to the object files to be generated in the current module.
</div>
<div class="Indented">
The use of <tt>$(D)</tt> to evaluate the <tt>make</tt> variable <tt>D</tt> on lines 8 and 9 implies that the object and source files are given in full as absolute paths. Giving object and source file names in full as absolute paths is essential to the technique being described.
</div>
<div class="Indented">
A <tt>rules.mk</tt> file for the module <tt>utils/</tt> written along the same lines looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">saved := $(D)
D := $(R)/utils
$(D)CFLAGS := $(CFLAGS) -fPIC
​
$(D)/%.o: $(D)/%.cpp
        $(CPP) $($(@D)CFLAGS)  -o $@ -c $&lt;
​
$(D)/utils.o: $(D)/utils.cpp
$(D)/Table.o: $(D)/Table.cpp
​
D := $(saved)
</pre>
</div>

</div>
<div class="Indented">
A Makefile in <tt>linking/linalg</tt> that builds the executable to test the LU solver looks as follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>include ../../root.mk #define R, CPP, CFLAGS
<span class="number-left">2</span>D := $(R)/linking/linalg
<span class="number-left">3</span>LIBS := -mkl=sequential
<span class="number-left">4</span>​
<span class="number-left">5</span>%.exe:
<span class="number-left">6</span>	$(CPP) -o $@ $(filter %.o,$^) $(LIBS)
<span class="number-left">7</span>​
<span class="number-left">8</span>include rules.mk
<span class="number-left">9</span>include $(R)/utils/rules.mk
<span class="number-left">10</span>​
<span class="number-left">11</span>test_lusolve.exe: $(D)/test_lusolve.o $(D)/lusolve.o
<span class="number-left">12</span>	$(R)/utils/utils.o $(R)/utils/Table.o
</pre>
</div>

</div>
<div class="Indented">
The <tt>root.mk</tt> file included on line 1 defines the variables <tt>R</tt>, <tt>CPP</tt>, and <tt>CFLAGS</tt>. It is similar to the <tt>makevars.mk</tt> file described earlier.
</div>
<div class="Indented">
The <tt>rules.mk</tt> file included on line 8 brings in the pattern rules and dependencies that govern object files and sources in the present <tt>linking/linalg</tt> module. The pattern rules for the <tt>utils/</tt> module are brought in using an <tt>include</tt> directive on line 9. The latter <tt>include</tt> directive uses an absolute path name.
</div>
<div class="Indented">
In the dependency rule with target <tt>test_lusolve.exe</tt> on lines 11 and 12, all object file names are given in full as absolute path names. The object files in the local module are prefixed with <tt>$(D)/</tt>. In contrast, the object files that are external to the module are given relative to the root with prefix <tt>$(R)/</tt>. By using absolute path names and pattern rules specific to each module, we avoid recursive make entirely and instead build a single dependency graph.
</div>
<div class="Indented">
One disadvantage of the technique just described is that when <tt>make</tt> runs and echoes the commands that it passes to the shell, the messages that appear have absolute path names that are almost unreadable. This deficiency can be remedied by resorting to complicated <tt>make</tt> syntax.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.3.8">2.3.8</a> Building your own library<a class="Label" name="sub:libmake-make-library"> </a>
</h3>
<div class="Unindented">
Our discussion of GNU<tt> make</tt> is uncommonly detailed and for a reason. Much of the time spent on C/C++ syntax is wasted without a fairly good knowledge of <tt>make</tt>. There is no modular programming in C/C++ without the <tt>make</tt> utility or an equivalent build system. The programmer is limited to single source files or awkward collections of source files in a single directory.
</div>
<div class="Indented">
We end our discussion of <tt>make</tt> by showing how to build and link static and shared libraries. Libraries provide a level of modularity beyond what is possible within a source tree. Any program that is linked against a library in effect treats the external library as a module.
</div>
<div class="Indented">
The <tt>utils/</tt> subdirectory in the source tree (see figure <a class="Reference" href="#fig:libmake-make-source-tree">2.2↑</a>) has <tt>utils.cpp</tt>, which provides basic facilities such as <tt>verify_dir()</tt>. The <tt>linking/aitken/</tt> subdirectory implements the Aitken iteration in the C source <tt>aitken.c</tt>. The source <tt>fft_mkl.cpp</tt> in <tt>linking/fft/</tt> provides an interface to part of MKL’s Fast Fourier Transform (FFT) facilities. The FFT is the topic of the next section. The Makefile below is in <tt>linking/lib</tt>/. It shows how to combine <tt>utils.o</tt>, <tt>aitken.o</tt>, and <tt>fft_mkl.o</tt> and build a shared or static library.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>include ../../makevars.mk
<span class="number-left">2</span>CFLAGS := $(CFLAGS) $(MKLINC)
<span class="number-left">3</span>######
<span class="number-left">4</span>MODS := ../../utils ../aitken ../fft
<span class="number-left">5</span>.PHONY: $(MODS)
<span class="number-left">6</span>$(MODS):
<span class="number-left">7</span>	@echo
<span class="number-left">8</span>	make --directory=$@ objl
<span class="number-left">9</span>	@echo
<span class="number-left">10</span>######
<span class="number-left">11</span>test_lib.o: test_lib.cpp
<span class="number-left">12</span>-include test_lib.d
<span class="number-left">13</span>######
<span class="number-left">14</span>libxmath.so: $(MODS)
<span class="number-left">15</span>	icpc -shared -o $@ ../../utils/utils.o	\
<span class="number-left">16</span>			   ../aitken/aitken.o 	\
<span class="number-left">17</span>			   ../fft/fft_mkl.o	
<span class="number-left">18</span>libxmath.a: $(MODS)
<span class="number-left">19</span>	ar rcs $@ ../../utils/utils.o		\
<span class="number-left">20</span>	          ../aitken/aitken.o 		\
<span class="number-left">21</span>		  ../fft/fft_mkl.o
<span class="number-left">22</span>######
<span class="number-left">23</span>#link against shared lib
<span class="number-left">24</span>#to link against static, rm .so file
<span class="number-left">25</span>test_lib.exe: test_lib.o
<span class="number-left">26</span>	icpc -o $@ $^ $(MKLLIBS) -L$(PWD) -lxmath 
</pre>
</div>

</div>
<div class="Indented">
Line 1 includes <tt>makevars.mk</tt>, so that the pattern rules we have discussed become effective in this Makefile. The three modules being combined are listed on line 4. Each module is a phony target (line 5). The phony target triggers recursive make in the appropriate subdirectory with the target <tt>objl</tt> (line 8) (lines 6 and 7 print empty lines to make the recursive invocation of <tt>make</tt> more visible as <tt>make</tt> runs). It is assumed that the Makefile in each directory will build the requisite object files when invoked on the phony target <tt>objl</tt>. 
</div>
<div class="Indented">
The rule for building the shared library is on lines 14 through 17. The target <tt>libxmath.so</tt> is the name of the shared library. Its prerequisites are the phony names for the three modules. The recipe here is almost the same as the recipe for linking. The only difference is the <tt>-shared</tt> option (line 15). The three object files referenced in the recipe are built using recursive make. The object files must be compiled with the <tt>-fPIC</tt> option. We explicitly showed the <tt>-fPIC</tt> option for <tt>aitken.o</tt> earlier in this section. The <tt>-fPIC</tt> options for the other two compilations are given through the Makefiles in the respective directories. 
</div>
<div class="Indented">
The rule for building the static library <tt>libxmath.a</tt> is on lines 18 through 21. The recipe uses the archive command <tt>ar</tt> with the options <tt>rcs</tt>. 
</div>
<div class="Indented">
The source <tt>test_lib.cpp</tt> has a simple program to test the functions in the <tt>xmath</tt> library. The target for building the corresponding executable is on line 25. This recipe will override the recipe in the pattern rule for <tt>%.exe</tt> targets with <tt>%.o</tt> prerequisites. The recipe (line 26) links the MKL libraries as it should. The <tt>icpc</tt> linker is told to look for libraries in the current directory (in addition to standard places) using the <tt>-L$(PWD)</tt> option (line 26). This is needed because both the static and share versions of <tt>xmath</tt> are built in the current directory. The <tt>-lxmath</tt> option to the <tt>icpc</tt> linker (line 17) makes the linker look for <tt>libxmath.so</tt> or <tt>libxmath.a</tt>. Shared libraries are linked preferentially. 
</div>
<div class="Indented">
Static libraries are conceptually simpler than shared libraries. A static library is simply an archive of object files. However, linking a static library is not the same as listing all the object files that are archived in it. If we list the object files explicitly, the executable will certainly include all the object files. In contrast, when a static library is linked, only those object files archived in the library that resolve undefined names in object files and libraries listed before it, as well as those undefined names that arise when object files archived in the same library are linked, are included. Thus, the order in which libraries are linked can be quite important.<span class="FootOuter"><span class="SupFootMarker"> [35] </span><span class="HoverFoot"><span class="SupFootMarker"> [35] </span>A bug related to the order in which libraries are linked persisted in the source code for this book for nearly <span class="formula">5</span> years. A program to compare the speed of FFTW and MKL was linked against both libraries, with MKL <i>first</i> and FFTW <i>later</i>. MKL implements many of the FFTW functions, and in fact, what we thought was FFTW was really again MKL. So the program was comparing MKL against MKL’s implementation of the FFTW interface. </span></span>
</div>
<div class="Indented">
When we invoke
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">test_lib.exe
</pre>
</div>

</div>
<div class="Indented">
at the command line, the executable initially does not have the definitions of the functions defined in <tt>xmath</tt>---if the linking is against the shared version of the library. It looks for the shared library at runtime. The search for shared libraries at runtime goes through a number of directories, but the directory containing <tt>libxmath.so</tt> is not one of them. We must add that directory to the shell variable <tt>LD_LIBRARY_PATH</tt> explicitly. 
</div>
<div class="Indented">
One advantage of shared libraries is that the system needs to load only one copy of the library if many processes are linked against the same shared library. Another advantage is that programs benefit automatically from updates and bug-fixes to shared libraries (at least in theory). The memory map and page tables of a process change when a shared library is loaded. Shared libraries must be supported by the operating system kernel. 
</div>
<div class="Indented">
The list of shared libraries available on a system may be obtained using the <tt>ldconfig -v</tt> command. The list can be long, and the only purpose it serves may be to overwhelm.
</div>
<div class="Indented">
The GNU/Linux <tt>ldd</tt> command can be used to find which shared libraries are being linked with the executable. There can be several versions of MKL or FFTW on a system, for example, making it uncertain which version of a library has been linked. If we say <tt>ldd test_lib.exe</tt> at the command prompt in GNU/Linux, we get the following sort of information:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">libmkl_sequential.so =&gt; /opt/intel/.../mkl/lib/...
libxmath.so =&gt; not found
</pre>
</div>

</div>
<div class="Indented">
Of the list of 10 libraries, we have shown only two. For the MKL library, the command outputs the full path to the shared library that will be linked dynamically at run time. For our <tt>libxmath.so</tt>, it says &ldquo;not found&rdquo; because the shell variable <tt>LD_LIBRARY_PATH</tt> has not been set to include the current directory. If this program is run, there will be an error at runtime because the <tt>xmath</tt> library cannot be found.
</div>
<div class="Indented">
Another useful command is <tt>ld</tt> with <tt>-verbose</tt> option. With that option, we may find out how the system looks for libraries . For example, if we say <tt>ld -verbose -lfftw3</tt>, we get a sense of what happens if the <tt>-lfftw3</tt> option is used to link <tt>fftw3</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">...
attempt to open //usr/local/lib64/libfftw3.so failed
attempt to open //usr/local/lib64/libfftw3.a failed
...
attempt to open //lib64/libfftw3.so failed
attempt to open //lib64/libfftw3.a failed
attempt to open //usr/lib/x86_64-linux-gnu/libfftw3.so 
succeeded
-lfftw3 (//usr/lib/x86_64-linux-gnu/libfftw3.so)
libm.so.6 needed by ...
//usr/lib/x86_64-linux-gnu/libfftw3.so
found libm.so.6 at //lib/x86_64-linux-gnu/libm.so.6
...
</pre>
</div>

</div>
<div class="Indented">
We see the order in which the loader goes through a number of directories looking for FFTW3. In every directory, it first looks for the shared and then the static version of FFTW3. When FFTW3 is found, it begins to look for shared libraries needed by FFTW3, and so on.
</div>
<div class="Indented">
The loader looks for shared libraries in the following order.<span class="FootOuter"><span class="SupFootMarker"> [36] </span><span class="HoverFoot"><span class="SupFootMarker"> [36] </span>For a far more detailed discussion, see <span class="bibcites">[<a class="bibliocite" name="cite-19" href="#biblio-19"><span class="bib-index">19</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-46" href="#biblio-46"><span class="bib-index">46</span></a>]</span>.</span></span> First, it looks at directories that may be explicitly embedded in the executable file using options such as <tt>-rpath</tt>. Second, it looks at directories in <tt>LD_LIBRARY_PATH</tt>. Third, it looks in the cache file <tt>/etc/ld.so.conf</tt>. The entries of this library cache file may be manipulated using the <tt>ldconfig</tt> command. Fourth, it looks at <tt>/usr/lib</tt>. 
</div>
<div class="Indented">
Understanding the manner in which programs using shared libraries are loaded and set up requires knowledge of the paging system, reviewed later in section <a class="Reference" href="#sec:memory-page-tables-virtual-memory">4.4↓</a>, and the concept of system calls, which are functions defined by the operating system kernel and which may be invoked by user programs (see section <a class="Reference" href="#sub:threads-easy-sys-call">5.4.1↓</a>). To complete our discussion of shared libraries, we anticipate later discussion of those topics and explain how shared libraries are set up.
</div>
<div class="Indented">
There are three parts to understanding how shared libraries work at the level of machine instructions. The first of these is the manner in which shared libraries are loaded into memory. Suppose a program <tt>abc.exe</tt>, which calls functions in shared libraries that in turn may call functions in other shared libraries, is invoked from the command line or in some other way. The Linux system call <tt>execve()</tt> is invoked with the file <tt>abc.exe</tt> as one of its arguments. Linux creates a process descriptor that holds administrative information about the program or process for its own use. It maps the contents of <tt>abc.exe</tt> to the virtual address space of the process with the intention of passing control to the process. However, before passing control it notices that the executable relies on shared libraries. Consequently, it loads the dynamic linker <tt>ld-linux.so</tt> or <tt>ld.so</tt> into the <i>same</i> virtual address space and passes control to the dynamic linker. The dynamic linker looks for the library in the file system in the order given above. Once the library is found, it issues the <tt>mmap()</tt> system call to load the library into the virtual address space of the process. If the library is already in physical memory, perhaps because it was loaded by some other process, <tt>mmap()</tt> will only update the page tables and not load the library again into physical memory. 
</div>
<div class="Indented">
The handling of global variables defined by shared libraries is the second part to be understood. References to global variables involve additional levels of indirection and go through the Global Offset Table (GOT). The GOT is set up by the dynamic linker.
</div>
<div class="Indented">
The final part to be understood is the handling of calls of functions defined in shared libraries. Here, too, additional levels of indirection are employed, and all function calls are routed through the Procedure Linkage Table (PLT). Unlike the GOT, PLT entries are not fully resolved by the dynamic linker to begin with. When a shared library function is called by the program, the program calls a PLT function. The first time a PLT function is called, the function sends the call to <tt>ld.so</tt>, which is still sitting in virtual memory, and it is only then that the reference to that shared library function is fully resolved. All later calls of the shared library function jump to the PLT entry and then directly to the library function.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:chapter1-make-exercise"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/make_exercise.png" alt="figure FIGS/chapter1/make_exercise.png" style="max-width: 113px; max-height: 52px;"/>

</div>
<div class="caption">
Figure 2.4 Dependencies between <tt>make</tt> targets.
</div>

</div>

</div>

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The dependencies between targets <tt>a</tt> through <tt>g</tt> are shown in figure <a class="Reference" href="#fig:chapter1-make-exercise">2.4↑</a>. Assume that the recipe to update the targets is as given below:<div class="listing">
<pre class="listing">a b c d e f g:
	echo $@
</pre>
</div>

</div>
<ul>
<li class="nested">
<ul>
<li>
Give exactly four dependency rules that capture all the dependencies shown in the figure.
</li>
<li>
If you say <tt>make a</tt> at the command line, explain why either <tt>g</tt> or <tt>e</tt> is the first to be printed.
</li>
<li>
Give the dependency rules in two ways, such that either <tt>e</tt> or <tt>g</tt> is the first to print.
</li>
<li>
Explain why there is no way to present the dependencies such that <tt>g</tt> prints first and <tt>e</tt> prints immediately afterward.
</li>

</ul>

</li>

</ul>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose you have the following Makefile:<div class="listing">
<pre class="listing">X := hello
Y := world
mesg:
	echo $(X) $(Y)
Y := universe
</pre>
</div>
If you say <tt>make mesg</tt>, will it print "hello world&rdquo; or &ldquo;hello universe&rdquo;?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Add a pattern rule to <tt>makevars.mk</tt> to generate <tt>.o</tt> files from <tt>.c</tt> files. Your pattern rule should be such that the entire Makefile to build <tt>leibniz.exe</tt> and <tt>logseries.exe</tt> in the directory <tt>linking-aitken</tt> can be reduced to the following three lines:<div class="listing">
<pre class="listing">include ../makevars.mk
leibniz.exe: leibniz.o aitken.o
logseries.exe: logseries.o aitken.o
</pre>
</div>

</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The <tt>icpc</tt> compiler/linker provides the options <tt>-mkl=sequential</tt> and <tt>-mkl=threaded</tt> to fetch header files and link against the MKL library. Modify <tt>makevars.mk</tt> to use these options.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Consider the Makefile to build <tt>test_lusolve.exe</tt> discussed in section <a class="Reference" href="#sub:libmake-make-recursive-end">2.3.6↑</a>. Suppose the phony target <tt>../utils</tt> is removed from the list of prerequisites of the target <tt>test_lusolve.exe</tt>. The object file <tt>utils.o</tt> will still be built correctly, but it will end up in the directory <tt>linking-linalg/</tt> instead of <tt>utils/</tt>, and the linking will fail. Explain why.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Record the size of the executable <tt>test_lib.exe</tt> when both <tt>libxmath</tt> and MKL are linked dynamically, when <tt>libxmath</tt> is linked statically, and when the <tt>-static</tt> option is used to link both <tt>libxmath</tt> and MKL statically. Explain why the size of the executable increases only slightly from the first linking to the second and by a lot more from the second linking to the third.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the <tt>ldd</tt> command to find all the shared libraries that <tt><div class="listing">
<pre class="listing">test_lusolve.exe
</pre>
</div>
</tt> is linked against. Use the <tt>nm</tt> command (also part of GNU’s binutils) to find the shared library in which <tt>dgetrf()</tt> is defined. Verify that <tt>dgetrf</tt>, <tt>dgetrf_</tt>, <tt>DGETRF</tt>, and <tt>DGETRF_</tt> are all bound to the same address in the text area, so that they are really  different names for the same function.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-2.4">2.4</a> The Fast Fourier Transform <a class="Label" name="sec:libmake-fft"> </a>
</h2>
<div class="Unindented">
So far in this book, modular programming in C/C++ has been the focus. Organization of sources into a source tree, Makefiles, and libraries is the basis of modular programming. The <tt>make</tt> utility, or an equivalent build system, is indispensable to modular programming in C/C++.
</div>
<div class="Indented">
Among programming languages, the C/C++ framework is the best---and often by far---for writing fast programs. C/C++ programs can be several hundred or even several thousand times faster than programs written in interpreted languages. In this last section, we look at the speed of a few implementations of the Fast Fourier Transform (FFT). Program speed is a major theme of the rest of this book.
</div>
<div class="Indented">
Program speed is influenced by many factors, including programming skill, compilers, the processor hardware, and the memory system. Each layer of software and hardware is heavily designed. Program speed is a discontinuous function of the design parameters. A small change in a program, or in the environment in which it runs, can result in unpredictable changes in program speed. In this section, we look at a few FFT implementations to gain an understanding of some of the factors that influence program speed. What does it mean to say that an implementation is optimal? To what extent does programming skill affect program speed? These are some of the questions we ask. We find already that programming skill has a great influence on program speed. Later chapters set forth many of the concepts that must be understood to produce efficient implementations of scientific programs.
</div>
<div class="Indented">
A program’s speed depends on the hardware configuration. In this chapter, we stick to a single processor core. In later chapters, we will see that even if a program is multithreaded or networked, understanding what happens on a single core is a big part of the game.
</div>
<div class="Indented">
About 100% of the computers in use for scientific programming use the x86 architecture. Thus, this book too adopts the x86 architecture. The x86 architecture evolves constantly. There are only a few particulars of the x86 architecture that will be of concern to us. These are discussed in the next chapter.
</div>
<div class="Indented">
For the most part, all that concerns us is the level of the instruction set, in particular, whether the instruction set is SSE2, AVX, AVX2, or AVX-512 (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↓</a> of the next chapter). Thus, the machines we use will be designated as <span class="formula">2.6</span>  GHz SSE2 or <span class="formula">2.2</span>  GHz AVX. The full names of the machines may be looked up from table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> of the appendix. The SSE2 machines support <span class="formula">128</span>-bit XMM registers, and the AVX/AVX2 machines support <span class="formula">256</span>-bit YMM registers.
</div>
<div class="Indented">
The clock signal that is fed into each processor core is the heart beat of the computer. The activities of the processor are synchronized with the clock signal. The memory system and other parts of the computer must accommodate themselves to the processor. If we measure program speed in cycles, we get a better sense of how well the program is exploiting the hardware.
</div>
<div class="Indented">
In this book, program speeds are reported using cycles. We use measures such as flops (floating point operations) per cycle for program speed and bytes per cycle for memory bandwidth. For some programs, we report the number of cycles consumed directly. Measuring program speed in terms of cycles is somewhat unconventional. It is more typical to see GFlops (Gigaflops per second) for arithmetic speed and GB/s (Gigabytes per second) for memory bandwidth. The second is a standard unit of time and its use is most appropriate when different hardware configurations are being compared. Our concern, which is to write efficient programs on a given hardware configuration, is quite different. Although we report timing measurements in cycles, they can be easily converted to seconds using the frequency in   GHz of the processor clock.
</div>
<div class="Indented">
The FFT is one of the most widely used algorithms in scientific computing and is fundamental to many areas of science. It is an appropriate starting point for the discussion of the speed of scientific programs. In section <a class="Reference" href="#sub:libmake-fft-algorithm-in-outline">2.4.1↓</a>, we introduce the FFT algorithm in outline. The purpose of this outline is to help understand the speed of FFT implementations. The two major FFT implementations in the MKL and FFTW libraries are introduced in sections <a class="Reference" href="#sub:libmake-FFT-using-MKL">2.4.2↓</a> and <a class="Reference" href="#sub:libmake-FFT-using-FFTW">2.4.3↓</a>, respectively.
</div>
<div class="Indented">
Programs run in a complex environment and the complexity of the environment influences program speed in ways we cannot fully grasp. The manner in which a program is timed can make a big difference. In addition, a program may behave quite differently from when it is used as part of a larger program to when it is timed by itself. The purpose of section <a class="Reference" href="#sub:libmake-fft-Cycles-and-histograms">2.4.4↓</a> is to give a sense of how the complexity of the environment influences program speed.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:libmake-fft-Optimality">2.4.5↓</a> compares MKL, FFTW, and another expertly coded FFT. The last of these is the type of program a good C/C++ programmer without knowledge of computer architecture may write. The optimized MKL and FFTW libraries can be even 10 times faster, illustrating both the value of optimized libraries and programming with a knowledge of computer architecture.<span class="FootOuter"><span class="SupFootMarker"> [37] </span><span class="HoverFoot"><span class="SupFootMarker"> [37] </span>Our discussion assumes that the FFTW library has been built correctly using <tt>--enable-avx</tt> (for machines with YMM registers) or <tt>--enable-sse</tt> (for machines with XMM registers). Without a proper build, FFTW can be too slow by a factor of <span class="formula">2</span> or even <span class="formula">4</span>. </span></span>
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.4.1">2.4.1</a> The FFT algorithm in outline<a class="Label" name="sub:libmake-fft-algorithm-in-outline"> </a>
</h3>
<div class="Unindented">
The discrete Fourier transform of <span class="formula"><i>f</i><sub>0</sub>, …, <i>f</i><sub><i>N</i> − 1</sub></span> is defined as <div class="formula">
<a class="eqnumber" name="eq:fft-fwd">(2.1) </a><i>f̂</i><sub><i>k</i></sub> = <span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator"><i>N</i></span><span class="ignored">)</span></span><span class="limits"><sup class="limit"><i>N</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>j</i> = 0</sub></span><i>ω</i><sup> − <i>jk</i></sup><i>f</i><sub><i>j</i></sub>,  <span class="text">for</span> <i>k</i> = 0, …, <i>N</i> − 1.
</div>
Here <span class="formula"><i>ω</i> = exp<span class="symbol">(</span>2<i>π</i><span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root"> − 1</span><span class="ignored">)</span></span> ⁄ <i>N</i><span class="symbol">)</span></span> is a primitive <span class="formula"><i>N</i></span>th root of unity. The <span class="formula"><i>f̂</i><sub><i>k</i></sub></span> are linear functions of <span class="formula"><i>f</i><sub><i>j</i></sub></span>. The discrete Fourier transform from <span class="formula"><i>f</i> = <span class="symbol">(</span><i>f</i><sub>0</sub>, …, <i>f</i><sub><i>N</i> − 1</sub><span class="symbol">)</span></span> to <span class="formula"><i>f̂</i> = <span class="symbol">(</span><i>f̂</i><sub>0</sub>, …, <i>f̂</i><sub><i>N</i> − 1</sub><span class="symbol">)</span></span> can be written as <span class="formula"><i>f̂</i> = <i>Mf</i></span>, where <span class="formula"><i>M</i></span> is the <span class="formula"><i>N</i> × <i>N</i></span> matrix whose <span class="formula">(<i>j</i>, <i>k</i>)</span>th entry is <span class="formula"><i>ω</i><sup> − <i>jk</i></sup> ⁄ <i>N</i></span>. 
</div>
<div class="Indented">
The inverse discrete Fourier transform is given by <div class="formula">
<a class="eqnumber" name="eq:fft-inverse">(2.2) </a><i>f</i><sub><i>j</i></sub> = <span class="limits"><sup class="limit"><i>N</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>k</i> = 0</sub></span><i>ω</i><sup><i>jk</i></sup><i>f̂</i><sub><i>k</i></sub>.
</div>
It too can be thought of as a matrix-vector product. 
</div>
<div class="Indented">
The discrete Fourier transform and its inverse have an intimate connection to Fourier series. Suppose <span class="formula"><i>f</i>(<i>x</i>)</span> is a function with period <span class="formula">2<i>π</i></span>. Its Fourier coefficients are defined by <div class="formula">
<i>c</i><sub><i>n</i></sub> = <span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator">2<i>π</i></span><span class="ignored">)</span></span><span class="limits"><sup class="limit">2<i>π</i></sup><span class="limit">⌠</span><span class="limit">⌡</span><sub class="limit">0</sub></span><i>f</i>(<i>x</i>)exp<span class="symbol">(</span> − <span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root"> − 1</span><span class="ignored">)</span></span><i>nx</i><span class="symbol">)</span> <i>dx</i> <span class="text">for</span> <i>n</i> = 0, ±1, ±2, …
</div>
If <span class="formula"><i>f</i><sub><i>j</i></sub> = <i>f</i><span class="symbol">(</span>2<i>π</i><i>j</i> ⁄ <i>N</i><span class="symbol">)</span></span>, then <span class="formula"><i>f̂</i><sub><i>k</i></sub> ≈ <i>c</i><sub><i>k</i></sub></span> for <span class="formula">0 ≤ <i>k</i> &lt; <i>N</i> ⁄ 2</span> and <span class="formula"><i>f̂</i><sub><i>k</i></sub> ≈ <i>c</i><sub><i>k</i> − <i>N</i></sub></span> for <span class="formula"><i>N</i> ⁄ 2 &lt; <i>k</i> ≤ <i>N</i> − 1</span>. If <span class="formula"><i>N</i></span> is even, <span class="formula"><i>f̂</i><sub><i>N</i> ⁄ 2</sub> ≈ <span class="symbol">(</span><i>c</i><sub><i>N</i> ⁄ 2</sub> + <i>c</i><sub> − <i>N</i> ⁄ 2</sub><span class="symbol">)</span> ⁄ 2</span>. Here <span class="formula"><i>f</i>(<i>x</i>)</span> is assumed to be a function that is integrable and sufficiently smooth. 
</div>
<div class="Indented">
The FFT is a method to effect the discrete transform <a class="Reference" href="#eq:fft-fwd">(2.1)↑</a> or its inverse <a class="Reference" href="#eq:fft-inverse">(2.2)↑</a> using <span class="formula"><span class="scriptfont">O</span><span class="symbol">(</span><i>N</i>log<sub>2</sub><i>N</i><span class="symbol">)</span></span> arithmetic operations, which is a considerable improvement over the <span class="formula"><span class="scriptfont">O</span><span class="symbol">(</span><i>N</i><sup>2</sup><span class="symbol">)</span></span> arithmetic operations required by direct matrix-vector multiplication.<span class="FootOuter"><span class="SupFootMarker"> [38] </span><span class="HoverFoot"><span class="SupFootMarker"> [38] </span>The modern discovery of the FFT is due to <span class="bibcites">[<a class="bibliocite" name="cite-20" href="#biblio-20"><span class="bib-index">20</span></a>]</span>. </span></span> The improvement in operation count is vital to making the FFT fast but is not the full story. The implementation can make a difference of more than a factor of <span class="formula">10</span> to the program speed. We present the structure of the power of <span class="formula">2</span> FFT but omit mathematical details. 
</div>
<div class="Indented">
Suppose <span class="formula"><i>N</i> = 2<sup><i>n</i></sup></span>. We assume the data to be <span class="formula"><i>N</i></span> complex numbers. The first step in the power of <span class="formula">2</span> FFT is to separate the data into even and odd parts as follows:<tt><div class="formula">
<i>f</i><sub>0</sub>, <i>f</i><sub>2</sub>, …, <i>f</i><sub><i>N</i> − 2</sub> <span class="text"> <span class="textnormal">and</span></span> <i>f</i><sub>1</sub>, <i>f</i><sub>3</sub>, …, <i>f</i><sub><i>N</i> − 1</sub>.
</div>
</tt>An <span class="formula"><i>N</i> ⁄ 2</span> FFT is applied separately to the even and odd parts. The odd part is multiplied by the twiddle factors <span class="formula">1, <i>ω</i>, …, <i>ω</i><sup><i>N</i> ⁄ 2 − 1</sup></span>. The FFT of size <span class="formula"><i>N</i></span> is generated by adding and subtracting corresponding points in the even and odd parts.
</div>
<div class="Indented">
Because <span class="formula"><i>N</i> ⁄ 2</span> is also a power of <span class="formula">2</span>, the FFTs of size <span class="formula"><i>N</i> ⁄ 2</span> are effected using the same strategy. Thus, the even part and odd part are once again separated into even and odd parts to obtain four lists of numbers. Repeated separation into even and odd parts leads to the bit-reversed permutation, which is perhaps the most important element in an efficient implementation of the FFT.
</div>
<div class="Indented">
The <span class="formula"><i>N</i></span> data items are indexed using binary numbers with <span class="formula"><i>n</i></span> bits. The bit-reversed permutation is obtained by reversing indices in binary and reordering the data using the reversed binary numbers. The last bit moves to the first position, and it is immediately evident that the bit reversal separates the data into even and odd parts. If <span class="formula"><i>N</i> = 8</span>, the bit-reversed permutation is given by<div class="formula">
<i>f</i><sub>0</sub>, <i>f</i><sub>4</sub>, <i>f</i><sub>2</sub>, <i>f</i><sub>6</sub>, <i>f</i><sub>1</sub>, <i>f</i><sub>5</sub>, <i>f</i><sub>3</sub>, <i>f</i><sub>7</sub>.
</div>
 Once the data is bit-reversed, the FFT begins by operating on successive pairs of numbers.
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:fft-pow2-structure">2.5↓</a> illustrates the power of <span class="formula">2</span> FFT assuming the data to be in bit-reversed order. The innermost (lowermost in the figure) step operates on consecutive pairs. The next levels operate on quartets, octets, and so on. At each level, the data items in solid squares are first multiplied by twiddle factors. At each level, the data item in a solid square is paired with another data item in an empty square (see figure). The iteration at a level is complete, when each pair of data items is replaced by their sum and difference. Bit reversal improves the locality of memory references.
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:fft-pow2-structure">2.5↓</a> assumes that each level is complete before we move to the next level. In fact, there is a lot more freedom in the way the operations are ordered. For example, we may complete operations on the first half of the bit reversed data before operating on any of the pairs, quartets, and so on in the second half. Judicious orderings of the operations result in better usage of the memory system and better performance. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:fft-pow2-structure"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/fft.png" alt="figure FIGS/chapter1/fft.png" style="max-width: 198px; max-height: 112px;"/>
<div class="caption">
Figure 2.5 Structure of the power of <span class="formula">2</span> FFT assuming bit reversal. At each level, the solid positions are multiplied by twiddle factors and either added to or subtracted from the corresponding unfilled positions.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
The power of <span class="formula">2</span> FFT moves through <span class="formula">log<sub>2</sub><i>N</i></span> levels. The number of arithmetic operations in each level consists of <span class="formula"><i>N</i> ⁄ 2</span> twiddle factor multiplications, <span class="formula"><i>N</i> ⁄ 2</span> additions, and <span class="formula"><i>N</i> ⁄ 2</span> subtractions. Additions and subtractions have the same cost and may both be counted as additions. A complex addition is equal to two floating point double precision additions and a complex multiplication is equal to <span class="formula">4</span> floating point multiplications and two additions. Therefore, a power of <span class="formula">2</span> FFT of size <span class="formula"><i>N</i></span> costs <span class="formula">3<i>N</i>log<sub>2</sub><i>N</i></span> floating point additions and <span class="formula">2<i>N</i>log<sub>2</sub><i>N</i></span> floating point multiplications. The total cost is <span class="formula">5<i>N</i>log<sub>2</sub><i>N</i></span>. There are other variants, such as the power of <span class="formula">4</span> FFT, which have slightly lower operation counts.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.4.2">2.4.2</a> FFT using MKL<a class="Label" name="sub:libmake-FFT-using-MKL"> </a>
</h3>
<div class="Unindented">
The header file <tt>mkl_dfti.h</tt> declares the FFT functions implemented in the MKL library. The following class simplifies application of the FFT and its inverse to complex data:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class fft_mkl{
private:
	int n;
	DFTI_DESCRIPTOR_HANDLE handle;
public:
	fft_mkl(int nin); 
	~fft_mkl();
	void fwd(double *f){
		DftiComputeForward(handle, f);
	}
	void bwd(double *f){
		DftiComputeBackward(handle, f);
	}
};
</pre>
</div>

</div>
<div class="Indented">
Unlike the <tt>Vector</tt> class from before, <tt>fft_mkl</tt> is a narrowly defined class. It does just one thing, which is to provide an interface to MKL’s FFT for complex one-dimensional data. It is typical of the way we use C++.
</div>
<div class="Indented">
The class may be used as shown below to apply the FFT to a complex data of size <span class="formula"><i>n</i></span>. It is assumed that <tt>v</tt> is of type <tt>double *</tt> pointing to <span class="formula">2<i>n</i></span> or more <tt>double</tt> locations.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">for(int i=0; i &lt; n; i++){
	v[2*i] = rand()*1.0/RAND_MAX - 0.5;
	v[2*i+1] = rand()*1.0/RAND_MAX - 0.5;
}
fft_mkl fft(n);
fft.fwd(v);
</pre>
</div>

</div>
<div class="Indented">
Here the array <tt>v[]</tt> is initialized with uniformly distributed random numbers and its FFT is taken. All the details of the MKL library are cleanly hidden away.
</div>
<div class="Indented">
The member functions <tt>fft_mkl::fwd()</tt> and <tt>fft_mkl::bwd()</tt> are defined completely within the class definition. These member functions correspond to <a class="Reference" href="#eq:fft-fwd">(2.1↑)</a> and <a class="Reference" href="#eq:fft-inverse">(2.2↑)</a>, respectively. 
</div>
<div class="Indented">
The two private data members of the class <tt>fft_mkl</tt> are <tt>n</tt>, for saving the dimension of the transform, and <tt>handle</tt>. The <tt>handle</tt> is initialized by the constructor and used for effecting the transform. The constructor is defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>fft_mkl::fft_mkl(int nin)
<span class="number-left">2</span>  :n(nin)
<span class="number-left">3</span>{
<span class="number-left">4</span>	DftiCreateDescriptor(&amp;handle,
<span class="number-left">5</span>				DFTI_DOUBLE,DFTI_COMPLEX, 1,n);
<span class="number-left">6</span>	DftiSetValue(handle, DFTI_FORWARD_SCALE, 1.0/n);
<span class="number-left">7</span>	DftiSetValue(handle, DFTI_PLACEMENT, DFTI_INPLACE);
<span class="number-left">8</span>	DftiCommitDescriptor(handle);
<span class="number-left">9</span>}
</pre>
</div>

</div>
<div class="Indented">
The colon initialization on line 2 shows C++ syntax we have not encountered so far. Its effect is to call the constructor for the class object <tt>n</tt> with <tt>nin</tt> as the argument. It is essential for initializing class members, which are themselves class objects. But <tt>n</tt> is an <tt>int</tt>, which is a basic type. Here the effect is the same as saying <tt>n=nin</tt> just after line 3. 
</div>
<div class="Indented">
The MKL call on lines 4 and 5 sets up <tt>handle</tt>. The function call tells MKL that the data is complex double precision, one-dimensional, and of size <tt>n</tt>. The <tt>handle</tt> is a pointer to a data structure called a descriptor. Within that data structure, MKL can store a variety of information. For example, it can precompute and store twiddle factors. 
</div>
<div class="Indented">
The forward transform computed by default is unnormalized. The division by <span class="formula"><i>N</i></span> in <a class="Reference" href="#eq:fft-fwd">(2.1↑)</a> is omitted. Line 6 tells MKL to normalize the forward transform. Line 7 tells MKL to compute in-place transforms. In-place transforms modify the array in place. The forward transform is effected by the MKL call
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">DftiComputeForward(handle, f);
</pre>
</div>

</div>
<div class="Indented">
in the class <tt>fft_mkl</tt> because the transform is in place. If the transform were out-of-place, we would need a call such as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">DftiComputeForward(handle, f, fout);
</pre>
</div>

</div>
<div class="Indented">
In this call, the data in <tt>f</tt> will be unchanged, and the result of the transform will be left in <tt>fout</tt>. The FFT is naturally implemented in place. The out-of-place FFT is slower because it touches more data. 
</div>
<div class="Indented">
The descriptor that <tt>handle</tt> points to becomes usable only after the commit on line 8. It is here that MKL actually computes and saves the twiddle factors, and so on. The order in which the FFT operations are applied may also be determined and saved here.
</div>
<div class="Indented">
The class destructor is defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">fft_mkl::~fft_mkl(){
	DftiFreeDescriptor(&amp;handle);
}
</pre>
</div>

</div>
<div class="Indented">
The definition of the <tt>fft_mkl</tt> class is now complete.
</div>
<div class="Indented">
The C language has a facility for defining functions with a variable number of arguments. An example of such a function is <tt>printf()</tt>. The first argument to <tt>printf()</tt> is a format string, and the types and number of the subsequent arguments depend on the format string. Like <tt>printf</tt>, the MKL function <tt>DftiComputeForward()</tt> uses the <tt>varargs</tt> facility to handle both in-place and out-of-place transforms.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.4.3">2.4.3</a> FFT using FFTW<a class="Label" name="sub:libmake-FFT-using-FFTW"> </a>
</h3>
<div class="Unindented">
The FFTW library<span class="FootOuter"><span class="SupFootMarker"> [39] </span><span class="HoverFoot"><span class="SupFootMarker"> [39] </span>See <tt>www.fftw.org</tt><span class="default"> as well as <span class="bibcites">[<a class="bibliocite" name="cite-22" href="#biblio-22"><span class="bib-index">22</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-26" href="#biblio-26"><span class="bib-index">26</span></a>]</span>.<i> </i></span></span></span> has a much cleaner interface than MKL, although that may not be clear from the one-dimensional complex-to-complex case we deal with. The header file is <tt>fftw3.h</tt>. The following tightly defined class offers a means to use FFTW functions for the FFT:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class fft_fftw{
private:
	int n;
	fftw_plan pf;
	fftw_plan pb;
public:
	fft_fftw(int nin); 
	~fft_fftw();
	void fwd(double *f){
		assrt((long)f%16 == 0);
		fftw_execute_dft(pf, (fftw_complex *)f, 
		                     (fftw_complex *)f);
		for(int i=0; i &lt; 2*n; i++)
			f[i] /= n;
	}
	void bwd(double *f){
		assrt((long)f%16 == 0);
		fftw_execute_dft(pb, (fftw_complex *)f, 
		                     (fftw_complex *)f);
	}
};
</pre>
</div>

</div>
<div class="Indented">
The sole task of the <tt>fft_fftw</tt> class is to offer an easy interface to FFTW transforms for one-dimensional complex data. FFTW stores <tt>fftw_plan</tt>s instead of a <tt>handle</tt> as with MKL. There are different plans for forward and backward transforms. The <tt>fwd()</tt> and <tt>bwd() </tt>member functions are implemented within the class definition.
</div>
<div class="Indented">
Both the member functions use <tt>assrt()</tt> (defined in <tt>utils/utils.hh</tt>) to verify that the pointer <tt>f</tt> is <span class="formula">16</span>-byte aligned. A pointer is <span class="formula">16</span>-byte aligned if it is divisible by <span class="formula">16</span> or, equivalently, if the last <span class="formula">4</span> bits are zero. Because of the way the FFTW plans are set up, the pointers must be <span class="formula">16</span>-byte aligned for correctness. FFTW recognizes that the transforms are in place because the same pointer <tt>f</tt> is used as the input and output argument to <tt>fftw_execute_dft()</tt>.
</div>
<div class="Indented">
FFTW does not offer a facility for normalizing the forward transform. Therefore, the member function <tt>fwd()</tt> normalizes explicitly using a for-loop. As we will see, this seemingly innocuous bit of code nearly halves the program speed.
</div>
<div class="Indented">
The constructor and destructor for <tt>fft_fftw</tt> are defined below. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">fft_fftw::fft_fftw(int nin)
	:n(nin)
{
	double *f = (double *)
			_mm_malloc(2*n*sizeof(double), 16);
	fftw_complex *ff  = (fftw_complex *)f;
	pf = fftw_plan_dft_1d(n, ff, ff, -1, 
							FFTW_MEASURE);
	pb = fftw_plan_dft_1d(n, ff, ff,  1, 
							FFTW_MEASURE);
	_mm_free(f);
}
​
fft_fftw::~fft_fftw(){
	fftw_destroy_plan(pf);
	fftw_destroy_plan(pb);
}
</pre>
</div>

</div>
<div class="Indented">
The constructor aligns the pointer <tt>f</tt> with <span class="formula">16</span>-byte boundaries by using <tt>_mm_malloc()</tt> instead of <tt>malloc()</tt>. The forward plan <tt>pf</tt> uses the flag <span class="formula"> − 1</span>, while the backward plan uses the flag <span class="formula">1</span>. These refer to the sign of the exponents in <a class="Reference" href="#eq:fft-fwd">(2.1↑)</a> and <a class="Reference" href="#eq:fft-inverse">(2.2↑)</a>, respectively. Both the forward and backward plans use the <tt>FFTW_MEASURE</tt> flag. The FFTW library uses the planning stage to measure different implementations of the FFT in an attempt to pick a good one. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.4.4">2.4.4</a> Cycles and histograms<a class="Label" name="sub:libmake-fft-Cycles-and-histograms"> </a>
</h3>
<div class="Unindented">
How many cycles does a one-dimensional complex FFT of dimension <span class="formula">2<sup>10</sup> = 1024</span> take? Program performance is influenced by so many factors that the question is too simple to be answered. First, we have to say how the measurement is taken and which implementation of the FFT is used. Here we assume the implementation to be from the MKL library. 
</div>
<div class="Indented">
The issue of measurement is more complicated. Suppose a single measurement is made. The cycle count is likely to be atypically large. Suppose a great number of measurements are made but while applying the FFT to the same data. This time the average or median cycle count is likely to be an underestimate. A great part of the expense of the FFT is in reading data from memory. If the same data is repeatedly transformed, the data locations will be cached near the processor core in cache memory. Caching reduces the expense of reading data. 
</div>
<div class="Indented">
We measure FFTs of dimension <span class="formula"><i>N</i> = 2<sup>10</sup></span> in a way that mimics what we consider to be a realistic scenario. We line up <span class="formula">10<sup>6</sup></span> problem instances in one long array of <span class="formula">2 × 2<sup>10</sup> × 10<sup>6</sup></span> double-precision numbers (the factor <span class="formula">2</span> at the front accounts for complex data). This array is <span class="formula">16</span> GB. We successively apply the inverse FFT to each problem instance and record <span class="formula">10<sup>6</sup></span> cycle counts. The median (or average) cycle count obtained is likely to be a fair estimate of the cost of an FFT of dimension <span class="formula">1024</span> in a large computation.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:fft-histogram-cycles"> </a><div class="multifigure">
<span class="float">
<div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/fft_trial2_view1-gray.png" alt="figure FIGS/chapter1/fft_trial2_view1-gray.png" style="width: 175px; max-width: 585px; height: 132px; max-height: 441px;"/>
<div class="caption">
(a) 
</div>

</div>

</div>

</span>
<span class="float">
<div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/fft_trial1_view1-gray.png" alt="figure FIGS/chapter1/fft_trial1_view1-gray.png" style="width: 175px; max-width: 585px; height: 132px; max-height: 441px;"/>

</div>
<div class="caption">
(b) 
</div>

</div>

</span>
<span class="float">
<div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/fft_trial1_view2-gray.png" alt="figure FIGS/chapter1/fft_trial1_view2-gray.png" style="width: 175px; max-width: 585px; height: 132px; max-height: 441px;"/>

</div>
<div class="caption">
(c) 
</div>

</div>

</span>
<span class="float">
<div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter1/fft_trial1_view3-gray.png" alt="figure FIGS/chapter1/fft_trial1_view3-gray.png" style="width: 175px; max-width: 585px; height: 132px; max-height: 441px;"/>
<div class="caption">
(d) 
</div>

</div>

</div>

</span>
<div class="caption">
Figure 2.6 Histograms of cycles of <span class="formula">10<sup>6</sup></span> trials of 1D FFT of size <span class="formula">1024</span> on <span class="formula">2.66</span> GHz SSE2 machines.
</div>

</div>

</div>

</div>
<div class="Indented">
The statistics of the <span class="formula">10<sup>6</sup></span> measurements does not follow the normal law or any such well-known probability distribution. The histograms in figure <a class="Reference" href="#fig:fft-histogram-cycles">2.6↑</a> show the peculiar nature of cycle statistics. Parts (a) and (b) of the figure show histograms of measurements taken on two distinct computers (<span class="formula">2.66</span> GHz SSE2 machines)<span class="FootOuter"><span class="SupFootMarker"> [40] </span><span class="HoverFoot"><span class="SupFootMarker"> [40] </span>For more information about the machine, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>.</span></span> of identical specifications. Both measurements have medians of around <span class="formula">20, 000</span> cycles. However, the histograms are noticeably different. On both computers, the histograms have a big bump near the median and a much tinier bump to the right of the median. The tinier bump is beyond <span class="formula">25, 000</span> cycles and barely visible in figure <a class="Reference" href="#fig:fft-histogram-cycles">2.6↑</a>(b). The tinier bump is much closer to the median in (a). 
</div>
<div class="Indented">
Part (c) of figure <a class="Reference" href="#fig:fft-histogram-cycles">2.6↑</a> enlarges the histogram of part (b) near its median. This histogram looks more like a smooth distribution, but it is quite unlike the normal law. There is a marked protrusion to the right of the median. Part (d) enlarges the histogram of part (b) near the tiny bump. 
</div>
<div class="Indented">
Why is the statistical distribution of the cycle counts so peculiar? The multi-bump nature of the histograms is a consequence of the multiple factors at play. The FFT fetches data from memory and subjects that data to a number of arithmetic operations. Fetching data from memory on to the processor is the job of memory controllers. Because the same data item is accessed multiple times during a single transform (see figure <a class="Reference" href="#fig:fft-pow2-structure">2.5↑</a>), the data locations are often sourced from caches. Moving data between the caches and the processor is handled by cache controllers. The execution units inside the processor apply arithmetic operations to data in the registers. The cycle count is influenced by the design of the memory controllers, cache controllers, and the execution units within the processor. It is reasonable to conjecture that the protrusion to the right of the median in figure <a class="Reference" href="#fig:fft-histogram-cycles">2.6↑</a>(c) is due to some feature of the memory system. There are a large number of discontinuities (similar to if-statements in C/C++ programs) in hardware design, implying that some features are only occasionally excited. 
</div>
<div class="Indented">
The tiny bump to the right of the median shown in figure <a class="Reference" href="#fig:fft-histogram-cycles">2.6↑</a>(d) appears to be due to the Linux kernel. Even on a computer system where <span class="formula">11</span> out of <span class="formula">12</span> processor cores are idling, the program receives timer interrupts once in several milliseconds. The kernel uses timer interrupts to ensure fairness in scheduling, gather statistics for its own use, and other purposes. It is conjectured that the tiny bump is due to timer interrupts or some other activity of the Linux kernel. If the program runs for a long time, the kernel will change the way it issues timer interrupts. To even speak of the distribution function of the number of cycles consumed by an FFT of size <span class="formula"><i>n</i> = 1024</span> may not be correct.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-2.4.5">2.4.5</a> Optimality of FFT implementations<a class="Label" name="sub:libmake-fft-Optimality"> </a>
</h3>
<div class="Unindented">
What is the fastest possible speed of an FFT implementation? The many system features that intrude into the histograms hint that this question may not have a straightforward answer. However, a discussion is worthwhile. It helps us understand what makes an FFT implementation efficient. The discussion is based on a <span class="formula">2.2</span> GHz AVX machine.<span class="FootOuter"><span class="SupFootMarker"> [41] </span><span class="HoverFoot"><span class="SupFootMarker"> [41] </span>For more information about the machine, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>.</span></span> 
</div>
<div class="Indented">
The power of <span class="formula">2</span> FFT performs <span class="formula">5<i>N</i>log<sub>2</sub><i>N</i></span> double-precision floating point operations. Additions and multiplications are in the ratio <span class="formula">3:2</span>. Thanks to instruction-level parallelism and <span class="formula">256</span>-bit YMM registers, a single AVX processor core can complete four additions and four multiplications every cycle. If we consider arithmetic operations alone, a theoretical lower bound is <span class="formula">.75<i>N</i>log<sub>2</sub><i>N</i></span> cycles.
</div>
<div class="Indented">
We would be justified in ignoring memory accesses if the number of arithmetic operation for each memory access were large. The total number of bytes that must be accessed is <span class="formula">16<i>N</i></span> (<span class="formula">16</span> bytes for each complex number). For large <span class="formula"><i>N</i></span>, we indeed have <span class="formula">0.75log<sub>2</sub><i>N</i> &gt; 16</span>. However, the structure of the FFT does not allow the outer iterations (upper levels in figure <a class="Reference" href="#fig:fft-pow2-structure">2.5↑</a>) to be cached if <span class="formula"><i>N</i></span> is large. The inner iterations (lower in the figure) operate on small packets of data, such as pairs or quartets. Caching can be effective for the inner iterations. As the size of the data packets in the outer iterations becomes comparable to cache size, caching becomes less and less effective. The FFT is caught between two opposing currents. On the one hand, large <span class="formula"><i>N</i></span> means more arithmetic operations per item of data. On the other hand, the data items accessed in the outer iterations cannot be cached as effectively.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:fft-cycles-mkl-fftw-nr"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>N</i></span>
</td>
<td align="center" valign="top">
MKL
</td>
<td align="center" valign="top">
FFTW
</td>
<td align="center" valign="top">
Numerical Recipes
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">32</span>
</td>
<td align="center" valign="top">
<span class="formula">0.86</span>
</td>
<td align="center" valign="top">
<span class="formula">1.6</span>
</td>
<td align="center" valign="top">
<span class="formula">8.12</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">64</span>
</td>
<td align="center" valign="top">
<span class="formula">1.14</span>
</td>
<td align="center" valign="top">
<span class="formula">1.62</span>
</td>
<td align="center" valign="top">
<span class="formula">6.71</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">80</span>
</td>
<td align="center" valign="top">
<span class="formula">1.72</span>
</td>
<td align="center" valign="top">
<span class="formula">1.58</span>
</td>
<td align="center" valign="top">
*
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">8 × 3 × 7</span>
</td>
<td align="center" valign="top">
<span class="formula">1.76</span>
</td>
<td align="center" valign="top">
<span class="formula">1.96</span>
</td>
<td align="center" valign="top">
*
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">192</span>
</td>
<td align="center" valign="top">
<span class="formula">1.82</span>
</td>
<td align="center" valign="top">
<span class="formula">1.84</span>
</td>
<td align="center" valign="top">
*
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024</span>
</td>
<td align="center" valign="top">
<span class="formula">1.83</span>
</td>
<td align="center" valign="top">
<span class="formula">1.51</span>
</td>
<td align="center" valign="top">
<span class="formula">5.69</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024 × 128</span>
</td>
<td align="center" valign="top">
<span class="formula">2.05</span>
</td>
<td align="center" valign="top">
<span class="formula">1.66</span>
</td>
<td align="center" valign="top">
<span class="formula">14.33</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024 × 1024</span>
</td>
<td align="center" valign="top">
<span class="formula">1.89</span>
</td>
<td align="center" valign="top">
<span class="formula">2.96</span>
</td>
<td align="center" valign="top">
<span class="formula">17.62</span>
</td>

</tr>

</table>
<div class="caption">
Table 2.3 Number of cycles consumed by the FFT divided by <span class="formula"><i>N</i>log<sub>2</sub><i>N</i></span> on a <span class="formula">2.2</span> GHz AVX machine.
</div>

</div>

</div>

</div>
Table <a class="Reference" href="#tab:fft-cycles-mkl-fftw-nr">2.3↑</a> shows the cycle counts for MKL, FFTW, and Numerical Recipes<span class="FootOuter"><span class="SupFootMarker"> [42] </span><span class="HoverFoot"><span class="SupFootMarker"> [42] </span><span class="bibcites">[<a class="bibliocite" name="cite-27" href="#biblio-27"><span class="bib-index">27</span></a>]</span> is a handy and wide-ranging, if brisk, work on numerical methods.. </span></span> for a variety of <span class="formula"><i>N</i></span>. Problem instances sufficiently numerous to occupy <span class="formula">8</span> GB of memory were used for each <span class="formula"><i>N</i></span>. The inverse FFT was successively applied to the problem instances. The reported figures are medians. The Numerical Recipes implementation applies only to <span class="formula"><i>N</i> = 2<sup><i>n</i></sup></span>.
</div>
<div class="Indented">
Numerical Recipes is <span class="formula">5</span> to <span class="formula">10</span> times worse than the optimized libraries. The sheer volume of effort needed to produce optimized implementations such as MKL and FFTW will become clear in later chapters. There is no doubt that the scientific programmer must seek out optimized libraries as far as possible. MKL is faster than FFTW for <span class="formula"><i>N</i> = 32</span> and <span class="formula"><i>N</i> = 64</span>. Overall, the two libraries are comparable.
</div>
<div class="Indented">
None of the operation counts match the theoretical lower bound of <span class="formula">0.75<i>N</i>log<sub>2</sub><i>N</i></span> cycles, although MKL gets quite close for <span class="formula"><i>N</i> = 32</span>. For large <span class="formula"><i>N</i></span>, the deviation from the theoretical bound is greater. We may be tempted to conclude that this is because of the memory references at the outer iterations of the FFT. In fact, there is so much cache on the <span class="formula">2.2</span> GHz AVX machine that all the data for even the problem instance with <span class="formula"><i>N</i> = 1024 × 1024</span> can comfortably fit in L3 cache. It is unclear whether optimizations to hide the cost of streaming data items from L3 cache are possible or whether they have been attempted. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:fft-in-cache-mkl-fftw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>N</i></span>
</td>
<td align="center" valign="top">
MKL
</td>
<td align="center" valign="top">
FFTW
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">32</span>
</td>
<td align="center" valign="top">
<span class="formula">0.81</span>
</td>
<td align="center" valign="top">
<span class="formula">1.62</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">64</span>
</td>
<td align="center" valign="top">
<span class="formula">0.67</span>
</td>
<td align="center" valign="top">
<span class="formula">1.16</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">192</span>
</td>
<td align="center" valign="top">
<span class="formula">0.90</span>
</td>
<td align="center" valign="top">
<span class="formula">1.04</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024</span>
</td>
<td align="center" valign="top">
<span class="formula">0.88</span>
</td>
<td align="center" valign="top">
<span class="formula">0.89</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024 × 128</span>
</td>
<td align="center" valign="top">
<span class="formula">1.16</span>
</td>
<td align="center" valign="top">
<span class="formula">1.48</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024 × 1024</span>
</td>
<td align="center" valign="top">
<span class="formula">1.50</span>
</td>
<td align="center" valign="top">
<span class="formula">2.51</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 2.4 Number of cycles consumed by in-cache FFT divided by <span class="formula"><i>N</i>log<sub>2</sub><i>N</i></span> on a <span class="formula">2.2</span> GHz AVX machine.
</div>

</div>

</div>

</div>
<div class="Indented">
In table <a class="Reference" href="#tab:fft-in-cache-mkl-fftw">2.4↑</a>, the cycle counts are much closer to the <span class="formula">0.75<i>N</i>log<sub>2</sub><i>N</i></span> lower bound. These in-cache cycle counts were obtained by applying the inverse FFT to the same array of <span class="formula">2<i>N</i></span> double-precision numbers initialized to <span class="formula">0</span>. In fact, MKL goes below the theoretical bound for <span class="formula"><i>N</i> = 4<sup>3</sup> = 64</span>. 
</div>
<div class="Indented">
The power of <span class="formula">4</span> FFT is applicable to <span class="formula"><i>N</i> = 4<sup>3</sup></span> and has a lower operation count than <span class="formula">5<i>N</i>log<sub>2</sub><i>N</i></span>.<span class="FootOuter"><span class="SupFootMarker"> [43] </span><span class="HoverFoot"><span class="SupFootMarker"> [43] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-43" href="#biblio-43"><span class="bib-index">43</span></a>]</span>.</span></span> MKL may be using an algorithm with an operation count lower than <span class="formula">5<i>N</i>log<sub>2</sub><i>N</i></span>. The <span class="formula">2.2</span> GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>) has an in-core frequency that can exceed <span class="formula">2.2</span> GHz. The in-core acceleration may as well be the reason for the observed speed of MKL’s <span class="formula"><i>N</i> = 64</span> FFT being greater than the theoretical bound.
</div>
<div class="Indented">
The in-cache numbers for MKL are better than for FFTW and sometimes significantly better. The fraction of scientific programs that remain in cache appears to be fairly large, and the in-cache advantage of MKL over FFTW is not insignificant. In the next chapter, we will discuss the type of instruction pipeline optimizations needed to achieve MKL speeds. MKL is certainly optimizing better for the instruction pipeline than FFTW.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:fft-fwd-mkl-fftw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>N</i></span>
</td>
<td align="center" valign="top">
MKL
</td>
<td align="center" valign="top">
FFTW
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">32</span>
</td>
<td align="center" valign="top">
<span class="formula">0.975</span>
</td>
<td align="center" valign="top">
<span class="formula">4.86</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">64</span>
</td>
<td align="center" valign="top">
<span class="formula">1.61</span>
</td>
<td align="center" valign="top">
<span class="formula">4.04</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1024</span>
</td>
<td align="center" valign="top">
<span class="formula">2.00</span>
</td>
<td align="center" valign="top">
<span class="formula">3.16</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 2.5 Number of cycles consumed by forward FFT divided by <span class="formula"><i>N</i>log<sub>2</sub><i>N</i></span> on a <span class="formula">2.2</span> GHz AVX machine.
</div>

</div>

</div>
So far, we have only been applying the inverse FFT. The inverse discrete transform <a class="Reference" href="#eq:fft-inverse">(2.2↑)</a> is unnormalized, and there is no need to divide by <span class="formula"><i>N</i></span> at the end. In table <a class="Reference" href="#tab:fft-fwd-mkl-fftw">2.5↑</a>, we turn to the forward FFT, which must be normalized by dividing by <span class="formula"><i>N</i></span>, and get a nasty surprise. The performance of FFTW has deteriorated by more than a factor of <span class="formula">4</span> in one instance and by about a factor of <span class="formula">2</span> in the other two instances.
</div>
<div class="Indented">
How can as simple an operation as dividing by <span class="formula"><i>N</i></span> cause such sharp deterioration? The answer will become clearer in the next chapter. The loop for dividing by <span class="formula"><i>N</i></span> in the member function <tt>fwd()</tt> of the class <tt>fft_fftw</tt> looks innocuous: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">for(int i=0; i &lt; 2*n; i++)
      f[i] /= n;
</pre>
</div>

</div>
<div class="Indented">
The member function <tt>fwd()</tt> is defined within the class  and therefore gets inlined. As we have already indicated in passing, <tt>icpc</tt> does not seem to optimize loops of inlined functions adequately.<span class="FootOuter"><span class="SupFootMarker"> [44] </span><span class="HoverFoot"><span class="SupFootMarker"> [44] </span>This statement about <tt>icpc</tt><span class="default"> has been verified for versions 10 through 15.</span></span></span> The loop for dividing by <span class="formula"><i>N</i></span> can be optimized by turning the division into multiplication by <span class="formula">1.0 ⁄ <i>N</i></span>. Multiplications are much faster than divisions. However, <tt>icpc</tt> does not do this optimization nor does it attempt to exploit loop-level parallelism---facts that can be ascertained by inspecting the assembly code.
</div>
<div class="Indented">
The next chapter shows how to inspect assembly code and what to expect from the compiler. Optimizations that may appear obvious to the programmer are sometimes not effected by compilers, as we just saw.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  If <span class="formula"><i>ω</i> = exp(2<i>π</i><span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root"> − 1</span><span class="ignored">)</span></span> ⁄ <i>N</i>)</span>, prove that <span class="formula"><i>ω</i><sup><i>k</i></sup> ≠ 1</span> and <span class="formula"><i>ω</i><sup><i>kN</i></sup> = 1</span> for <span class="formula"><i>k</i> = 1, 2, …, <i>N</i> − 1</span>. Prove that <div class="formula">
<span class="limits"><sup class="limit"><i>N</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>j</i> = 0</sub></span><i>ω</i><sup><i>kj</i></sup> = 0
</div>
for <span class="formula"><i>k</i> = 1, 2, …, <i>N</i> − 1</span>. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the previous exercise to argue that the matrix with <span class="formula">(<i>j</i>, <i>k</i>)</span>th entry equal to <span class="formula"><i>ω</i><sup><i>jk</i></sup> ⁄ <span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root"><i>N</i></span><span class="ignored">)</span></span></span> has orthonormal columns. Conclude that the transforms <a class="Reference" href="#eq:fft-fwd">(2.1↑)</a> and <a class="Reference" href="#eq:fft-inverse">(2.2↑)</a> are inverses of each other.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The inverse discrete Fourier transform is defined by <div class="formula">
<i>f</i><sub><i>j</i></sub> = <span class="limits"><sup class="limit"><i>N</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>k</i> = 0</sub></span><i>ω</i><sup><i>jk</i></sup><i>f̂</i><sub><i>k</i></sub>, 
</div>
where <span class="formula"><i>ω</i> = exp(2<i>π</i><span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root"> − 1</span><span class="ignored">)</span></span> ⁄ <i>N</i>)</span>. Suppose <span class="formula"><i>N</i></span> is even and <span class="formula"><i>N</i> = 2<i>n</i></span>. We may write <span class="formula"><i>k</i> = 2ℓ + <i>p</i></span>, with <span class="formula"><i>p</i> = 0</span> or <span class="formula"><i>p</i> = 1</span>, and decompose the sum as <div class="formula">
<span class="environment"><span class="arrayrow">
<span class="arraycell align-r">
<i>f</i><sub><i>j</i></sub>
</span>
<span class="arraycell align-l">
 = <span class="limits"><sup class="limit"><i>n</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit">ℓ = 0</sub></span><span class="limits"><sup class="limit">1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>p</i> = 0</sub></span><i>ω</i><sup>2<i>j</i>ℓ</sup><i>ω</i><sup><i>jp</i></sup><i>f̂</i><sub>2ℓ + <i>p</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
 
</span>
<span class="arraycell align-l">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">

</span>
<span class="arraycell align-l">
 = <span class="limits"><sup class="limit"><i>n</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit">ℓ = 0</sub></span><i>ω</i><sup>2<i>j</i>ℓ</sup><i>f̂</i><sub>2ℓ</sub> + <i>ω</i><sup><i>j</i></sup><span class="limits"><sup class="limit"><i>n</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit">ℓ = 0</sub></span><i>ω</i><sup>2<i>k</i>ℓ</sup><i>f̂</i><sub>2ℓ + 1</sub>.
</span>

</span>
</span>
</div>
Notice that <span class="formula"><i>ω</i><sup>2</sup> = exp(2<i>π</i><span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root"> − 1</span><span class="ignored">)</span></span> ⁄ <i>n</i>)</span> and interpret the two summations above as inverse discrete Fourier transforms of dimension <span class="formula"><i>n</i></span>. Explain how to reduce a transform of dimension <span class="formula"><i>N</i> = 2<i>n</i></span> to two transforms of dimension <span class="formula"><i>n</i></span>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose <span class="formula"><i>N</i> = 2<sup><i>n</i></sup>.</span> The array <tt>a[]</tt> of dimension <span class="formula"><i>N</i></span> may be indexed using bit sequences of length <span class="formula"><i>n</i></span> with the index ranging from <span class="formula">0 = 00...0</span> to <span class="formula"><i>N</i> − 1 = 11...1</span>. In the bit-reversed permutation, <tt>a[j]</tt> and <tt>a[k]</tt> are interchanged if the bit sequence of length <span class="formula"><i>n</i></span> corresponding to <span class="formula"><i>k</i></span> is the reversal of the one corresponding to <span class="formula"><i>j</i></span>.
</div>
<ul>
<li class="nested">
<ul>
<li>
Write a program to effect the bit-reversed permutation of an array of <tt>double</tt>s in place.
</li>
<li>
Assume that <tt>a[]</tt> is an array of complex numbers, with each complex number represented using two adjacent <tt>double</tt>s. Write a program to effect the bit-reversed permutation of <tt>a[]</tt> in place.
</li>

</ul>

</li>

</ul>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Let <span class="formula"><i>f</i>(<i>x</i>) = |sin(<i>x</i>)|</span>, <span class="formula"><i>x</i><sub><i>j</i></sub> = 2<i>π</i><i>j</i> ⁄ <i>N</i></span>, and <span class="formula"><i>f</i><sub><i>j</i></sub> = <i>f</i>(<i>x</i><sub><i>j</i></sub>)</span>. Graph the discrete Fourier transform of <span class="formula"><i>f</i><sub><i>j</i></sub></span> with <span class="formula"><i>N</i> = 10<sup>4</sup></span>. Repeat with <span class="formula"><i>f</i>(<i>x</i>) = sin(sin(<i>x</i>))</span>. What do you observe?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Let <span class="formula"><i>N</i> = 1024</span> and initialize a complex array of size <span class="formula"><i>N</i></span> to <span class="formula">0</span>. Apply MKL’s inverse FFT to the same array <span class="formula">10<sup>6</sup></span> times. Histogram the <span class="formula">10<sup>6</sup></span> cycle counts (you will need the <tt>TimeStamp</tt> class described in the next chapter). If your machine has L1 data cache of at least <span class="formula">16</span> KB, you will observe something closer to the normal distribution than the plots in figure <a class="Reference" href="#fig:fft-histogram-cycles">2.6↑</a>. Why? Fit the normal law and calculate the mean and variance of the fit.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Tables <a class="Reference" href="#tab:fft-cycles-mkl-fftw-nr">2.3↑</a> and <a class="Reference" href="#tab:fft-in-cache-mkl-fftw">2.4↑</a> report the number of cycles consumed by an in-place, complex one-dimensional FFT of dimension <span class="formula"><i>N</i></span>, the latter with data in cache and the former with data out of cache. For each value of <span class="formula"><i>N</i></span> in the tables, find the bandwidth to memory realized in bytes/cycles as well as GB/s, under the assumption that all the extra cycles for out-of-cache FFT are due to data access. Investigate the possibility that the in-cache numbers are artificially low because the FFT operates on an array that is always zero.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Assuming versions 11 or 12 of the <tt>icpc</tt> compiler, the member function <tt>fwd()</tt> of the class <tt>fft_fftw</tt> may be sped up as follows. Remove the for-loop for dividing the array <tt>f[]</tt> by <span class="formula"><i>n</i></span>. Instead, call a function <tt>scale_fwd(double *f, int n)</tt>. The function first calculates <tt>double x = 1.0/n</tt> and then multiplies each of the <span class="formula">2<i>n</i></span> <tt>double </tt>entries of <tt>f[]</tt> by <tt>x</tt>. Compile using <tt>-fno-inline-function</tt>s. Recalculate table <a class="Reference" href="#tab:fft-fwd-mkl-fftw">2.5↑</a> and show that the forward transform with FFTW is now much faster.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-2.5">2.5</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-13"><span class="bib-index">13</span></a>] </span> <span class="bib-authors">C. van Loan</span>: <i><span class="bib-title">Computational Frameworks for the Fast Fourier Transform</span></i>. <span class="bib-publisher">SIAM</span>, <span class="bib-year">1992</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-14"><span class="bib-index">14</span></a>] </span> <span class="bib-authors">C.L. Lawson, R.J. Hanson, D.R. Kincaid, F.T. Krogh</span>: “<span class="bib-title">Basic linear algebra subprograms for Fortran usage</span>”, <i><span class="bib-journal">ACM TOMS</span></i>, pp. <span class="bib-pages">308-323</span>, <span class="bib-year">1979</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-15"><span class="bib-index">15</span></a>] </span> <span class="bib-authors">E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, D. Sorensen</span>: <i><span class="bib-title">LAPACK User's Guide</span></i>. <span class="bib-publisher">SIAM</span>, <span class="bib-year">1999</span>. <span class="bib-note">1st ed. 1987</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-16"><span class="bib-index">16</span></a>] </span> <span class="bib-authors">E. Elmroth, F. Gustavson, I. Jonsson, B. Kågström</span>: “<span class="bib-title">Recursive blocked algorithms and hybrid data structures for dense matrix library software</span>”, <i><span class="bib-journal">SIAM Review</span></i>, pp. <span class="bib-pages">3-45</span>, <span class="bib-year">2004</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-17"><span class="bib-index">17</span></a>] </span> <span class="bib-authors">J.J. Dongarra, J. Du Croz, S. Hammarling, I. Duff</span>: “<span class="bib-title">A set of level 3 basic linear algebra subprograms</span>”, <i><span class="bib-journal">ACM TOMS</span></i>, pp. <span class="bib-pages">1-17</span>, <span class="bib-year">1990</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-18"><span class="bib-index">18</span></a>] </span> <span class="bib-authors">J.J. Dongarra, J. Du Croz, S. Hammarling, R.J. Hanson</span>: “<span class="bib-title">An extended set of Fortran basic linear algebra subprograms</span>”, <i><span class="bib-journal">ACM TOMS</span></i>, pp. <span class="bib-pages">1-17</span>, <span class="bib-year">1988</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-19"><span class="bib-index">19</span></a>] </span> <span class="bib-authors">J.R. Levine</span>: <i><span class="bib-title">Linkers and Loaders</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">1999</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-20"><span class="bib-index">20</span></a>] </span> <span class="bib-authors">J.W. Cooley, J.W. Tukey</span>: “<span class="bib-title">An algorithm for the machine calculation of complex Fourier series</span>”, <i><span class="bib-journal">Mathematics of Computation</span></i>, pp. <span class="bib-pages">297-301</span>, <span class="bib-year">1965</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-21"><span class="bib-index">21</span></a>] </span> <span class="bib-authors">J.W. Demmel</span>: <i><span class="bib-title">Applied Numerical Linear Algebra</span></i>. <span class="bib-publisher">SIAM</span>, <span class="bib-year">1997</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-22"><span class="bib-index">22</span></a>] </span> <span class="bib-authors">M. Frigo</span>: “<span class="bib-title">A fast Fourier transform compiler</span>”, <i><span class="bib-journal">Proc. 1999 ACM SIGPLAN conference</span></i>, pp. <span class="bib-pages">169-180</span>, <span class="bib-year">1999</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-23"><span class="bib-index">23</span></a>] </span> <span class="bib-authors">M. Kerrisk</span>: <i><span class="bib-title">The Linux Programming Interface</span></i>. <span class="bib-publisher">No Starch Press</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-24"><span class="bib-index">24</span></a>] </span> <span class="bib-authors">N.J. Higham</span>: <i><span class="bib-title">Accuracy and Stability of Numerical Algorithms</span></i>. <span class="bib-publisher">SIAM</span>, <span class="bib-year">2002</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-25"><span class="bib-index">25</span></a>] </span> <span class="bib-authors">S. Toledo</span>: “<span class="bib-title">Locality of reference in LU decomposition with partial pivoting</span>”, <i><span class="bib-journal">SIAM J. Matrix Anal. Appl</span></i>, pp. <span class="bib-pages">1065-1081</span>, <span class="bib-year">1997</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-26"><span class="bib-index">26</span></a>] </span> <span class="bib-authors">S.G. Johnson, M. Frigo</span>: “<span class="bib-title">The design and implementation of FFTW3</span>”, <i><span class="bib-journal">Proc. of the IEEE special issue on Program Generation, Optimization, and Platform Adaptation</span></i>, pp. <span class="bib-pages">216-233</span>, <span class="bib-year">2005</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-27"><span class="bib-index">27</span></a>] </span> <span class="bib-authors">W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery</span>: <i><span class="bib-title">Numerical Recipes 3rd edition: The Art of Scientific Computating</span></i>. <span class="bib-publisher">Cambridge University Press</span>, <span class="bib-year">2007</span>.
</p>

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-3">3</a> The Processor<a class="Label" name="chap:The-processor"> </a>
</h1>
<div class="Unindented">
The 80286 processor of 1982 had a mere 134,000 transistors. The Pentium 4 of 2001 had 4.2 million transistors. Processor packages in the x86 line for sale in 2015 have several billion transistors packed into less than <span class="formula">0.25 <span class="text"> cm</span><sup>2</sup></span>.<span class="FootOuter"><span class="SupFootMarker"> [45] </span><span class="HoverFoot"><span class="SupFootMarker"> [45] </span>This data is for Intel Xeon E5 v3 (Haswell) family.</span></span> These advances in processor technology are correlated with the rise of computer technology as a whole.
</div>
<div class="Indented">
Early x86 computers, such as the 80286 or the Pentium 4, featured a single central processor. Modern processor packages may have a dozen or so processors, and this number is constantly increasing. As a result of the explosion in the number of transistors, the design of each processor has changed considerably. A great deal of parallelism is built into modern processors, and this parallelism is active even when the thread of execution is serial.
</div>
<div class="Indented">
In contrast to this rapid progress in hardware, the software environment of the scientific programmer is nearly the same as what it was 15 years ago. In recent years, innovation in software has been driven by the Internet and mobile gadgetry and largely sidesteps the great difficulties in programming modern hardware optimally. If a compiler of 20 years ago was likely to generate excellent code for the processor of its day, that was less so in the case of a compiler of 10 years ago. Even for many simple programs, the compilers of today do not generate optimal code or anything like it.
</div>
<div class="Indented">
Good, or nearly optimal, programs for modern platforms can be written only with a knowledge of the nuances of the computer’s hardware. With that in mind, this chapter, indeed the rest of this book, will introduce programming models in close concert with computer architecture. 
</div>
<div class="Indented">
This chapter is organized into three sections. Section <a class="Reference" href="#sec:proc-overviewofx86">3.1↓</a> is an overview of x86 architecture and assembly programming. The x86 line has held sway for more than three decades to the extent that nearly 100% of the computers in use outside of the mobile world in 2015 belong to this line. The x86 architecture is vast,<span class="FootOuter"><span class="SupFootMarker"> [46] </span><span class="HoverFoot"><span class="SupFootMarker"> [46] </span>The x86 instruction set architecture is documented in Intel 64 and IA-32 Architectures’ Software Developer’s Manual, Volumes 1, 2A, 2B, 3A, 3B, and AMD64 Architecture Programmer’s Manual, Volumes 1, 2, 3, 4, 5. The entire documentation runs to more than 5,000 pages.</span></span> and our interest is limited to a tiny part of it. 
</div>
<div class="Indented">
To understand program speed and to determine whether compilers are doing a good job, it is essential to know a little about the register set and about the instruction mix. Our interest in sections <a class="Reference" href="#subsec:processor-part1-begin">3.1.1↓</a> and <a class="Reference" href="#subsec:processor-x86-assembly-prog">3.1.2↓</a> is limited to this part of the x86 architecture and no more. Although we introduce assembly programming, we do so only to the extent necessary to identify assembly code generated by compilers for inner loops and to make sense of function calls. There is no suggestion in those sections that the reader should program in assembly. The need to program in assembly is only for the rare expert. However, inasmuch as our intention is to gain understanding of programming and program speed on a rational basis, it is essential to understand the assembly code generated by compilers for inner loops.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:proc-Compiler-optimizations">3.2↓</a> is one of the more important parts of this book. Sections <a class="Reference" href="#subsec:proc-compileropt-unrolling">3.2.2↓</a> through <a class="Reference" href="#subsec:proc-compileropt-theory">3.2.7↓</a> have a single focus, which is to expose some of the inner workings of the compiler. In these sections, we inspect the assembly code generated by compilers to gain an understanding of the quality of assembly code generated for inner loops. Only a limited understanding of assembly programming is needed. It suffices to identify the type of registers and instructions that are being used in the innermost loop for the most part. In section <a class="Reference" href="#subsec:proc-compileropt-prelims">3.2.1↓</a>, we explain how to gather statistics and make tables and plots easily in C/C++. Such preliminaries simplify programming. 
</div>
<div class="Indented">
Sections <a class="Reference" href="#subsec:proc-compileropt-unrolling">3.2.2↓</a> through <a class="Reference" href="#subsec:proc-compileropt-theory">3.2.7↓</a> put to rest a popular belief that Fortran programs are faster than C/C++. If the <tt>restrict</tt> qualifier is used appropriately, compilers can effect every loop optimization applicable in Fortran in C/C++. Section <a class="Reference" href="#subsec:proc-compileropt-C++-overhead">3.2.6↓</a> directly takes on the false belief that C++ programs are condemned to be slower than Fortran. In that section, we write a naive C++ program for multiplying matrices using general classes for vectors and matrices and show that it is more than an order of magnitude slower. The culprit here is the naivety of the programmer and not the C++ language. A C++ program written with an idea of how compilers work, and the kind of instruction stream they may generate, will be faster than a Fortran program written without that knowledge. The C/C++ programming languages provide every available means for writing optimized programs, and there is no other programming language or paradigm, including Fortran, that is remotely comparable to them in this respect. 
</div>
<div class="Indented">
The skill of writing loops explained in section <a class="Reference" href="#sec:proc-Compiler-optimizations">3.2↓</a> is about 40% of what the typical scientific programmer needs to know to produce well-optimized programs. Another 40% is about optimizing memory accesses in a single-core program, a closely related topic discussed in the next chapter. Many subtle concepts and techniques come up when we study threads and networks. However, in optimizing scientific programs, much of the game is in structuring loops carefully, as explained in this chapter, and in organizing memory accesses, as explained in the next chapter. For example, when we optimize a program to effect a matrix transpose in chapter <a class="Reference" href="#chap:Networks-and-message">6↓</a>, much of the optimization boils down to structuring loops and organizing accesses to memory, even though the program makes sophisticated use of networking and threading.
</div>
<div class="Indented">
The brief summary, in the preceding paragraph, of the relative worth of various techniques of optimization is for scientific programs, which generally tend to work with known amounts of data that are laid out in regular patterns. In computer science, data tends to be dynamic and highly unpredictable in extent. Dynamic data structures such as linked lists, trees, and graphs are commonly employed. When such data structures are used, the relative worth of various optimization techniques changes considerably. Optimizing for memory, which is the topic of the next chapter, is considerably more important. Loop optimizations discussed in this chapter are not as important. However, more and more scientific programs are taking on qualities of systems programming, and, conversely, some newer areas of computer science such as data analysis and image processing give rise to programs that are similar to traditional scientific programs.
</div>
<div class="Indented">
In section <a class="Reference" href="#sec:libmake-fft">2.4↑</a>, we found that optimized libraries such as MKL and FFTW can be nearly an order of magnitude faster than Numerical Recipes. Why are optimized libraries nearly an order of magnitude faster? A big part of the answer has to do with optimization for the instruction pipeline and the register set. 
</div>
<div class="Indented">
Optimizing for the processor’s register set and instruction pipeline is a difficult skill, and section <a class="Reference" href="#sec:proc-Optimizing-for-the-pipeline">3.3↓</a> explains how to do it. Although sections <a class="Reference" href="#subsec:proc-instruction-pipelines">3.3.1↓</a> and <a class="Reference" href="#subsec:proc-optim-Chipsets">3.3.2↓</a> are not overly technical, the following two sections, <a class="Reference" href="#subsec:proc-optim-peak-flops">3.3.3↓</a> and <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↓</a>, are not for the faint of heart. This is the only part of the book where assembly programming is essential.
</div>
<div class="Indented">
The value of optimizing for the instruction pipeline and the register set is well known in dense numerical linear algebra and, as already pointed out, in computational Fourier analysis. However, even in these areas, the technical knowledge essential for effecting such optimizations is known only to a small number of cognoscenti. There can be little doubt that wider knowledge of such techniques will uncover more opportunities for such optimizations. Even for the Fast Fourier Transform (FFT), theoretical lower bounds are well below actual out-of-cache performance, and there could be room for additional investigations in this gap. Knowledge of computer architecture creates more possibilities for the programmer and opens a whole new point of view. 
</div>
<div class="Indented">
To conclude this introduction, we turn to a question that may be on the reader’s mind. If we are delving as deeply into processor architecture as we do in this chapter, won’t much of this chapter become irrelevant as the hardware changes? The answer is no, and emphatically no as long as the x86 dominance persists. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:proc-sse2-avx-avx2"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="left" valign="top">
ISA
</td>
<td align="center" valign="top">
Year
</td>
<td align="center" valign="middle" style="width: 1.2in;">
Family
</td>
<td align="center" valign="middle">
Registers
</td>
<td align="center" valign="middle" style="width: 1.4in;">
Instructions
</td>

</tr>
<tr>
<td align="left" valign="top">
SSE
</td>
<td align="center" valign="top">
1999
</td>
<td align="center" valign="middle" style="width: 1.2in;">
Pentium
</td>
<td align="center" valign="middle">
XMM (128 bit)
</td>
<td align="center" valign="middle" style="width: 1.4in;">
ADDPS, MULPS
</td>

</tr>
<tr>
<td align="left" valign="top">
SSE2
</td>
<td align="center" valign="top">
2001
</td>
<td align="center" valign="middle" style="width: 1.2in;">
Pentium 4, Nehalem, Westmere, Opteron, Athlon
</td>
<td align="center" valign="middle">
XMM (128 bit)
</td>
<td align="center" valign="middle" style="width: 1.4in;">
ADDPD, MULPD
</td>

</tr>
<tr>
<td align="left" valign="top">
AVX
</td>
<td align="center" valign="top">
2011
</td>
<td align="center" valign="middle" style="width: 1.2in;">
Sandy Bridge, Ivy Bridge, Bulldozer, Jaguar
</td>
<td align="center" valign="middle">
YMM (256 bit)
</td>
<td align="center" valign="middle" style="width: 1.4in;">
VADDPD, VMULPD, 
</td>

</tr>
<tr>
<td align="left" valign="top">
AVX2
</td>
<td align="center" valign="top">
2013
</td>
<td align="center" valign="middle" style="width: 1.2in;">
Haswell, Broadwell
</td>
<td align="center" valign="middle">
YMM (256 bit)
</td>
<td align="center" valign="middle" style="width: 1.4in;">
VADDPD, VMULPD, VFM*ADDPD
</td>

</tr>
<tr>
<td align="left" valign="top">
AVX-512
</td>
<td align="center" valign="top">
2015
</td>
<td align="center" valign="middle" style="width: 1.2in;">
Skylake 
</td>
<td align="center" valign="middle">
ZMM (512 bit)
</td>
<td align="center" valign="middle" style="width: 1.4in;">
VADDPD, VMULPD, VFM*ADDPD
</td>

</tr>

</table>
<div class="caption">
Table 3.1 Generations of x86 instruction set architectures (ISAs) and machines implementing them. The machine families typically include several machines. For example, Xeon-5650 and Xeon-5670 are machines in the Westmere family (or microarchitecture). This table was prepared by consulting several Wikipedia pages. For a list of machines used in this book, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> in the appendix. Not all versions of Skylake support AVX-512. 
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
A look at table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>, to which we will return many times, will help explain why that is so. It shows the gradual evolution in instruction set architectures from 1999 to 2016/2017 and beyond. Section <a class="Reference" href="#sec:proc-Compiler-optimizations">3.2↓</a> connects the workings of the compiler to XMM (SSE2) and YMM (AVX/AVX2) registers. This section will hardly change when the ZMM registers and their successors come along, except that the kind of concepts discussed in it will increase in importance. None of the C/C++ programs discussed in that section needs to be changed. Indeed, many of the same points could have been made in 2001.
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a> shows the gradual evolution in instruction set architectures from Streaming SIMD Extensions (SSE) to SSE2 to Advanced Vector Extensions (AVX) to AVX2. The expansion in register width from <span class="formula">128</span>-bit XMM registers to <span class="formula">256</span>-bit YMM registers to <span class="formula">512</span>-bit ZMM registers is one part of the evolution. Another part of the evolution is the addition of new instructions. In the instruction <tt>vaddpd</tt>, <tt>v</tt> stands for vector, as may be expected, and <tt>pd</tt> for packed double. The packed double instructions operate on all the double-precision numbers stored in a vector register simultaneously. Thus, a single <tt>vaddpd</tt> instruction applied to YMM registers results in four additions. The AVX2 instruction set architecture introduced fused-multiply-add instructions, namely, <tt>fmadd132pd</tt>, <tt>fmadd213pd</tt>, and <tt>fmadd231pd</tt>. These instructions implement operations of the form <span class="formula"><i>c</i> = <i>c</i> + <i>ab</i></span> or <span class="formula"><i>a</i> = <i>c</i> + <i>ab</i></span> while operating on three vector registers. Thus, a <tt>fmadd*pd</tt> instruction applied to three YMM registers results in four additions and four multiplications.
</div>
<div class="Indented">
We will refer to machines as <span class="formula">2.6</span> GHz SSE2 or <span class="formula">3.6</span> GHz AVX2. The full information about the machine may be found by looking up table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> in the appendix. The instruction set is the part of the computer’s architecture most pertinent to program speed, which explains the nomenclature we adopt. 
</div>
<div class="Indented">
The type of processor design we see today is a product of research that happened as early as the 1960s, with important additions in the 1970s and 1980s.<span class="FootOuter"><span class="SupFootMarker"> [47] </span><span class="HoverFoot"><span class="SupFootMarker"> [47] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span>. The 1960s reference is to Tomasulo’s algorithm, which is mentioned later in this chapter.</span></span> These design principles were incorporated into the x86 line in the 1990s, and the underlying design principles have not changed greatly since then. The x86 instruction set has evolved gradually and methodically. Therefore, not only do the fundamental concepts of optimization for processor architectures remain the same, the particular form they take within C/C++ programs does not change much either.
</div>
<div class="Indented">
In addition, there has been a great deal of convergence in processor technology since 2000 so that nearly 100% of desktop processors are now x86 based. The pertinence of the particular examples, as well as that of general principles of program optimization described in this chapter, is unlikely to diminish anytime soon. In fact, the pertinence of computer architecture to program optimization appears to be growing.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:proc-Optimizing-for-the-pipeline">3.3↓</a> shows how to optimize for the instruction pipeline for SSE2 machines. The instruction pipeline depends on the instruction set architecture and to a far lesser extent on its microarchitectural realization (family in table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>). Because the programs in this section are in assembly, they will need to be updated for machines with YMM and ZMM registers (AVX, AVX2, AVX-512). The updating for the YMM registers is the topic of several exercises. ZMM registers are discussed in chapter <a class="Reference" href="#chap:The-Xeon-Phi">7↓</a> on the Xeon Phi. Using XMM registers has the advantage that the programs we discuss will run on almost any computer, including AVX and AVX2 machines.
</div>
<div class="Indented">
The payoff from this type of optimization is increasing rapidly. A matrix multiplication program written in 2010 in C/C++ approached nearly a third of the speed of a program optimized for the SSE2 processor pipeline. Five years later, the C/C++ program does not even come within a tenth of the speed of a program optimized for the AVX2 processor pipeline. 
</div>
<div class="Indented">
In section <a class="Reference" href="#subsec:proc-compileropt-mmult">3.2.5↓</a>, we explain why the same C/C++ program has become so much worse, when compared to a fully optimized program, over a span of five years. There are two related reasons. After clock speeds flat-lined around 2003, and even before, processors have used greater and greater numbers of transistors for greater and greater parallelism. At one level, that translates to more and more processor cores in a single processor package. Inside a processor, the same trend translates to vector registers and instruction pipelines that can simultaneously execute multiple instructions. Even as there is a greater pay-off in optimizing for such pipelines, it becomes harder for the compilers to do so.
</div>
<div class="Indented">
In 2015, nearly 100% of non-mobile computers and laptops run on x86 processors. It would take a cataclysmic event (cataclysmic for Intel Corporation), such as Apple Computers switching its MacBooks and desktops from the x86 line to ARM, for the x86 line to be truly challenged.<span class="FootOuter"><span class="SupFootMarker"> [48] </span><span class="HoverFoot"><span class="SupFootMarker"> [48] </span>I thank Hans Johnston for this point.</span></span> Intel is not as invulnerable to market forces as it used to be one or two decades ago, but it still dominates the processor benchmarks overwhelmingly. Such a cataclysmic event is not on the horizon as of 2015. 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-3.1">3.1</a> Overview of the x86 architecture<a class="Label" name="sec:proc-overviewofx86"> </a>
</h2>
<div class="Unindented">
This section is a basic introduction to the x86 instruction architecture, which has dominated since its introduction in 1978. We look at registers and a little bit of assembly programming. Our intention is to lay the foundation for the next section, where we look at compilation and show how to tell whether a compiler has generated good code or not. All that is required is a familiarity with the names of some of the registers and a few instructions. The instruction names are often self-explanatory. 
</div>
<div class="Indented">
Accordingly, the assembly programming we exhibit is basic. Yet one of the programs we look at is quite useful. This program implements a C++ class <tt>TimeStamp</tt> for accessing the Time Stamp Counter, which is one of the best methods to measure time on a computer. This C++ class is used throughout the book.
</div>
<div class="Indented">
The x86 assembly language is known for its huge complexity. Understanding assembly programming is a mammoth task meant for compiler writers and computer architects, not for the average scientist. Our intention here is to gain a basic understanding only---in other words, our aim is to splash a little water, not to become expert swimmers. Without some understanding of assembly programming, many aspects of computer architecture as well as the workings of compilers and linkers take on a remote quality. In addition, it is impossible to gain an appreciation of the value of optimized scientific codes.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.1.1">3.1.1</a> 64-bit x86 architecture<a class="Label" name="subsec:processor-part1-begin"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:proc-land-coordinates"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter2/figCompArchIntelLandCoord.png" alt="figure FIGS/chapter2/figCompArchIntelLandCoord.png" style="width: 182px; max-width: 304px; height: 174px; max-height: 290px;"/>
<span class="hspace" style="width: 1cm;"> </span><img class="embedded" src="FIGS/chapter2/Xeon5400.png" alt="figure FIGS/chapter2/Xeon5400.png" style="width: 163px; max-width: 204px; height: 161px; max-height: 202px;"/>
<div class="caption">
Figure 3.1 Land coordinates of Intel’s Xeon 5400 processor and a photograph of the same processor. The land coordinates are used to identify pins, which are shown in the photo.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Perhaps a good way to begin is by looking at a processor package. Figure <a class="Reference" href="#fig:proc-land-coordinates">3.1↑</a> shows what a processor package looks like from the outside.<span class="FootOuter"><span class="SupFootMarker"> [49] </span><span class="HoverFoot"><span class="SupFootMarker"> [49] </span>The land coordinates in the figure are from Quad-core Intel Xeon Processor 5400 Series: Datasheet.</span></span> There are pins to transfer addresses and data to and from memory, pins to interrupt the processors, and so on. The processor package shown has only four processor cores. More recent packages have many more cores, and it takes multiple pages to describe their land coordinates. 
</div>
<div class="Indented">
When we write programs, we do so with an awareness of memory. Every variable name is ultimately the name for a segment of memory. However, in most programming, we have no awareness of the registers at all. Registers are locations that reside on the chip that are capable of holding data or addresses. They are very special locations because they are wired into the circuits for carrying out arithmetic and logical operations. When arithmetic operations are executed by the processor, all the operands may be registers. To add two numbers in memory, for instance, typically we have to first move one or both of them to registers, add the two registers, and move the result from a register to a memory location. The addresses of the memory locations can also be held in the registers. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:proc-register-diagram"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter2/registers.png" alt="figure FIGS/chapter2/registers.png" style="max-width: 290px; max-height: 227px;"/>

</div>
<div class="caption">
Figure 3.2 Partial register diagram of an x86 processor. The general-purpose registers (RAX to R15) are all <span class="formula">64</span>-bit wide. The XMM, YMM, and ZMM registers are <span class="formula">128, </span> <span class="formula">256</span>, and <span class="formula">512</span> bit wide, respectively.
</div>

</div>

</div>

</div>
<h? class="Subsubsection">
<b><u>Registers</u></b>
</h?>
<div class="Unindented">
The x86 64-bit architecture is first of all a specification of registers (see figure <a class="Reference" href="#fig:proc-register-diagram">3.2↑</a>). The architecture was first introduced by AMD and later adopted by Intel. Some variable names may only correspond to specific registers. The following are some of the registers in the 64-bit x86 architecture: 
</div>
<ul>
<li>
Eight <span class="formula">64</span>-bit registers, including <tt>%rax</tt>, <tt>%rbx</tt>, <tt>%rcx</tt>, <tt>%rdx</tt>, <tt>%rdi</tt>, <tt>%rbp</tt>, <tt>%rsp</tt>. 
</li>
<li>
If the <tt>r</tt> in the above names is replaced by <tt>e</tt>, we get names for <span class="formula">32</span>-bit registers. These are the lower halves of the <span class="formula">64</span>-bit registers. 
</li>
<li>
Eight more <span class="formula">64</span>-bit general-purpose registers <tt>%r8</tt> through <tt>%r15</tt>. 
</li>
<li>
To access only the lower <span class="formula">32</span> bits, we can use <tt>%r8d</tt> through <tt>%r15d</tt>. Here <tt>d</tt> stands for double word. 
</li>
<li>
Floating point register stack with eight 80-bit registers called <tt>%st(0)</tt> through <tt>%st(7)</tt>. Eight 64-bit MMX registers <tt>%mm0</tt> through <tt>%mm7</tt>. We will never use these registers.
</li>
<li>
Sixteen <span class="formula">128</span>-bit registers <tt>%xmm0</tt> through <tt>%xmm</tt>15. Each register can hold two double-precision numbers. These are the so-called SSE registers.
</li>
<li>
Each XMM register is extended to a <span class="formula">256</span>-bit YMM register in AVX (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>). The YMM registers are <tt>%ymm0</tt> through <tt>%ymm15</tt>. The YMM registers have in turn been extended in AVX-512 (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>).
</li>

</ul>
<div class="Unindented">
The AMD/Intel manuals use slightly different names for the registers. For instance, <tt>%rax</tt> becomes <tt>RAX</tt>. The naming in our list anticipates our use of GAS (the GNU assembler). The partial list omits a number of registers. From some of the registers we have listed, we can get <span class="formula">16</span>-bit or word-length registers and <span class="formula">8</span>-bit or byte-length integers by changing the names appropriately.
</div>
<div class="Indented">
This list of registers may appear long and a bit intimidating. To master it all can indeed be quite a task. For our purposes, it suffices to know that there are <span class="formula">16</span> general-purpose registers and <span class="formula">16</span> XMM/YMM registers. The different uses that these registers are put to will emerge as this chapter progresses. The XMM/YMM registers are the basis of fast scientific programs.
</div>
<div class="Indented">
There is a <span class="formula">64</span>-bit register called <tt>%rip</tt> that users normally do not access. That register is the instruction pointer. It is the offset of the address of the next instruction to be executed relative to some base. But that interpretation can be problematic because of instruction pipelining and out-of-order execution.
</div>
<h? class="Subsubsection">
<b><u>Backward binary compatibility</u></b>
</h?>
<div class="Unindented">
The 64-bit x86 architecture carries within it ghosts from decades past. Some of these ghosts are easily seen in the naming of the register set, which is the most basic feature of all processor architectures. At first there seems to be no good reason to call a register <tt>rax</tt> or <tt>rbx</tt>. However, when the first 8086 16-bit processor was introduced back in 1978, large numbers of registers must have seemed a distant dream. So it was natural to use the first few letters of the alphabet for the registers. When the 32-bit 80386 was introduced in 1985, names such as <tt>ax</tt> were changed to <tt>eax</tt>, signaling the enhancements to the size of the registers. The register names in the 64-bit x86 architecture reflect this evolution. 
</div>
<div class="Indented">
Binary compatibility is the reason for this peculiar evolution. It may make no sense to expect a 64-bit x86 processor of today to run a 16-bit binary from 1978, but it makes sense to expect it to run the 32-bit binaries of a few years ago. All the 64-bit x86 processors have a 32-bit mode that can be turned on by the operating system. If they did not have such a mode, nearly every one of the many 32-bit software systems available would have failed when the 64-bit architecture was first introduced. Backward binary compatibility gives software developers, especially developers of operating systems, time to catch up. Backward compatibility means that an instruction such as MOVL EAX EBX that made sense in the 32-bit architecture must also make sense in the 64-bit x86 architecture. Not only that, its encoding using bits must be exactly the same (its encoding is <tt>89 C3</tt> in hexadecimal---the first byte is the opcode for MOVL and the second byte, which encodes the source and destination registers, is the so-called ModR/M byte). 
</div>
<div class="Indented">
The disadvantage of backward binary compatibility is that it constrains the design of the instruction set and the microarchitecture used to implement it in hardware. Every time the instruction set is extended, the binary encoding of the earlier instruction set as well as its semantics must be preserved intact. This burden can be considerable. For instance, programs written in 1990 could rely on the register <tt>%eip</tt> to point to the address of the next instruction to be executed and manipulate that register to jump to some other point in the program. Not only must today’s x86 architecture include that register, it should also give the same effect when the <tt>%eip</tt> is altered to jump to some other location in the program. Having to bear this considerable burden has meant that the x86 architecture is infamous for its extremely complicated binary encodings of instructions. 
</div>
<div class="Indented">
History has shown that the disadvantages of backward binary compatibility are outweighed by the advantage of being readily usable. The x86 architecture always seeks to be usable. A new design that is brought out today must run yesterday’s binaries. This emphasis on usability has meant that the vast majority of today’s computers (excluding mobile devices) use x86 processors.<span class="FootOuter"><span class="SupFootMarker"> [50] </span><span class="HoverFoot"><span class="SupFootMarker"> [50] </span>The ARM architecture has a much simpler instruction set and has achieved wide use on mobile gadgets. The number of ARM devices is greater than the number of x86 computers. Thus, the ARM architecture does not suffer from a lack of widespread adoption and the consequent lack of economy of scale, which has been a bane of previous challengers to x86. However, the terms of commercial licensing of ARM are such that the benefits of its wide adoption are shared by many companies. </span></span>
</div>
<div class="Indented">
Yet one must not exaggerate the benefits of backward binary compatibility in relation to the x86 architecture. Intel, the foremost champion of the x86 line, is no longer the invincible behemoth it once was. Intel is tied to the PC business, which is in gradual decline, and has no control over hardware or software platforms in the rapidly growing mobile computing and networking sectors. Despite the enormous investment in design accumulated over many years, the volume of profit being generated in the computer business is now so great that a legitimate challenger to the x86 line is not inconceivable. In a business that is growing exponentially, what happens in the next five years can be of more consequence than what happened in the past four decades.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.1.2">3.1.2</a> 64-bit x86 assembly programming<a class="Label" name="subsec:processor-x86-assembly-prog"> </a>
</h3>
<div class="Unindented">
Each processor core fetches instructions from memory and executes them. These instructions are stored in memory as a sequence of bytes. In assembly language, the bytes that encode machine instructions are replaced by mnemonics. For example, an instruction to move a quad word, which is eight bytes in size, from register <tt>r8</tt> to register <tt>rax</tt> is coded as the following hex sequence <tt>4C 89 C0</tt>. The first byte <tt>4C</tt> is the so-called REX byte, the second byte <tt>89</tt> is the opcode, and the third byte <tt>C0</tt> encodes the fact that the source register is <tt>r8</tt> and the destination is <tt>rax</tt>.<tt><span class="FootOuter"><span class="SupFootMarker"> [51] </span><span class="HoverFoot"><span class="SupFootMarker"> [51] </span>The third byte is given by the AMD manuals. </span></span> </tt>In the 64-bit x86 architecture, opcodes can be one, two, or three bytes. A single instruction can be as many as 17 bytes. 
</div>
<div class="Indented">
Unlike processors, we can’t just look at bits and make sense of them, which is why the assembly languages provide mnemonics. In the GNU assembler, which is called GAS, the mnemonic for the instruction to move a quad word from <tt>r8</tt> to <tt>rax</tt> is <tt>movq %r8, %rax</tt>. To move a double word, which is four bytes stored in the lower halves of the 64-bit registers, the mnemonic is <tt>movl %r8d, %eax</tt>. These mnemonics vary with the assembler. In the MASM assembler, the mnemonic for moving a quad word from <tt>r8</tt> to <tt>rax</tt> would be <tt>MOVQ RAX R8</tt>---notice that the registers are given in reverse order.
</div>
<div class="Indented">
There is no standardization in the world of x86 assembly languages. The documentation for the instruction set, which include the mnemonics for various instructions, is published by AMD and Intel. However, the mnemonics can change in a predictable fashion depending on the assembly language. For instance, a register referred to as <tt>R8</tt> in the AMD/Intel documentation becomes <tt>%r8</tt> in GAS. One point is important to keep in mind. In the AMD/Intel manuals, the destination precedes the source. In GAS, the destination follows the source. We will always use the GAS convention. On Linux computers, even the Intel compilers use the GNU assembler. 
</div>
<div class="Indented">
The reader may find it a little puzzling that &ldquo;double&rdquo; (or &ldquo;long&rdquo;) is used for <span class="formula">32</span>-bit operands and &ldquo;quad&rdquo; for <span class="formula">64</span>-bit operands. A word in the original 8086 machine of 1978 was <span class="formula">16</span> bits. Thus, double words are <span class="formula">32</span> bits and quad words are <span class="formula">64</span> bits, as a result of a choice made long ago.
</div>
<h? class="Subsubsection">
<b><u>Getting started</u></b>
</h?>
<div class="Unindented">
For our first assembly program, we begin with the following simple C code:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;stdio.h&gt;
int main(){
  int x, y, z;
  x = 1;
  y = 2;
  z = 3;
  printf("The sum of %d and %d is %d \n", x, y, z);
}
</pre>
</div>

</div>
<div class="Indented">
 It is compiled using 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">gcc -S -O3 add.c
</pre>
</div>

</div>
<div class="Indented">
The <tt>-S</tt> flag tells the compiler to generate assembly only and leave it in the file <tt>add.s</tt>. Optimization is turned on with the -<tt>O3</tt> flag so that the compiler generates cleaner code. 
</div>
<div class="Indented">
Below is a listing of the <tt>addtwo.s</tt> assembly code. The listing is a cleaned up version of the code generated by the compiler. In particular, we removed some code at the bottom related to an error handler. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>.file	"addtwo.s"
<span class="number-left">2</span>	.section	.mydata,"aMS",@progbits,1
<span class="number-left">3</span>.LC0:
<span class="number-left">4</span>	.string	"The sum of %d and %d is %d\n"
<span class="number-left">5</span>	.text
<span class="number-left">6</span>.globl main
<span class="number-left">7</span>	.type	main, @function
<span class="number-left">8</span>main:
<span class="number-left">9</span>	movl	$3, %ecx
<span class="number-left">10</span>	movl	$2, %edx
<span class="number-left">11</span>	movl	$1, %esi
<span class="number-left">12</span>	movl	$.LC0, %edi
<span class="number-left">13</span>	xorl	%eax, %eax
<span class="number-left">14</span>	call	printf
<span class="number-left">15</span>	ret
</pre>
</div>

</div>
<div class="Indented">
The instructions are specified using mnemonics in the <tt>main</tt> block, which begins at line 9. The earlier lines are assembler directives. The syntax for the directives can vary considerably between different assemblers.
</div>
<div class="Indented">
The  first line, which gives the name of the file, is a directive. The second line asks the compiler to begin a section called <tt>.mydata</tt>. One has to dig through the GAS documentation to decipher the portion that follows. The second line states that the section is meant to hold data, that each character is one byte, and that the strings will be terminated by <tt>0</tt>. The strings are terminated by <span class="formula">0</span>, anticipating the need to pass them to the <tt>printf</tt> function in the C library. In C, strings are always NULL or <tt>0 </tt>terminated.
</div>
<div class="Indented">
<tt>.LC0</tt> is the name of the string specified in line 4. It corresponds to a location in memory. Here the assembler takes on the responsibility of storing the specified string in the data section and gives the location the name <tt>.LC0</tt>.
</div>
<div class="Indented">
The text section, which is not named, begins on line 5. The text section will contain instructions. The name <tt>main</tt> is specified to be a global, and the name of a function in lines 6 and 7---<tt>main</tt> will be visible outside the file and is the name for an address in the text segment, which corresponds to line 8.
</div>
<div class="Indented">
In the following code, the 32-bit registers <tt>%esi</tt>, <tt>%edx</tt>, and <tt>%ecx</tt> are filled with 1, 2, and 3, respectively. These are the second, third, and fourth arguments in the call to printf. Because <tt>.LC0</tt> is the address of a location in the data segment, it can only be <span class="formula">32</span> bits. Thus, the <tt>movl</tt> instruction is used to move it into the register <tt>%edi</tt>. The <tt>xorl</tt> instruction zeros the <tt>eax</tt> register as a precaution. 
</div>
<div class="Indented">
The 64-bit x86 architecture requires the first four arguments to be placed in <tt>%edi</tt>, <tt>%esi</tt>, <tt>%edx</tt>, and <tt>%ecx</tt>. The call to <tt>printf</tt> is made with those arguments. Here <tt>printf</tt> is an externally defined name and has to be resolved during linking.
</div>
<div class="Indented">
To assemble,<span class="FootOuter"><span class="SupFootMarker"> [52] </span><span class="HoverFoot"><span class="SupFootMarker"> [52] </span>The <tt>icc</tt> compiler uses the GNU Assembler (GAS). For its documentation, see (Using as, D. Elsner and J. Fenlason).</span></span> we may use 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc -c addtwo.s 
</pre>
</div>

</div>
<div class="Indented">
to produce the object file <tt>addtwo.o</tt> (we are back to using <tt>icc</tt>). The object file can be linked to produce an executable using 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icc -o addtwo.exe addtwo.o
</pre>
</div>

</div>
<div class="Indented">
The <tt>icc</tt> linker takes care of resolving the reference to <tt>printf</tt> in <tt>addtwo.o</tt>. When the executable <tt>addtwo.exe</tt> is run, we are told that 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">The sum of 1 and 2 is 3
</pre>
</div>

</div>
<h? class="Subsubsection">
<b><u>More about function calls</u></b>
</h?>
<div class="Unindented">
The output of the next listing, which is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">The sum of 1 2 3 4 5 6 7 8 is 36
</pre>
</div>

</div>
<div class="Indented">
is hardly more exciting. However, it illustrates function calls made after placing some arguments on the stack.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>.file	"addmany.s"	
<span class="number-left">2</span>	.section	.mydata,"aMS",@progbits,1
<span class="number-left">3</span>.LC0:
<span class="number-left">4</span>	.string	"The sum of %d %d %d %d %d %d %d %d  is %d \n"
<span class="number-left">5</span>	.text
<span class="number-left">6</span>.globl main
<span class="number-left">7</span>	.type	main, @function
<span class="number-left">8</span>main:
<span class="number-left">9</span>	subq	$32, %rsp
<span class="number-left">10</span>	movl	$36, 24(%rsp)
<span class="number-left">11</span>	movl	$8, 16(%rsp)
<span class="number-left">12</span>	movl	$7, 8(%rsp)
<span class="number-left">13</span>	movl	$6, (%rsp)
<span class="number-left">14</span>	movl	$5, %r9d
<span class="number-left">15</span>	movl	$4, %r8d
<span class="number-left">16</span>	movl	$3, %ecx
<span class="number-left">17</span>	movl	$2, %edx
<span class="number-left">18</span>	movl	$1, %esi
<span class="number-left">19</span>	movl	$.LC0, %edi
<span class="number-left">20</span>	xorl	%eax, %eax
<span class="number-left">21</span>	call	printf
<span class="number-left">22</span>	addq	$32, %rsp
<span class="number-left">23</span>	ret
</pre>
</div>

</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:proc-asm-callstack"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter2/figCompArchStack.png" alt="figure FIGS/chapter2/figCompArchStack.png" style="width: 4in; max-width: 263px; height: auto; max-height: 120px;"/>
<div class="caption">
Figure 3.3 The stack pointer is moved downward by <span class="formula">32</span> and the numbers 6, 7, 8, and 36 are placed on the stack before a call to<i> <tt></tt></i>printf. The labeling on the right assumes that the stack pointer has been moved downward.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
The <tt>addmany.s</tt> program calls <tt>printf()</tt> (on line 21) with a total of <span class="formula">10</span> arguments. These are first the format string, next the numbers <span class="formula">1</span> through <span class="formula">8</span>, and finally their sum, which is <span class="formula">36</span>. The last four of these arguments are pushed onto the program stack (see figure <a class="Reference" href="#fig:proc-asm-callstack">3.3↑</a>) and the first six are placed in registers as before. It helps to think of the registers as an extension of the stack.
</div>
<div class="Indented">
Each argument pushed onto the stack is an <tt>int</tt> and therefore <span class="formula">4</span> bytes. Thus, we would need <span class="formula">16</span> bytes on the stack. However, in 64-bit x86 architecture, each entry on the stack must be a multiple of <span class="formula">8</span> bytes. Therefore, we need <span class="formula">32</span> bytes on the stack. 
</div>
<div class="Indented">
By convention, <tt>%rsp</tt> is the pointer to the top entry of the stack. On line 9, the stack pointer <tt>%rsp</tt> is decremented by <span class="formula">32</span> bytes to make room for four more entries. In x86, the stack grows downward from a high address as shown in figure <a class="Reference" href="#fig:proc-asm-callstack">3.3↑</a>. The arguments are pushed onto the stack in reverse order. Thus, the final argument <span class="formula">36</span> is pushed first onto the stack (line 10). That is followed by the preceding three arguments <span class="formula">8</span>, <span class="formula">7</span>, and <span class="formula">6</span> (lines 11, 12, and 13, respectively). For addressing modes of the type <tt>(%rsp)</tt> and <tt>8(%rsp)</tt>, see figure <a class="Reference" href="#fig:proc-asm-callstack">3.3↑</a>. They are explained in greater detail below. The stack pointer <tt>%rsp</tt> points to the top, which is <span class="formula">6</span> (see the right part of figure <a class="Reference" href="#fig:proc-asm-callstack">3.3↑</a>).
</div>
<div class="Indented">
We may think of the registers used to pass arguments as an extension of the stack. These are <tt>%r9</tt>, <tt>%r8</tt>, <tt>%rcx</tt> (or <tt>%ecx</tt>), <tt>%rdx</tt> (or <tt>%edx</tt>), <tt>%rsi</tt> (or <tt>%esi</tt>), and <tt>%rdi</tt> (or <tt>%edi</tt>), in bottom to top order (lines 14 through 19). Thus, the top six entries of the call stack are in registers, with <tt>%rdi</tt> at the top by convention. If a function call has six or fewer arguments, the entire call stack is stored in registers. 
</div>
<div class="Indented">
When the call to <tt>printf</tt> is made, the function has to retrieve some arguments from the registers and some from the stack. Line 22 moves the stack pointer back to its original location: the caller cleans up the stack after the function that is called returns.
</div>
<h? class="Subsubsection">
<b><u>Addressing modes</u></b>
</h?>
<div class="Unindented">
Let us go back to the listing and look at usages such as <tt>(%rsp)</tt> and <tt>8(%rsp)</tt> once again. The first usage <tt>(%rsp)</tt> is a memory reference using a base register. Here the base register is <tt>%rsp</tt> and it stores a 64-bit address. When memory is referenced in this way using a base register, it is as if the base register is a pointer that is dereferenced.
</div>
<div class="Indented">
The memory reference <tt>8(%rsp)</tt> uses a base register and a displacement. The displacement is <span class="formula">8</span> and the base register is <tt>%rsp</tt>. The memory location referred to is the one whose address is the content of the base register with the displacement added to it. 
</div>
<div class="Indented">
Memory references in machine language are similar to the way pointers are handled in C. If <tt>rsp</tt> is thought of as a pointer-type variable in C, the meaning of <tt>(%rsp)</tt> is the same as<tt> *rsp</tt>. In other words, an address is dereferenced and an rvalue converted to an lvalue. Likewise, <tt>8(%rsp)</tt> has the same meaning as <tt>rsp[8]</tt> or <tt>*(rsp+8)</tt> in C, but we must be a little careful here. In C, <span class="formula"> + 8</span> would be <span class="formula"> + 64</span> if <tt>rsp</tt> pointed to <tt>double</tt> (<span class="formula">8</span> bytes) and <span class="formula"> + 32</span> if <tt>rsp</tt> pointed to <tt>int</tt> (<span class="formula">4</span> bytes). In assembly, there is no notion of types, and <span class="formula"> + 8</span> is always <span class="formula"> + 8</span>.
</div>
<div class="Indented">
Although <span class="formula"> + 8</span> is always <span class="formula"> + 8</span> in assembly, the analogy to C goes further. A reference such as <tt>8(%rax,%rdx)</tt> that fits the format
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">displacement(base register, index register)
</pre>
</div>

</div>
<div class="Indented">
evaluates to the memory location whose address is the sum of the base, index, and the displacement. Thus, <tt>(%rax, %rdx)</tt> is equivalent to <tt>rax[rdx]</tt> in C syntax, with the caveat about <span class="formula"> + 8</span> being <span class="formula"> + 8</span>. In x86 assembly, a memory location can be either a destination (lvalue) or a source (rvalue).
</div>
<div class="Indented">
Yet another memory addressing mode is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">displacement(base register, index register, scale)
</pre>
</div>

</div>
<div class="Indented">
Its meaning is almost self-explanatory. Thus, <tt>8(%rax, %rbx, 16)</tt> is equivalent to <tt>rax[16*rbx+8]</tt> in C syntax, with the caveat stated above. The <tt>scale</tt> is allowed to take only certain values such as <span class="formula">1, 2, 4, 8, 16</span>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.1.3">3.1.3</a> The Time Stamp Counter<a class="Label" name="subsec:processor-time-stamp"> </a>
</h3>
<div class="Unindented">
To do anything useful with assembly code, it helps to have a method to make it part of a C or C++ program. The <tt>asm</tt> directive allows us to embed assembly code in a C or C++ program. The implementation of the <tt>asm</tt> directive varies with the compiler. AMD’s PGI compiler manual describes inline assembly in detail. The implementations of inline assembly by Intel and GNU compilers seem to correspond to the PGI documentation. 
</div>
<div class="Indented">
<tt>RDTSC</tt> or <tt>rdtsc</tt> is an x86 machine instruction for reading the Time Stamp Counter. The behavior of the Time Stamp Counter varies. Its basic function is to record the clock cycles. After the instruction is executed, the number of clock cycles is saved as a 64-bit number using two 32-bit registers, namely, <tt>eax</tt> and <tt>edx</tt>, with the higher bits in <tt>edx</tt>.
</div>
<div class="Indented">
Why are 64 bits needed? A 2 GHz clock would tick <span class="formula">5 × 10<sup>9</sup></span> times in two and a half seconds, whereas the maximum unsigned integer that we can represent using 32 bits is <span class="formula">2<sup>32</sup> − 1 ≈ 4.3 × 10<sup>9</sup></span>. A 2 GHz clock would wrap around in less than 10 seconds if the counter were to use only 32 bits. With 64 bits, the Time Stamp Counter is guaranteed not to wrap around for 10 years. However, the counter is reset every time the machine is booted. 
</div>
<div class="Indented">
The following inline assembly can be embedded into a C or C++ program:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">asm volatile("rdtsc" : "=a" (a1), "=d" (d1));
</pre>
</div>

</div>
<div class="Indented">
Before this statement, <tt>a1</tt> and <tt>d1</tt> must be defined to be of type <tt>unsigned int</tt>. After the <tt>rdtsc</tt> instruction is completed, the registers <tt>eax</tt> and <tt>edx</tt> are copied to the variables <tt>a1</tt> and <tt>d1</tt>, respectively. The segment that follows the colon is the output part of the inline assembly statement. In this case, the output part is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">="a"(a1),="d"(d1)
</pre>
</div>

</div>
<div class="Indented">
The <tt>=a</tt> and <tt>=d</tt> are saying that the <tt>%eax</tt> and <tt>%edx</tt> registers must be output, using <tt>a</tt> as the code for <tt>eax</tt> and <tt>d</tt> as the code for <tt>edx</tt>, as may be verified by checking the documentation. The destinations for the output are given as <tt>(a1)</tt> and <tt>(d1)</tt>, where <tt>a1</tt> and <tt>d1</tt> are variables of type <tt>unsigned int</tt> defined in the C or C++ program. In this way, we are able to execute a machine instruction and bring in information into variables that can be accessed within the C or C++ program. 
</div>
<div class="Indented">
The <tt>volatile</tt> qualifier asks the compiler not to change the assembly code as part of an optimization. In addition, if two inline assembly statements have the <tt>volatile</tt> qualifier, the compiler will not move the two statements past each other. 
</div>
<div class="Indented">
In some processors, such as those of the Pentium M family, the Time Stamp Counter is incremented every clock cycle. However, the frequency of clock ticks can vary. To decrease energy consumption, computers vary the frequency of clock ticks and the operating voltage. In other processors, such as the Intel Atom, the Time Stamp Counter is incremented at a constant rate regardless of the operating voltage and the actual frequency of the clock. The architectural behavior ‘‘moving forward’’ has been stated to be of the latter type.
</div>
<div class="Indented">
Between two calls to <tt>rdtsc</tt> made on the same processor core, the second call is guaranteed to return a higher count.<span class="FootOuter"><span class="SupFootMarker"> [53] </span><span class="HoverFoot"><span class="SupFootMarker"> [53] </span>Processes may be moved from core to core by the operating system. When that happens, the second reading of the Time Stamp Counter may be lesser than the first. See <a class="FlexURL" href="http://en.wikipedia.org/wiki/Time_Stamp_Counter">http://en.wikipedia.org/wiki/Time_Stamp_Counter</a>. </span></span> However, <tt>rdtsc</tt> is not a serializing instruction. In the presence of multiple pipelines and out-of-order scheduling, some instructions that occur after <tt>rdtsc</tt> may complete sooner, although some instructions that occur before may complete later. However, one does not need to worry too much about such possibilities. With careful use, the Time Stamp Counter can be used to time events that take only a few nanoseconds. 
</div>
<div class="Indented">
It is of course a nuisance if we have to think about machine instructions every time we want to read the Time Stamp Counter in a C++ program. A simple C++ class greatly simplifies the use of the Time Stamp Counter. The interface of the <tt>TimeStamp</tt> class is given below.<tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class TimeStamp{
public:
  TimeStamp(){};
  void tic(); 
  double toc();
};
</pre>
</div>

</div>
<div class="Indented">
The entire <tt>TimeStamp</tt> class is defined in the header file <tt>TimeStamp.hh</tt> to avoid function call overhead using function inlining. The constructor does nothing. A call to the member function <tt>tic()</tt> will issue the <tt>rdtsc</tt> instruction and record the counter in two variables of type <tt>unsigned int</tt>. A call to the member function <tt>toc()</tt> will read the Time Stamp Counter in the same way and will return the number of cycles elapsed since the last call to <tt>tic()</tt> as a <tt>double</tt>. The calculation to turn the information from the two readings of the Time Stamp Counter into a <tt>double</tt> is not shown. 
</div>
<div class="Indented">
The <tt>TimeStamp</tt> class can be used as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">TimeStamp ts;
ts.tic();
//... code to be timed ...
double cycles = ts.toc();
</pre>
</div>

</div>
<div class="Indented">
The <tt>cycles</tt> variable will hold the number cycles between <tt>tic()</tt> and <tt>toc()</tt>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.1.4">3.1.4</a> Cache parameters and the CPUID instruction<a class="Label" name="subsec:processor-cpuid-cache-parameters"> </a>
</h3>
<div class="Unindented">
The registers on a processor are few in number. Large arrays and other large data structures cannot be stored in their entirety using registers. They are typically stored in memory (DRAM). However, memory is outside the processor chip. Although it takes only a few cycles to operate on registers, it takes a few hundred cycles to fetch a word from memory. To hide the cost of the large number of cycles required to access a word from memory, some memory words are <i>cached </i>on the chip itself. Cache memory is organized into several levels.
</div>
<div class="Indented">
We will take a closer look at cache memory in the next chapter. Here we will use the <tt>cpuid</tt> instruction to ask the processor to report information about its cache and thus give us a preliminary idea of cache memory. As we look at assembly code and explore certain facets of the processor, it will help to have some idea of cache memory. Caches influence the performance of nearly every program.
</div>
<div class="Indented">
The <tt>cpuid</tt> instruction can be used to extract a variety of information about the processor. If you type <tt>cat /proc/cpuinfo</tt> on a Linux computer, the command returns information about the processor’s instruction set, power consumption, caches, support for specific technologies, and clock speed. Much of that information is obtained by the operating system using <tt>cpuid</tt>. The <tt>cpuid</tt> instruction has a manual of its own. The AMD and Intel processors use different conventions for <tt>cpuid</tt>.
</div>
<div class="Indented">
The inline assembly statement for extracting information about caches using the <tt>cpuid</tt> instruction is given below. <tt>eax</tt>, <tt>ebx</tt>, and <tt>ecx</tt> must be defined as variables of type <tt>unsigned int</tt>, and <tt>i</tt> must be an integer value.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  
asm volatile("cpuid"                //instruction
	:"=a"(eax), "=b"(ebx), "=c"(ecx)//output list
	:"a"(0x04), "c"(i)              //input list
	:"edx");                        //clobber list                       
</pre>
</div>

</div>
<div class="Indented">
This inline assembly statement has more complicated syntax than the one we used to read the Time Stamp Counter. The instruction, which is given as <tt>cpuid</tt>, is followed by three colons and not just one. We do not need to pass any parameters to the <tt>rdtsc</tt> instruction and all the data items created by the instruction are output to program variables. In the case of the <tt>cpuid</tt> instruction, we need to leave some data in the registers to tell the <tt>cpuid</tt> instruction what to do. In addition, not all the data items returned by the <tt>cpuid</tt> instruction are output to program variables. The syntax of this inline assembly statement is more complicated for these reasons.
</div>
<div class="Indented">
The three colons in this inline assembly statement divide the part inside the parentheses into four segments. The first segment, which is before the first colon, gives the instruction to be executed. The second, third, and fourth segments give the output list, the input list, and the so-called clobber list, respectively. In this case, the output list specifies that the contents of the registers <tt>%eax</tt>, <tt>%ebx</tt>, and <tt>%ecx</tt>, which are indicated by <tt>=a</tt>, <tt>=b</tt>, and <tt>=c</tt>, respectively, must be output to the program variables <tt>eax</tt>, <tt>ebx</tt>, and <tt>ecx</tt>, respectively. 
</div>
<div class="Indented">
The input list specifies that the register <tt>%eax</tt> must be loaded with the hexadecimal number <tt>04</tt>. This hexadecimal code tells <tt>cpuid</tt> to get information about cache. By loading other codes into <tt>%eax</tt>, <tt>cpuid</tt> can be asked to return information about performance counters, power managment, processor name, and other features of the processor. The input list loads the integer value <tt>i</tt> into the <tt>%ecx</tt> register. A processor typically has multiple levels of cache, and an integer value is loaded into <tt>%ecx</tt> to tell <tt>cpuid</tt> which cache it must get information about. 
</div>
<div class="Indented">
<tt>CPUID</tt> returns information in four registers: <tt>%eax</tt>, <tt>%ebx</tt>, <tt>%ecx</tt>, and <tt>%edx</tt>. Of these, only three are output to program variables. We have elected not to output the <tt>%edx</tt> register. Therefore, the clobber list, which is the segment following the third and last colon, includes <tt>%edx</tt> to tell the compiler that the instruction will write over that register. That way the compiler knows it must not save information in <tt>%edx </tt>with plans of using it later. The registers <tt>%eax</tt>, <tt>%ebx</tt>, and <tt>%ecx</tt> are also clobbered by the <tt>cpuid</tt> instruction. However, these should not be in the clobber list because the compiler can figure out that they are being clobbered from the input and output segments. 
</div>
<div class="Indented">
A program that uses the <tt>cpuid</tt> instruction to extract information about cache memory follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>  cout&lt;&lt;"CPUID with code 04h"&lt;&lt;endl;
<span class="number-left">2</span>  unsigned int eax, ebx, ecx;
<span class="number-left">3</span>  for(int i=0;;i++){
<span class="number-left">4</span>    asm volatile("cpuid"
<span class="number-left">5</span>		 :"=a"(eax), "=b"(ebx), "=c"(ecx)
<span class="number-left">6</span>		 :"a"(0x04), "c"(i)
<span class="number-left">7</span>		 :"edx");
<span class="number-left">8</span>    if((eax&amp;0xF)==0)
<span class="number-left">9</span>      break;
<span class="number-left">10</span>    printf("cache type = %x\n",eax&amp;0xF);
<span class="number-left">11</span>    printf("cache level = %x\n", (eax&gt;&gt;5)&amp;(0x7));
<span class="number-left">12</span>    printf("ways of associativity = %u\n",
<span class="number-left">13</span>	   ((ebx&gt;&gt;22)&amp;0xFFF)+1);
<span class="number-left">14</span>    printf("physical line partitions = %x\n", 
<span class="number-left">15</span>	   ((ebx&gt;&gt;12)&amp;0x3FF)+1);
<span class="number-left">16</span>    printf("cache line size = %u\n",(ebx&amp;0xFFF)+1);
<span class="number-left">17</span>    printf("number of sets = %u\n",ecx+1);
<span class="number-left">18</span>    printf("cache size in bytes = %u\n\n",
<span class="number-left">19</span>	   (((ebx&gt;&gt;22)&amp;0xFFF)+1)* //ways of associativity
<span class="number-left">20</span>	   (((ebx&gt;&gt;12)&amp;0x3FF)+1)* //physical line partns
<span class="number-left">21</span>	   ((ebx&amp;0xFFF)+1)* //cache line size
<span class="number-left">22</span>	   (ecx+1)); //number of sets
<span class="number-left">23</span>  }
<span class="number-left">24</span>​
</pre>
</div>

</div>
<div class="Indented">
Lines 4 to 7 issue the <tt>cpuid</tt> instruction with <span class="formula"><i>i</i> = 0, 1, …</span> For each value of <tt>i</tt>, <tt>cpuid</tt> returns information about a different cache. It signals that there are no more caches by putting zeros in the last four bits of <tt>%eax</tt>. On line 8, <tt>(eax&amp;0xF)</tt> extracts the last four bits of <tt>eax</tt> using a bitwise and operation with the hexadecimal number <tt>0xF</tt>. If <tt>cpuid</tt> has signaled that there are no more caches, line 9 uses <tt>break</tt> to exit from the loop. Lines 10 to 22 extract information about the cache.
</div>
<div class="Indented">
When the <tt>cpuid</tt> instruction was used to query a <span class="formula">2.6</span> GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines), it reported 32 KB of L1 instruction cache, 32 KB of L1 data cache, .26 MB of L2 cache, and 12.6 MB of L3 cache. On a <span class="formula">2.2</span> GHz AVX machine, the L3 cache is <span class="formula">20</span> MB. On a <span class="formula">3.6</span> GHz AVX2 machine, the L3 cache is <span class="formula">4</span> MB. These are some of the machines we use for timing programs.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Find the hex codes for the following instructions:
</div>
<ul>
<li class="nested">
<ul>
<li>
<tt>movq %rax</tt>, <tt>%rcx</tt>
</li>
<li>
<tt>movq %rax, %r15</tt>
</li>
<li>
<tt>movq %r9, %r10 </tt>
</li>

</ul>
<div class="--Separator--">
In MOVQ, Q stands for quad word, which is equal to 64 bits. In the Intel convention used by AMD/Intel manuals, the destination register comes first. In the ATT convention used by Unix machines, which we follow, the source comes first and the destination register comes last. You may consult the AMD and Intel manuals to answer this question. Alternatively, you may use the <tt>objdump</tt> utility. How many bytes is each instruction? Are all three instructions of the same length?
</div>

</li>

</ul>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why the arguments in a function call are pushed onto the stack in reverse order.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a function in assembly code that works as follows. The first argument is an integer <span class="formula"><i>n</i></span>. It is assumed that the next <span class="formula"><i>n</i></span> arguments are also integers, and the function should return the sum of these arguments if <span class="formula"><i>n</i> &gt; 0</span>. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Look up the hexadecimal opcodes of <tt>rdtsc</tt>, <tt>cpuid</tt>, and <tt>CALL</tt>. Which of these are serializing instructions? 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In a file called <tt>dummy.cpp</tt>, define a function <tt>void dummy()</tt>, which returns after doing nothing. In another file write a for-loop that makes <span class="formula"><i>n</i></span> calls to <tt>dummy()</tt>. Compile the files separately and then link them together to ensure that the compiler does not eliminate the function call. Verify that the function call is present by inspecting the assembly.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Time the for-loop using the Time Stamp Counter and with <span class="formula"><i>n</i></span> varying from <span class="formula">1</span> to <span class="formula">10<sup>9</sup></span>. To get a good reading for each <span class="formula"><i>n</i></span>, you can do several timings and then take the median. Graph the number of cycles as a function of <span class="formula"><i>n</i></span>.
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Fit the graph to the line <span class="formula"><i>An</i> + <i>B</i></span>. How good is the fit? Assuming that all the cycles are consumed by function calls and the overhead of timing, interpret <span class="formula"><i>A</i></span> and <span class="formula"><i>B</i></span>. Is <span class="formula"><i>B</i></span> positive or negative? The assumption is not perfectly valid, however. Some of the cycles will be consumed by instructions that increment and test the loop counter, and the branch prediction introduces some overhead as well. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the command <tt>cat /proc/cpuinfo</tt> to see some cpu parameters. Be aware that this command doubles the number of cores on certain Intel machines if hyper-threading is turned on. Hyper-threading is not good for some scientific applications. How much of the output can you verify using <tt>cpuid</tt>?  
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-3.2">3.2</a> Compiler optimizations<a class="Label" name="sec:proc-Compiler-optimizations"> </a>
</h2>
<div class="Unindented">
Anyone who seeks to write fast programs must begin by grasping what compilers do. C/C++ programs are turned into machine instructions by the compiler. The speed of the program can vary by a factor of <span class="formula">10</span>, or even more, depending on the instruction stream the compiler generates. While generating machine instructions, compilers can alter the structure of the program considerably.
</div>
<div class="Indented">
In sections <a class="Reference" href="#subsec:proc-compileropt-unrolling">3.2.2↓</a> through <a class="Reference" href="#subsec:proc-compileropt-mmult">3.2.5↓</a>, we go over a number of compiler optimizations of loops and loop nests. The two big lessons are to use the <tt>restrict</tt> qualifier, where appropriate, and to present loops to the compiler in as simple a form as possible. What is meant by simple is a little vague and will become clear during the discussion.
</div>
<div class="Indented">
Compilers can be highly unpredictable. The only way to get a sense of whether the compiler has done as well as expected is to look at the assembly code. Although writing assembly programs can be difficult, scanning the assembly code to find inner loops and checking the instruction mix is much easier to do. Thus, our discussion of compiler optimization is not so much about the optimizations themselves as it is about what to look for in the assembly code. There can be no rational discussion of program speed without an idea of what the assembly code looks like. 
</div>
<div class="Indented">
A dramatic example of the unpredictability of compilers occurs in section <a class="Reference" href="#subsec:proc-compileropt-mmult">3.2.5↓</a>. In that section, we look at a few simple programs for multiplying matrices. The programs are nearly identical, but their speeds can be quite different. That is not the dramatic part, however. The same programs are run on an SSE2 machine, an AVX machine, and an AVX2 machine that is theoretically more than <span class="formula">4</span> times faster than the SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines). Surprisingly, our simple matrix multiplication programs are faster on the SSE2 machine, which leaves us wondering whatever happened to the speedup of a factor of <span class="formula">4</span>. The SSE2 instruction set stabilized over a period of 10 years before it was superseded. The AVX2 instruction set is only a couple of years old as of this writing. Although AVX2 programs can be <span class="formula">4</span> times faster, compiler technology has not yet caught up.
</div>
<div class="Indented">
In section <a class="Reference" href="#subsec:proc-compileropt-C++-overhead">3.2.6↓</a>, we look at a C++ program for multiplying matrices, in which matrices and vectors are represented using classes. This program is slow because C++ constructs are used without an understanding of how they map to machine instructions. C++ programs can be as fast as programs in any other language, or they can be slow, depending on the skill and intention of the programmer. 
</div>
<div class="Indented">
Compilation is a difficult task because compilers have to look at a program character by character, token by token, and statement by statement until they have an internal representation of the whole program. The programmer’s view of a computer program is from the reverse direction. The programmer often starts with a problem to be solved and with a global view of a solution to it. The global view, which is so easy for a human programmer, is very hard to attain for the compiler. In the final section, <a class="Reference" href="#subsec:proc-compileropt-theory">3.2.7↓</a>, we explain a little theory to show how compilers go about building a global view. 
</div>
<div class="Indented">
Before getting into the meat of this section, we begin by discussing a few preliminaries.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.1">3.2.1</a> Preliminaries<a class="Label" name="subsec:proc-compileropt-prelims"> </a>
</h3>
<div class="Unindented">
Cache effects in simple trials can give unrealistically good figures for program speed. Cache flush, which we describe here, is one way to eliminate cache effects. We also review compiler options.
</div>
<div class="Indented">
The C++ classes <tt>PyPlot</tt> for plotting, <tt>StatVector</tt> for gathering simple statistics, and <tt>Table</tt> for making tables are used extensively in the source code for this book. However, they appear only rarely in the text. All three classes are described in the appendix.
</div>
<h? class="Subsubsection">
<b><u>Cache flush</u></b>
</h?>
<div class="Unindented">
When programs are timed, one may inadvertently ignore the effects of caching and come up with unrealistically good numbers. Suppose we initialize three matrices to time algorithms for matrix-matrix multiplication.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">    for(int i=0; i &lt; dim; i++) //intialize a, b and c
      for(int j=0; j &lt; dim; j++){
	    a[i+j*dim] = (1.0*rand()/RAND_MAX-0.5)*2;
	    b[i+j*dim] = (1.0*rand()/RAND_MAX-0.5)*2;
	    c[i+j*dim] = (1.0*rand()/RAND_MAX-0.5)*2;
      }
</pre>
</div>

</div>
<div class="Indented">
The C library function <tt>rand()</tt> generates a pseudorandom integer between zero and <tt>RAND_MAX</tt>. It is handy for timing and testing but may not be a well-tested random number generator.<span class="FootOuter"><span class="SupFootMarker"> [54] </span><span class="HoverFoot"><span class="SupFootMarker"> [54] </span>The general opinion that <tt>rand()</tt> is of poor quality appears to be from decades before. Whether that really applies to modern <tt>gcc</tt> and <tt>icpc</tt> runtime libraries is uncertain and possibly untrue.</span></span> When pseudorandom numbers with specific properties, such as uniformity and independence, are needed, a good library of statistical functions must be used instead. For our purposes here, <tt>rand()</tt> is adequate, but the manner in which the matrices <tt>a</tt>, <tt>b</tt>, and <tt>c</tt> are initialized means that they will all be in cache if <tt>dim</tt> is small. If <tt>dim=100</tt>, for example, the matrices will remain in the <tt>L2</tt> cache of any of the processors of table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>, which is more than 0.2 MB. In a realistic program, even small matrices are unlikely to be in cache when they are needed because the program may touch a lot of data before it starts operating on the small matrices. Thus, timing immediately after initialization can give unrealistically good numbers. 
</div>
<div class="Indented">
The matrices can be evicted from cache as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">    //clear a, b, c from cache
	for(int i=0; i &lt; dim; i++) 
      for(int j=0; j &lt; dim; j++){
	    _mm_clflush(a+i+j*dim);
	    _mm_clflush(b+i+j*dim);
	    _mm_clflush(c+i+j*dim);
      }
</pre>
</div>

</div>
<div class="Indented">
The function <tt>_mm_clflush()</tt> is an <i>intrinsic</i>. It corresponds directly to the instruction <tt>CLFLUSH</tt>. The effect of cache flush is to evict the entire cache line that corresponds to its argument. Its argument must be a pointer. All transfers to and out of cache occur in blocks or lines. A cache line is typically 64 bytes. The declaration of <tt>_mm_clflush()</tt> is made visible through the header file <tt>ia64intrin.h</tt> by the <tt>icpc</tt> compiler. After <tt>CLFLUSH</tt>, the cache line is found in DRAM memory but not in any of the caches. 
</div>
<div class="Indented">
The use of <tt>CLFLUSH</tt> can introduce artifacts and give pessimistic timing figures. There is no such thing as a perfect timing protocol. A better way is to arrange inputs to the program in a long array, which is larger than the size of the cache, and apply the program to the inputs in succession. For some programs, such as matrix multiplication, such a precaution is not really necessary, and the use of <tt>CLFLUSH</tt> may be much more convenient.
</div>
<h? class="Subsubsection">
<b><u>Compiler options</u></b>
</h?>
<div class="Unindented">
The other preliminary topic is the use of compiler options. We have recommended the options 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">-xHost -O3 -prec-div -no-ftz -restrict
</pre>
</div>

</div>
<div class="Indented">
for the <tt>icpc</tt> compiler. Here we add a few more comments. The <tt>icpc</tt> compiler has a default <tt>-fast</tt> option, which includes <tt>-xHost</tt> and <tt>-O3</tt>. The <tt>-xHost</tt> option ensures that the compiler generates instructions assuming the highest capability of the machine. The <tt>-O3</tt> option sets the optimization level. However, the <tt>-fast</tt> option is not suitable for our use. For one thing, it uses <tt>-no-prec-div</tt>, which we decided to abjure earlier. It turns on <tt>-ipo</tt> for interprocedural optimization, which interferes with many of the points we seek to make throughout this book. It is uncertain whether interprocedural optimization yields a measurable improvement for well-written programs. 
</div>
<div class="Indented">
An important compiler option shown above is <tt>-restrict</tt>. It is discussed in greater depth later. It enables the <tt>restrict</tt> qualifier, which is essential for making programs fast.
</div>
<div class="Indented">
Typically, we turn off function inlining using the <tt>-fno-inline-functions</tt> option (not shown above). Function inlining changes the structure of the code in ways that make discussion of the corresponding assembly code difficult. On many occasions, we have noticed that loops are not well optimized when functions are inlined. However, function inlining should probably be enabled in some programs. It is too important a part of C++ design to be simply turned off without thought. 
</div>
<h? class="Subsubsection">
<b><u></u></b>
</h?>
<h? class="Subsubsection">
<b><u></u></b>
</h?>
<div class="--Separator--">

</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.2">3.2.2</a> Loop unrolling<a class="Label" name="subsec:proc-compileropt-unrolling"> </a>
</h3>
<div class="Unindented">
A program to compute the <tt>n</tt>th partial sum of the Leibniz series follows:<a class="Label" name="proc-leibniz-partial-sum-cpp"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>//sum of first n terms of 4(1-1/3+1/5-1/7+1/9-...)
<span class="number-left">2</span>double leibniz(long int n){
<span class="number-left">3</span>	long int i;
<span class="number-left">4</span>	double ans;
<span class="number-left">5</span>	for(i=0; i &lt; n; i++)
<span class="number-left">6</span>		if(i==0)
<span class="number-left">7</span>			ans = 4.0;
<span class="number-left">8</span>		else if(i%2==1)
<span class="number-left">9</span>			ans -=  4.0/(2.0*i+1);
<span class="number-left">10</span>		else
<span class="number-left">11</span>			ans +=  4.0/(2.0*i+1);
<span class="number-left">12</span>	return ans;
<span class="number-left">13</span>}
</pre>
</div>

</div>
<div class="Indented">
This function will be run for large values of <tt>n</tt>, such as <span class="formula"><i>n</i> = 10<sup>9</sup> <span class="text"> or</span> 10<sup>10</sup></span>. With <span class="formula"><i>n</i> = 10<sup>9</sup></span>, the partial sum is generated in a few seconds.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:proc-compileropt-unroll-Leibniz"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="right" valign="top">

</td>
<td align="center" valign="top">
Cycles
</td>

</tr>
<tr>
<td align="right" valign="top">
Unoptimized code
</td>
<td align="center" valign="top">
32
</td>

</tr>
<tr>
<td align="right" valign="top">
With <tt>-xHost -03</tt> optimization
</td>
<td align="center" valign="top">
14
</td>

</tr>
<tr>
<td align="right" valign="top">
With <tt>-xHost -03</tt> optimization and loop unrolling
</td>
<td align="center" valign="top">
7
</td>

</tr>

</table>

</div>
<div class="caption">
Table 3.2 Number of cycles per term of the Leibniz series on a <span class="formula">3.6</span> GHz machine with AVX2 instructions (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full name of the machine).
</div>

</div>

</div>

</div>
<div class="Indented">
Normally, if a program is compiled with optimizations turned on, the resulting code can be two to three times faster. This speedup is not only because the compilers are clever, which they sometimes are, but also because the unoptimized code can be long and roundabout. Table <a class="Reference" href="#tab:proc-compileropt-unroll-Leibniz">3.2↑</a> shows that turning on compiler optimization doubles the speed of the code. Rewriting <tt>leibniz()</tt> to enable loop unrolling, which is one of the most important optimizations, doubles the speed once again. We will use the <tt>leibniz()</tt> function and its variants to explain how loop unrolling works. 
</div>
<div class="Indented">
First, we will understand why unoptimized code is nearly always quite slow. The first few lines of the unoptimized assembly of <tt>leibniz()</tt> are as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">        pushq     %rbp                                        
        movq      %rsp, %rbp                                  
        subq      $32, %rsp                                   
        movq      %rdi, -32(%rbp)                             
        movq      $0, -24(%rbp)                               
        movq      -24(%rbp), %rax                             
        movq      -32(%rbp), %rdx                             
        cmpq      %rdx, %rax                                  
        jl        ..B1.4        
        jmp       ..B1.9        
</pre>
</div>

</div>
<div class="Indented">
The function <tt>leibniz()</tt> receives its argument <tt><span class="formula"><i>n</i></span></tt> in the register <tt>%rdi</tt>. This code fragment is checking whether <span class="formula">0</span> is less than <span class="formula"><i>n</i></span>. If <span class="formula"><i>n</i> &gt; 0</span>, the program jumps to the address <tt>..B1.4</tt>. If not, it jumps to the address <tt>..B1.9</tt>, where it terminates quickly by returning <span class="formula">0</span> as the answer. Deciphering this code fragment is left to the reader. It is a  roundabout way to check whether <span class="formula"><i>n</i> &gt; 0</span> or not.
</div>
<div class="Indented">
The optimized code performs the same action as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">        testq     %rdi, %rdi  
        jle       ..B2.9  
</pre>
</div>

</div>
<div class="Indented">
The <tt>testq</tt> instruction ands the <tt>%rdi</tt> register, which contains the argument <span class="formula"><i>n</i></span>, with itself and sets certain flags. The instruction discards the result of anding and does not alter the contents of <tt>%rdi</tt>. If <span class="formula"><i>n</i></span> is negative, the result will be negative as well, and the sign flag is set. If <span class="formula"><i>n</i></span> is zero, the result is zero as well, and the zero flag is set. The next instruction <tt>jle</tt> jumps to a location where the program terminates with a <tt>ret</tt> statement if either the sign or zero flag is set, in other words, if <span class="formula"><i>n</i> ≤ 0</span>. The optimized code is checking whether <span class="formula"><i>n</i> ≤ 0</span> or <span class="formula"><i>n</i> &gt; 0</span> using a single instruction, while the unoptimized code takes more than half a dozen to do the same check. 
</div>
<div class="Indented">
We get a sense here of why unoptimized assembly is nearly always slow. Unoptimized assembly code is characterized by a certain listlessness. The compiler takes a local view of the program.
</div>
<div class="Indented">
Thus, if an expression such as <span class="formula">(<i>j</i> + 1)(<i>j</i> + 2)</span> occurs in multiple statements, the nonoptimizing compiler may fail to recognize that each instance evaluates to the same value. If the same variable is used repeatedly in a loop, the nonoptimizing compiler may fail to assign the variable to a register. Poor use of registers is typical of unoptimized code. Even when more than a dozen registers are available, unoptimized code often uses only a few. 
</div>
<div class="Indented">
With the <tt>-xHost -O3</tt> options, the <tt>icpc </tt>compiler generates code, which is more than twice as fast. As shown in table <a class="Reference" href="#tab:proc-compileropt-unroll-Leibniz">3.2↑</a>, the number cycles of per term of the Leibniz series decreases from <span class="formula">32</span> for the unoptimized code to <span class="formula">14</span> for the optimized code. The listing below displays the assembly of the for-loop of the <tt>leibniz()</tt> partial sum function on page <a class="Reference" href="#proc-leibniz-partial-sum-cpp">1↑</a>. We shall not examine the listing line by line. Our intention is to figure out whether the the compiler has done a good job. For that purpose, it suffices to scan the assembly of the inner loop. After the listing, we explain how to generate the assembly and locate the inner loop in it.
</div>
<div class="Indented">
<a class="Label" name="leibniz-inner-loop-assembly"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">..B3.4:
        vxorpd    %xmm3, %xmm3, %xmm3
        lea       1(%rax), %rsi
        movq      %rsi, %rdx
        vcvtsi2sdq %rsi, %xmm3, %xmm3
        shrq      $63, %rdx
        vfmadd213sd .L[omit]pkt.4(%rip), %xmm1, %xmm3
        lea       1(%rax,%rdx), %rcx
        andq      $-2, %rcx
        cmpq      %rax, %rcx
        je        ..B3.6
..B3.5:
        vdivsd    %xmm3, %xmm2, %xmm3
        vaddsd    %xmm3, %xmm0, %xmm0
        jmp       ..B3.7
..B3.6:
        vdivsd    %xmm3, %xmm2, %xmm3
        vsubsd    %xmm3, %xmm0, %xmm0
..B3.7:
        movq      %rsi, %rax
        cmpq      %rdi, %rsi
        jb        ..B3.4  
</pre>
</div>

</div>
<div class="Indented">
To generate the assembly, the compiler is invoked with the 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">-S -fno-verbose-asm
</pre>
</div>

</div>
<div class="Indented">
options, in addition to the compilation options, as in the pattern rule in section <a class="Reference" href="#sub:libmake-pattern-rules">2.3.4↑</a>. The assembly of the entire <tt>leibniz.cpp</tt> program, which contains the <tt>leibniz()</tt> function of page <a class="Reference" href="#proc-leibniz-partial-sum-cpp">1↑</a>, is more than 1,300 lines. Locating the inner loop within that assembly is an easy skill. One has to search for &ldquo;leibniz&rdquo; or its mangled name &ldquo;_Z7leibnizl&rdquo; (which may be found as explained in section <a class="Reference" href="#sec:libmake-mixedlang">2.1↑</a>), and the beginning of the function definition will be obvious. 
</div>
<div class="Indented">
Within the function definition, paying a little attention to the jump statements helps identify the inner loop. In our listing, the final line is a <tt>jb</tt> (jump if below) instruction, and it is jumping back to the instruction labeled <tt>..B3.4</tt>, which is the top of the loop. In between, we have two more jump instructions. The <tt>je</tt> (jump if equal) instruction jumps to the case where the term of the Leibniz series is negative and must be subtracted. The <tt>jmp</tt> (unconditional jump) instruction follows the addition of a positive term of the Leibniz series. It jumps to <tt>..B3.7</tt>, where the loop termination condition is verified before possibly jumping back to the top of the loop.
</div>
<div class="Indented">
Even in this short snippet of assembly code, there are many unfamiliar instructions. Because our purpose is only to understand the quality of the code generated by the compiler, we do not need to understand many of these. The crucial part for us is first the snippet
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">..B3.5:
        vdivsd    %xmm3, %xmm2, %xmm3
        vaddsd    %xmm3, %xmm0, %xmm0
</pre>
</div>

</div>
<div class="Indented">
In this assembly snippet, the first instruction divides the <tt>%xmm2</tt> register by the <tt>%xmm3</tt> register and stores the result in the register <tt>%xmm3</tt>. The source registers are <tt>%xmm3</tt> and <tt>%xmm2</tt>, and the destination register is <tt>%xmm3</tt>. The <tt>%xmm2</tt> has <span class="formula">4.0</span> and <tt>%xmm3</tt> has <span class="formula">2<i>i</i> + 1</span> (as a <tt>double</tt>). So the effect is to compute the <span class="formula">4 ⁄ (2<i>i</i> + 1)</span> term of the Leibniz series. In this case, <span class="formula"><i>i</i></span> is even. Therefore, the second instruction is adding the computed answer to <tt>%xmm0</tt>. The answer is being accumulated in <tt>%xmm0</tt>, and the approximation to <span class="formula"><i>π</i></span> will be found in this register at the end of the loop. The second snippet tackles the case where <span class="formula"><i>i</i></span> is even. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">..B3.6:
        vdivsd    %xmm3, %xmm2, %xmm3
        vsubsd    %xmm3, %xmm0, %xmm0
</pre>
</div>

</div>
<div class="Indented">
The second instruction here is <tt>vsubsd</tt> and not <tt>vaddsd</tt>. Therefore, the <span class="formula">4 ⁄ (2<i>i</i> + 1)</span> term is being subtracted and not added.
</div>
<div class="Indented">
How do we tell whether the compiled assembly is satisfactory? First, we may notice that the assembled code is using XMM registers even though wider YMM registers are available on this AVX2 machine. In fact, it is worse than that. Even the <span class="formula">128</span>-bit XMM registers (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>) are not being used fully. Each XMM register is wide enough to hold two <tt>double</tt>s. Yet the trailing <tt>sd</tt> in the <tt>vdivsd</tt>, <tt>vaddsd</tt>, and <tt>vsubsd</tt> instructions, which stands for &ldquo;single double,&rdquo; is indicating that only one half of the XMM registers is being used. The leading <tt>v</tt> in these instructions indicates that these are vector instructions that operate on the vector XMM/YMM/ZMM registers (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>). 
</div>
<div class="Indented">
To use the vector registers effectively, the compiler has to find parallelism in the loop. In this case, it has found no parallelism. The <span class="formula">4 ⁄ (2<i>i</i> + 1)</span> terms are being alternately added and subtracted in sequential order, just as in our program.
</div>
<div class="Indented">
The other deficiency in this assembly are the two branch statements (<tt>je</tt> and <tt>jmp</tt> instructions) in the loop body. Branches are undesirable because they interfere with instruction-level parallelism. The first two instructions in our assembly listing are as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">        vxorpd    %xmm3, %xmm3, %xmm3
        lea       1(%rax), %rsi
</pre>
</div>

</div>
<div class="Indented">
These two instructions operate on entirely different registers and may be executed in parallel. Very probably, that is just what the x86 processor does. The processor decodes multiple instructions simultaneously in a single cycle. In addition, it has multiple units to execute instructions. The processor constantly looks ahead in the instruction stream. Wherever it finds opportunities for parallelism, it schedules instructions for parallel execution. Thus, even a sequential program is executed in parallel. Much of the improvement in single-processor speeds over the last <span class="formula">15</span> years may be attributed to greater and greater instruction-level parallelism.
</div>
<div class="Indented">
Branches (or jump statements) interfere with instruction-level parallelism because it is impossible to tell in advance whether a conditional branch will be taken or not. This uncertainty may mean the processor cannot look ahead and execute instructions in parallel. In fact, branches are so common inside loops (in particular, there is always a branch at the end of the loop body) that instruction-level parallelism would be almost completely ineffective without branch prediction. Fortunately, much of the branching is highly predictable. In our assembly snippet, the <tt>je</tt> branch is alternately taken or not taken, and the <tt>jb</tt> branch at the end is always taken except at loop termination. Processors can predict such branches quite easily. If there is a misprediction, so that the wrong sequence of instructions have been executed, there are mechanisms to recover and resume with a correct instruction stream.<span class="FootOuter"><span class="SupFootMarker"> [55] </span><span class="HoverFoot"><span class="SupFootMarker"> [55] </span>For a great deal more about instruction-level parallelism and branch prediction, see <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span>. </span></span>
</div>
<div class="Indented">
If branches are likely to be predicted with nearly 100% accuracy in our assembled code, why are branches a problem here? There is a misprediction overhead that should be negligible for our program. To answer this question more fully, we have to go back to the C++ code of the <tt>leibniz()</tt> partial summation function on page <a class="Reference" href="#proc-leibniz-partial-sum-cpp">1↑</a>. The loop body of that function splits into three cases: <span class="formula"><i>i</i> = 0</span>, <span class="formula"><i>i</i></span> odd, and <span class="formula"><i>i</i></span> even. The compiler in fact has moved the <span class="formula"><i>i</i> = 0</span> case out of the loop. Only two cases occur within the loop body in the assembled code, so that the overhead of checking whether <span class="formula"><i>i</i></span> is zero does not occur in every iteration. The problem with the branching in the loop body is that it prevents the compiler from finding parallelism in the loop body. This is the reason the code is stuck with XMM registers and single double instructions. In general, it is a good idea to move <tt>if</tt> statements out of loop bodies in C/C++.
</div>
<div class="Indented">
In the following C++ definition, we have manually removed the even/odd branch from the loop body to illustrate how the compiler exploits parallelism in the loop body.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double leibnizX(long int n){
	long int i;
	double ans0=4.0;
	double ans1=0;
	for(i=2; i &lt; n; i=i+2)
		ans0 += 4.0/(2.0*i+1);
	for(i=1; i &lt; n; i=i+2)
		ans1 += 4.0/(2.0*i+1);
	return ans0-ans1;
}
</pre>
</div>

</div>
<div class="Indented">
The assembly code of this function is more than <span class="formula">300</span> lines and almost impossible to understand. The compiler goes to feast on this function and optimizes it very well. We show only three instructions in the body of the first loop to verify that the compiler has done a good job.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">vfmadd132pd .L_2il0floatpacket.8(%rip), %ymm0, %ymm6
vdivpd    %ymm6, %ymm2, %ymm3
vaddpd    %ymm7, %ymm3, %ymm9
</pre>
</div>

</div>
<div class="Indented">
In <tt>vfmadd132pd</tt>, <tt>vdivpd</tt>, and <tt>vaddpd</tt>, the <tt>v</tt> stands for vector as before, and <tt>pd</tt> stands for &ldquo;packed double.&rdquo; This means that the instructions are operating on all four <tt>double</tt>s that fit into a <span class="formula">256</span>-bit YMM register. Not only are YMM registers being used now, but they are being used in full. That is the main reason that this program takes only seven cycles per term and is twice as fast as the earlier program (see table <a class="Reference" href="#tab:proc-compileropt-unroll-Leibniz">3.2↑</a>).
</div>
<div class="Indented">
We have explained the <tt>vdivpd</tt> and <tt>vaddpd</tt> instructions above. In <tt>vfmadd132pd</tt>, the <tt>fmadd</tt> stands for fused multiply add (FMA). The code &ldquo;132&rdquo; indicates that the first and third operands are multiplied and added to the second source operand. The result is stored in the first operand (the destination). In our code snippet, the operands must be read in reverse (because of the difference between Intel and GNU/Linux conventions in assembly code). The first (or destination) operand is <tt>%ymm6</tt>, the second operand is <tt>%ymm0</tt>, and the third operand is a float packet (stored constant in text area). The effect of this instruction is <span class="formula"><i>i</i> → 2<i>i</i> + 1</span>.<span class="FootOuter"><span class="SupFootMarker"> [56] </span><span class="HoverFoot"><span class="SupFootMarker"> [56] </span>To decipher this assembly snippet, one needs its complete context, which is omitted from the text. Deciphering the meaning of the float packet is crucial. To decipher the float packet, one may search for it in the assembly code and find its definition to be &ldquo;4000000000000000&rdquo; in hexadecimal. A web service such as <a class="FlexURL" href="http://babbage.cs.qc.cuny.edu/IEEE-754.old/64bit.html">http://babbage.cs.qc.cuny.edu/IEEE-754.old/64bit.html</a> may be used to convert from the binary IEEE 754 format to decimal and discover that the float packet is <span class="formula">2.0</span>. Similarly, one may uncover that <tt>%ymm0</tt> is storing <span class="formula">1.0</span>, at which point it is almost obvious that <tt>%ymm2</tt> must be storing <span class="formula">4.0</span>.</span></span> The FMA instruction is not found in SSE or AVX (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>). 
</div>
<div class="Indented">
The compiler has &ldquo;unrolled&rdquo; the loop partially. Because the assembly code of the unrolled loop is rather opaque, we give a listing of a C++ definition that unrolls the loop manually. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double leibnizXX(long int n){
	long int i;
	double ans[10]={0};
	for(i=0; i &lt; n; i+=10){
		ans[0] += 4.0/(2.0*i+1);
		ans[1] += 4.0/(2.0*i+3);
		ans[2] += 4.0/(2.0*i+5);
		ans[3] += 4.0/(2.0*i+7);
		ans[4] += 4.0/(2.0*i+9);
		ans[5] += 4.0/(2.0*i+11);
		ans[6] += 4.0/(2.0*i+13);
		ans[7] += 4.0/(2.0*i+15);
		ans[8] += 4.0/(2.0*i+17);
		ans[9] += 4.0/(2.0*i+19);
	}
	return ans[0]+ans[2]+ans[4]+ans[6]+ans[8]
		-ans[1]-ans[3]-ans[5]-ans[7]-ans[9];
}
</pre>
</div>

</div>
<div class="Indented">
In this program, the terms of the Leibniz series are grouped into 10 sets, and each iteration of the loop updates the sum over each set. The statements inside the loop body are independent of each other and may be executed in parallel using the packed double instructions. Loop unrolling is one of the most popular and effective loop transformations.
</div>
<div class="Indented">
In the assembly of the loop bodies of <tt>leibnizX()</tt>, in fact four sets of packed double instructions operate on YMM registers. Because each YMM register is wide enough for four <tt>doubles</tt>, each loop has been unrolled by a factor of <span class="formula">16</span>. It is as if the <tt>i=i+2</tt> increment of the loop counter in each for-loop has been replaced by <tt>i=i+32</tt> and the loop body expanded to treat <span class="formula">16</span> terms in parallel.<span class="FootOuter"><span class="SupFootMarker"> [57] </span><span class="HoverFoot"><span class="SupFootMarker"> [57] </span>It may occur to the reader that incrementing the loop counter <span class="formula"><i>i</i></span> by <span class="formula">32</span> instead of <span class="formula">2</span> may cause a problem if the number of loop iterations is not a multiple of <span class="formula">16</span>. Indeed, it does, and the compiler has to generate code at either the beginning or  end to handle the case where the number of iterations is not a multiple of <span class="formula">16</span>. </span></span>
</div>
<div class="Indented">
As shown in table <a class="Reference" href="#tab:proc-compileropt-unroll-Leibniz">3.2↑</a>, the program with unrolled loops (with the unrolling done either manually or by the optimizing compiler) takes seven cycles per term, which is half that of the <tt>leibniz()</tt> function, which branched inside the loop body. It must be said that seven cycles per term is probably far from the optimum. Optimizing the program for the instruction pipeline, which is the topic of section <a class="Reference" href="#sec:proc-Optimizing-for-the-pipeline">3.3↓</a>, may even cut that number in half.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.3">3.2.3</a> Loop fusion
</h3>
<div class="Unindented">
Because the speed of modern x86 computers relies on instruction-level parallelism and vector registers, it is often a good idea to have a lot of parallelism in the innermost loops. If sets of instructions in the body of the innermost loop are independent of each other, the processor is likely to execute them in parallel. 
</div>
<div class="Indented">
Loop fusion addresses the situation where we have two distinct loops. It is assumed that the iterations of each loop have dependencies that make loop unrolling ineffective. In such a scenario, merging the bodies of the two loops may be the best way to produce a loop body that is amenable to instruction-level parallelism. The transformation where distinct loop bodies are merged is called loop fusion.
</div>
<div class="Indented">
For a simple example, we consider the power series for sine and cosine.<span class="FootOuter"><span class="SupFootMarker"> [58] </span><span class="HoverFoot"><span class="SupFootMarker"> [58] </span>These series were discovered by Madhava around 1400 AD according to <span class="bibcites">[<a class="bibliocite" name="cite-30" href="#biblio-30"><span class="bib-index">30</span></a>]</span>.</span></span> <div class="formula">
<span class="environment"><span class="arrayrow">
<span class="arraycell align-r">
sin<i>x</i>
</span>
<span class="arraycell align-l">
 = <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>x</i></span><span class="ignored">)/(</span><span class="denominator">1!</span><span class="ignored">)</span></span> − <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>x</i><sup>3</sup></span><span class="ignored">)/(</span><span class="denominator">3!</span><span class="ignored">)</span></span> + <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>x</i><sup>5</sup></span><span class="ignored">)/(</span><span class="denominator">5!</span><span class="ignored">)</span></span> − ⋯
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
 
</span>
<span class="arraycell align-l">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-r">
cos<i>x</i>
</span>
<span class="arraycell align-l">
 = 1 − <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>x</i><sup>2</sup></span><span class="ignored">)/(</span><span class="denominator">2!</span><span class="ignored">)</span></span> + <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>x</i><sup>4</sup></span><span class="ignored">)/(</span><span class="denominator">4!</span><span class="ignored">)</span></span> − ⋯
</span>

</span>
</span>
</div>
The following program computes the sine and cosine of <tt>x</tt> and returns them in the reference variables <tt>c</tt> and <tt>s</tt>. The number of terms of the series to be used is input to the function as <tt>n</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void sincos(double x, int n, double&amp; c, double&amp; s){
	c = s = 0;
	double ci = 1;
	for(int i=0; i&lt;n; i++){
		c += ci;
		ci *= -x*x/(2.0*i+2)/(2.0*i+1);
	}
	double si = x;
	for(int i=0; i&lt;n; i++){
		s += si;
		si *= -x*x/(2.0*i+3)/(2.0*i+2);
	}
}
</pre>
</div>

</div>
<div class="Indented">
The cosine loop accumulates the answer in <tt>c</tt> and stores the current term in <tt>ci</tt>. Each iteration updates the current term <tt>ci</tt>. When we look at the terms of the series for <span class="formula">cos<i>x</i></span>, we see a lot of parallelism because the terms can be grouped and summed in many different ways. However, when the loop presented to the compiler generates each term by updating the previous term, it is quite difficult for a compilation algorithm to automatically detect such parallelism. Similar comments apply to the sine loop.
</div>
<div class="Indented">
Thus, simply unrolling the loops will not introduce much parallelism into the instruction stream and will not enable the compiler to generate packed double instructions. 
</div>
<div class="Indented">
A better idea may be to fuse the two loops. To a human observer, it is obvious that the two loop bodies can be merged, assuming <tt>ci</tt> and <tt>si</tt> are appropriately initialized. However, even if the loops are fused, it will be hard for the compiler to generate packed double instructions. It is true that <tt>c</tt> and <tt>s</tt>, as well as <tt>ci</tt> and <tt>si</tt>, can be packed into the two halves of an XMM register. However, when the terms are updated, we will need an XMM register that saves <span class="formula"> − <i>x</i><sup>2</sup></span> in both its halves and, perhaps more problematically, another XMM register that stores <span class="formula">(2<i>i</i> + 2)(2<i>i</i> + 1)</span> and <span class="formula">(2<i>i</i> + 2)(2<i>i</i> + 3)</span> in its two halves. A human programmer can easily see how to get around the problem. One XMM register can hold <span class="formula">2<i>i</i> + 2</span> in both its halves, and a second XMM register can hold <span class="formula">2<i>i</i> + 1</span> and <span class="formula">2<i>i</i> + 3</span> in its two halves. The two XMM registers can be multiplied to get <span class="formula">(2<i>i</i> + 2)(2<i>i</i> + 1)</span> and <span class="formula">(2<i>i</i> + 2)(2<i>i</i> + 3)</span> and updated by adding <span class="formula">(2, 2)</span> saved in either an XMM register or a cached location. 
</div>
<div class="Indented">
All this proves to be too much for the <tt>icpc</tt> compiler, however. Here are some of the instructions in the body of the cosine inner loop on an SSE2-capable machine (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>):
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">        addsd     %xmm6, %xmm5                       
        subsd     %xmm1, %xmm8                       
        addsd     %xmm6, %xmm7                       
        addsd     %xmm6, %xmm5                       
        addsd     %xmm6, %xmm7                       
        divsd     %xmm5, %xmm8                       
        divsd     %xmm7, %xmm8                       
        mulsd     %xmm8, %xmm2                        
</pre>
</div>

</div>
<div class="Indented">
There is no need to decipher the assembly code. All that matters is that the cosine and sine loops are not fused, and both loops use single double instructions in the loop body (notice the <tt>sd</tt> in instruction mnemonics). On an AVX2 machine, the compiler does generate fused-multiply-add instructions, but the instructions are still single double, and loops are not fused.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.4">3.2.4</a> Unroll and jam
</h3>
<div class="Unindented">
In this section, we will study a compiler optimization that applies to nested loops. So far we have considered loop unrolling and loop fusion. In loop nests, it may be desirable to unroll the outer loop and fuse several copies of the inner loop that result. That is the unroll and jam transformation.
</div>
<div class="Indented">
The following C++ program computes a sine table:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">//sine table for x = (pi/(2*n))*i
//with i=0,...,n.
//therefore stab must be of size n+1.
void sinetable(int n, double *restrict stab){
	double dx = 3.14159265358979323846/(2*n);
	for(int i=0; i &lt;= n; i++){
		double x =  i*dx;
		stab[i] = 0;
		double si = x;
		for(int j=0; j &lt; 20; j++){
			stab[i] += si;
			si *= -x*x/((2*j+3)*(2*j+2));
		}
	}
}
</pre>
</div>

</div>
<div class="Indented">
This program computes <span class="formula">sin<i>x</i></span> for <span class="formula"><i>x</i> = 0, <i>π</i> ⁄ (2<i>n</i>), …, (<i>n</i> − 1)<i>π</i> ⁄ (2<i>n</i>), <i>π</i> ⁄ 2</span> and leaves the <span class="formula"><i>n</i> + 1</span> computed values in the array <tt>stab</tt>. If we choose <span class="formula"><i>n</i> = 360</span>, the resulting table corresponds to the chord table presented by Ptolemy in his treatise on mathematical astronomy, which became known as the <i>Almagest</i>.<span class="FootOuter"><span class="SupFootMarker"> [59] </span><span class="HoverFoot"><span class="SupFootMarker"> [59] </span>Chord tables are equivalent to sine tables, and one of the earliest chord tables was computed by Hipparchus around 150 BC. The first book of the <i>Almagest</i>, a textbook of mathematical astronomy written by Claudius Ptolemy of Alexandria around 150 AD, has a table of chords in sexagesimal. See <span class="bibcites">[<a class="bibliocite" name="cite-28" href="#biblio-28"><span class="bib-index">28</span></a>]</span>.</span></span> Ptolemy did not use power series, however. One of his key tools was a generalization of the Pythagoras theorem to cyclic quadrilaterals. 
</div>
<div class="Indented">
This program gives the type of <tt>stab</tt> as <tt>double *restrict</tt>. The <tt>restrict</tt> qualifier tells the compiler that any location addressed using the pointer will not be addressed using some other pointer. Its use is not significant here because only one pointer is in sight. However, in general, judicious use of the <tt>restrict</tt> qualifier can help the compiler generate good code. We will discuss the <tt>restrict</tt> qualifier in greater detail later.
</div>
<div class="Indented">
In this program, the outer loop variable <span class="formula"><i>i</i></span> generates different values for <span class="formula"><i>x</i></span> in the range <span class="formula">[0, <i>π</i> ⁄ 2]</span>. The inner loop variable <span class="formula"><i>j</i></span> generates terms of the power series of <span class="formula">sin<i>x</i></span>. We have fixed the number of terms at <span class="formula">20</span>. So the numerical bounds of the inner loop variable <span class="formula"><i>j</i></span> are known to the compiler. 
</div>
<div class="Indented">
If the compiler chooses to, it can unroll the inner loop completely. However, not much can be gained by unrolling the inner loop. The new value of <tt>si</tt>, which holds a term of the power series, is obtained using its previous value. If the loop is unrolled, this chain, in which each iteration of the inner loop depends on the previous iteration, remains intact, making it quite hard for the compiler to generate packed double instructions. 
</div>
<div class="Indented">
The assembly generated by the compiler does not unroll the inner loop nor does it unroll the outer loop on an SSE2-capable machine. Some of the instructions in the body of the inner loop are shown below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">        pxor      %xmm5, %xmm5                       
        subsd     %xmm1, %xmm5                       
        addsd     %xmm2, %xmm3                       
        cvtsi2sd  %r8d, %xmm4                        
        divsd     %xmm4, %xmm5                       
        mulsd     %xmm5, %xmm2                            
</pre>
</div>

</div>
<div class="Indented">
On an AVX2-capable machine, the compiler does unroll the inner loop, but the instructions are all single double, as they are here. So there is no real gain from the compiler’s code transformation. 
</div>
<div class="Indented">
Our experience with unroll and jam, so far, is similar to that with loop fusion. We wrote a program expecting the compiler to make a certain loop transformation, and it did not. This time we will persist and alter the program to get the compiler to generate better code.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void sinetable(int n, double *restrict xlist, 
	       double *restrict stab){
	for(int i=0; i &lt;= n; i++){
		stab[i] = 0;
		double si = xlist[i];
		for(int j=0; j &lt; 20; j++){
		  stab[i] += si;
		  si *= -xlist[i]*xlist[i]/((2*j+3)*(2*j+2));
		}
	}
}
</pre>
</div>

</div>
<div class="Indented">
This program takes in the list of values at which the sine function must be computed in the array <tt>xlist[0...n]</tt>, which is of length <span class="formula"><i>n</i> + 1</span>. The use of the <tt>restrict</tt> qualifier is crucial here. It tells the compiler that the arrays <tt>xlist</tt> and <tt>stab</tt> do not overlap in memory. Without that qualifier, the compiler has to allow for the case in which <tt>xlist</tt> and <tt>stab</tt> are indexed to address the same location in memory. Having to preserve the semantics of the program when the two arrays are aliased would preclude loop transformations. The assembly of the inner most loop follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">..B3.4:                         
        pxor      %xmm3, %xmm3                       
        lea       2(%rdi,%rdi), %r8d                 
        pxor      %xmm4, %xmm4                       
        lea       3(%rdi,%rdi), %r9d                 
        subpd     %xmm0, %xmm4                       
        addpd     %xmm1, %xmm2                       
        imull     %r8d, %r9d                         
        mulpd     %xmm0, %xmm4                       
        cvtsi2sd  %r9d, %xmm3                        
        incl      %edi                               
        unpcklpd  %xmm3, %xmm3                       
        cmpl      $20, %edi                          
        divpd     %xmm3, %xmm4                       
        mulpd     %xmm4, %xmm1                       
        jb        ..B3.4        
</pre>
</div>

</div>
<div class="Indented">
We do see packed double instructions here, indicating that the unroll-and-jam optimization has been carried out. On AVX/AVX2-capable machines, however, packed double instructions are not generated.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.5">3.2.5</a> Loop interchange<a class="Label" name="subsec:proc-compileropt-mmult"> </a>
</h3>
<div class="Unindented">
Loop interchange is the next compiler optimization for discussion, and matrix multiplication is the example we use to bring it out. In our discussion of matrix multiplication and loop interchanging, the following points will emerge:
</div>
<ul>
<li>
One should use <tt>restrict</tt> pointers as far as possible. The use of <tt>restrict</tt>-qualified pointers can speed up a program by more than a factor of <span class="formula">2</span>.
</li>
<li>
It is best to present loops to compilers in a simple and transparent form.
</li>
<li>
Although AVX2-capable machines are faster than SSE2-capable machines, generating AVX2 code is a bigger struggle for a compiler. Therefore, much of the advertised advantage of an AVX2 machine over an SSE2 machine may not be realized.
</li>
<li>
Compilers are capricious, and no assumptions about the generated assembly code can be made without inspecting it. 
</li>
<li>
Optimizing for the instruction pipeline can yield far greater speedups on more recent processors than on earlier ones.
</li>

</ul>
<div class="Unindented">
Some of these points have come up already. Many of these points are much broader in scope than any particular compiler optimization. 
</div>
<div class="Indented">
Before we begin, we make some remarks about the peak capabilities of SSE2- and AVX2-capable machines. As shown in table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>, SSE2 provides XMM registers wide enough to hold two <tt>double</tt>s. The <tt>mulpd</tt> (or <tt>vmulpd</tt>) instruction applied to XMM registers carries out two multiplications in a single instruction. Similarly, the <tt>addpd</tt> (or <tt>vaddpd</tt>) instruction carries out two additions in a single cycle. Typical SSE2 processors can simultaneously issue an <tt>addpd</tt> and a <tt>mulpd</tt> to separate execution units in the same cycle. Therefore, the peak capability of a single SSE2 processor core is <span class="formula">4</span> flops (floating points operations) per cycle. 
</div>
<div class="Indented">
The AVX2 processor has YMM registers, which are twice as wide as XMM. In addition, it has the <tt>fmadd*pd</tt> instruction, which does a fused multiply add of the type <span class="formula"><i>c</i> = <i>c</i> + <i>ab</i></span> on the entire width of the YMM registers. A single <tt>fmadd*pd</tt> instruction is equal to <span class="formula">8</span> flops. The AVX2 processor can issue two of these instructions to separate execution units in the same cycle. Therefore, the peak capability of a single AVX2 processor is <span class="formula">16</span> flops per cycle.
</div>
<div class="Indented">
Some Intel processors accelerate the clock in-core. For such machines, the actual theoretical limit can be slightly greater than the bounds derived above. Neither the <span class="formula">2.6</span> GHz SSE2 machine nor the <span class="formula">3.6</span>  GHz AVX2 machine of table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> features in-core acceleration of the clock.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:proc-flops-ijk-mmult"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top" style="width: 0.8in;">
<tt>dim=1000 </tt>(SSE2)
</td>
<td align="center" valign="top" style="width: 0.8in;">
<tt>dim=2000</tt> (SSE2)
</td>
<td align="center" valign="top" style="width: 0.8in;">
<tt>dim=1000 </tt>(AVX2)
</td>
<td align="center" valign="top" style="width: 0.8in;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
<tt>dim=2000</tt><br/>
(AVX2)<tt> </tt>
</div>

</td>

</tr>
<tr>
<td align="center" valign="top">
<tt>multijk()</tt>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.37</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.22</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.62</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.46</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<tt>multijkx()</tt>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.40</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.33</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.76</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.53</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<tt>multIJK()</tt>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.72</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.62</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.30</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.21</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<tt>multIJKX()</tt>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.72</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.62</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.31</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">1.21</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
MKL BLAS
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">3.81</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">3.84</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">14.0</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">14.6</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 3.3 Floating point operations per cycle in the multiplication of square matrices. The SSE2 and AVX2 processors used here had clocks of <span class="formula">2.6</span> and <span class="formula">3.6</span> GHz, respectively (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines used).
</div>

</div>

</div>

</div>
<div class="Indented">
All the points we wish to make in this section emerge from table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>. One of the points is so glaring that we will begin by commenting on it. The point is that matrix multiplication is much faster using Intel’s MKL library. On SSE2, MKL is more than twice as fast as functions such as <tt>multijk()</tt> and <tt>multIJK()</tt> that we write in C/C++. The programs that we write are the type one can write in C/C++ and are not optimized for the instruction pipeline. MKL’s optimization for the instruction pipeline, which must be coded in assembly, yields a far greater speedup of more than a factor of <span class="formula">10</span> on AVX2 machines. In both cases, MKL comes close to the peak floating point throughput, while the C/C++ programs fall short. In fact, the C/C++ programs do better in SSE2 than in AVX2, although the peak capability of the more recent AVX2 architecture is four times as high.
</div>
<div class="Indented">
We step through four simple implementations of matrix multiplications. These are the sort of C/C++ programs one may be expected to write. For each implementation, we explain how the compiler views the program and what it does with it. The sort of optimizations that yield the amazing speed of MKL BLAS are discussed in section <a class="Reference" href="#sec:proc-Optimizing-for-the-pipeline">3.3↓</a>. 
</div>
<div class="Indented">
The <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of the square matrix <tt>a</tt> of dimension <tt>dim</tt> is <tt>a[i+j*dim]</tt> (we assume that the leading dimension is equal to <tt>dim</tt>). Here <tt>a</tt> is of type <tt>double *</tt> and points to the first location of the contiguous segment of memory where the <tt>dim*dim</tt> entries of the matrix are stored. As always, we assume that matrices are stored columnwise. 
</div>
<div class="Indented">
The function <tt>multijk()</tt> defined below multiplies matrices <tt>a</tt> and <tt>b</tt> and adds their product to the matrix <tt>c</tt>.<a class="Label" name="multijk-function"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void multijk(double *a, double *b, double *c, int dim){
	for(int i=0; i &lt; dim; i++)
		for(int j=0; j &lt; dim; j++)
			for(int k=0; k &lt; dim; k++)
				c[i+j*dim] += a[i+k*dim]*b[k+j*dim];
}
</pre>
</div>

</div>
<div class="Indented">
None of the pointers is qualified as <tt>restrict</tt>. Thus, the arrays <tt>a</tt>, <tt>b</tt>, and <tt>c</tt> could be indexed to point to the same location, and the compiler has to preserve program semantics even in the event of such aliasing. There is no room for the compiler to apply loop transformations.
</div>
<div class="Indented">
On both SSE2 and AVX2, the compiler-generated assembly code (not shown) closely follows the C code of <tt>multijk()</tt>. There is no dramatic code transformation. From table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>, we see that AVX2 is nearly twice as fast for <tt>multijk()</tt>. That speedup is easily explained. On both SSE2 and AVX2, the compiler generates only single double instructions, and nothing more can be expected. On SSE2, the single double instructions are <tt>vaddsd</tt> and <tt>vmulsd</tt>. On AVX2, they are <tt>fmadd*pd</tt>. The speedup of two results because the fused-multiply-add combines addition and multiplication into a single instruction.
</div>
<div class="Indented">
Next we make a slight change to the matrix multiplication program.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void multijkx(double *a,double *b,double *c,int dim){
	for(int i=0; i &lt; dim; i++)
		for(int j=0; j &lt; dim; j++){
			double x = c[i+j*dim];
			for(int k=0; k &lt; dim; k++)
				x += a[i+k*dim]*b[k+j*dim];
			c[i+j*dim] = x;
		}
}
</pre>
</div>

</div>
<div class="Indented">
This program does not use <tt>restrict</tt> pointers either. However, it alters the innermost loop and makes it accumulate its computation in the scalar variable <span class="formula"><i>x</i></span>. Even if the arrays are aliased, the addition operations in the innermost loop can be done or grouped in any order, addition being commutative and associative (modulo rounding errors).<span class="FootOuter"><span class="SupFootMarker"> [60] </span><span class="HoverFoot"><span class="SupFootMarker"> [60] </span>On very rare occasions, code transformations effected by compilers under the assumption that machine arithmetic is commutative and associative may become problematic.</span></span> The <tt>icpc</tt> compiler exploits that room to unroll the innermost loop and generate packed double instructions on both SSE2 and AVX2. From table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>, we see that this optimization results in better performance for both matrices of dimension <span class="formula">1000</span> and <span class="formula">2000</span>, although the improvement is minor. AVX2 is again about twice as fast and for the same reason as before.
</div>
<div class="Indented">
The programs below are exactly the same except all the pointers are qualified with <tt>restrict</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void multIJK(double *restrict a, double *restrict b, 
	     double *restrict c, int dim){
	for(int i=0; i &lt; dim; i++)
		for(int j=0; j &lt; dim; j++)
			for(int k=0; k &lt; dim; k++)
				c[i+j*dim] += a[i+k*dim]*b[k+j*dim];
}
​
void multIJKX(double *restrict a, double *restrict b, 
	      double *restrict c, int dim){
	for(int i=0; i &lt; dim; i++)
		for(int j=0; j &lt; dim; j++){
			double x = c[i+j*dim];
			for(int k=0; k &lt; dim; k++)
				x += a[i+k*dim]*b[k+j*dim];
			c[i+j*dim] = x;
    }
}
</pre>
</div>

</div>
<div class="Indented">
These programs are nearly four times faster on SSE2 and twice as fast on AVX2 (see table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>). We may wonder why they are four times faster on SSE2 and only twice as fast on AVX2. Inspection of the assembly code will bring out the explanations. In fact, overall, the program is faster on SSE2 than AVX2, although the peak capability of the latter is four times as high (the peak capability factor is more than <span class="formula">4</span> if we allow for the clock speeds reported in table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>). 
</div>
<div class="Indented">
The crucial point here is that because <tt>a[]</tt>, <tt>b[]</tt>, and <tt>c[]</tt> are all <tt>restrict</tt> qualified, the compiler knows that the arrays are not supposed to alias. The compiler may assume that the arrays are nonoverlapping and distinct areas of memory. 
</div>
<div class="Indented">
On SSE2, the compiler takes full advantage of those assumptions and changes the order of nesting of the loops so that the <span class="formula"><i>i</i></span>-loop becomes innermost and the <span class="formula"><i>j</i></span>-loop becomes outermost. The innermost loop is unrolled, and packed double instructions are generated. 
</div>
<div class="Indented">
Why does the compiler make the <span class="formula"><i>i</i></span>-loop the innermost (on SSE2) and why does that lead to a faster program? In our discussion of compiler optimization so far, we have focused on arithmetic operations. We have seen that parallelism and packed double instructions in the innermost loops are likely to lead to faster programs. In addition to instruction-level parallelism and the judicious ordering of arithmetic, an optimizing compiler will do well to consider the pattern of memory access.
</div>
<div class="Indented">
For matrix multiplication, the pattern of memory access has a big impact on program performance. When the <span class="formula"><i>k</i></span>-loop is innermost, each successive location of the <tt>a[]</tt> array accessed in the innermost loop is separated from the previous location by a stride of length <tt>dim</tt>. Similarly, if the <span class="formula"><i>j</i></span>-loop is the innermost, the accesses of the <tt>b[]</tt> in the innermost loop have a stride equal to <tt>dim</tt>. Striding has the disadvantage that when a <tt>double</tt> word is brought into memory, we do not immediately use the other <tt>double</tt> words in the same cache line. 
</div>
<div class="Indented">
When the <tt>i</tt>-loop is innermost, successive iterations of the innermost loop touch the same location in the <tt>c[]</tt> array but move through the <tt>a[]</tt> and <tt>b[]</tt> arrays with a stride equal to <span class="formula">1</span>. That leads to better utilization of the cache line. The cache line is typically <span class="formula">64</span> bytes (large enough for <span class="formula">8</span> <tt>double</tt>s). 
</div>
<div class="Indented">
Unfortunately, on AVX2, the compiler does not interchange the loops. It simply unrolls the inner loop and generates packed double instructions. Cache line utilization is therefore poor. The failure to unroll loops on AVX2 could be because the <tt>icpc </tt>compiler is not as mature for AVX2 as it is for the earlier SSE2 architecture. The size of the assembly code for SSE2 is nearly a 1,000 lines. For AVX2, it is fewer than <span class="formula">200</span> lines, showing that compilers fare worse with more modern architectures.
</div>
<div class="Indented">
However, it may not be simply a case of compiler maturity. As computer architecture advances, for example, with vector registers getting wider, compilation becomes harder, and the compiled assembly code is more likely to fall short of being optimal. This point may be illustrated by comparing against the AVX case, which was omitted from table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>. On a <span class="formula">2.2</span>  GHz AVX machine, the compiler generates assembly code for <tt>multIJK()</tt> that is very similar to the assembly code for SSE2, except with YMM registers in place of XMM registers. Nevertheless, the speed of the compiled code is less than 15% of MKL, whereas on the SSE2 machine of table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>, the compiled code runs at more than 40% of MKL’s speed. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.6">3.2.6</a> C++ overhead<a class="Label" name="subsec:proc-compileropt-C++-overhead"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:proc-flops-ijk-cpp"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<tt>multijk()</tt>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<div class="PlainVisible">
<tt>dim</tt>=1000
</div>
<div class="PlainVisible">
(SSE2)
</div>

</td>
<td align="center" valign="top" style="width: 0.8in;">
<div class="PlainVisible">
<tt>dim</tt>=2000
</div>
<div class="PlainVisible">
(SSE2)
</div>

</td>
<td align="center" valign="top" style="width: 0.8in;">
<div class="PlainVisible">
<tt>dim</tt>=1000
</div>
<div class="PlainVisible">
(AVX2)
</div>

</td>
<td align="center" valign="top" style="width: 0.8in;">
<div class="PlainVisible">
<tt>dim</tt>=2000
</div>
<div class="PlainVisible">
(AVX2)
</div>

</td>

</tr>
<tr>
<td align="center" valign="top">
Using <tt>Matrix</tt> objects
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">.10</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">.07</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.18</span>
</td>
<td align="center" valign="top" style="width: 0.8in;">
<span class="formula">0.15</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 3.4 Floating point operations per cycle. Compare with table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>.
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:proc-flops-ijk-cpp">3.4↑</a> shows performance data for two implementations of matrix multiplication in C++. The implementations of section <a class="Reference" href="#subsec:proc-compileropt-mmult">3.2.5↑</a> are also in C++, but the syntax and style properly belong to C. They are C++ functions mainly because C++ is an extension of C. The functions were coded using arrays and pointer arithmetic. In contrast, the C++ function timed in table <a class="Reference" href="#tab:proc-flops-ijk-cpp">3.4↑</a> uses <tt>Matrix</tt> class objects (see chapter <a class="Reference" href="#chap:C/C++:-Review">1↑</a> for the definition of the <tt>Vector</tt> class, the <tt>Matrix</tt> class is similar).<span class="FootOuter"><span class="SupFootMarker"> [61] </span><span class="HoverFoot"><span class="SupFootMarker"> [61] </span>The timing information was collected with function inlining enabled.</span></span> 
</div>
<div class="Indented">
If matrices are multiplied using objects of the class <tt>Matrix</tt>, the C++ syntax mimics mathematical syntax quite closely as shown below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void multijk(Matrix&amp; A, Matrix&amp; B, Matrix&amp; C){
	int l = A.getm();
	int m = A.getn();
	assrt(B.getm()==m);
	int n = B.getn();
	assrt(C.getm()==l);
	assrt(C.getn()==n);
	for(int i=0; i &lt; l; i++)
		for(int j=0; j &lt; m; j++)
			for(int k=0; k &lt; n; k++)
				C(i,k) += A(i,j)*B(j,k);
}
</pre>
</div>

</div>
<div class="Indented">
The price paid for staying close to mathematical syntax is very poor performance. Comparing tables <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a> and <a class="Reference" href="#tab:proc-flops-ijk-cpp">3.4↑</a>, we find that on AVX2, the C++ is slower than MKL BLAS by a whopping factor that is nearly <span class="formula">100</span>!
</div>
<div class="Indented">
The matrix multiplication program here may look nice to a human reader. But it looks far more complex than a function such as <tt>multIJK()</tt> defined earlier to the compiler. A usage such as <tt>C(i,j)</tt> is in fact an overloaded function call. It is more difficult for the compiler to figure out the pattern in which the indices are mapping to locations in the array. If we want the compiler to do a good job, we have to make the loops easy to read and optimize for the compiler.
</div>
<div class="Indented">
Of course, the fault here is not that of C++. C++ provides a gigantic vocabulary and numerous modes of expression. It is for the programmer to decide how to use it. 
</div>
<div class="Indented">
To end this discussion, we turn to the persistent belief in some quarters that Fortran programs are faster than C or C++ programs. The basis of this belief is that arrays do not alias in Fortran the way pointers can alias in C/C++. As we have seen in this chapter, the assurance of no aliasing allows the compiler to generate better code. Given that the <tt>restrict</tt> qualifier was introduced only around 2000 and was implemented by compilers only several years after that, one has to acknowledge that there is some basis to that belief. Modern C++ compilers such as <tt>icpc</tt> and <tt>g++</tt> support <tt>restrict</tt> pointers. So the point about aliasing is no longer valid.
</div>
<div class="Indented">
Once C and C++ are well understood, their advantages are considerable. The C language often encourages and seldom impedes thinking of how program variables and objects are laid out in memory. A variety of data structures ranging from linked lists to trees to graphs to hash tables can be implemented with a suppleness that is not approached by languages such as Fortran. While C and C++ are routinely used to implement device drivers, operating systems, compilers, and network protocols, the limitations of Fortran’s idiom would become immediately apparent if anyone attempted to use it for such complicated programming tasks. Even when the programming task is not so complicated, C/C++ have much to offer if the idiom native to these languages is understood.
</div>
<div class="--Separator--">

</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.2.7">3.2.7</a> A little compiler theory<a class="Label" name="subsec:proc-compileropt-theory"> </a>
</h3>
<div class="Unindented">
The compiler’s view of a program is quite different from that of a human programmer. A human programmer has a problem to be solved and an idea to solve that problem that is expressed as a computer program. What the compiler sees is a sequence of statements that obey the syntactic rules of the programming language. The global view of what the program does is lost.
</div>
<div class="Indented">
To generate good assembly, the compiler has to grasp which variables are being used heavily and other global aspects of the program. Uncovering global information from a program, which is presented to the compiler as a sequence of statements, is quite hard. In this section, we will consider some of the ideas on which optimizing compilers rely.<span class="FootOuter"><span class="SupFootMarker"> [62] </span><span class="HoverFoot"><span class="SupFootMarker"> [62] </span>Our main reference is <span class="bibcites">[<a class="bibliocite" name="cite-32" href="#biblio-32"><span class="bib-index">32</span></a>]</span>. In it may be found bibliographic discussions of the work of Kuck, Bannerjee, Kennedy, and other researchers. Our earlier discussion of compiler optimizations drew heavily from this reference.</span></span>
</div>
<div class="Indented">
In a sequential program, the statements depend on one another. We assume that <tt>x</tt>, <tt>y</tt>, and <tt>z</tt> are program variables. The following is an example:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">y=x;
...
z=x;
</pre>
</div>

</div>
<div class="Indented">
Here the second statement has an input dependence on the first because both of them read the same variable <tt>x</tt>. The input dependence is also called Read After Read (RAR).<span class="FootOuter"><span class="SupFootMarker"> [63] </span><span class="HoverFoot"><span class="SupFootMarker"> [63] </span>The RAR, RAW, WAR, and WAW terminology for dependencies is from <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span>.</span></span> RAR is the mildest form of dependence and poses virtually no obstacle to the code transformations the compilation algorithm may want to attempt. If we have a sequence of statements and the only dependencies between them are RAR, the statements can be permuted in any manner without changing the semantics of the program.
</div>
<div class="Indented">
The Read After Write (RAW) dependence is also called a true dependence.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">x=y;
...
z=x;
</pre>
</div>

</div>
<div class="Indented">
Here the variable <tt>x</tt> is read by the second statement, which assigns it to <tt>z</tt>, after the first statement writes into <tt>x</tt> by assigning <tt>y</tt> to it. For a compiler, RAW is the most problematic dependence. When a statement consumes the output produced by a previous statement, the ordering of the statements cannot be changed. The notion of what it means for a program to be sequential is closely tied to the RAW dependence.
</div>
<div class="Indented">
There are two more kinds of dependencies. In Write After Read (WAR), a location is read and then overwritten as in the example below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">y=x;
...
x=z;
</pre>
</div>

</div>
<div class="Indented">
Here the variable <tt>x</tt> is read by the first statement and then overwritten by the second statement. WAR is also called an antidependence. Write After Write (WAW) is just what the reader may expect it to be, as shown by the example below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">x=y;
...
x=z;
</pre>
</div>

</div>
<div class="Indented">
In this example, the two statements have a WAW dependence because both write over WAW. The WAW dependence is also called an output dependence. If we interchange two statements with a WAR or WAW dependence, the semantics of the program may be affected because of a statement in the passage in between that has a RAW dependence on the first statement. Once again, we see how important RAW dependencies are for program correctness. A sequential program is sequential because of RAW dependencies.
</div>
<div class="Indented">
Programs spend little time executing straight line code. Much of the time is spent inside loops. Therefore, the compiler has to pay special attention to dependencies carried by loops. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  s = 0;
  for(int i=0; i &lt; n; i++)
    s = s + a[i];
</pre>
</div>

</div>
<div class="Indented">
This loop computes the sum of an array. To find the loop-carried dependencies, it is helpful to think of the loop in its fully unrolled form.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  s = s + a[0];
  s = s + a[1];
  s = s + a[2];
  ...
  s = s + a[n-1];
</pre>
</div>

</div>
<div class="Indented">
In this sequence of statements, each statement reads and writes into the program variable <tt>s</tt>. Because the dependencies emerge after the loop is unrolled, we say that the statement within the for-loop has loop-carried RAR, RAW, WAR, and WAW dependencies on itself. 
</div>
<div class="Indented">
The following loop computes the cumulative sum of the entries of an array:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(int i=1; i &lt; n; i++)
    a[i] = a[i]+a[i-1];
</pre>
</div>

</div>
<div class="Indented">
The only statement in this for-loop has loop-carried RAR and RAW dependencies on itself. 
</div>
<div class="Indented">
Such dependence analysis is used by compilers to figure out whether statements can be reordered and whether loop transformations of the type we studied in earlier sections may be applicable. For example, if there are no loop-carried dependencies at all, the iterations of the loop can be executed in any order. A for-loop that doubles every entry of an array has no loop-carried dependencies. 
</div>
<div class="Indented">
If there are no cyclic dependencies within the body of a loop, the loop can be split into independent loops. In the loop below, we assume that there is no aliasing between the <tt>a</tt>, <tt>b</tt>, and <tt>c</tt> arrays. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(int i=0; i &lt; n-1; i++){
    a[i+1] = b[i] + c[i];
    b[i] = a[i];
  };
</pre>
</div>

</div>
<div class="Indented">
This loop carries a RAW dependence. However, there is no cycle of dependencies between the two statements in the body of the loop. Therefore, it is legal to rewrite the code as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(int i=0; i &lt; n-1; i++)
    a[i+1] = b[i] + c[i];
  for(int i=0; i &lt; n-1; i++)
    b[i] = a[i];
</pre>
</div>

</div>
<div class="Indented">
The theory of dependence analysis is used by compilers to ascertain the validity of loop transformations. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  We may write a function <tt>double</tt> <tt>leibniz(int n)</tt> that returns the partial sum of the first <span class="formula"><i>n</i></span> terms of the Leibniz series. We may alter the definition of <tt>leibniz()</tt> and make <tt>n</tt> a <tt>const</tt> variable whose value is <span class="formula">20</span> instead of passing it as an argument. Examine the assembly code generated by <tt>icpc</tt> assuming such a modification and comment.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose that <tt>n</tt> is a <tt>const</tt> whose value is <span class="formula">10<sup>9</sup></span> and not <span class="formula">20</span> as in the previous exercise. Time <tt>leibnizX()</tt>. How many cycles per term does it take? Discuss the observed cycle count by relating to the assembly code generated by the compiler.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Compile the programs for summing the Leibniz series using <tt>-fast</tt>, thus replacing <tt>-prec-div</tt> we normally use by <tt>-no-prec-div</tt>. Is there any improvement in speed?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a function in C that takes a double-precision array <tt>a[0..n-1]</tt> as input, determines the sums <span class="formula"><span class="limits"><span class="limit">∑</span></span><span class="scripts"><sup class="script"><i>i</i> &lt; (<i>n</i> − <i>r</i>) ⁄ 4</sup><sub class="script"><i>i</i> = 0</sub></span><i>a</i><sub>4<i>i</i> + <i>r</i></sub></span> for <span class="formula"><i>r</i> = 0, 1, 2, 3</span>, and return the value of <span class="formula"><i>r</i></span> for which the sum is maximum. Make sure that the loops are unrolled and that packed double instructions are being issued.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a function in C that takes a double-precision array <tt>a[0..n-1]</tt> as input and replaces <span class="formula"><i>a</i><sub><i>i</i></sub></span> by the sum <span class="formula"><span class="limits"><span class="limit">∑</span></span><span class="scripts"><sup class="script"><i>j</i> = <i>i</i></sup><sub class="script"><i>j</i> = 0</sub></span><i>a</i><sub><i>j</i></sub></span> for <span class="formula"><i>i</i> = 0, …, <i>n</i> − 1</span>. Can you get the compiler to unroll the loop and issue packed double instructions in this example?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Look up the <tt>icpc</tt> compiler documentation and figure out what <tt>-fast</tt> does. Suppose both <tt>-fast</tt> and <tt>-prec-div</tt> are given as compiler options. Which will take precedence? Suppose both <tt>-fast</tt> and <tt>-O2</tt> are given as compiler options. Which will take precedence?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Change the program for computing the sine and cosine at the value <span class="formula"><i>x</i></span> to a program that computes the cosine at two input values <span class="formula"><i>x</i></span> and <span class="formula"><i>y</i></span>. Present two separate loops to the compiler as in the program above. Does the compiler fuse the loops and generate packed double instructions?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Fuse the two loops in the program that computes sine and cosine of <span class="formula"><i>x</i></span> manually. Try to write the body of the loop in such a way that the compiler generates packed double instructions. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Utilize the parallelism within the power series expansions of <span class="formula">sin<i>x</i></span> and <span class="formula">cos<i>x</i></span> to write loops that are partially unrolled and for which the compiler can easily generate packed double instructions. Compare the execution times of programs that compile to packed double instructions with those that compile to single double instructions. It is useful to try large values of <span class="formula"><i>n</i></span> when timing the loops even though the series converge rapidly, making any value of <span class="formula"><i>n</i></span> above <span class="formula">20</span> or so superfluous.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Change the first definition of <tt>sinetable()</tt> so that the loops are interchanged. Does the compiler now generate packed double instructions?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In the second definition of <tt>sinetable()</tt>, can the computation of <span class="formula"> − <i>x</i><sup>2</sup></span> from <span class="formula"><i>x</i></span> be moved out of the inner loop in the generated assembly code? 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program that replaces the columns of a square matrix with the cumulative column sum. In other words, the <span class="formula"><i>k</i></span>th column must be replaced by the sum of columns <span class="formula">0</span> through <span class="formula"><i>k</i></span>. Assuming the program uses a loop over rows and a loop over columns, what is the better way to order the loops? If the loops are not ordered the better way, does the compiler switch the order of loops?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program that computes the cumulative row sums of a square matrix and answer the same questions as above.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Alter the implementation of the <tt>Vector</tt> and <tt>Matrix</tt> classes so that member functions that overload the function call operator <tt>()</tt> cannot be inlined. How does the matrix multiplication function of section <a class="Reference" href="#subsec:proc-compileropt-C++-overhead">3.2.6↑</a> perform after the alterations?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why the for-loop<div class="listing">
<pre class="listing">  for(int i=0; i &lt; n-1; i++){
    a[i+1] = b[i] + c[i];
    b[i] = a[i];
  };
</pre>
</div>
carries the RAW dependence.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Give examples of for-loops whose bodies have just a single statement and that exhibit loop-carried dependence of exactly one of the types RAR, RAW, WAR, and WAW.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why the following loop<div class="listing">
<pre class="listing">  for(int i=1; i &lt; n; i++){
    a[i] = b[i]+c[i];
    d[i] = a[i-1];
  }
</pre>
</div>
carries the RAW dependence. Rewrite the loop so that there is no loop-carried dependence.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Assume that the array <tt>a</tt> stores a square matrix of size <tt>dim</tt>. In the loop nest<div class="listing">
<pre class="listing">  for(int i=1; i &lt; n; i++)
    for(int j=1; j &lt; n; j++)
      a[i+j*dim] = a[i-1+j*dim]+a[i+(j-1)*dim];
</pre>
</div>
explain why both the inner and outer loops carry dependencies. Visualize the computation graphically and show how to transform the loop nest so that the inner loop carries no dependence.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In the <tt>multIJK()</tt> program of the previous section, a single statement comprises the body of a loop nest of depth 3. List and explain the dependencies carried by the <span class="formula"><i>i</i></span>, <span class="formula"><i>j</i></span>, and <span class="formula"><i>k</i></span> loops. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a C++ program for Aitken extrapolation with transparent loops that enable the compiler to generate good assembly code. The loop bodies should not have any <tt>if</tt> statements. Compare the generated assembly code with that of the C++ programs of chapter 1. By what factor is the program with transparent loops faster?
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-3.3">3.3</a> Optimizing for the instruction pipeline<a class="Label" name="sec:proc-Optimizing-for-the-pipeline"> </a>
</h2>
<div class="Unindented">
Earlier, we found that MKL’s fast Fourier transform and matrix multiplication can be <span class="formula">10</span> times faster than ordinary C/C++ programs. If the C++ program is written without care, the speedup can even be a factor of <span class="formula">100</span>. What does Intel’s MKL do to be so much faster than ordinary C/C++ programs?
</div>
<div class="Indented">
The biggest part of the answer to that question is optimizing for the instruction pipeline, which is the topic of the present section. The compiler converts a C/C++ program into a stream of machine instructions. When the program runs, this instruction stream is consumed by the processor. From a programmer’s point of view, the instructions are executed one by one, but that is not the way processors consume instructions. The x86 processors consumed instructions in that manner before 1990. If processors still worked that way, they would be slower by at least a factor of <span class="formula">10</span>.
</div>
<div class="Indented">
All modern processors consume instructions using a pipeline, although semantically it is as if the instructions were executed one after another. The pipeline has different stages for fetching, decoding, execution, and accessing memory. Each of these stages has considerable parallelism built into it. Modern x86 processors fetch and decode multiple instructions in a single cycle. For example, an AVX2 processor of the Haswell family (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>) has seven execution ports. Therefore, seven instructions can be dispatched to execution units simultaneously on that processor.
</div>
<div class="Indented">
The point of optimizing for the instruction pipeline is to keep the execution units working in parallel as far as possible.<span class="FootOuter"><span class="SupFootMarker"> [64] </span><span class="HoverFoot"><span class="SupFootMarker"> [64] </span>The chief source for technical details of the x86 pipeline is <i>Intel® 64 and IA-32 Architectures Optimization Reference Manual</i>, 2013.</span></span> Like register sets, the instruction pipeline is invisible within the confines of C/C++. Unlike the registers, the instruction pipeline is invisible even within assembly code. It appears impossible to get compilers to generate instruction streams that are optimized for the instruction pipeline. The difficulties here are of a fundamental kind, and it is unlikely that compilers can ever do this type of optimization satisfactorily. Optimizing for the instruction pipeline involves accounting for instruction size and alignment, usage of register ports, and many other factors that are completely invisible in a C/C++ program. The disparity between architectural design and the abstract view of the computer in C/C++ appears too great to be bridged automatically and perfectly by a compiler. The disparity is growing rapidly, increasing the importance of optimizing for the instruction pipeline. That is true even though one of the aims of architectural design is to be an easy target for compilation.
</div>
<div class="Indented">
In sections <a class="Reference" href="#subsec:proc-instruction-pipelines">3.3.1↓</a> and <a class="Reference" href="#subsec:proc-optim-Chipsets">3.3.2↓</a>, we give a general overview of the processor pipeline and related matters. This overview is specialized to x86. A distinction must be made between instruction set architectures such as SSE2 and AVX2 (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>) and microarchitectures. The microarchitecture specifies the type of pipeline implemented in hardware to consume instructions. Even when the instruction set architecture is the same, microarchitectures can differ. The differences in microarchitecture can be of importance in optimizing for the instruction pipeline. Therefore, we pay attention to the microarchitecture in addition to the instruction set.
</div>
<div class="Indented">
Sections <a class="Reference" href="#subsec:proc-optim-peak-flops">3.3.3↓</a> and <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↓</a> get into the gritty details, which can be hair-rising but also exciting. The thrill of making nontrivial programs faster by factors of <span class="formula">2</span> or <span class="formula">10</span> by understanding how machine instructions map to the microarchitecture is undeniable. However, the thrill is not easily attained. Part of the difficulty is that some details of the microarchitecture need to be discovered through reverse-engineering. 
</div>
<div class="Indented">
The chief example in sections <a class="Reference" href="#subsec:proc-optim-peak-flops">3.3.3↓</a> and <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↓</a> is matrix multiplication. We have already seen that MKL can be more than twice (on SSE2) or more than <span class="formula">10</span> times (on AVX2) faster than ordinary C/C++ programs (see table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3↑</a>). Our aim is to understand how that speedup comes about. 
</div>
<div class="Indented">
The number of arithmetic operations in <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span>, if all matrices are square of dimension <span class="formula"><i>n</i></span>, is <span class="formula">2<i>n</i><sup>3</sup></span> with equally many additions and multiplications. Because each <tt>double</tt> is <span class="formula">8</span> bytes, the total amount of memory used by the three matrices is <span class="formula">24<i>n</i><sup>2</sup></span> bytes. As <span class="formula"><i>n</i></span> increases, the number of arithmetic operations increases superlinearly, as the <span class="formula">1.5</span>th power, against the size of data in memory. This is better than with the FFT, where the superlinear factor is only logarithmic. 
</div>
<div class="Indented">
There are two stages in writing a good matrix multiplication. The first stage is to produce a microkernel that multiplies small matrices while using instruction pipeline resources optimally. The second stage is to use the microkernel to code multiplication of larger matrices while hiding the cost of memory access. The cost of memory accesses can be almost completely hidden because the number of arithmetic operations grows superlinearly in data size. Thus, coding a good microkernel is decisive. The second stage, like most memory optimizations, can be implemented in C/C++ and is deferred to the next chapter. In this chapter, we focus on the microkernel.
</div>
<div class="Indented">
The exposition assumes the SSE2 instruction set (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>). Although the SSE2 instruction set has been superseded by AVX and AVX2, all the main principles of optimizing for the instruction pipeline are brought out in our exposition. An advantage of using SSE2 is that our programs will run on almost any computer. In addition, the exposition is simplified because SSE2 instructions are less complex. Because our aim is to bring out the principles of optimizing for the instruction pipeline, the programs we discuss come close to MKL speeds (on SSE2 machines) but stay away from additional details and clutter necessary to actually reach such speeds. Optimizing for the AVX2 pipeline is covered in the exercises.
</div>
<div class="Indented">
Computer architecture, systems software, program compilation, and program optimization are all full of minute details. Among these, computer architecture changes at the most rapid pace. Yet the understanding gained using one particular microarchitecture applies across past and future generations. That is so because the fundamental principles of instruction pipeline design have not changed for decades. Thus, the concepts and techniques that we learn using an SSE2 pipeline in section <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↓</a> carry over to more modern AVX2 pipelines, as illustrated in our discussion and as evident from the exercises, as well as to AVX-512 pipelines of the future.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.3.1">3.3.1</a> Instruction pipelines<a class="Label" name="subsec:proc-instruction-pipelines"> </a>
</h3>
<h? class="Subsubsection">
<b><u>General remarks </u></b>
</h?>
<div class="Unindented">
The automobile assembly line offers a useful point of comparison to clarify concepts. An automobile assembly line may take 48 hours to assemble a single car. However, thanks to pipelining and multiple assembly lines, the factory may produce a car every minute. For such a factory, the latency would be 48 hours and the bandwidth 1 car per minute. 
</div>
<div class="Indented">
The assembly of the car is broken down into a number of steps that are executed sequentially in the assembly line. Crucially, these steps or pipeline stages are independent of each other. Therefore, car B can be pushed onto the first stage as soon as car A completes the first stage and moves to the second. When car A moves to the third stage and car B to the second, car C is pushed to the first stage. Ideally, the various stages of the assembly line should take nearly the same time. The bandwidth of the assembly line is constrained by the slowest stage. If the number of stages in the assembly line is increased, the bandwidth increases (assuming that the stages take the same amount of time), even though the latency is unchanged. 
</div>
<div class="Indented">
The analogy to car manufacturing omits many complications that arise in processor pipelines. Instructions are not as independent as cars. If there is a RAW dependence between two instructions, the second instruction cannot begin to execute until the first completes. Even ignoring dependencies, it takes a lot of design to make the stages of the instruction pipeline relatively independent. For example, the instruction cache should be separate from data cache if the instruction fetch stage is to be kept relatively independent of the stage where operands are read from memory. Another source of complication comes from interrupts or exceptions raised by an external device or the operating system. If the interrupt is of high enough priority, the entire pipeline must be abandoned to service the interrupt and restored after the interrupt is serviced.
</div>
<div class="Indented">
Although RAW dependencies cannot be eliminated, modern processors eliminate WAW and WAR dependencies on the fly. Those dependencies are eliminated using register renaming.<span class="FootOuter"><span class="SupFootMarker"> [65] </span><span class="HoverFoot"><span class="SupFootMarker"> [65] </span>For a detailed account of Tomasulo’s algorithm for register renaming, see <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span>.</span></span> Suppose we have two instructions as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">movq %r8 %rax
...
movq %r9 %rax
</pre>
</div>

</div>
<div class="Indented">
Evidently, the second instruction has WAW dependence on the first because both instructions write into the <tt>%rax</tt> register. It would be illegal for the processor to execute the second instruction before the first because there may be some other instruction in the middle that reads from <tt>%rax</tt> and therefore has RAW dependence on the first instruction. But suppose the processor dynamically renames the second <tt>%rax</tt> to some other internal register. Suppose as well that the processor renames <tt>%rax</tt> to the same internal register in all later instructions that read what the second instruction writes into <tt>%rax</tt>. If the processor does such renaming of registers, it can go ahead and execute the second instruction before the first.
</div>
<div class="Indented">
Using register renaming, the processor can eliminate WAR and WAW dependencies and greatly increase available parallelism in the instruction stream. Instructions can be scheduled out of order and even executed in parallel.
</div>
<div class="Indented">
Because of the sophistication of the algorithms used by processors to execute instructions, it is a misconception to think that processor performance is somehow proportional to clock speed. The 3.2  GHz Pentium 4 from 2004 used between <span class="formula">1.19</span> and <span class="formula">5.85</span> cycles per instruction for <span class="formula">10</span> programs in the SPEC CPU benchmark.<span class="FootOuter"><span class="SupFootMarker"> [66] </span><span class="HoverFoot"><span class="SupFootMarker"> [66] </span>See sections 2.10 and 1.8 of <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span> (4th ed.).</span></span>  The 2.66  GHz AMD Opteron of that time used fewer cycles per instruction by a factor of <span class="formula">1.27</span>. As a result, the 2.66  GHz Opteron performed slightly better than the 3.2  GHz Pentium. The use of deep pipelines by the 3.2  GHz Pentium to accommodate a higher clock rate resulted in more pipeline stalls and more cycles per instruction, thus outweighing the advantage of greater clock speed.
</div>
<div class="Indented">
For the SPEC CPU benchmark programs, the Pentium 4 did not do better than <span class="formula">1.19</span> cycles per instruction, although its peak bandwidth is <span class="formula">1 ⁄ 3</span> of a cycle per instruction. Unsurprisingly, typical performance is well short of the peak bandwidth. Apart from making simplifying assumptions about the instruction stream, the definition of the peak bandwidth entirely ignores the cost of memory accesses. Yet processor bandwidth is a useful metric. From 1982 to 2001, the processor bandwidth increased by a factor of 2,250. The processor performance increased by a factor close to 2,250 (an annual rate of about 50%) in the 19 years from 1982 to 2001, as measured by the SPECint benchmarks.
</div>
<h? class="Subsubsection">
<b><u>AVX2 and SSE2 pipelines</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:proc-pipeline-nehalem"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter2/figNehalemPipeline.png" alt="figure FIGS/chapter2/figNehalemPipeline.png" style="width: 360px; max-width: 400px; height: 380px; max-height: 423px;"/>
<div class="caption">
Figure 3.4 Instruction pipeline (two copies for two processor cores) on an SSE2-capable machine. The pipeline here corresponds to the Nehalem/Westmere microarchitecture.  
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:proc-pipeline-haswell"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter2/haswell.png" alt="figure FIGS/chapter2/haswell.png" style="max-width: 414px; max-height: 296px;"/>
<div class="caption">
Figure 3.5 Instruction pipeline on an AVX2-capable machine. The pipeline here corresponds to the Haswell microarchitecture. 
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Figures <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a> and <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a> show instruction pipelines for the SSE2-capable Nehalem/Westmere microarchitecture and the AVX2-capable Haswell microarchitecture. If we want to write programs that approach MKL speeds for matrix multiplication and other tasks, such microarchitectures must be understood in some detail.
</div>
<div class="Indented">
The pipelines shown have four parts. The first part, called the front-end, fetches and decodes instructions. Certain instructions, mainly instructions that store in memory, may be broken up into micro-ops. The front-end can fetch and decode multiple instructions in a single cycle.
</div>
<div class="Indented">
The second part, called the reorder engine, is responsible for renaming registers as well as scheduling instructions. Instructions may be scheduled for execution out of order. Predicting branches and recovering from mispredictions is also the responsibility of this part. The reorder engine includes reorder buffers. The reorder buffer holds partially complete instructions or &ldquo;instructions in flight.&rdquo; It corresponds roughly to renamed registers.
</div>
<div class="Indented">
The third part is for execution. The AVX2 pipeline has seven execution units, while the older SSE2 pipeline has only five. Of the seven execution units in the AVX2 pipeline, two can execute <tt>fmadd*pd</tt> instructions (port 0 and port 1 in figure <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a>). If peak capability of <span class="formula">16</span> flops per cycle is to be approached during matrix multiplication, we have to make sure that two<tt> </tt>fused-multiply-add instructions that operate on YMM registers are dispatched to execution ports 0 and 1 almost every cycle. That will involve finding sufficient parallelism in the instruction stream as well as understanding how the front-end and the reorder engine work. 
</div>
<div class="Indented">
The final part in the instruction pipeline is for memory access. Each part is broken up into multiple stages, and there is considerable parallelism at every level of the pipeline. As evident from figures <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a> and <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a>, the stages within a pipeline are not linearly laid out, contrary to the image conjured by the word &ldquo;pipeline,&rdquo; but have more complicated interconnections with each other.
</div>
<div class="Indented">
The SSE2 pipeline shown in figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a> is for the Nehalem/Westmere architecture. The AVX2 pipeline shown in figure <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a> is for the Haswell microarchitecture. The front-end, the reorder engine, and the execution units can differ even when the instruction set is the same. However, such differences in microarchitecture are not always of the greatest significance for program optimization, although they could be on occasion. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.3.2">3.3.2</a> Chipsets <a class="Label" name="subsec:proc-optim-Chipsets"> </a>
</h3>
<div class="Unindented">
So far we have mainly been looking at the processor core. We are about to go even more deeply into the processor. So let us pause for a moment and look at the rest of the computer.
</div>
<div class="Indented">
The processors are central to the computer but are only one among the many components that make up a computer or compute node. Some of the other components are DRAM memory, graphics processor, hard disk, solid state storage, network adapter, keyboard, and monitor. To understand how the processor is connected to all the components, we have to look at chipsets. Chipsets are chips used to assemble computers from many components.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:proc-Block-diagram-of-5500P"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter2/figIntel5500-gray.png" alt="figure FIGS/chapter2/figIntel5500-gray.png" style="width: 326px; max-width: 408px; height: 422px; max-height: 528px;"/>
<div class="caption">
Figure 3.6 Block diagram of the Intel 5000P chipset used to build computers of the Xeon 5500  or 5600 series.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:proc-Block-diagram-of-5500P">3.6↑</a> shows the block diagram of the Intel 5500P chipset and gives a good idea of the layout of a computer. We pick a particular chipset, but many others would do just as well at the level of generality of our discussion. The basic problem in assembling a computer is that the various components operate at very different speeds. The processors are very fast, DRAM memory is not so fast, peripherals such as keyboards are very slow, and the network is capricious. The task of reconciling components that operate at very different speeds falls partly on the chipset.
</div>
<div class="Indented">
Two processor packages are shown in figure <a class="Reference" href="#fig:proc-Block-diagram-of-5500P">3.6↑</a> as P1 and P2. Each processor package fits into a socket on the motherboard. Each processor package may have four or six cores in this instance. On more recent machines using other chipsets, each processor package may even have <span class="formula">16</span> cores.
</div>
<div class="Indented">
From figure <a class="Reference" href="#fig:proc-Block-diagram-of-5500P">3.6↑</a>, we see that DDR3 memory is connected directly to the sockets housing the processor chips. The processor packages contain multiple levels of cache and memory controllers to handle transfer of data to and from the DRAM channels. It is obvious from visual inspection that some DRAM memory channels are closer to certain processor cores. In particular, DRAM memory that is connected to the other socket will be slower to access from a processor core than DRAM memory that is connected to the same socket. If the DRAM memory is on the other socket, it has to come through the memory controller on the other chip. The two processors are connected to each other (with QPI or Quick Path Interconnect), as shown in the figure. 
</div>
<div class="Indented">
This sort of organization of memory makes memory access nonuniform. Non-uniform memory access (NUMA) architecture was introduced by AMD into its x86 product line in 2003. Intel followed suit in 2008.
</div>
<div class="Indented">
The graphics processor is connected to I/O handler (IOH) via a PCI express link. Although certain graphics processors can deliver processing and memory bandwidths that can rival the processor cores, the graphics processor is no more than just another peripheral device as far as the organization of the computer is concerned. 
</div>
<div class="Indented">
Like the graphics processors, the adapters for the Infiniband network are connected using PCI express links. The Infiniband network is used to build high-performance clusters from compute nodes. Ethernet cards and adapters used for Internet connectivity may also use PCI express links.
</div>
<div class="Indented">
Hard disks, audio, keyboard, and other I/O devices are connected to I/O controller handler (ICH). The way in which the chipset allows the processing cores to talk to I/O devices and yet work at something like their normal speed involves substantial involvement of the operating system. Device drivers and interrupt handlers are the operating system components that mediate between the processor and the I/O devices. 
</div>
<div class="Indented">
Direct memory access is one of the major functions of the chipset. Suppose a processor wants to transfer 10 GB from hard disk to memory. Because the hard disk is very slow, the transfer would slow down the processor greatly. So the processor delegates the transfer to the chipset and starts executing some other instruction stream, while the chipset is working on direct memory transfer. During direct memory transfer, data is transferred directly between the hard disk and DRAM memory without processor intervention. 
</div>
<div class="Indented">
Direct memory access is used for transfers between the processor’s memory and graphics coprocessor memory or Xeon Phi coprocessor memory. It is used by Infiniband adapters to directly transfer data between memory and the network. The use of direct memory access in such situations must respect the operating system’s role in memory management, as we will explain later. Direct memory access enables parallelism between network activity and processor activity (chapter 6) or between coprocessor activity and processor activity (chapters 7 and 8).
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.3.3">3.3.3</a> Peak floating point performance<a class="Label" name="subsec:proc-optim-peak-flops"> </a>
</h3>
<div class="Unindented">
The SSE2 instruction set architecture (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>) provides for <span class="formula">16</span> XMM registers on each processor. Each XMM register is 128 bits wide and capable of holding two <tt>double</tt>s. A single instruction that uses an XMM register as source and another XMM register as destination can carry out two additions, if the instruction is <tt>addpd</tt>, or two subtractions, if it is <tt>subpd,</tt> or two multiplications, if it is <tt>mulpd</tt>. The addition and multiplication instructions are dispatched using separate ports, which means that an addition and a multiplication instruction can be issued as well as completed in the same cycle. Thus, the peak rate at which double-precision floating point operations (flops) are executed by a single SSE2 processor is <span class="formula">4</span> flops per cycle. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:proc-flops-ijk-mmult">3.3 on page 1↑</a> showed us that the matrix multiplication routines of the MKL library achieve more than <span class="formula">3.8</span> flops per cycle. However, to write such a program is no simple matter. It requires intimate knowledge of the processor pipeline, which is shown in figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>.
</div>
<div class="Indented">
Our objective is to understand matrix multiplication routines of the sort implemented by MKL. We will not aim to match MKL’s performance. Such a thing would require us to write more assembly code than is pedagogically desirable or appropriate. Knowledge of the processor pipeline is one aspect of optimizing matrix multiplication. Memory hierarchy is equally important. Memory is the topic of the next chapter. 
</div>
<div class="Indented">
In this section, we write a few programs that do nothing meaningful but that get close to the peak performance of <span class="formula">4</span> flops per cycle, with the <span class="formula">4</span> flops comprised of two additions and two multiplications of double-precision floating point numbers. Although the programs are not required to be meaningful, getting close to peak performance is not easy. This exercise requires us to understand many aspects of how instructions are decoded and then dispatched to execution units using several ports. Instruction latencies and throughputs, register read stalls, and register renaming are other aspects of instruction-level parallelism we encounter during the exercise. Where appropriate, we shall look back to figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a> to make the discussion concrete, even though the SSE2/Nehalem pipeline is too complicated for a schematic sketch of the type given in that figure to be either complete or totally accurate.
</div>
<div class="Indented">
This section is a prelude to the discussion of matrix multiplication, which begins in the next section. We understand matrix multiplication on modern processors in two steps. In the next section, we take the first step by writing programs or microkernels<span class="FootOuter"><span class="SupFootMarker"> [67] </span><span class="HoverFoot"><span class="SupFootMarker"> [67] </span>The microkernel nomenclature was introduced by R.A. van de Geijn. The concept may be found in <span class="bibcites">[<a class="bibliocite" name="cite-40" href="#biblio-40"><span class="bib-index">40</span></a>]</span>. I thank Robert van de Geijn for this information.</span></span> to multiply <span class="formula">4 × <i>n</i></span> matrices with an <span class="formula"><i>n</i> × 4</span> matrices for values of <span class="formula"><i>n</i></span> such as <span class="formula"><i>n</i> = 1, 4, 200</span>. The microkernels with <span class="formula"><i>n</i> = 50, 100, 200</span> approach peak performance if the matrices are assumed to be in cache. The final stage, which is to use a microkernel as the building block of a program for multiplying large matrices in DRAM memory, is one of the examples discussed in the next chapter. 
</div>
<div class="Indented">
Understanding matrix multiplication on modern processors will take us more deeply into computer hardware than is customary in textbooks on scientific computing or indeed even computer architecture. The programs are run on a <span class="formula">2.6</span>  GHz SSE2 processor and a <span class="formula">3.6</span>  GHz AVX2 processor (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for their full names; neither processor has in-core acceleration of the clock). Although the programs are not optimized for the AVX2 pipeline, they are still more than three times faster than compiled code. 
</div>
<div class="Indented">
Optimization for the more recent AVX2 instruction set is dealt with in the exercises. The AVX-512 instruction set, which is yet to be introduced in the main x86 line, is dealt with in chapter <a class="Reference" href="#chap:The-Xeon-Phi">7↓</a>. An advantage of using SSE2 is that our programs will run on almost all x86 computers. Because the SSE2 instructions are simpler than AVX2 instructions, there is considerable simplification in the discussion. Most of the points that we make about latencies and throughputs of instructions, such as <tt>addpd</tt> and <tt>mulpd</tt>, remain valid on AVX and AVX2 machines, sometimes with minor modifications. It is a little surprising that the SSE2 programs we discuss have not become dated even with the advent of AVX and AVX2. Of course, these programs no longer approach peak performance. Yet the concepts they bring out related to instruction-level parallelism remain valid in the context of the specific programs found in our discussion, and the programs will continue to run.
</div>
<h? class="Subsubsection">
<b><u>XMM registers and packed double instructions<a class="Label" name="sub:danger-of-inline-assembly-macro"> </a></u></b>
</h?>
<div class="Unindented">
We have examined compiler-generated assembly code on a number of occasions earlier in this chapter. It appears impossible to write C/C++ programs in such a way that the compiled code achieves peak floating point performance. C and C++ give us a uniform view of memory. One can write cache-aware programs in C/C++, as we will in the next chapter. As we saw in earlier sections, we can write C/C++ programs with an eye on the kind of instruction stream the compiler will generate and the processor executes. However, to achieve peak floating point performance, we must pay attention to instruction-level parallelism and the instruction pipeline as well as the cache hierarchy. The design of the instruction pipeline is quite intricate and constrains the instruction stream needed to approach peak performance in many ways. Within the confines of the abstract view of the computer provided by C/C++, it appears impossible to heed such constraints on the instruction stream.
</div>
<div class="Indented">
We use macros and the inline assembly facility to embed instructions that manipulate XMM registers into C/C++ programs. The first of these macros is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">/* R must be "%xmmi" 0&lt;=i&lt;=15 */
#define zeroxmm(R)				\
  asm volatile("xorps %" R ", %" R "\n\t":::R);
</pre>
</div>

</div>
<div class="Indented">
The 16 XMM registers are <tt>%xmm0</tt> through <tt>%xmm15</tt> in GNU assembly (GAS). To zero one of them, say <tt>%xmm7</tt>, we say <tt>zeroxmm(&ldquo;%xmm7&rdquo;)</tt> inside a C/C++ program. Macro expansion and inline assembly produce the machine instruction 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">xorps %xmm7, %xmm7
</pre>
</div>

</div>
<div class="Indented">
followed by a newline and a tab for better formatting. The mnemonic <tt>xorps</tt> may be read as XOR packed single. Its two operands must both be 128 bits wide. It treats each operand as four packed single-precision floating point numbers. Its effect is to exclusive or (xor) the two operands bit by bit. The xor of two bits is <span class="formula">1</span> when exactly one of the two bits is <span class="formula">1</span>, the other being <span class="formula">0</span>. Because we are xor-ing <tt>%xmm7</tt> with itself, we get <span class="formula">0</span> for each of the <span class="formula">128</span> bits.
</div>
<div class="Indented">
There are other ways to set a register to zero. For example, we can move the constant value <span class="formula">0</span> into each part of the <span class="formula">128</span>-bit XMM register, or we can store <span class="formula">0</span> in memory and load it to the XMM register. The use of <tt>xorps</tt> is preferred because the instruction is three bytes, if the register is one of <tt>%xmm0</tt> through <tt>%xmm7</tt>, or four bytes, if the register is one of <tt>%xmm8</tt> through <tt>%xmm15</tt>. Other options may lead to longer instructions. For example, the <tt>xorpd</tt> instruction is a byte longer. In addition, the <tt>xorps</tt> instruction takes only a single cycle to execute.
</div>
<div class="Indented">
Four more macros that expand to inline assembly statements follow.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">//R1 and R2 must be "%xmmi" 0&lt;=i&lt;=15 
#define addxmm(R1, R2)					\
  asm volatile("addpd %" R1 ", %" R2 "\n\t":::R1, R2);
​
//R1 and R2 must be "%xmmi" 0&lt;=i&lt;=15 
#define mulxmm(R1, R2)					\
  asm volatile("mulpd %" R1 ", %" R2 "\n\t":::R1, R2);
​
//R = "%xmmi" 0&lt;=i&lt;=15
//a = double * (16 byte aligned)
#define loadxmm(a, R)					\
  asm volatile("movaps %0, %" R "\n\t"::"m"(*(a)):R);	
​
//R = "%xmmi" 0&lt;=i&lt;=15xmm
//a = double * (16 byte aligned)
#define storexmm(R, a)					\
  asm volatile("movaps %" R ", %0 \n\t":"=m"(*(a))::R);	
</pre>
</div>

</div>
<div class="Indented">
The macros generate <tt>addpd</tt>, <tt>mulpd</tt>, and <tt>movaps</tt> instructions. <tt>PD</tt> stands for packed double. <tt>APS</tt> in <tt>movaps</tt> stands for aligned packed single. The <tt>movaps</tt> instruction is one byte shorter than the <tt>movapd</tt> instruction. The <tt>movaps</tt> instruction can be used to move an XMM register to another XMM register, store an XMM register to memory, or load an XMM register from memory. The memory address must be <span class="formula">16</span>-byte aligned.
</div>
<div class="Indented">
The macros help us write easily readable assembly code. For example, if we write
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  __declspec(align(16)) double a[2]={1,2};
  __declspec(align(16)) double b[2]={-1,-2};
  loadxmm(a, "%xmm0");
  loadxmm(b, "%xmm1");
  addxmm("%xmm1", "%xmm0");
  storexmm("%xmm0", a);
  printf("%f %f \n", a[0], a[1]);
</pre>
</div>

</div>
<div class="Indented">
it is evident that the <tt>printf()</tt> statement will print two zeros. The four macros expand to the following assembly statements:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">        movaps    (%rsp), %xmm0                                
        movaps    16(%rsp), %xmm1                              
        addpd     %xmm1, %xmm0                                 
        movaps    %xmm0, (%rsp)                                
</pre>
</div>

</div>
<div class="Indented">
This assembly code was extracted after compiling the C++ program with the <tt>-S</tt> option. It is evident that the arrays <tt>a</tt> and <tt>b </tt>begin at the locations <tt>(%rsp)</tt> and <tt>16(%rsp)</tt>. The value of <tt>a</tt> equals the content of the <tt>%rsp</tt> register and the value of <tt>b</tt> equals <tt>%rsp+16</tt>. Both pointers are 16-byte aligned, but that information is not found in the extract.
</div>
<div class="Indented">
Another way to recover the assembly instructions corresponding to the four macros is to disassemble the object file. The command <tt>objdump -d file.o</tt> disassembles the object file <tt>file.o</tt>. The machine instructions are written in binary format in the object file. Disassembly converts it to more familiar assembly language mnemonics. An extract from the output of <tt>objdump -d peakflops.o</tt>, where <tt>peakflops.o</tt> is the object file compiled from the C++ program that adds two entries of <tt>b</tt> to two entries of <tt>a</tt>, follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"> 69:	0f 28 04 24          	movaps (%rsp),%xmm0
 6d:	0f 28 4c 24 10       	movaps 0x10(%rsp),%xmm1
 72:	66 0f 58 c1          	addpd  %xmm1,%xmm0
 76:	0f 29 04 24          	movaps %xmm0,(%rsp)
</pre>
</div>

</div>
<div class="Indented">
Here we see the address at which each instruction begins on the left (the addresses are given in hexadecimal), followed by the hexadecimal code for the instruction followed by the assembly statement. 
</div>
<div class="Indented">
The load and store macros assume the memory address to be <span class="formula">16</span>-byte aligned. The last hexadecimal digit of the address must be zero. We can generate <span class="formula">16</span>-byte-aligned arrays by prefixing <tt>__declspec(align(16))</tt> if the array is statically defined as shown above. For memory that is allocated dynamically, versions of <tt>malloc()</tt> such as <tt>_mm_malloc()</tt> allow alignment to be specified.
</div>
<div class="Indented">
The macros such as <tt>zeroxmm()</tt> are certainly convenient. However, their use exposes us to a <i>dangerous error</i>. If we have two lines such as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  addxmm("%xmm1", "%xmm0");
  storexmm("%xmm0", a);
</pre>
</div>

</div>
<div class="Indented">
the compiler is allowed to use <tt>%xmm0</tt> for its own purposes in between the two lines. The inline assembly statements are guaranteed to appear in order, but the compiler may use <tt>%xmm0</tt> for some temporary computation in between the two lines. This danger is especially great when inline assembly statements written in this style appear within the loop body.
</div>
<div class="Indented">
We use macros given here to generate inline assembly statements for expository convenience. In a real program, it is better to write functions entirely in assembly or to use a single <tt>asm volatile</tt> statement for entire blocks of assembly instructions. Where macros are used for expository convenience, we assume that the assembly code is examined to ensure that dangerous side effects are not present. 
</div>
<h? class="Subsubsection">
<b><u>Instruction latencies and throughputs</u></b>
</h?>
<div class="Unindented">
Instruction latency (according to Intel manuals) is the number of cycles spent by the instruction in an execution unit of the instruction pipeline. Instruction throughput is the maximum rate at which an instruction can be dispatched to the execution units. 
</div>
<div class="Indented">
The definition of the C++ function <tt>addreg()</tt> follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void addreg(double *a, double *b, long int nitns){
  long int i;
  loadxmm(a, "%xmm0");
  loadxmm(b, "%xmm1");
  for(i=0; i &lt; nitns; i++)
    addxmm("%xmm0","%xmm1");
  storexmm("%xmm1", a);
}
</pre>
</div>

</div>
<div class="Indented">
This function replaces <tt>a[0]</tt> and <tt>a[1]</tt> by <tt>b[0]+nitns*a[0]</tt> and <tt>b[1]+nitns*a[1]</tt>, respectively. 
</div>
<div class="Indented">
What the function does is irrelevant to the point we want to make. When such functions arise later, only the relevant part of the code is given. For <tt>addreg()</tt>, the relevant part of the code is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(i=0; i &lt; nitns; i++)
    addxmm("%xmm0","%xmm1");
</pre>
</div>

</div>
<div class="Indented">
For simplicity, when code fragments such as this one arise, we may assume that all XMM registers are initialized to zero. The number of iterations is assumed to be large.
</div>
<div class="Indented">
If this code fragment is timed, we find that it takes slightly more than <span class="formula">3</span> cycles per iteration. Why <span class="formula">3</span> cycles? The answer is that the latency of <tt>addpd</tt> is <span class="formula">3</span> cycles. Like every instruction, the <tt>addpd</tt> instruction probably begins its life in the instruction pipeline at the instruction fetch stage of the pipeline (see figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>), which is the first stage in the pipeline. The instruction fetch stage fetches instruction from memory. If the instruction is inside a loop, as in this instance, almost all the fetches are from L1 cache.<span class="FootOuter"><span class="SupFootMarker"> [68] </span><span class="HoverFoot"><span class="SupFootMarker"> [68] </span>The Nehalem/Westmere pipeline has a loop stream detector that skips instruction fetch for loops with a small body.</span></span> Once the operands are available, the scheduler dispatches the instruction to the execution unit. The latency is the number of cycles spent by the instruction in the execution unit. 
</div>
<div class="Indented">
For <tt>i&gt;0</tt>, every <tt>addpd</tt> instruction receives one of its operands from the previous <tt>addpd</tt> instruction. Thus, each <tt>addpd</tt> instruction can be scheduled only after the previous iteration is complete. Thus, the observed speed of <span class="formula">3</span> cycles per iteration is the best possible.
</div>
<div class="Indented">
Although each <tt>addpd</tt> instruction spends <span class="formula">3</span> cycles in the execution unit, it is possible to dispatch an <tt>addpd</tt> to the execution unit every cycle. This assumes of course that the instruction dispatched does not need to wait for some other instruction that is currently executing to complete. Thus, there should be a sufficient level of parallelism in the instruction stream for the maximum throughput of one <tt>addpd</tt> instruction per cycle to be realized. The following program takes <span class="formula">3</span> cycles per iteration.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(i=0; i &lt; nitns; i++){
    addxmm("%xmm0","%xmm1");
    addxmm("%xmm2","%xmm3");
    addxmm("%xmm4","%xmm5");
  }
</pre>
</div>

</div>
<div class="Indented">
The three <tt>addpd</tt> instructions in the loop body are completely independent of each other. Each instruction in a given iteration of the loop body can be scheduled only after the same instruction has completed in the previous iteration. However, the maximum throughput is realized because three independent instruction streams can be executed in parallel.
</div>
<div class="Indented">
On both SSE2/Nehalem (see figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>) and AVX2/Haswell (see figure <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a>), the latencies of <tt>addpd</tt> and <tt>mulpd</tt> are <span class="formula">3</span> cycles and <span class="formula">5</span> cycles, respectively. The maximum throughput of <tt>addpd</tt> is <span class="formula">1</span> cycle per instruction, meaning that at most one <tt>addpd</tt> can be issued in a single cycle on both platforms. The throughput of <tt>mulpd</tt> is <span class="formula">1</span> cycle per instruction on SSE2/Nehalem but <span class="formula">0.5</span> cycles per instruction (meaning that <span class="formula">2</span> <tt>mulpd</tt>s can be issued in a single cycle) on AVX2/Haswell. 
</div>
<h? class="Subsubsection">
<b><u>Multiple dispatch ports and register read stalls</u></b>
</h?>
<div class="Unindented">
The <tt>mulpd</tt> instructions are dispatched for execution on port 0 and the <tt>addpd</tt> instructions are dispatched on port 1 (see figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>, where the units are numbered). Because the dispatch ports are separate, it is possible to dispatch a <tt>mulpd</tt> and an <tt>addpd</tt> in the same cycle. In fact, even on the more recent AVX/AVX2 architectures, at most one <tt>mulpd</tt> can be simultaneously dispatched with an <tt>addpd</tt>. So much of the discussion remains valid.
</div>
<div class="Indented">
Our first attempt to observe this type of parallelism is the following program:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(i=0; i &lt; nitns; i++){
    addxmm("%xmm8", "%xmm0");
    addxmm("%xmm9", "%xmm1");
    addxmm("%xmm10", "%xmm2");
    mulxmm("%xmm11", "%xmm3");
    mulxmm("%xmm12", "%xmm4");
    mulxmm("%xmm13", "%xmm5");
    mulxmm("%xmm14", "%xmm6");
    mulxmm("%xmm15", "%xmm7");
  }
</pre>
</div>

</div>
<div class="Indented">
In each block of five cycles, we may expect the five <tt>mulpd</tt> instructions and three <tt>addpd</tt> instructions in the loop body to be scheduled. There is certainly enough parallelism in the loop body to permit such a schedule. Thus, we may expect <span class="formula">5</span> cycles per iteration.
</div>
<div class="Indented">
The observed number is <span class="formula">8</span> cycles per iteration and is greater than our expectation. The initial observation is in fact slightly more than <span class="formula">9</span> cycles per iteration. By unrolling the loop, we get close to <span class="formula">8</span> cycles per iteration. 
</div>
<div class="Indented">
It is as if the <tt>addpd</tt> and <tt>mulpd</tt> instructions cannot be dispatched in the same cycle even though they have separate execution ports. Why is that happening? Because of register read stalls.<span class="FootOuter"><span class="SupFootMarker"> [69] </span><span class="HoverFoot"><span class="SupFootMarker"> [69] </span>The five-part optimization manual, made available by Agner Fog on his web page (see <a class="FlexURL" href="http://www.agner.org/optimize/">http://www.agner.org/optimize/</a>.), is a valuable source on x86 processors. Fog’s discussion of register read stalls was essential input for writing this section. </span></span> 
</div>
<div class="Indented">
During every cycle, it is possible to read only three registers from the register file in the SSE2/Nehalem microarchitecture. Here the <tt>addpd</tt> would need to read two registers and the <tt>mulpd</tt> would need to read two other registers. The initial answer is to point out that one of the two instructions cannot be dispatched because four registers cannot be read in the same cycle.
</div>
<div class="Indented">
That is not a complete answer, however. After an instruction completes, it spends some time in the reorder buffer (ROB) (see figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>) before it is retired. If an operand needed to dispatch an instruction can be captured directly as the<i> output</i> of an instruction that is waiting in ROB, there is no need to use a register port. In fact, in the earlier Core 2 or Merom microarchitecture, the program takes only <span class="formula">5</span> cycles per iteration as expected, although the Core 2 microarchitecture provides only two ports to read from the register file. It appears that each <tt>addpd</tt> and <tt>mulpd</tt> instruction in the loop body captures the result of the previous iteration directly from ROB on Core 2. The reason that SSE2/Nehalem does not read from ROB as well as Core 2 is unknown.
</div>
<div class="Indented">
Another confirmation that register read stalls are responsible for slowing the program down to <span class="formula">8</span> cycles per iterations, instead of the expected <span class="formula">5</span>, may be found by running the same program on an AVX/AVX2 machine. On those more modern machines, there are more register read ports, and therefore register read stalls are not an issue. The same program realizes <span class="formula">5</span> cycles per iteration.
</div>
<div class="Indented">
Our second attempt is the following program:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(i=0; i &lt; nitns; i++){
    addxmm("%xmm8", "%xmm0");
    addxmm("%xmm8", "%xmm1");
    addxmm("%xmm8", "%xmm2");
    mulxmm("%xmm8", "%xmm3");
    mulxmm("%xmm8", "%xmm4");
    mulxmm("%xmm8", "%xmm5");
    mulxmm("%xmm8", "%xmm6");
    mulxmm("%xmm8", "%xmm7");
  }
</pre>
</div>

</div>
<div class="Indented">
In this program, each <tt>addpd</tt> and <tt>mulpd</tt> instruction uses <tt>%xmm8</tt> as its source register. Only three registers need to be read from the register file to dispatch an <tt>addpd</tt> and <tt>mulpd</tt> in the same cycle. There is no register read stall, and after suitable unrolling, the number of cycles per iteration is only slightly greater than <span class="formula">5</span>. 
</div>
<h? class="Subsubsection">
<b><u>Peak performance without loads or stores</u></b>
</h?>
<div class="Unindented">
The loop body of the following program issues five <tt>addpd</tt> instructions and five <tt>mulpd</tt> instructions. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(long int i=0; i &lt; nitns; i++){
    addxmm("%xmm0", "%xmm1");
    mulxmm("%xmm1", "%xmm2");
    addxmm("%xmm3", "%xmm4");
    mulxmm("%xmm4", "%xmm5");
    addxmm("%xmm6", "%xmm7");
    mulxmm("%xmm7", "%xmm8");
    addxmm("%xmm9", "%xmm10");
    mulxmm("%xmm10", "%xmm11");
    addxmm("%xmm12", "%xmm13");
    mulxmm("%xmm13", "%xmm14");
  }
</pre>
</div>

</div>
<div class="Indented">
The source operand of each <tt>mulpd</tt> is the destination operand of the <tt>addpd</tt> above it. The hope is that this source operand is read directly from ROB, thus relieving pressure on the register ports. If that hope bears out, we should observe <span class="formula">5</span> cycles per iteration.
</div>
<div class="Indented">
If the loop is unrolled 10 times, we observe <span class="formula">5.13</span> cycles per iteration in line with expectation. Some operands are indeed being read from ROB instead of from the register file. 
</div>
<div class="Indented">
If the loop is not unrolled, it takes <span class="formula">7</span> cycles per iteration. Why does unrolling help? The assembly code for the loop includes an instruction to increment the loop counter <tt>i</tt>, which is stored in a register such as <tt>%rax</tt>. It includes another instruction to compare the loop counter with <tt>nitns</tt>. The variable <tt>nitns</tt> is stored in a register. These instructions create additional pressure on the register ports. An overhead of <span class="formula">2</span> cycles is not unreasonable. With unrolling, the overhead is amortized. 
</div>
<div class="Indented">
On a more modern AVX2/Haswell machine, the same program takes <span class="formula">5.8</span> cycles per iteration, even with unrolling. Evidently this program interacts with the AVX2/Haswell pipeline in a different manner. Although the AVX2/Haswell pipeline is newer, it does not seem to recapture operands from ROB as effectively.
</div>
<div class="Indented">
A total of <span class="formula">20</span> flops are carried out in the loop body. Therefore, <span class="formula">5.13</span> cycles per iteration corresponds to <span class="formula">3.89</span> flops per cycle. One can approach the theoretical bound of <span class="formula">4</span> flops per cycle much more closely with a shortfall that is less than <span class="formula">.01</span>. The program which does that uses a more complicated staggered pattern of instructions with more parallelism in the loop body and more reuse of registers that have been recently modified. Because the program is long and makes no fundamentally new point, it is not given here. 
</div>
<h? class="Subsubsection">
<b><u>Peak performance with loads</u></b>
</h?>
<div class="Unindented">
The load instructions are dispatched on port 2. A SSE2/Nehalem program is capable of dispatching a <tt>mulpd</tt> on port 0, an <tt>addpd</tt> on port 1, and a load instruction on port 2 in the same cycle (the statement so far is true on AVX2/Haswell as well, as shown by figure <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a>), assuming the number of reads from the register file is not more than three. The following program demonstrates the capability:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(long int i=0; i &lt; nitns; i++){
    loadxmm(a, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm2");
    
    loadxmm(a+12, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm3");
​
    loadxmm(a+24, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm4");
​
    loadxmm(a+36, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm5");
​
    loadxmm(a+48, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm6");
  }
</pre>
</div>

</div>
<div class="Indented">
After unrolling by a factor of four, this program takes <span class="formula">5.01</span> cycles per iteration. On AVX2/Haswell, it reaches <span class="formula">5.0</span> cycles per iteration with or without unrolling. 
</div>
<div class="Indented">
Each load instruction uses <tt>%xmm0</tt> as the destination. It may seem that each triplet of load, multiply, and add has to wait for the previous triplet to complete. But such WAW dependencies are eliminated using register renaming. The processor may, for example, dynamically rename the <tt>%xmm0</tt> in <tt>loadxmm(a+12,&rdquo;%xmm0&rdquo;)</tt> to something else, possibly an internally maintained reservation station.<span class="FootOuter"><span class="SupFootMarker"> [70] </span><span class="HoverFoot"><span class="SupFootMarker"> [70] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span>.</span></span> In that case, the processor will automatically rename the references to <tt>%xmm0</tt> in the next two instructions to the same thing. Register renaming is a stage in the instruction pipeline (see figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>). Each triplet of load, multiply, and add is completely independent of other triplets in the same iteration. Register renaming allows the processor to exploit this high degree of parallelism in the instruction stream.
</div>
<div class="Indented">
It is interesting to think of how the instructions may get scheduled across cycles. Many different schedules are admissible. At the beginning, the scheduler cannot exploit the parallelism in the instruction stream completely. The locations that are loaded may not be in L1 cache at the beginning. There must be a kind of dynamics to the way the scheduling changes as the iteration count increases. In a case such as this, one may expect the instruction schedule to reach a single &ldquo;steady state.&rdquo; However, one wonders whether there could be periodic, quasi-periodic, or chaotic oscillations in the instruction schedule as the iteration count increases.
</div>
<h? class="Subsubsection">
<b><u>Peak performance with loads and stores</u></b>
</h?>
<div class="Unindented">
We have seen that the instruction pipeline can schedule two or three instructions in the same cycle. For such a capability to be effective, every stage in the pipeline must be capable of pushing several instructions per cycle to the next stage. If instructions are fetched at a slow rate, for example, instruction fetch will be the bottleneck, and the capability of executing several instructions in the same cycle may not be fully utilized.
</div>
<div class="Indented">
Instruction fetch and decode are early stages of the pipeline (see figure <a class="Reference" href="#fig:proc-pipeline-nehalem">3.4↑</a>). The SSE2/Nehalem microarchitecture fetches instructions in <span class="formula">16</span>-byte blocks. The blocks are aligned in memory. If an instruction crosses a <span class="formula">16</span>-byte boundary, it will take two cycles to fetch the instruction. The maximum throughput of these early stages of the pipeline is six instructions per cycle. 
</div>
<div class="Indented">
Occasionally, the instruction fetch and decode may limit the throughput. Below is an example.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  for(long int i=0; i &lt; nitns; i++){
    loadxmm(a, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm2");
    storexmm("%xmm2", a+100);
​
    loadxmm(a+12, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm3");
    storexmm("%xmm3", a+200);
​
    loadxmm(a+24, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm4");
    storexmm("%xmm4", a+300);
​
    loadxmm(a+36, "%xmm0");
    mulxmm("%xmm1", "%xmm0");
    addxmm("%xmm0", "%xmm5");
    storexmm("%xmm5", a+400);
​
    loadxmm(a+48, "%xmm0");
    mulxmm("%xmm1", "%xmm0"); 
    addxmm("%xmm0", "%xmm6");
    storexmm("%xmm6", a+500);
  }
</pre>
</div>

</div>
<div class="Indented">
Each block of load, multiply, add, and store is independent of all others. Each store instruction is sent to ports 3 and 4. If we reason as before, expecting register renaming to help exploit the high degree of parallelism, we may conclude that this program will take <span class="formula">5</span> cycles per iteration. 
</div>
<div class="Indented">
The observed throughput (after unrolling) was <span class="formula">6.6</span> cycles per iteration. Even in theory, this program cannot attain <span class="formula">5.0</span> cycles per iteration. For a single block of load, multiply, add, and store, <tt>objdump -d</tt> gives the following information:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"> ef5:	0f 28 47 60          movaps 0x60(%rdi),%xmm0
 ef9:	66 0f 59 c1          mulpd  %xmm1,%xmm0
 efd:	66 0f 58 d8          addpd  %xmm0,%xmm3
 f01:	0f 29 9f 40 06 00 00 movaps %xmm3,0x640(%rdi)
</pre>
</div>

</div>
<div class="Indented">
The load, multiply, and add instructions are <span class="formula">4</span> bytes long. The store instruction is <span class="formula">7</span> bytes long. There is no way that four of these instructions can be fetched and decoded in a single cycle on SSE2/Nehalem. On AVX2/Haswell, the observed throughput was <span class="formula">5.75</span> cycles per iteration.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-3.3.4">3.3.4</a> Microkernel for matrix multiplication<a class="Label" name="subsec:proc-optim-microkernel"> </a>
</h3>
<div class="Unindented">
Let <span class="formula"><i>A</i></span> be an <span class="formula"><i>l</i> × <i>m</i></span> matrix and <span class="formula"><i>B</i></span> an <span class="formula"><i>m</i> × <i>n</i></span> matrix. The operation <span class="formula"><i>C</i>: = <i>C</i> + <i>AB</i></span>, where <span class="formula"><i>C</i></span> is a <span class="formula"><i>l</i> × <i>n</i></span> matrix, requires <span class="formula">2<i>lmn</i></span> flops, half of which are additions and half of which are multiplications.<span class="FootOuter"><span class="SupFootMarker"> [71] </span><span class="HoverFoot"><span class="SupFootMarker"> [71] </span>Algorithms such as Strassen’s matrix multiplication achieve a lower operation count. See <span class="bibcites">[<a class="bibliocite" name="cite-48" href="#biblio-48"><span class="bib-index">48</span></a>]</span>.</span></span> A single <tt>addpd</tt> instruction performs two additions and a single <tt>mulpd</tt> performs two multiplications. If <tt>addpd</tt> and <tt>mulpd</tt> instructions are issued nearly every cycle, the matrix multiplication can be completed in slightly more than <span class="formula"><i>lmn</i> ⁄ 2</span> cycles. 
</div>
<div class="Indented">
We will abuse terminology slightly and refer to the operation <span class="formula"><i>C</i>: = <i>C</i> + <i>AB</i></span> as matrix multiplication. In the special case where <span class="formula"><i>C</i></span> is initialized to zero, this operation coincides with matrix multiplication.
</div>
<div class="Indented">
In this section, we write programs that multiply matrices of dimensions <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> using slightly more than <span class="formula">8<i>n</i></span> cycles. The microkernel with <span class="formula"><i>n</i> = 200</span> is the basis of matrix multiplication routines given in the next chapter. For <span class="formula"><i>n</i> = 200</span>, the desired cycle count is 1,600. Our  gets to 1,840. With better optimization, microkernels that get closer to the ideal cycle count can be written.
</div>
<div class="Indented">
The one we present has a particularly simple design and uses only a few instructions. Yet it brings out many of the essential features of this type of programming. One of these is the tension between two constraints that must be simultaneously satisfied by such s. On the one hand, to utilize multiple dispatch ports and execute instructions in parallel, it is favorable to interleave segments of the instruction stream that are independent of each other. On the other hand, too much independence would mean that instructions dispatched during the same cycle are more likely to have unrelated operands. Register read stalls would result from too much independence in the instruction stream. We need the instructions to be independent so that they can be dispatched in parallel, and, at the same time, we want each instruction to be reading an operand that was written to recently by another instruction or we want two instructions scheduled during the same cycle to have common operands.
</div>
<div class="Indented">
Working with such constraints on the instruction stream will require us to get into many aspects of the microarchitecture. We limit ourselves to a relatively simple microkernel, partly to keep the exposition tractable and partly because many details of the microarchitecture are unknown to us. Figuring out the microarchitecture calls for laborious and time-consuming experimentation. For the most part, we stick to those aspects of the microarchitecture that have already been uncovered in the previous section. We have seen the <tt>movaps</tt> instruction, used for storing, loading, and moving one register to another, as well as <tt>addpd</tt> and <tt>mulpd</tt>. Our microkernel uses these three instructions and <tt>shufpd</tt> but no others. The instruction 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">shufpd $1, %xmm0, %xmm0
</pre>
</div>

</div>
<div class="Indented">
flips the upper and lower halves of <tt>%xmm0</tt>, or of whichever XMM register that appears in place of <tt>%xmm0</tt>, and that is all we need about <tt>shufpd</tt>.
</div>
<h? class="Subsubsection">
<b><u>Product of <span class="formula">4 × 1</span> and <span class="formula">1 × 4</span> matrices</u></b>
</h?>
<div class="Unindented">
If <span class="formula"><i>A</i></span> is <span class="formula">4 × 1</span> and <span class="formula"><i>B</i></span> is <span class="formula">1 × 4</span>, then <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span> updates <span class="formula"><i>C</i></span> with an outer product. This instance of matrix multiplication takes <span class="formula">32</span> flops, and we will try to do it using slightly more than <span class="formula">8</span> cycles. The microkernel is built upon this outer product.
</div>
<div class="Indented">
The assignment of the entries of <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span> to registers is hinted at by the following diagram:
</div>
<div class="Indented">
<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>c</i><sub>0</sub>
</span>
<span class="arraycell align-c">
↗
</span>
<span class="arraycell align-c">
<i>c</i><sub>2</sub>
</span>
<span class="arraycell align-c">
↗
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>c</i><sub>1</sub>
</span>
<span class="arraycell align-c">
↘
</span>
<span class="arraycell align-c">
<i>c</i><sub>3</sub>
</span>
<span class="arraycell align-c">
↘
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>c</i><sub>4</sub>
</span>
<span class="arraycell align-c">
↗
</span>
<span class="arraycell align-c">
<i>c</i><sub>6</sub>
</span>
<span class="arraycell align-c">
↗
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>c</i><sub>5</sub>
</span>
<span class="arraycell align-c">
↘
</span>
<span class="arraycell align-c">
<i>c</i><sub>7</sub>
</span>
<span class="arraycell align-c">
↘
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span> +  = <span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<span class="boxed"><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>0</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
∗
</span>

</span>
</span></span>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<span class="boxed"><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>1</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
∗
</span>

</span>
</span></span>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<span class="symbol">(</span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<span class="boxed"><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub>0</sub>
</span>
<span class="arraycell align-c">
∗
</span>

</span>
</span></span>
</span>
<span class="arraycell align-c">
<span class="boxed"><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub>1</sub>
</span>
<span class="arraycell align-c">
∗
</span>

</span>
</span></span>
</span>

</span>
</span><span class="symbol">)</span>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
</span>
</div>
Eight XMM registers are used to store <span class="formula"><i>C</i></span>, which is <span class="formula">4 × 4</span>. The <span class="formula"><i>c</i><sub>0</sub></span> register holds <span class="formula"><i>c</i><sub>0</sub></span> and the entry that is immediately southeast, the <span class="formula"><i>c</i><sub>1</sub></span> register holds <span class="formula"><i>c</i><sub>1</sub></span> and the entry that is immediately northeast, and so on. Loading the <span class="formula"><i>C</i></span> matrix into registers and then storing the registers in the matrix becomes particularly convenient if the matrix is stored in &ldquo;skew&rdquo; order in memory. If the <span class="formula">2 × 2</span> matrix <div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i>
</span>
<span class="arraycell align-c">
<i>b</i>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>c</i>
</span>
<span class="arraycell align-c">
<i>d</i>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>
</div>
is skewed, it becomes 
</div>
<div class="Indented">
<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i>
</span>
<span class="arraycell align-c">
<i>c</i>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>d</i>
</span>
<span class="arraycell align-c">
<i>b</i>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>.
</div>
A matrix of dimension <span class="formula">2<i>m</i> × 2<i>n</i></span> is said to be skewed if each of the <span class="formula"><i>m</i> × <i>n</i></span> blocks of size <span class="formula">2 × 2</span> is skewed. Notice that if a skewed matrix is skewed twice, we get back the original matrix. 
</div>
<div class="Indented">
As far as matrix multiplication is concerned, assuming <span class="formula"><i>C</i></span> to be stored in skew order is a minor point in terms of performance but simplifies the exposition. We shall assume <span class="formula"><i>C</i></span> to be stored in skew order.
</div>
<div class="Indented">
The diagram indicates that the register <span class="formula"><i>a</i><sub>0</sub></span> holds <span class="formula"><i>a</i><sub>0</sub></span> and the entry immediately below it. The contents of the registers <span class="formula"><i>a</i><sub>1</sub></span>, <span class="formula"><i>b</i><sub>0</sub></span>, and <span class="formula"><i>b</i><sub>1</sub></span> follow from the diagram in the same way.
</div>
<div class="Indented">
We assume <span class="formula"><i>C</i></span> to be stored in a contiguous array of size <span class="formula">16</span> in column-major order but after skewing. We assume <span class="formula"><i>A</i></span> and <span class="formula"><i>B</i></span> to be stored in contiguous arrays of size <span class="formula">4</span>.
</div>
<div class="Indented">
If the XMM register holding <span class="formula"><i>b</i><sub>0</sub></span> is multiplied into <span class="formula"><i>a</i><sub>0</sub></span> using <tt>mulpd</tt> and the result is added to <span class="formula"><i>c</i><sub>0</sub></span> using <tt>addpd,</tt> we have completed updating <span class="formula"><i>c</i><sub>0</sub></span>. If <span class="formula"><i>a</i><sub>0</sub></span> is flipped, using <tt>shufpd</tt> to interchange its lower and upper half, and then multiplied by <span class="formula"><i>b</i><sub>0</sub></span> and added to <span class="formula"><i>c</i><sub>1</sub></span>, we have updated <span class="formula"><i>c</i><sub>1</sub></span>. The <span class="formula"><i>c</i></span> registers with an even subscript do not require flipping. The ones with an odd subscript require flipping.
</div>
<div class="Indented">
Using the notation introduced so far, we give the method for implementing <span class="formula"><i>C</i>: = <i>C</i> + <i>AB</i></span> in a kind of pseudocode. The assembly code given later corresponds closely to this pseudocode. There is some vagueness in the way the <span class="formula"><i>a</i><sub><i>i</i></sub></span> are assigned to registers, which is cleared up by the assembly code (see figure <a class="Reference" href="#fig:asm4x1x4">3.7↓</a>). 
</div>
<ol>
<li>
Load <span class="formula"><i>c</i><sub>0</sub>, …, <i>c</i><sub>7</sub></span> from memory. 
</li>
<li>
Load <span class="formula"><i>a</i><sub>0</sub></span> from memory.
</li>
<li>
Load <span class="formula"><i>b</i><sub>0</sub></span> from memory.
</li>
<li>
Use <tt>mulpd</tt> to replace <span class="formula"><i>a</i><sub>0</sub></span> by <span class="formula"><i>b</i><sub>0</sub>∗<i>a</i><sub>0</sub></span> (entrywise product of two XMM registers).
</li>
<li>
Load <span class="formula"><i>a</i><sub>1</sub></span>. 
</li>
<li>
Move <span class="formula"><i>a</i><sub>1</sub></span> to <span class="formula"><i>aa</i><sub>1</sub></span>, which is another XMM register.
</li>
<li>
Replace <span class="formula"><i>a</i><sub>1</sub></span> by <span class="formula"><i>b</i><sub>0</sub>∗<i>a</i><sub>1</sub></span> using <tt>mulpd</tt>.
</li>
<li>
Add <span class="formula"><i>a</i><sub>0</sub></span> to <span class="formula"><i>c</i><sub>0</sub></span> using <tt>addpd</tt>.
</li>
<li>
Add <span class="formula"><i>a</i><sub>1</sub></span> to <span class="formula"><i>c</i><sub>4</sub></span> using <tt>addpd</tt>.
</li>
<li>
Load <span class="formula"><i>b</i><sub>1</sub></span>.
</li>
<li>
Replace <span class="formula"><i>aa</i><sub>1</sub></span> by <span class="formula"><i>b</i><sub>1</sub>*<i>aa</i><sub>1</sub></span>.
</li>
<li>
Load <span class="formula"><i>a</i><sub>0</sub></span>.
</li>
<li>
Replace <span class="formula"><i>a</i><sub>0</sub></span> by <span class="formula"><i>b</i><sub>1</sub>∗<i>a</i><sub>0</sub></span>.
</li>
<li>
Add <span class="formula"><i>a</i><sub>0</sub></span> to <span class="formula"><i>c</i><sub>2</sub></span>.
</li>
<li>
Add <span class="formula"><i>aa</i><sub>1</sub></span> to <span class="formula"><i>c</i><sub>6</sub></span>.
</li>
<li>
Load <span class="formula"><i>a</i><sub>0</sub></span>.
</li>
<li>
Flip <span class="formula"><i>a</i><sub>0</sub></span>.
</li>
<li>
Move <span class="formula"><i>a</i><sub>0</sub></span> to <span class="formula"><i>aa</i><sub>0</sub></span>.
</li>
<li>
Replace <span class="formula"><i>aa</i><sub>0</sub></span> by <span class="formula"><i>b</i><sub>1</sub>∗<i>aa</i><sub>0</sub></span>.
</li>
<li>
Replace <span class="formula"><i>a</i><sub>0</sub></span> by <span class="formula"><i>b</i><sub>0</sub>∗<i>a</i><sub>0</sub></span>.
</li>
<li>
Add <span class="formula"><i>aa</i><sub>0</sub></span> to <span class="formula"><i>c</i><sub>3</sub></span>.
</li>
<li>
Add <span class="formula"><i>a</i><sub>0</sub></span> to <span class="formula"><i>c</i><sub>1</sub></span>.
</li>
<li>
Load <span class="formula"><i>a</i><sub>1</sub></span>.
</li>
<li>
Flip <span class="formula"><i>a</i><sub>1</sub></span>.
</li>
<li>
Move <span class="formula"><i>a</i><sub>1</sub></span> to <span class="formula"><i>aa</i><sub>1</sub></span>.
</li>
<li>
Replace <span class="formula"><i>a</i><sub>1</sub></span> by <span class="formula"><i>b</i><sub>0</sub>*<i>a</i><sub>1</sub></span>.
</li>
<li>
Replace <span class="formula"><i>aa</i><sub>1</sub></span> by <span class="formula"><i>b</i><sub>1</sub>∗<i>aa</i><sub>1</sub></span>.
</li>
<li>
Add <span class="formula"><i>a</i><sub>1</sub></span> to <span class="formula"><i>c</i><sub>5</sub></span>.
</li>
<li>
Add <span class="formula"><i>aa</i><sub>1</sub></span> to <span class="formula"><i>c</i><sub>7</sub></span>.
</li>
<li>
Store <span class="formula"><i>c</i><sub>0</sub>, …, <i>c</i><sub>7</sub></span>.
</li>

</ol>
<div class="Unindented">
Each of the 28 items from 2 to 29 corresponds to exactly one instruction. Items 1 and 30 correspond to eight load and eight store instructions, respectively. 
</div>
<div class="Indented">
Each of the eight registers <span class="formula"><i>c</i><sub>0</sub></span> to <span class="formula"><i>c</i><sub>7</sub></span> used to store the <span class="formula">16</span> entries of <span class="formula"><i>C</i></span> is updated using one <tt>mulpd</tt> and one <tt>addpd</tt> instruction. Thus, there are eight <tt>addpd</tt> and eight <tt>mulpd</tt> instructions.
</div>
<div class="Indented">
The cost of loading and storing <span class="formula"><i>C</i></span> is amortized when matrices of dimensions <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> are multiplied for large <span class="formula"><i>n</i></span>. There are no store instructions except those used to write to <span class="formula"><i>C</i></span> in item 30. If the instructions used to load <span class="formula"><i>C</i></span> in item 1 are ignored, there are 7 load instructions. Two of these are used to load <span class="formula"><i>b</i><sub>0</sub></span> and <span class="formula"><i>b</i><sub>1</sub></span>. The others are for loading <span class="formula"><i>A</i></span>.
</div>
<div class="Indented">
There are 3 move instructions (items 6, 18, and 25) and 2 <tt>shufpd</tt> instructions for flipping an XMM register (items 17 and 24).
</div>
<div class="Indented">
Thus, the <span class="formula">28</span> instructions from items 2 to 29 are comprised of <span class="formula">16</span> arithmetic instructions, <span class="formula">7</span> loads, <span class="formula">3</span> moves, and <span class="formula">2</span> <tt>shufpd</tt>s. The matrix <span class="formula"><i>A</i></span> can be loaded using just <span class="formula">2</span> instructions, and the number of loads can be reduced to <span class="formula">2</span>. But then the number of moves increases to <span class="formula">8</span>. The move instructions as well as the shuffle instructions are dispatched using port 5. Having a total of <span class="formula">10</span> move and shuffle instructions creates too much pressure on port 5. Another reason to favor the load instructions is that they seem to work better with respect to register renaming and the capture of modified registers from ROB. 
</div>
<div class="Indented">
Among the <span class="formula">28</span> instructions left after disregarding the first and last items, there are <span class="formula">8</span> <tt>mulpd</tt>s that are dispatched on port 0, <span class="formula">8</span> <tt>addpd</tt>s that are dispatched on port 1, <span class="formula">7</span> store instructions that are dispatched on port 2, and <span class="formula">5</span> move or shuffle instructions dispatched on port 5. Ports 3 and 4 are unused because there are no store instructions.
</div>
<div class="Indented">
If the <span class="formula">28</span> instructions of items 2 to 29 are put inside a loop body, in effect we have a program that adds the product <span class="formula"><i>AB</i></span> to <span class="formula"><i>C</i></span> repeatedly. Given the way the instructions are distributed between ports, it is not unreasonable to expect each iteration to take <span class="formula">8</span> cycles.
</div>
<div class="Indented">
But of course there are other constraints on the instruction stream to be considered. One of these is the availability of only <span class="formula">3</span> ports to read from the register file. The load instructions do not create any pressure on the register read ports. However, all the other instructions do. So if we are to get to <span class="formula">8</span> cycles per iteration, it is absolutely necessary to ensure that some of the operands are read from ROB. 
</div>
<div class="Indented">
Each of the <tt>addpd</tt> instructions is likely to capture one of its operands from the corresponding <tt>mulpd</tt>, which is placed not too far above it. The <tt>mulpd</tt> of item 7 is likely to capture one of its operands from item 5, which is a load instruction. Similarly, the <tt>mulpd</tt>s of items 11 and 13 may capture an operand from item 10, which is a load instruction. 
</div>
<div class="Indented">
Instruction lengths impose yet another constraint. If the average instruction were <span class="formula">5</span> bytes long, it would take more than <span class="formula">8</span> cycles to fetch the instructions because each instruction fetch brings in an aligned block of <span class="formula">16</span> bytes. The majority of instructions are only <span class="formula">4</span> bytes, however. Instruction lengths and instruction alignment may have a significant effect when the program is unrolled to multiply matrices of size <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> for <span class="formula"><i>n</i> &gt; 1</span>. We do not pay much attention to this constraint, preferring to keep the microkernel relatively simple. If items 2 through 29 are put in a loop, it takes <span class="formula">8.53</span> cycles per iteration if the top of the loop is aligned at a <span class="formula">16</span>-byte boundary. That is not bad considering that the instructions to increment the loop counter and compare against the loop count create some pressure on the register read ports. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:asm4x1x4"> </a><div class="figure">
<div class="PlainVisible">
.align 16, 0x90<br/>
.globl 	asm4x1x4<br/>
asm4x1x4:<br/>
# parameter 1: %rdi (a)<br/>
# parameter 2: %rsi (b)<br/>
# parameter 3: %rdx (c)<br/>
	#loading c<br/>
	movaps	(%rdx), %xmm4                                 <br/>
        movaps	32(%rdx), %xmm5                               <br/>
        movaps	64(%rdx), %xmm6                               <br/>
        movaps	96(%rdx), %xmm7                               <br/>
        movaps	16(%rdx), %xmm8                               <br/>
        movaps	48(%rdx), %xmm9                               <br/>
        movaps	80(%rdx), %xmm10                              <br/>
        movaps	112(%rdx), %xmm11
</div>
<div class="PlainVisible">
	#mult 4x1x4<br/>
        movaps	(%rdi), %xmm2                                 <br/>
        movaps	(%rsi), %xmm0                                 <br/>
        mulpd	%xmm0, %xmm2                                  <br/>
        movaps	16(%rdi), %xmm3                               <br/>
        movaps	%xmm3, %xmm12                                 <br/>
        mulpd	%xmm0, %xmm3                                  <br/>
        addpd	%xmm2, %xmm4                                  <br/>
        addpd	%xmm3, %xmm8                                  <br/>
        movaps	16(%rsi), %xmm1                               <br/>
        mulpd	%xmm1, %xmm12                                 <br/>
        movaps	(%rdi), %xmm2<br/>
        mulpd	%xmm1, %xmm2                                  <br/>
        addpd	%xmm2, %xmm6                                  <br/>
        addpd	%xmm12, %xmm10<br/>
        movaps	(%rdi), %xmm3 <br/>
        shufpd	$1, %xmm3, %xmm3                              <br/>
        movaps	%xmm3, %xmm2                                  <br/>
        mulpd 	%xmm1, %xmm3                                  <br/>
        mulpd 	%xmm0, %xmm2                                  <br/>
        addpd 	%xmm3, %xmm7                                  <br/>
        addpd 	%xmm2, %xmm5                                  <br/>
        movaps	16(%rdi), %xmm2                               <br/>
        shufpd	$1, %xmm2, %xmm2                              <br/>
        movaps 	%xmm2, %xmm3                                  <br/>
        mulpd 	%xmm0, %xmm2                                  <br/>
        mulpd	%xmm1, %xmm3                                  <br/>
        addpd 	%xmm2, %xmm9                                  <br/>
        addpd	%xmm3, %xmm11<br/>
	#storing c<br/>
        movaps 	%xmm4, (%rdx)                                 <br/>
        movaps	%xmm5, 32(%rdx)                               <br/>
        movaps	%xmm6, 64(%rdx)                               <br/>
        movaps	%xmm7, 96(%rdx)                               <br/>
        movaps	%xmm8, 16(%rdx)                               <br/>
        movaps	%xmm9, 48(%rdx)                               <br/>
        movaps	%xmm10, 80(%rdx)                              <br/>
        movaps	%xmm11, 112(%rdx)                             <br/>
	ret<br/>
.align 16, 0x90<br/>
.type asm4x1x4, @function
</div>
<div class="caption">
Figure 3.7 Assembly function <tt>asm4x1x4()</tt> for multiplying matrices of dimensions <span class="formula">4 × 1</span> and <span class="formula">1 × 4</span>.
</div>

</div>

</div>

</div>
<h? class="Subsubsection">
<b><u>Product of <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> matrices</u></b>
</h?>
<div class="Unindented">
The complete assembly program for multiplying a <span class="formula">4 × 1</span> matrix with a <span class="formula">1 × 4</span> matrix is shown in figure <a class="Reference" href="#fig:asm4x1x4">3.7↑</a>. It corresponds closely to the pseudocode and is the building block for programs to multiply matrices of dimensions <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> with <span class="formula"><i>n</i> &gt; 1</span>. The C/C++ declaration of this function is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">extern "C"{
	void asm4x1x4(double *a, double *b, double *c);
}
</pre>
</div>

</div>
<div class="Indented">
Each instruction in the middle block labeled <tt>#mult 4x1x4</tt> (see figure <a class="Reference" href="#fig:asm4x1x4">3.7↑</a>) corresponds to exactly one of the items 2 to 29 in the pseudocode.
</div>
<div class="Indented">
There are many conventions that govern assembly programming on GNU/Linux. These conventions specify which registers are used to pass arguments, which registers are used to return the function value, which registers are caller saved, which are callee saved, and the manner in which the stack must be used.<span class="FootOuter"><span class="SupFootMarker"> [72] </span><span class="HoverFoot"><span class="SupFootMarker"> [72] </span>By far the best guide to the calling conventions of Linux or Windows is part 5 of Agner Fog’s optimization document posted on his web page:  <a class="FlexURL" href="http://www.agner.org/optimize/">http://www.agner.org/optimize/</a>.</span></span> The three arguments to <tt>asm4x1x4() </tt>are pointers (to arrays that hold the matrices <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span>) and therefore <span class="formula">64</span> bits wide. They are passed using the registers <tt>%rdi</tt>, <tt>%rsi</tt>, and <tt>%rdx</tt>, respectively. Fortunately, we need to know very little of the calling conventions. The XMM registers are all caller saved, and <tt>asm4x1x4()</tt> is free to use them to perform its calculations.
</div>
<div class="Indented">
The definition of <tt>asm4x1x4()</tt> as an assembly program has three blocks. The first block is for loading <span class="formula"><i>C</i></span>, the second block multiplies <span class="formula"><i>A</i></span> and <span class="formula"><i>B</i></span> and adds the product to the registers that hold <span class="formula"><i>C</i></span>, and the last block is for storing <span class="formula"><i>C</i></span>. The code to multiply matrices of dimensions <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> is obtained essentially by replicating the middle block <span class="formula"><i>n</i></span> times but with some modifications. This is possible because the product<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>11</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>12</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>a</i><sub>1<i>n</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>21</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>22</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>a</i><sub>2<i>n</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>31</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>32</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>a</i><sub>3<i>n</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>32</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>42</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>a</i><sub>4<i>n</i></sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub>11</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>12</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>13</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>14</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub>21</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>22</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>23</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>24</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>1</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>2</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>3</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>3</sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>
</div>
is a sum of outer products:<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>11</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>21</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>31</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>41</sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<span class="symbol">(</span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub>11</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>12</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>13</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub>14</sub>
</span>

</span>
</span><span class="symbol">)</span>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
</span> + ⋯ + <span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>1<i>n</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>2<i>n</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>3<i>n</i></sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>4<i>n</i></sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<span class="symbol">(</span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>1</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>2</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>3</sub>
</span>
<span class="arraycell align-c">
<i>b</i><sub><i>n</i>4</sub>
</span>

</span>
</span><span class="symbol">)</span>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>

</span>
</span>
</div>
The matrices <span class="formula"><i>A</i></span> and <span class="formula"><i>B</i></span> are assumed to be stored in arrays of size <span class="formula">4<i>n</i></span> double-precision numbers. The matrix <span class="formula"><i>A</i></span> is assumed to be stored column after column. In contrast, the matrix <span class="formula"><i>B</i></span> is assumed to be stored row after row, making it easier to access the columns and rows for each outer product. To form the <span class="formula"><i>k</i></span>th outer product, where <span class="formula">0 ≤ <i>k</i> &lt; <i>n</i></span>, we may add <span class="formula">4<i>k</i></span> to the pointers <span class="formula"><i>A</i></span> and <span class="formula"><i>B</i></span> to advance to the <span class="formula"><i>k</i></span>th column of <span class="formula"><i>A</i></span> and the <span class="formula"><i>k</i></span>th row of <span class="formula"><i>B</i></span>.
</div>
<div class="Indented">
To multiply matrices of dimensions <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span>, we modify <tt>asm4x1x4()</tt>. The middle block is replicated <span class="formula"><i>n</i></span> times. In the second block, the memory references 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">(%rdi), (%rsi), 16(%rdi), 16(%rsi), (%rdi), (%rdi), 16(%rdi) 
</pre>
</div>

</div>
<div class="Indented">
are replaced by
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">32(%rdi), 32(%rsi), 48(%rdi), 48(%rsi), 32(%rdi), 32(%rdi), 48(%rdi) 
</pre>
</div>

</div>
<div class="Indented">
Adding <span class="formula">32</span> to the displacement corresponds to moving forward by <span class="formula">4</span> doubles because each double is <span class="formula">8</span> bytes. The third, fourth, and fifth replications are treated similarly by adding <span class="formula">64</span>, <span class="formula">96</span>, and <span class="formula">128</span> to the displacement fields. After the fifth replication, we add <span class="formula">160</span> to <tt>%rdi</tt> and <tt>%rsi</tt> using the instructions 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	addq 	$160, %rdi
	addq 	$160, %rsi
</pre>
</div>

</div>
<div class="Indented">
and repeat the first five replications. Note that <span class="formula">160</span> bytes equals <span class="formula">20</span> doubles. This design tries to balance competing requirements for shorter instructions and fewer instructions.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:proc-asm4xnx4-table"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
Flops per cycle
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">20</span>
</td>
<td align="center" valign="top">
<span class="formula">3.19</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">40</span>
</td>
<td align="center" valign="top">
<span class="formula">3.33</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">100</span>
</td>
<td align="center" valign="top">
<span class="formula">3.43</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">200</span>
</td>
<td align="center" valign="top">
<span class="formula">3.48</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 3.5 Floating point performance for routines that multiply a <span class="formula">4 × <i>n</i></span> matrix into an <span class="formula"><i>n</i> × 4</span> matrix for various <span class="formula"><i>n</i></span> on SSE2/Nehalem pipeline (the <span class="formula">2.6</span>  GHz SSE2 processor of table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>

</div>
<div class="Indented">
The function <tt>asm4x200x4()</tt> defined in this manner achieves <span class="formula">3.48</span> flops per cycle (see table <a class="Reference" href="#tab:proc-asm4xnx4-table">3.5↑</a>) on SSE2/Nehalem. It serves as the microkernel for programs that multiply larger matrices in the next chapter. One might ask, why stop at <span class="formula"><i>n</i> = 200</span>? Further unrolling does not improve the performance significantly, and we are close to the limit of the <span class="formula">4 × 1 × 4</span> design used to build the microkernel. The code for <tt>ams4x200x4()</tt> occupies <span class="formula">24</span> KB and fills three quarters of the L1 instruction cache. In addition, the matrices multiplied by the microkernel are assumed to fit comfortably with room to spare in L1 data cache, which is <span class="formula">32</span> KB, when the microkernel is used to build efficient programs to multiply large matrices  in the next chapter. The choice of <span class="formula"><i>n</i></span> is limited by the data cache as well as the instruction cache.
</div>
<div class="Indented">
Throughout the design of the <tt>asm4x200x4()</tt> microkernel, we have emphasized register read stalls. Verification that register read stalls are a key issue may be found by running the same program on AVX2/Haswell (the <span class="formula">3.6</span>  GHz AVX2 processor of table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). On AVX2/Haswell the program realizes <span class="formula">3.98</span> flops per cycle, which is much closer to the best possible <span class="formula">4.0</span> flops per cycle for a program built using <tt>mulpd</tt> and <tt>addpd</tt> instructions than what we see in table <a class="Reference" href="#tab:proc-asm4xnx4-table">3.5↑</a>. Evidently, the greater number of register ports available on AVX2/Haswell is helping. 
</div>
<div class="Indented">
This finding might suggest that writing an optimal microkernel is easier on AVX2 than SSE2. That hope is unfortunately unlikely to be true. An optimal microkernel on AVX2 must issue two <tt>fmadd*pd</tt> instructions operating on YMM registers every cycle to realize <span class="formula">16</span> flops per cycle (these are scheduled on ports 0 and 1; see figure <a class="Reference" href="#fig:proc-pipeline-haswell">3.5↑</a>). The <tt>fmadd*pd</tt> instructions operate on three registers, and not just two, as with <tt>addpd</tt> or <tt>mulpd</tt>, creating greater pressure on register ports. 
</div>
<div class="Indented">
A point that came up in our discussion of the <tt>asm4x200x4()</tt> microkernel for SSE2/Nehalem is that independence in the instruction stream is good because instructions can be scheduled in parallel. However, too much independence is bad because it leads to register read stalls. This tension persists in an optimal microkernel for AVX2/Haswell (and may be expected to persist in architectures yet to be released) and remains the key issue to be dealt with in optimizing for the instruction pipeline.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Summing the Leibniz series with loop unrolling takes <span class="formula">7</span> cycles per term on an AVX2 machine, as we saw in section <a class="Reference" href="#subsec:proc-compileropt-unrolling">3.2.2↑</a>. Examine the assembly code and determine the realized bandwidth in terms of instructions consumed per cycle.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Why does having a deep pipeline with many stages help accommodate a faster clock?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write macros <tt>zeroymm()</tt>, <tt>addymm()</tt>, <tt>mulymm()</tt>, <tt>storeymm()</tt>, and <tt>loadymm()</tt> to issue instructions to manipulate YMM registers. The respective instructions are <tt>vxorps</tt>, <tt>vaddpd</tt>, <tt>vmulpd,</tt> and <tt>vmovaps</tt>. You may consult <i>Intel® 64 and IA-32 Architectures Software Developer’s Manual</i> for information about these instructions. Keep in mind that the order of sources and destinations is reversed between the Intel manuals and the GNU/Linux assembler. Write a simple program to test your macros.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use GNU’s <tt>objdump</tt> utility to find out the size in bytes of a <tt>vmovaps</tt> instruction, with both source and destination being YMM registers, with the source an address in memory, and with the destination an address in memory.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The AVX2 instruction set provides for three fused-multiply-add instructions <tt>vfmadd231pd</tt>, <tt>vfmadd132pd</tt>, and <tt>fmadd213pd</tt>. Write macros to issue these instructions. Why are there exactly three fused-multiply-add instructions?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Look up the latency and throughput of <tt>vmulpd</tt> and <tt>vaddpd</tt> for your AVX2 microarchitecture in <i>Intel® 64 and IA-32 Architectures Optimization Reference Manual </i>or its AMD equivalent. Write programs that verify that information.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  On the Haswell/Broadwell microarchitectures implementing AVX2, the latency of any of the <tt>fmadd*pd</tt> instructions is <span class="formula">5</span> cycles. The throughput is <span class="formula">0.5</span> cycles, implying that <span class="formula">2</span> <tt>vfmadd*pd</tt> instructions can be simultaneously dispatched to execution units. Write programs verifying the latency and throughput of fused-multiply-add instructions. To complete this exercise, it is crucial to find out the number of register read ports available. Finding that out may require some reverse-engineering.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program using <tt>vfmadd*pd</tt> instructions that reaches peak floating point performance of <span class="formula">16</span> flops per cycle and loads into a YMM register once for each <tt>vfmadd*pd</tt> instruction issued.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Code a program that issues <tt>vfmadd*pd</tt> instructions and that is impeded from reaching peak performance of <span class="formula">16</span> flops per cycle because instructions are too long and cannot be fetched fast enough.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Investigate the <tt>vperm</tt> instructions for permuting the contents of a YMM register. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Code a microkernel for multiplying <span class="formula">4 × <i>n</i></span> and <span class="formula"><i>n</i> × 4</span> matrices that approaches peak performance on an AVX2 machine for a suitable value of <span class="formula"><i>n</i>.</span>
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Try another microkernel design, this time for multiplying <span class="formula">8 × <i>n</i></span> and <span class="formula"><i>n</i> × 8</span> matrices. For a suitable <span class="formula"><i>n</i></span>, does it get closer to AVX2 peak performance of <span class="formula">16</span> flops per cycle than the previous design?
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-3.4">3.4</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-28"><span class="bib-index">28</span></a>] </span> <span class="bib-authors">G.I. Toomre</span>: <i><span class="bib-title">Ptolemy's Almagest</span></i>. <span class="bib-publisher">Princeton University Press</span>, <span class="bib-year">1998</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-29"><span class="bib-index">29</span></a>] </span> <span class="bib-authors">J.L. Hennessy, D.A. Patterson</span>: <i><span class="bib-title">Computer Architecture: A Quantitative Approach</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">1990-2011</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-30"><span class="bib-index">30</span></a>] </span> <span class="bib-authors">K.V. Sarma</span>: <i><span class="bib-title">A History of the Kerala School of Hindu Astronomy</span></i>. <span class="bib-publisher">Vishveshvaranand Institute</span>, <span class="bib-year">1992</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-31"><span class="bib-index">31</span></a>] </span> <span class="bib-authors">M. Kerrisk</span>: <i><span class="bib-title">The Linux Programming Interface</span></i>. <span class="bib-publisher">No Starch Press</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-32"><span class="bib-index">32</span></a>] </span> <span class="bib-authors">R. Allen, K. Kennedy</span>: <i><span class="bib-title">Optimizing Compilers for Modern Architectures</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">2002</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-33"><span class="bib-index">33</span></a>] </span> <span class="bib-authors">S. Goto, R. A. van de Geijn</span>: “<span class="bib-title">Anatomy of high performance matrix multiplication</span>”, <i><span class="bib-journal">ACM TOMS</span></i>, pp. <span class="bib-pages">art:12</span>, <span class="bib-year">2008</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-34"><span class="bib-index">34</span></a>] </span> <span class="bib-authors">T.H. Cormen, C.E. Lieserson, R.L. Rivest</span>: <i><span class="bib-title">Introduction to Algorithms</span></i>. <span class="bib-publisher">MIT Press</span>, <span class="bib-year">2001</span>.
</p>

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-4">4</a> Memory<a class="Label" name="chap:Memory"> </a>
</h1>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:Memory-hierarchy-is"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/MemoryH.png" alt="figure FIGS/chapter3/MemoryH.png" style="max-width: 288px; max-height: 144px;"/>

</div>
<div class="caption">
Figure 4.1 Memory hierarchy is a pyramid of decreasing capacity and increasing speed.
</div>

</div>

</div>

</div>
<div class="Indented">
The memory pyramid shown in figure <a class="Reference" href="#fig:Memory-hierarchy-is">4.1↑</a> has hard disk at the bottom and registers at the top. Hard disk can have capacity well into the terabytes, while the registers are few in number. It can take milliseconds to access the hard disk, while the registers can be accessed in less than a nanosecond. As we ascend the memory pyramid, capacity decreases and speed increases. Registers have been dealt with in the previous chapter. Here we discuss caches, Dynamic Random Access Memory (DRAM), and hard disk. Much of the time memory refers to DRAM memory.
</div>
<div class="Indented">
The stored program concept is regarded as an early landmark in the development of computers. Instead of thinking of instructions and data as separate entities, the stored program slightly blurs the distinction between the two, with both programs and data stored in the same computer memory. In modern computers, including supercomputers, desktops, laptops, and all kinds of mobile devices, memory invariably refers to DRAM. Although file systems reside on hard disk or some other storage medium, when a program is running, much of the data that is handled is from DRAM. DRAM sits external to the processor as shown in section <a class="Reference" href="#subsec:proc-optim-Chipsets">3.3.2↑</a> (see figure <a class="Reference" href="#fig:proc-Block-diagram-of-5500P">3.6↑</a>). It is estimated that 40% of the instructions in a typical computer program are loads and stores, and all these instructions address locations in DRAM. Thus, within a program, for most purposes, memory means DRAM memory. In DRAM memory, every bit is stored on a single microscopically small capacitor, which is accessed using a single transistor.
</div>
<div class="Indented">
DRAM technology is cheap, ubiquitous, and relatively stable. In section <a class="Reference" href="#sec:memory-dram-cache">4.1↓</a>, we take a look at how DRAM hardware and caches are configured. Accessing a word from DRAM can be more than 100 times slower than accessing a register. Although memory available through registers is only of the order of kilobytes (KB), a register can be accessed in a single cycle. In contrast, memory available through DRAM runs into tens, even hundreds, of gigabytes (GB), but it can take more than <span class="formula">100</span> cycles to access a word from DRAM. A great part of computer design is an attempt to mitigate this extreme gap in speed and capacity. The aim or hope is to approximate the speed of registers with the capacity of DRAM. A key idea is to store caches on the processor itself. The cache is a record of those words in DRAM that are most frequently accessed by the processor. Words that are cached may be accessed directly within the processor itself without exchanging signals with DRAM units external to the processor. 
</div>
<div class="Indented">
On modern computers, caches can be quite large. It is not unusual for a processor package to have more than <span class="formula">10</span> megabytes (MB) of cache. Both cache and DRAM capacities are increasing inexorably. The day when caches are large enough to comfortably hold a million grid point computation is firmly in the past. In a lot of scientific computing, computations do not go out of cache. Caches are implemented using Static Random Access Memory (SRAM). In SRAM, each bit is stored using multiple transistors with circuitry to switch the bit on or off rapidly. Caches are intermediate between DRAM and registers in expense, speed, and capacity.
</div>
<div class="Indented">
Companies that make DRAM are about as profitable as low-end grocery stores, if they are lucky enough to make a profit. DRAM is as close to a commodity, mass-market technology as there exists in the world of computing. Its low cost has made it ubiquitous across the whole computing spectrum. Engineers have been able to meet market demand for increasing capacity at a low cost, and the potential for disruptive innovations in this area appears limited.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:memory-dram-cache">4.1↓</a> gives an overview of DRAM and cache memory. The organization of DRAM into memory channels and of caches into multiple levels influences programming technique in several ways. For example, the organization of memory into channels is under the assumption that memory accesses from separate processor cores (of the same system) are relatively independent and far apart. If in fact the memory accesses are tightly correlated and reference nearby locations, there can be a severe penalty.
</div>
<div class="Indented">
DRAM memory is shared by multiple programs running on the same computer. Virtual memory creates the illusion that every program has its own exclusive memory. Virtual memory, which is implemented by the operating system kernel with help from the processor hardware, influences programming speed in several ways too. In section <a class="Reference" href="#sec:memory-dram-cache">4.1↓</a>, we give a basic introduction to virtual memory. In section <a class="Reference" href="#sec:memory-dram-cache">4.1↓</a>, we describe a program to measure the latency to DRAM memory. This program exposes many aspects of the memory system, such as parallelism of memory access, cache line size, and virtual memory, which are vital to efficient programming. In addition, latency to memory is  a figure of much importance, especially when dynamic data structures such as linked lists and trees are employed. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:memory-optimizing">4.2↓</a> presents many techniques to optimize memory access. These techniques have not changed greatly over the years. This section could have been written even 15 years ago in mostly the same form. Of course the parameters used for successive levels of blocking, which are related to the sizes of caches and other aspects of computer architecture, would have been different. But all the principles are the same with one important exception. Even that one principle (packing data in caches to reduce TLB misses) would have applied in an identical manner to machines of the past, but it was not known 15 years ago.
</div>
<div class="Indented">
Sections <a class="Reference" href="#sec:memory-dram-cache">4.1↓</a> and <a class="Reference" href="#sec:memory-optimizing">4.2↓</a> may well be the most important parts of this book. Optimizing memory access is just as relevant to your cell phone as it is to supercomputers. In addition, the principles are the same. Its importance only increases when programs are threaded or networked. The techniques of memory optimization described in section <a class="Reference" href="#sec:memory-optimizing">4.2↓</a> are applicable to regular and structured data, as in images and grids for solving partial differential equations. When dynamic data structures such as linked lists and graphs are used, one needs to be more mindful of latency to memory. 
</div>
<div class="Indented">
Memory optimizations can be done in C, without resorting to assembly code, to a far greater extent than instruction pipeline optimizations. Like the previous chapter, this chapter too discusses optimizations in the context of SSE2, AVX, and AVX2 machines (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>), although the role of the instruction set is not as great. Much of the time the distinction between these instruction sets does not matter greatly. All programs are written in C. The compiled code may be suboptimal as before, but the penalty to pay for the suboptimality of the compiler is not as high. Although the penalty can exceed a factor of <span class="formula">10</span> for instruction pipeline optimizations, as shown in the previous chapter, the penalty with regard to memory optimizations seldom exceeds a factor of <span class="formula">1.5</span>.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:memory-diskio">4.3↓</a> explains how to write and read from hard disk. If DRAM memory can run into hundreds of GB, hard disk can run into tens of terabytes on even a small computer. However, hard disk can be very slow, with latencies of the order of milliseconds. The operating system kernel plays a great role in determining the observed speed of disk input/output. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sec:memory-page-tables-virtual-memory">4.4↓</a>, we return to virtual memory and look at its implementation inside the Linux kernel. All programs run at the mercy of the operating system kernel. The systems programming perspective and knowledge of the paging system, which implements virtual memory, help understand disk input/output, network programming, and multithreaded programming at an advanced level.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-4.1">4.1</a> DRAM and cache memory<a class="Label" name="sec:memory-dram-cache"> </a>
</h2>
<div class="Unindented">
DRAM was invented by Intel in 1973. In DRAM, a single capacitor is used to store a single bit, and each capacitor is equipped with an access transistor. DRAM technology has evolved over the years to the point where it is the primary form of memory in almost all computing and mobile devices. For a schematic illustration of where DRAM memory fits into the computer as a whole, see figure <a class="Reference" href="#fig:proc-Block-diagram-of-5500P">3.6↑</a>, where it is labeled as system memory.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:memory-dram">4.1.1↓</a> is an overview of DRAM technology. At the finest level, DRAM is an array of bits. Arrays of bits are organized into banks, ranks, and channels. There is a memory controller on each processor package that drives the memory channels. Most of the details of the memory controller are entirely hidden from the programmer (as well as the operating system). Fortunately, one does not need knowledge of the hardware at the level of the memory controllers or channels to write optimized programs. Thus, the principal purpose of the information in section <a class="Reference" href="#sub:memory-dram">4.1.1↓</a> is to provide context. The information is not directly useful in writing actual programs. However, its indirect implications can be of considerable importance.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:memory-cache">4.1.2↓</a>, we look at caches. Unlike DRAM, caches reside on the processor package. Accessing cached memory is much faster than accessing DRAM. Caching is done automatically by the hardware, and in principle, the programmer does not even need to know that caches exist. However, a basic knowledge of cache organization, including items such as the cache line size, is essential for writing optimized programs. 
</div>
<div class="Indented">
When the same data item is stored in DRAM memory as well as multiple caches, there is the problem of keeping caches coherent. In the next chapter, we will find that cache coherence is the basis of multithreaded programming. Inasmuch as multithreaded programming is a central paradigm from scientific computing to web servers and mobile apps, it is inadvisable to program with an ignorance of caches. Technically, multithreaded programs can be written without heeding caches. Doing so would imply inefficiencies as well as the danger of falling into error without realizing it. 
</div>
<div class="Indented">
DRAM memory is a shared resource. At any given point in time, dozens of programs on a computer could all be using DRAM memory. Each of these programs is written as if the program owns its own memory. The Linux kernel implements virtual memory, with the help of the processor hardware. Thanks to virtual memory, programs can be written assuming a tractable memory model. Section <a class="Reference" href="#sub:memory-virtual">4.1.3↓</a> is an introduction to virtual memory. The way virtual memory is set up can have major implications for program speed, especially in the multithreaded context, as we will see on several occasions later.
</div>
<div class="Indented">
Sections <a class="Reference" href="#sub:memory-latency-first">4.1.4↓</a> and <a class="Reference" href="#sub:memory-latency">4.1.5↓</a> utilize knowledge of DRAM, cache lines, as well as virtual memory to measure latency to DRAM. Latency to DRAM is the number of cycles between issuing an instruction to load a word into a register from DRAM and the completion of that instruction. Finding the latency to DRAM might appear straightforward. It might appear that all we need to do is time a load or store instruction.
</div>
<div class="Indented">
In fact, determining latency to memory is not so straightforward. Just as in register pipelines, there is a great deal of parallelism in the memory system. Although this parallelism is a great boon, it gets in the way of finding latency. Similarly, the virtual memory system can also get in the way. Thus, this simple exercise of finding latency to DRAM exposes several important elements of the memory system.
</div>
<div class="Indented">
To access a register, the latency is <span class="formula">1</span> cycle. To access the L1 cache, the latency is <span class="formula">4</span> cycles. The latency to DRAM, however, can be hundreds of cycles. It is estimated that 40% of the instructions access DRAM, and therefore hiding this large latency to DRAM is a major part of computer architectural design. It is important for the programmer to understand when this latency to DRAM can be hidden and when it cannot be hidden, but that is a point we will turn to in the next section. Briefly, the latency to DRAM can be effectively hidden when the data access is sequential and predictable. It cannot be hidden in linked lists and other dynamic data structures because the location of the next item is determined by a link from the present item. However, even in such situations, programming techniques can mitigate the latency to DRAM.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.1.1">4.1.1</a> DRAM memory<a class="Label" name="sub:memory-dram"> </a>
</h3>
<div class="Unindented">
By opening the cover of a computer and peering inside, we can look at memory plugged into the mother board. The devices that are plugged in are called Dual Inline Memory Modules (DIMMs). DIMMs can be purchased to add more memory to the computer. Each DIMM is a package of several little chips. The little chips are DRAM devices.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:memory-dram-channel-array"> </a><div class="multifigure">
<span class="float">
<div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/dram_array.png" alt="figure FIGS/chapter3/dram_array.png" style="max-width: 152px; max-height: 195px;"/>
<div class="caption">
(a) An array of bits with <span class="formula">16, 386</span> (<span class="formula">2<sup>14</sup></span>) rows and <span class="formula">2048</span> (<span class="formula">2<sup>11</sup></span>) columns.
</div>

</div>

</div>

</span>
<span class="hspace" style="width: 1cm;"></span><span class="float">
<div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/sixchannels.png" alt="figure FIGS/chapter3/sixchannels.png" style="max-width: 220px; max-height: 156px;"/>
<div class="caption">
(b) Twelve processors and six memory channels of an SSE2/Westmere computer.
</div>

</div>

</div>

</span>
<div class="caption">
Figure 4.2 Organization of DRAM memory.
</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:memory-dram-channel-array">4.2↑</a> shows two levels in the organization of DRAM memory: the finest and the outermost.<span class="FootOuter"><span class="SupFootMarker"> [73] </span><span class="HoverFoot"><span class="SupFootMarker"> [73] </span>The treatise by <span class="bibcites">[<a class="bibliocite" name="cite-35" href="#biblio-35"><span class="bib-index">35</span></a>]</span> has much more information than we give here.</span></span> On the left, it shows a DRAM array of bits. The DRAM array is the finest level of organization. Each DRAM channel is composed of several DRAM arrays of size <span class="formula">16<i>K</i> × 2<i>K</i></span>. On the right, it depicts six channels. The memory controllers that reside on the two packages drive the six channels. There are several other levels of organization between the DRAM array and the channel to memory. The memory channels are driven by memory controllers that reside on the processor packages as shown in the figure. 
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:memory-dram-channel-array">4.2↑</a> assumes a typical array size, which can of course vary and vary a lot, and six channels. Some of the more recent AVX2 computers have eight channels to memory, with four on each processor package. The number of channels can also vary quite a bit. 
</div>
<h? class="Subsubsection">
<b><u>Banks, ranks, channels, and DRAM technology</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:dram-2-organization"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/rank_bank.png" alt="figure FIGS/chapter3/rank_bank.png" style="max-width: 347px; max-height: 132px;"/>

</div>
<div class="caption">
Figure 4.3 A single row of <span class="formula">2<sup>6</sup></span> arrays in this figure constitutes a bank. All arrays in the same bank operate in concert. In the figure, a rank is composed of <span class="formula">2<sup>3</sup></span> banks. The <span class="formula">2<sup>3</sup> × 2<sup>6</sup></span> arrays are split across <span class="formula">16</span> devices, with each bank spanning all <span class="formula">16</span> devices. 
</div>

</div>

</div>

</div>
<div class="Indented">
To access a single bit in a <span class="formula">2<sup><i>m</i></sup> × 2<sup><i>n</i></sup></span> DRAM array takes <span class="formula"><i>m</i> + <i>n</i></span> address bits. These are the lowest bits in the memory address. In figure <a class="Reference" href="#fig:memory-dram-channel-array">4.2↑</a>, <span class="formula"><i>m</i> + <i>n</i> = 25</span>. The DRAM arrays are organized in banks so that <span class="formula">2<sup><i>a</i></sup></span> arrays constitute a bank. In figure <a class="Reference" href="#fig:dram-2-organization">4.3↑</a>, <span class="formula"><i>a</i> = 6</span> so that each bank consists of <span class="formula">64</span> arrays.
</div>
<div class="Indented">
When <span class="formula"><i>m</i> + <i>n</i></span> address bits are used to pick a bit from an array, the same <span class="formula"><i>m</i> + <i>n</i></span> bits are applied to every one of the arrays in a bank. Thus, the output from applying <span class="formula"><i>m</i> + <i>n</i></span> address bits will be <span class="formula">2<sup><i>b</i></sup></span> bits of data. In figure <a class="Reference" href="#fig:dram-2-organization">4.3↑</a>, the output would be <span class="formula">2<sup>6</sup></span> bits or <span class="formula">8</span> bytes. 
</div>
<div class="Indented">
There is one point related to banks of DRAM arrays that can have a significant impact on program speed. The way DRAM arrays work, it is easy to move from one bit in a given row to some other bit in the same row. Therefore, typically, consecutive addresses within a bank map to the same row. However, if a program generates memory addresses that require frequent switching between rows, there will be bank conflicts. Such bank conflicts slow down the program. 
</div>
<div class="Indented">
Although bank conflicts slow down programs, there is not much a programmer can do to avoid them. The organization of memory varies quite a bit from computer to computer, and it can be difficult to find out the parameters of memory organization.<span class="FootOuter"><span class="SupFootMarker"> [74] </span><span class="HoverFoot"><span class="SupFootMarker"> [74] </span>To determine the parameters of memory organization, such as the number of arrays in a bank, on a given computer, one may start with the GNU/Linux command <tt>dmidecode</tt> and then look up the corresponding JEDEC manuals.</span></span> Even if the parameters of DRAM organization are known, the manner in which the memory controllers split memory accesses between channels can be almost impossible to determine. 
</div>
<div class="Indented">
As shown in figure <a class="Reference" href="#fig:dram-2-organization">4.3↑</a>, the next level of organization after the bank is the rank. If <span class="formula">2<sup><i>b</i></sup></span> banks constitute a rank, then <span class="formula"><i>b</i> + <i>m</i> + <i>n</i></span> address bits are used to pick a bank within a rank and then extract a word from the arrays that constitute that bank. The <span class="formula"><i>b</i></span> bits used to pick the bank are higher than the other <span class="formula"><i>m</i> + <i>n</i></span> bits. In figure <a class="Reference" href="#fig:dram-2-organization">4.3↑</a>, <span class="formula"><i>b</i> = 3</span>. 
</div>
<div class="Indented">
The ranks are grouped further to obtain a memory channel, and the entire address space is split between the memory channels. Typically, the maximum amount of memory that may be installed will be much less than the maximum amount addressable. For example, a <span class="formula">48</span>-bit-wide address bus can address <span class="formula">256</span> terabytes of memory, which is 1,000 times more than what even a high-end machine may provide for. 
</div>
<div class="Indented">
Given the speed at which modern computers operate, an error rate of one in a billion would imply several memory errors every second. DRAM almost always provides for error correction, a point we did not go into here.
</div>
<div class="Indented">
Almost all DRAM in use today is double data rate (DDR), and it has been that way for a long time. DDR was a major advance in DRAM technology. In earlier technology, a single bit of data was pushed out to a single pin (connected to a single line of the data bus) during one period of the clock signal. DDR initiates a data transfer during the rising edge as well as the falling edge of the clock (see figure <a class="Reference" href="#fig:dram-3-ddr">4.4↓</a>). Two bits of data are pushed out in a single period of the clock, doubling the bandwidth.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:dram-3-ddr"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/ddr.png" alt="figure FIGS/chapter3/ddr.png" style="max-width: 310px; max-height: 144px;"/>

</div>
<div class="caption">
Figure 4.4 Each diamond in this figure stands for a bit. DDR technology doubles the data rate by transferring a bit at both the rising and falling edges of the clock signal. 
</div>

</div>

</div>

</div>
<div class="Indented">
DDR memory is often advertised to be DDR3 or DDR5 and so on. The numerical suffixes <span class="formula">3</span> or <span class="formula">5</span> give the prefetch length. In DDR3, when <span class="formula"><i>m</i> + <i>n</i></span> address bits are sent to a bank of <span class="formula">2<sup><i>a</i></sup></span> arrays, the DRAM outputs not just <span class="formula">2<sup><i>a</i></sup></span> data bits, with <span class="formula">1</span> bit for each array, but <span class="formula">2<sup><i>a</i> + 3</sup></span> bits. It outputs the <span class="formula">2<sup><i>a</i></sup></span> bit word corresponding to the <span class="formula"><i>m</i> + <i>n</i></span> bit address as well as <span class="formula">2<sup>3</sup></span> words corresponding to <span class="formula">8</span> consecutive addresses, including the one that was sent to it. The assumption here is that when a program asks for a certain word, it will probably also ask for the next word. So when <span class="formula">2<sup>3</sup></span> words are sent to the memory controllers, the memory controllers may be able to service multiple load/store instructions while triggering a single DRAM access. This technique improves peak bandwidth assuming sequential, or nearly sequential, data access.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.1.2">4.1.2</a> Cache memory<a class="Label" name="sub:memory-cache"> </a>
</h3>
<div class="Unindented">
An instruction such as <tt>movq %rax, %rdx</tt>, which moves one register to another, takes a single cycle. However, a load instruction such as <tt>movq (%rsi), %rdx</tt>, which moves a quad word from memory to a register, can take more than <span class="formula">100</span> cycles. An instruction that writes can take even longer. Computer processors cache frequently used parts of DRAM memory so that they can execute instructions at the speed of the registers, although nearly half the instructions are loads and stores from the much slower DRAM devices.<span class="FootOuter"><span class="SupFootMarker"> [75] </span><span class="HoverFoot"><span class="SupFootMarker"> [75] </span>The classic by <span class="bibcites">[<a class="bibliocite" name="cite-38" href="#biblio-38"><span class="bib-index">38</span></a>]</span> has a thorough discussion of caches. There is a lot of information in the Intel manuals as well.</span></span>
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:cache-memory-1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/BarcelonaCore.png" alt="figure FIGS/chapter3/BarcelonaCore.png" style="width: 210px; max-width: 180px; height: 210px; max-height: 180px;"/>

</div>
<div class="caption">
Figure 4.5 Layout of a single core of AMD Opteron (code named Barcelona) (layout based on Patterson and Hennessy). 
</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:cache-memory-1">4.5↑</a> shows the layout of a single processor core. About half the area of the chip is taken up by cache memory and the memory controller. The latency to L1 cache memory is a mere <span class="formula">4</span> cycles. It is much faster to access cache than to access DRAM memory.
</div>
<div class="Indented">
The basic principle of cache organization is data locality. Once a word is accessed from DRAM memory, computer programs have a tendency to access the same word or nearby words repeatedly. Thus, it makes sense to keep the word in a fast cache after it is first accessed. Later accesses can be serviced quickly from the cache without contacting the slow DRAM devices. 
</div>
<div class="Indented">
All transfers between DRAM memory and the processors is in cache line multiples. A cache line is <span class="formula">64</span> bytes or <span class="formula">512</span> bits on most x86 computers today. Thus, a cache line is big enough to hold <span class="formula">8</span> <tt>double</tt>s or <span class="formula">16</span> <tt>int</tt>s. The cache lines are aligned in memory to begin at addresses whose last <span class="formula">6</span> bits (notice that <span class="formula">2<sup>6</sup> = 64</span>) are zero. 
</div>
<h? class="Subsubsection">
<b><u>Cache parameters</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:cache-memory-3"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/cache_xeon5650.png" alt="figure FIGS/chapter3/cache_xeon5650.png" style="max-width: 289px; max-height: 181px;"/>
<div class="caption">
Figure 4.6 Schematic sketch of the cache hierarchy of typical SSE2/AVX/AVX2 processors. The L1 and L2 parameters do not vary much (e.g., these are the same for all the machines of table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>), but the L3 size can vary a lot. The L3 cache as shown is <span class="formula">12</span> MB here but can be four times higher or only one quarter as much. 
</div>

</div>

</div>

</div>
Figure <a class="Reference" href="#fig:cache-memory-3">4.6↑</a> shows cache organization on typical x86 processors. Each processor core has its own L1 and L2 caches. The L1 cache is smaller than L2 but faster to access. All processor cores on the same package share L3 cache. The L3 cache is much bigger in size than the L2 cache.
</div>
<div class="Indented">
Having multiple levels of cache implies that the penalty of a miss at a certain level is not too high if the cache line is found at the next level. Thus, the cost of a cache miss worsens gradually.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-cache-parameters"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="left" valign="top">

</td>
<td align="right" valign="top">
Number of Sets
</td>
<td align="right" valign="top">
Associativity
</td>
<td align="right" valign="top">
Size
</td>

</tr>
<tr>
<td align="left" valign="top">
L1 (instruction)
</td>
<td align="right" valign="top">
64 
</td>
<td align="right" valign="top">
8 way
</td>
<td align="right" valign="top">
32 KB
</td>

</tr>
<tr>
<td align="left" valign="top">
L1 (data)
</td>
<td align="right" valign="top">
128
</td>
<td align="right" valign="top">
4 way
</td>
<td align="right" valign="top">
32 KB
</td>

</tr>
<tr>
<td align="left" valign="top">
L2 
</td>
<td align="right" valign="top">
512
</td>
<td align="right" valign="top">
8 way
</td>
<td align="right" valign="top">
262 KB
</td>

</tr>
<tr>
<td align="left" valign="top">
L3
</td>
<td align="right" valign="top">
12,288
</td>
<td align="right" valign="top">
16 way
</td>
<td align="right" valign="top">
12 MB
</td>

</tr>

</table>

</div>
<div class="caption">
Table 4.1 Cache parameters of typical SSE2/AVX/AVX2 processor packages. The L1 and L2 parameters are the same for all the machines of table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>. The size of the L3 cache varies quite a bit, the other parameters less so.
</div>

</div>

</div>

</div>
<div class="Indented">
The cache parameters of typical x86 computers are given in table <a class="Reference" href="#tab:memory-cache-parameters">4.1↑</a>. The parameters were found using the <tt>cpuid</tt> instruction. The size of the cache in bytes is equal to the product of the number of sets, associativity, and <span class="formula">64</span>, which is the number bytes in a single cache line. Cache size and cache associativity influence program speed, with greater being better. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:cache-memory-4"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/l1cache_assoc.png" alt="figure FIGS/chapter3/l1cache_assoc.png" style="max-width: 340px; max-height: 182px;"/>

</div>
<div class="caption">
Figure 4.7 Depiction of the L1 data cache with <span class="formula">128</span> sets and 4-way associativity assuming a cache line to be <span class="formula">64</span> bytes or <span class="formula">512</span> bits (which is very typical).  
</div>

</div>

</div>

</div>
<div class="Indented">
To understand what sets and associativity mean in the context of cache organization, we turn to figure <a class="Reference" href="#fig:cache-memory-4">4.7↑</a>. Suppose the processor wants to look for a byte whose physical address is <tt><span class="formula"><i>α</i></span></tt> in L1 data cache. The address of its cache line will be <div class="formula">
<span class="fraction"><span class="ignored">(</span><span class="numerator"><i>α</i> − <i>α</i><span class="mathrm">mod</span>64</span><span class="ignored">)/(</span><span class="denominator">64</span><span class="ignored">)</span></span>
</div>
because a cache line begins every <span class="formula">64</span>th byte. Cache lines are mapped to sets cyclically. The cache line containing the byte whose address is <span class="formula"><i>α</i></span> maps to set number <div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="fraction"><span class="ignored">(</span><span class="numerator"><i>α</i> − <i>α</i><span class="mathrm">mod</span>64</span><span class="ignored">)/(</span><span class="denominator">64</span><span class="ignored">)</span></span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><span class="mathrm"> mod</span>128
</div>
if there are <span class="formula">128</span> sets in the L1 data cache. The cache line could be in any of the four slots in that set. The processor will check the four slots simultaneously for a match. If there is a match, the processor will extract the byte from the cache line in the matching slot.
</div>
<div class="Indented">
If too many addresses map to the same set, there will be cache conflicts even if other sets of the cache are not heavily used. If the most frequently used addresses in a program segment map to only a few sets, the effectiveness of the cache is reduced, and cache conflicts become more likely. A fully associative cache, which has a single set and in which a cache line can be stored in any of the slots, is the best for reducing cache conflicts. However, such an ideal cache is too expensive to implement.
</div>
<div class="Indented">
The effect of cache conflicts is not as easy to detect in modern computers as used to be the case. Multiple levels of caching, instruction-level parallelism, the large size of the caches, and the sheer complexity of the memory system can make the effect of cache conflicts on strided accesses difficult to detect. 
</div>
<h? class="Subsubsection">
<b><u>Cache protocols</u></b>
</h?>
<div class="Unindented">
How do the cache controllers decide which cache lines to keep in cache and which ones to evict from cache? Suppose a certain cache line is accessed. If the cache line is already in cache, the access is serviced without altering the mapping of cache lines from DRAM to the cache. If the cache line is not in cache, it will be brought into cache. Some other cache line has to be evicted from the set the newly accessed cache line maps to. A popular strategy is to evict the least recently used cache line.
</div>
<div class="Indented">
Reads from memory are handled differently from writes to memory. If a processor writes to a cache line that is in cache, usually the cache line is modified in cache but the write is not propagated to DRAM memory. A dirty bit is turned on to inform the cache controllers that the cache line must be written back to memory when it is evicted. This policy of handling writes is called <i>write back</i>. An alternative is <i>write through</i>. In this policy, the write is propagated to DRAM memory. Yet another alternative is to propagate writes that modify the L1 cache to L2 cache but not to DRAM memory.
</div>
<div class="Indented">
An implication of the <i>write back</i> policy is that cache lines in DRAM memory may become invalid. The same cache line may be stored in the L1 or L2 caches of multiple processor cores. If one of them writes to its cache, the cache lines in other processor cores become invalid. The cache controller on each processor has to &ldquo;snoop&rdquo; on the traffic in the other processors to maintain a coherent cache.
</div>
<div class="Indented">
As a consequence of the write back policy, a single write instruction can trigger two DRAM accesses. Consider a write instruction of the type <tt>movq %rax, (%rdi)</tt>. If the quad word (<span class="formula">8</span> bytes) that <tt>%rdi</tt> points to is not in cache, the corresponding cache line must be brought in triggering the first DRAM access. Future instructions that read and write from the same cache line will be serviced without accessing DRAM. However, a second DRAM access is triggered when the cache line is evicted. In contrast, if all operations a cache line is subjected to are reads, there is no need for the second DRAM transfer. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.1.3">4.1.3</a> Physical memory and virtual memory<a class="Label" name="sub:memory-virtual"> </a>
</h3>
<div class="Unindented">
Virtual memory is implemented by the operating system and the processor hardware working in concert. Its main purpose is to prevent processes from interfering with each other’s memory. A single process will have only part of the DRAM memory for itself. The part of the DRAM memory that is available to a specific process is determined only when the process is loaded and can change as it runs. 
</div>
<div class="Indented">
Virtual memory is an illusion that simplifies programming, compilation, and linking in addition to keeping programs from interfering with each other. As the program runs, the memory addresses generated by a program are automatically mapped to physical addresses in the DRAM memory. So, for example, if a program issues an instruction such as <tt>movq %rax, (%rdi)</tt>, the address in <tt>%rdi</tt> is a virtual address. During instruction execution, the page tables are consulted by the hardware to map that address to a physical address. A separate map is maintained for each process to keep the processes from interfering with each other. The map is typically hierarchical and stored in multiple page tables. Page tables are the essence of virtual memory. 
</div>
<h? class="Subsubsection">
<b><u>Virtual memory in action</u></b>
</h?>
<div class="Unindented">
Let us consider an instruction that generates the memory reference <tt>(%rax, %rsi, 2)</tt>. The reference is to the memory location whose address is <tt>rax+2*rsi</tt>. The registers <tt>%rax</tt> and <tt>%rsi</tt> are <span class="formula">64</span>-bit. Therefore, a <span class="formula">64</span>-bit virtual address is formed. Strictly speaking, the virtual address is the last <span class="formula">48</span> bits of the address. If we print a pointer in a C program, the program prints <span class="formula">12</span> hex digits because a virtual address is <span class="formula">48</span> bits. A <span class="formula">48</span>-bit virtual address should get us past 2020 and can then be extended without changes to the instruction set architecture.
</div>
<div class="Indented">
How does the hardware look up an actual word in memory using a virtual address? The answer is complicated. The first step on x86 computers is to form a <span class="formula">64</span>-bit linear address by adding a segment register. This step is trivial and we will ignore it.
</div>
<div class="Indented">
The next and far more important step is to map virtual addresses to physical addresses. Once a physical address is formed, it may be used to look up DRAM or the caches. 
</div>
<div class="Indented">
To map addresses from virtual to physical memory, virtual memory is partitioned into pages. A page is typically 4,096 bytes (the command <tt>getconf PAGESIZE</tt> may be used to find out the page size). Thus, in a virtual address of <span class="formula">48</span> bits, the first <span class="formula">36</span> bits constitute a page address, and the following <span class="formula">12</span> bits are the address within that page. Correspondingly, DRAM memory is broken up into page frames, each of which is of the same size as a page. Page tables map page addresses to page frame addresses. 
</div>
<div class="Indented">
The manner in which page tables are set up does not concern us here. They are set up by the operating system kernel and left in a place where the processor hardware can look them up.The translation look-aside buffer (TLB) is a cache of the page tables. When an address such as <tt>(%rax, %rsi, 2)</tt> is formed, the next step is to look up the TLB to convert it to a physical address. Each entry in the TLB maps exactly one page to a page frame. If there is a TLB miss, the processor looks up the page tables.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-TLB-parameters"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="left" valign="top">

</td>
<td align="right" valign="top">
Number of Sets
</td>
<td align="right" valign="top">
Associativity
</td>
<td align="right" valign="top">
Size
</td>

</tr>
<tr>
<td align="left" valign="top">
Instruction TLB
</td>
<td align="right" valign="top">
32/16/32
</td>
<td align="right" valign="top">
4/8/4 way
</td>
<td align="right" valign="top">
128 entries
</td>

</tr>
<tr>
<td align="left" valign="top">
Data TLB
</td>
<td align="right" valign="top">
16
</td>
<td align="right" valign="top">
4 way
</td>
<td align="right" valign="top">
64 entries
</td>

</tr>
<tr>
<td align="left" valign="top">
Level 2 TLB (Shared)
</td>
<td align="right" valign="top">
128/128/_
</td>
<td align="right" valign="top">
4/8/_ way
</td>
<td align="right" valign="top">
512/1024/_ entries
</td>

</tr>

</table>

</div>
<div class="caption">
Table 4.2 TLB parameters for three x86 processors: <span class="formula">2.6</span>  GHz SSE2, <span class="formula">2.2</span>  GHz AVX, and <span class="formula">3.6</span>  GHz AVX2 (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines). The <span class="formula">3.6</span>  GHz AVX2 machine does not have level 2 TLB.
</div>

</div>

</div>

</div>
<div class="Indented">
TLB organization is similar to cache organization. Table <a class="Reference" href="#tab:memory-TLB-parameters">4.2↑</a> shows TLB parameters for a few machines. When the processor switches from one process to another, it is the operating system kernel’s responsibility to furnish a new set of page tables and flush the TLB. 
</div>
<div class="Indented">
Even a partial picture of what it takes to resolve a single memory reference such as <tt>(%rax, %rsi, 2)</tt> is mind-bogglingly complex. As noted already, <span class="formula">40</span>% of the instructions are estimated to be loads or stores. For every such instruction, the processor first forms a virtual address. To map the virtual address to the physical address, it looks up the TLB; if there is TLB miss, it must look up the page tables. Looking up the page tables may trigger additional DRAM accesses if the page tables are not in cache. 
</div>
<div class="Indented">
The page table lookup may trigger a page fault if the virtual page is yet to be mapped to a page frame. If so, the page fault handler, which is an important component of the operating system kernel, is invoked. The page fault handler allocates an actual page frame so that a physical address may be formed. Once a physical address is formed, the processor can look by the caches and trigger a DRAM access if necessary. 
</div>
<div class="Indented">
We think of a simple instruction such as <tt>movq (%rax, %rsi, 2), %rbx</tt> as taking a few cycles. If there is a page fault, the actual cost can be in the millions of cycles. In fact, there can be two page faults from this single instruction if the quad word crosses page boundaries. Even if there is no page fault, there can be a TLB miss, which can consume of the order of <span class="formula">100</span> cycles.
</div>
<h? class="Subsubsection">
<b><u>Virtual memory and program speed</u></b>
</h?>
<div class="Unindented">
Given that the virtual memory setup is exceedingly complex and is invoked almost every other instruction, it follows that the mechanisms used to implement virtual memory may influence program speed in many ways. 
</div>
<div class="Indented">
When memory is dynamically allocated by a program using <tt>malloc()</tt>, <tt>new[]</tt>, or <tt>_mm_malloc()</tt>, the last of which is preferred if memory needs to be aligned, the allocation function returns an address in virtual memory, and pages may be allocated only in virtual memory. There is always a page fault the first time a page is accessed. The pages are mapped to page frames by the page fault handler. If there is enough DRAM memory, page faults do not occur except when pages are accessed for the first time. 
</div>
<div class="Indented">
On most computers, the memory controllers predict memory accesses and prefetch words to cache. The prefetching normally does not cross page boundaries to prevent page faults from being triggered.
</div>
<div class="Indented">
Another scenario where the paging system is in play is when large amounts of data are transferred from DRAM memory to the network card or to graphics processor memory. During the transfer, some of the pages may get moved out of DRAM memory into hard disk by the memory management unit of the operating system kernel, complicating the transfer. One way around is to copy the data to be transferred to kernel buffers and incur a substantial overhead. Another way is to request the operating system kernel to keep the pages &ldquo;pinned&rdquo; to DRAM memory. 
</div>
<div class="Indented">
There are many more ways in which virtual memory impacts program speed. The role of cache and TLB misses is explained later in this chapter. Other aspects related to multithreaded programming are found in the next chapter.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.1.4">4.1.4</a> Latency to DRAM memory: First attempts<a class="Label" name="sub:memory-latency-first"> </a>
</h3>
<div class="Unindented">
In this section, we make our first attempts at measuring the access time to DRAM memory. All our attempts fail, but we are led into certain aspects of the memory system that have a bearing on program performance.
</div>
<div class="Indented">
The organization of memory is such that if we attempt to investigate one part of it, we need to be aware of the other parts as well. All the parts of the memory system are interrelated. Thus, to measure the access time to DRAM memory, we need to know about the size of the cache. We begin by giving a basic picture of the memory system as a whole. The measurement of latency brings to light some other parts of the memory system, and as we progress, we fill in some of the details in the picture.
</div>
<div class="Indented">
A significant point is that all the traffic between DRAM and the processor packages is in multiples of the cache line size. The cache line is 64 bytes or 512 bits. If we attempt to read a single byte or a single word (2 bytes or 16 bits) or a double word or a quad word from memory, the memory system will bring the entire cache line  into cache, anticipating that we will access other locations in the same cache line soon.
</div>
<div class="Indented">
Latency to DRAM memory is defined as the time between issuing a load/store instruction that triggers a DRAM access and its completion. The latency of writing to memory can differ from the latency of reading for certain types of DRAM. We limit ourselves to read latency for simplicity. 
</div>
<div class="Indented">
Measuring the latency to DRAM memory is a more complicated matter than one may realize at first sight. Many techniques are used to hide the latency to DRAM memory. Among these, the two most important are instruction-level parallelism and caching. Instruction-level parallelism enables the processor to issue a sequence of load or store instructions to set up a pipeline of memory accesses. If the pipeline can hold <span class="formula">10</span> memory accesses in various stages, the effective latency of a long stream of DRAM accesses is cut to a tenth. For example, if the latency to DRAM is <span class="formula">200</span> cycles, the <span class="formula"><i>n</i></span>th memory access may be initiated during cycle number <span class="formula">20<i>n</i></span> and completed during cycle number <span class="formula">20<i>n</i> + 199</span>. This sort of overlapping is possible only if the memory accesses are independent and there is sufficient parallelism in the instruction stream. Caching greatly reduces the effective latency of DRAM memory access as well. These techniques for hiding latency to memory are so effective that many programs do not realize how large the latency to DRAM memory can be.
</div>
<div class="Indented">
Here is a first and not very careful attempt at measuring latency.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int unitstride(int *a){
	int sum=0;
	for(int i=0; i &lt; 1000*1000*1000; i++)
		sum += a[i];
	return sum;
}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>unitstride()</tt> loads <span class="formula">10<sup>9</sup></span> entries of <tt>a[]</tt> and computes their sum. The compiler unrolls the loop and introduces parallelism in the loop body. As a result, the running time of this function is determined by instructions that load entries of <tt>a[]</tt> and not by arithmetic.
</div>
<div class="Indented">
The function <tt>unitstride()</tt> is timed as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void time_unitstride(){
<span class="number-left">2</span>	int a[1000*1000*1000];
<span class="number-left">3</span>	for(int i=0; i &lt; 1000*1000*1000; i++)
<span class="number-left">4</span>		a[i] = 0;
<span class="number-left">5</span>	TimeStamp clk;
<span class="number-left">6</span>	clk.tic();
<span class="number-left">7</span>	unitstride(a);
<span class="number-left">8</span>	double cycles = clk.toc();
<span class="number-left">9</span>	cout&lt;&lt;"cycles/access = "&lt;&lt;
<span class="number-left">10</span>	   cycles/(1000*1000*1000)&lt;&lt;endl; 
<span class="number-left">11</span>}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>time_unitstride()</tt> defines <tt>a[]</tt> statically, initializes every entry of <tt>a[]</tt> to <span class="formula">0</span>, and makes a single call to <tt>unitstride()</tt> on line 7. The function call is timed, and the number of cycles it consumes is divided by <span class="formula">10<sup>9</sup></span> to derive the cycles consumed per access. Each array dereferencing <tt>a[i]</tt> in <tt>unitstride()</tt> is counted as a memory access.
</div>
<div class="Indented">
The two functions <tt>unitstride()</tt> and <tt>time_unitstride()</tt> are defined in separate files and compiled separately. As usual, we do not use the <tt>-ipo</tt> option for interprocedural optimization during compilation, and we use <tt>-fno-inline-functions</tt> to eliminate function inlining. If the two functions are in the same file or if interprocedural optimization is turned on, the compiler will figure out that the call to <tt>unitstride()</tt> is doing nothing meaningful and simply eliminate the call.
</div>
<div class="Indented">
Initializing the array to zero on line 4 ensures that page frames in physical memory have been allocated to the entire array before the function <tt>unitstride()</tt> is called.
</div>
<div class="Indented">
The program reports <span class="formula">0.92</span> cycles per access. Cycle counts are quite similar on SSE2, AVX, and AVX2 machines. Therefore, we only report the numbers for a <span class="formula">2.6</span>  GHz SSE2 machine (for the full name of the machine, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). The <tt><span class="formula">0.92</span> </tt>cycles per access figure is a woeful underestimate of the latency to DRAM memory. The single for-loop in <tt>unitstride()</tt> has a loop-carried RAW dependency. The excellent <tt>icpc</tt> compiler has little difficulty recognizing that a list of numbers can be grouped before it is summed to introduce parallelism into the loop body. The generated code uses the XMM (or YMM) registers, each of which is large enough to pack four (or eight) <tt>int</tt>s, to decrease the number of instructions used to load entries of <tt>a[]</tt> into the registers. Why is the access time here so much smaller than the latency to DRAM? One reason is that a cache line is large enough to hold <span class="formula">16</span> <tt>int</tt>s, and only one of <span class="formula">16</span> accesses of the array is fetching data from outside the cache.
</div>
<div class="Indented">
Any program that accesses data sequentially with unit stride will benefit from the size of the cache line. To avoid cache hits, the program below accesses only one of every <span class="formula">16</span> locations.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int stride16(int *a){
	int sum=0;
	for(int i=0; i &lt; 1000*1000*1000; i+=16)
		sum += a[i];
	return sum;
}
</pre>
</div>

</div>
<div class="Indented">
In <tt>stride16()</tt>, the cost of accessing a single entry of the array is about <span class="formula">13</span> cycles, which is once again much smaller than the latency to DRAM. As many as <span class="formula">10</span> memory reads can be in flight at the same time. Strided accesses benefit from parallelism in the instruction pipeline and in the memory system. Strided accesses are particularly easy to predict. It is likely that the prefetch engines in the memory controller are able to prefetch many of the locations accessed in <tt>stride16()</tt> into cache ahead of time.
</div>
<div class="Indented">
A more refined attempt follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>//List must be 64 byte aligned
<span class="number-left">2</span>void accessList(long int *List, long int n, int count,
<span class="number-left">3</span>		double &amp;x){
<span class="number-left">4</span>	long int  index = 0;
<span class="number-left">5</span>	for(int i=0; i &lt; count; i++){
<span class="number-left">6</span>		x += List[index];
<span class="number-left">7</span>		index = List[index]%n;
<span class="number-left">8</span>		index = index - index%8;
<span class="number-left">9</span>	}   
<span class="number-left">10</span>}
</pre>
</div>

</div>
<div class="Indented">
The loop defined in lines 5 through 9 works as follows. It reads the entry <tt>List[index]</tt> on lines 6 and 7, and the next entry that is read is determined by the earlier entry on lines 7 and 8. It is assumed that all entries of <tt>List[]</tt> are initialized to be non-negative. The next load instruction cannot be issued until the earlier load instruction is complete because the address of the next load instruction is determined by the result of the earlier load instruction. This way of accessing memory breaks instruction-level parallelism.
</div>
<div class="Indented">
The function <tt>accessList()</tt> is called with <span class="formula"><i>n</i> = 10<sup>9</sup></span> and <span class="formula"><span class="text"><tt>count</tt></span> = </span>6,000. The array <tt>List</tt> is initialized with pseudorandom numbers. We verified explicitly (verification not shown here) that the 6,000 entries accessed in the body of the loop were all distinct. Line 8 ensures that every <tt>index</tt> used to look up <tt>List[]</tt> is a multiple of <span class="formula">8</span>. 
</div>
<div class="Indented">
All transfers between DRAM memory and cache are done in cache lines. A single cache line is <span class="formula">64</span> bytes. The array <tt>List</tt> is assumed to be <span class="formula">64</span> byte aligned (the last <span class="formula">6</span> bits of the address <tt>List</tt> are zero). Because every <tt>index</tt> used to look up <tt>List[]</tt> is a multiple of <span class="formula">8</span> and a <tt>long int</tt> is <span class="formula">8</span> bytes, every memory reference on line 6 brings in a new cache line. No cache line is accessed more than once. The total number of memory accesses is 6,000.
</div>
<div class="Indented">
When the cycles consumed by the entire function was divided by 6,000, we got <span class="formula">270</span> cycles. So is the latency to DRAM memory about <span class="formula">270</span> cycles? Not really. We were careful to break instruction-level parallelism. We flushed the entire array out of cache (not shown here) before calling the function and respected cache line boundaries. However, we forgot to think about a third element. Memory management is done using pages, page frames, page directories, and page tables. Because the array is so large, every new memory reference is likely to hit a new page. The page table entry for that page is unlikely to be in TLB and may have to be brought in from DRAM memory. What we thought was one memory access is likely to be two or more. 
</div>
<div class="Indented">
This little example may give us a sense of the complex way in which parts of the memory system interact. Cache line size, parallelism in memory access, and the cost of TLB misses are all of much significance to program optimization.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.1.5">4.1.5</a> Latency to DRAM<a class="Label" name="sub:memory-latency"> </a>
</h3>
<div class="Unindented">
Our earlier attempts to measure latency to DRAM memory failed because we did not account for the overhead of creating and accessing page table entries. The more careful program in this section breaks instruction-level parallelism, ensures that none of the cache lines accessed is from L1, L2, or L3 cache, and accesses all of the 256 cache lines within four pages of memory (so that TLB misses are not a factor).
</div>
<div class="Indented">
To begin with, we look at the function <tt>randomp()</tt>, which initializes the <span class="formula"><i>n</i></span> entries of the array <tt>List[]</tt> to be a random permutation of the numbers <span class="formula">0, 1, …, <i>n</i> − 1</span>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void randomp(int *List, int n){
<span class="number-left">2</span>	for(int i=0; i &lt; n; i++)
<span class="number-left">3</span>		List[i] = i;
<span class="number-left">4</span>	for(int i=0; i &lt; n; i++){
<span class="number-left">5</span>		int j = rand()%(n-i)+i;
<span class="number-left">6</span>		int temp = List[j];
<span class="number-left">7</span>		List[j] = List[i];
<span class="number-left">8</span>		List[i] = temp;
<span class="number-left">9</span>	}
<span class="number-left">10</span>}
</pre>
</div>

</div>
<div class="Indented">
On lines 2 and 3, the array <tt>List[]</tt> is initialized to be the identity permutation <span class="formula">0, 1, …, <i>n</i> − 1</span>. The loop from lines 4 through 9 picks <span class="formula"><i>j</i></span> to be a random number from the set <span class="formula"><i>i</i>, …, <i>n</i> − 1</span> and swaps <tt>List[i]</tt> and <tt>List[j]</tt> for each value of <span class="formula"><i>i</i></span> from <span class="formula">0</span> to <span class="formula"><i>n</i> − 1</span>. The random number generator <tt>rand()</tt> used on line 5 is convenient, being part of the standard C libraries, and sufficient for our purposes here. However, faster and more rigorously tested random number generators are available.
</div>
<div class="Indented">
The program for measuring latency uses two methods for clearing the array used for measurement from cache memory. If the preprocessor variable <tt>MEMWALK</tt> is undefined at the top of the file using 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#undef MEMWALK
</pre>
</div>

</div>
<div class="Indented">
then it uses the <tt>CLFLUSH</tt> instruction issued using an intrinsic. However, if <tt>MEMWALK</tt> is defined at the top of the file, it uses the following function: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void dummy(double *a, int len){
<span class="number-left">2</span>	for(int i=0; i &lt; len; i++)
<span class="number-left">3</span>		a[i] = (i%77)*(i*1001);
<span class="number-left">4</span>}
</pre>
</div>

</div>
<div class="Indented">
A large array <tt>a[]</tt> is passed to this function, which writes something into every entry on line 3. The array <tt>a[]</tt> is not used for measuring latency. Its sole purpose is to occupy the cache completely and evict the array used for measuring latency from the cache.<span class="FootOuter"><span class="SupFootMarker"> [76] </span><span class="HoverFoot"><span class="SupFootMarker"> [76] </span>To be certain that the cache is occupied, one has to check the assembly and make sure that instructions such as <tt>MOVNTPD</tt>, which avoid cache pollution, are not generated.</span></span> The function <tt>dummy()</tt> must be defined in a separate compilation unit to prevent the compiler from eliminating the function call. 
</div>
<div class="Indented">
The function <tt>latency()</tt>, whose listing follows, touches all the cache lines in four pages of memory and prints an estimate of the latency to DRAM memory.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void latency(){
<span class="number-left">2</span>	int List[256];
<span class="number-left">3</span>	randomp(List,256); 
<span class="number-left">4</span>	int LList[256];
<span class="number-left">5</span>	for(int i=0; i &lt; 256; i++)
<span class="number-left">6</span>		LList[List[i]]=List[(i+1)%256];
<span class="number-left">7</span>	//int is 4 bytes
<span class="number-left">8</span>	__declspec(align(4096)) int FourPages[4096];
<span class="number-left">9</span>	for(int i=0; i &lt; 256; i++)
<span class="number-left">10</span>		FourPages[16*i] = LList[i];
<span class="number-left">11</span>#ifdef MEMWALK
<span class="number-left">12</span>	double a[1000*1000*100];
<span class="number-left">13</span>	dummy(a, 1000*1000*100);
<span class="number-left">14</span>#else
<span class="number-left">15</span>	for(int i=0; i &lt; 4096; i++)
<span class="number-left">16</span>		_mm_clflush(FourPages+i);
<span class="number-left">17</span>#endif
<span class="number-left">18</span>	int index = 17;
<span class="number-left">19</span>	TimeStamp clk;
<span class="number-left">20</span>	clk.tic();
<span class="number-left">21</span>	for(int i=0; i &lt; 256; i++){
<span class="number-left">22</span>		index = FourPages[16*index];
<span class="number-left">23</span>	}
<span class="number-left">24</span>	double cycles = clk.toc();
<span class="number-left">25</span>	cout&lt;&lt;"index = "&lt;&lt;index&lt;&lt;endl;
<span class="number-left">26</span>	cout&lt;&lt;"cycles per access = "&lt;&lt;cycles/256&lt;&lt;endl;
<span class="number-left">27</span>}
</pre>
</div>

</div>
<div class="Indented">
The key to this function are the three arrays <tt>List[]</tt>, <tt>LList[]</tt>, and <tt>FourPages[]</tt> defined on lines 2, 4, and 8, respectively. The array <tt>List[]</tt> is set to be a random permutation of the numbers <span class="formula">0, …, 255</span> on line 3. The array <tt>LList[]</tt> is initialized using <tt>List[]</tt>, and <tt>FourPages[]</tt> is initialized using <tt>LList[]</tt>. 
</div>
<div class="Indented">
The key idea for breaking instruction-level parallelism is to access the entries of the array <tt>List[]</tt> in the following order:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">List[i], List[List[i]], List[List[List[i]]],...
</pre>
</div>

</div>
<div class="Indented">
If the entries are accessed in this manner, no access can be initiated before the earlier access is complete. However, there is the problem that one of the entries may cycle back to <tt>List[i]</tt>. For example, if <tt>List[i]</tt> is <tt>j</tt> and <tt>List[j</tt>] is <tt>i</tt>, the sequence above will repeat with a period of just <span class="formula">2</span>. 
</div>
<div class="Indented">
We need a permutation that is one big cycle. Lines 5 and 6 create and initialize the array <tt>LList[]</tt> to be such a permutation. The sequence
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">LList[i], LList[LList[i]], LList[LList[LList[i]]],...
</pre>
</div>

</div>
<div class="Indented">
will have a period of <span class="formula">256</span> for any <span class="formula"><i>i</i></span>, <span class="formula">0 ≤ <i>i</i> &lt; 256</span>. However, each <tt>int</tt> is 4 bytes, and <span class="formula">16</span> of the entries of <tt>LList[]</tt> will be in the same cache line. Although the sequence above breaks instruction-level parallelism and generates entries of <tt>LList[]</tt> in an order that is random enough to preempt cache prefetches, the same cache line is accessed <span class="formula">16</span> times. 
</div>
<div class="Indented">
Line 8 declares <tt>FourPages[]</tt> to be an array of 4,096 <tt>int</tt>s. Because each <tt>int</tt> is 4 bytes, the array is four pages long. The declaration qualifier on line 8 (which is valid only with the <tt>icpc</tt> compiler) ensures that <tt>FourPages</tt> is 4,096-byte aligned or page aligned. Every <span class="formula">16</span>th entry of <tt>FourPages[]</tt> is set to an entry of <tt>List[]</tt> in line 10. The loop from lines 21 to 23 accesses only those entries of <tt>FourPages[]</tt> whose index is a multiple of <span class="formula">16</span>. Every cache line is accessed only once.
</div>
<div class="Indented">
Before the entries are accessed and the program is timed, the array <tt>FourPages[]</tt> must be evicted from cache. Lines 12 and 13 define a large array and write to each entry of the array and indirectly remove <tt>FourPages[]</tt> from cache. Lines 15 and 16 use the <tt>CLFLUSH</tt> instruction to explicitly flush the cache lines of <tt>FourPages[]</tt> from cache memory.
</div>
<div class="Indented">
If the program is correct, line 25 should print <tt>index</tt> to be <span class="formula">17</span>, same as the value it was assigned on line 18. Because the permutation has period <span class="formula">256</span>, the last index generated by the for-loop on lines 17 to 19 must equal the first index. The print statement on line 25 forces the compiler to generate code for the entire program. Otherwise, the compiler can easily figure out that the program is doing nothing useful and ignore all the trouble we have taken to set up <tt>FourPages[]</tt> and then cycle through it.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-latency-to-DRAM-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="right" valign="top">
# of pages
</td>
<td align="center" valign="top">
mwalk
</td>
<td align="center" valign="top">
clflush
</td>
<td align="right" valign="top">
# of pages
</td>
<td align="center" valign="top">
mwalk
</td>
<td align="center" valign="top">
clflush
</td>

</tr>
<tr>
<td align="right" valign="top">
4
</td>
<td align="center" valign="top">
122
</td>
<td align="center" valign="top">
94
</td>
<td align="right" valign="top">
60
</td>
<td align="center" valign="top">
247
</td>
<td align="center" valign="top">
254
</td>

</tr>
<tr>
<td align="right" valign="top">
8
</td>
<td align="center" valign="top">
126
</td>
<td align="center" valign="top">
93
</td>
<td align="right" valign="top">
1000
</td>
<td align="center" valign="top">
251
</td>
<td align="center" valign="top">
257
</td>

</tr>
<tr>
<td align="right" valign="top">
16
</td>
<td align="center" valign="top">
130
</td>
<td align="center" valign="top">
95
</td>
<td align="right" valign="top">
10000
</td>
<td align="center" valign="top">
257
</td>
<td align="center" valign="top">
256
</td>

</tr>
<tr>
<td align="right" valign="top">
32
</td>
<td align="center" valign="top">
148
</td>
<td align="center" valign="top">
100
</td>
<td align="right" valign="top">
100000
</td>
<td align="center" valign="top">
265
</td>
<td align="center" valign="top">
263
</td>

</tr>
<tr>
<td align="right" valign="top">
40
</td>
<td align="center" valign="top">
247
</td>
<td align="center" valign="top">
248
</td>
<td align="right" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>

</tr>

</table>

</div>
<div class="caption">
Table 4.3 Latency to DRAM memory on a <span class="formula">3.6</span>  GHz AVX2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full name of the machine). Latencies on older SSE2 or AVX machines are similar. For example, on a <span class="formula">2.6</span>  GHz SSE2 machine, the latency is around <span class="formula">100</span> cycles when the number of pages is <span class="formula">16</span> or fewer and increases to <span class="formula">180</span> thereafter. Data is cleared from cache by either accessing a long array (mwalk) or using the cache flush instruction (clflush). 
</div>

</div>

</div>
Table <a class="Reference" href="#tab:memory-latency-to-DRAM-1">4.3↑</a> reports several measurements of latency. The program was modified to be able to handle <span class="formula">4<i>n</i></span> pages for <span class="formula"><i>n</i> ≥ 1</span>. The numbers reported are medians obtained from a large number of measurements. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:memory-latency-to-DRAM-1">4.3↑</a> shows that the measured latency depends on the number of pages accessed. On an AVX2 machine, the latency is of the order of <span class="formula">100</span> cycles when the number of pages accessed is <span class="formula">32</span> or less. When the number of pages used in our experiment is <span class="formula">40</span> or more, the latency jumps to around <span class="formula">250</span> cycles. We are not certain of the explanation. Our best guess is that the jump in latency may have something to do with the number of arrays in a bank of DRAM memory as well as the number of columns in each array. When the number of pages used is few, it is possible that all the pages map to the same row in a bank of DRAM memory.
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:memory-latency-to-DRAM-1">4.3↑</a> reports measurements of latency using two different techniques. In &ldquo;mwalk,&rdquo; the array <tt>FourPages[]</tt> is evicted from cache by writing to some other large array (line 13). In &ldquo;clflush,&rdquo; the array is evicted using the <tt>CLFLUSH</tt> instruction (line 16). When the number of pages is <span class="formula">32</span> or less, the &ldquo;mwalk&rdquo; figure is noticeably higher. That appears to be because writing to some other array to evict <tt>FourPages[]</tt> from cache implies write-backs of that other array when <tt>FourPages[]</tt> is used to measure latency. The write-backs are likely to cause row switching within a bank.<span class="FootOuter"><span class="SupFootMarker"> [77] </span><span class="HoverFoot"><span class="SupFootMarker"> [77] </span>To test this hypothesis, we modified &ldquo;mwalk&rdquo; to read from a large array rather than write to a large array. Reading does not trigger write-backs. As expected, the &ldquo;mwalk&rdquo; figures after this modification are close to the &ldquo;clflush&rdquo; figures.</span></span>
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  What is the size of DRAM memory on your machine? Write a simple C program to find out the maximum amount of memory that can be allocated with <tt>malloc()</tt> or <tt>_mm_malloc()</tt> on your machine. Does that limit depend on how many other programs are running on your system and how much memory they are using?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  How many DRAM arrays constitute a bank in your machine’s memory? Is your memory DDR3, DDR5, or DDR with some other prefetch parameter? How many bytes of data are transferred between DRAM and the memory controllers after a single load instruction?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the CPUID instruction to determine the cache and TLB parameters on your machine. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Why do processors prefer to have separate L1 caches for instructions and data? Similarly, what may be desirable in having separate level 1 TLBs for instruction and data? 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Upon <tt>malloc()</tt>, virtual memory is allocated, but the pages of virtual memory are not mapped to page frames in physical memory. Therefore, the first access of every page triggers a page fault. Write a C program to demonstrate this phenomenon and determine the cost of invoking the page fault handler.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In section <a class="Reference" href="#sub:memory-latency-first">4.1.4↑</a>, we determined the cost per access of an <tt>int</tt> assuming sequential access, strided access that reads just one <tt>int</tt> per cache line, and a more complex pattern that triggers TLB misses. Repeat these measurements on your machine. Determine the cost per access if each access is a write instead of a read.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Measure the latency to L1, L2, and L3 caches.<span class="FootOuter"><span class="SupFootMarker"> [78] </span><span class="HoverFoot"><span class="SupFootMarker"> [78] </span>For latency and bandwidth to DRAM memory as well as caches, see <span class="bibcites">[<a class="bibliocite" name="cite-36" href="#biblio-36"><span class="bib-index">36</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-41" href="#biblio-41"><span class="bib-index">41</span></a>]</span>.</span></span>
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Modify the program to measure latency given in section <a class="Reference" href="#sub:memory-latency">4.1.5↑</a> so that the four pages are not contiguous in virtual memory. How does that affect the measurement?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The function <tt>latency() </tt>prints the final value of index as well as the measured latency. Modify the program so that neither quantity is printed within the function itself. Does the change make a difference to the measured latency?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program that measures the latency of writes to memory and another program that measures the latency to memory when writes and reads are intermixed.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Does the validity of the program to measure latency given in section <a class="Reference" href="#sub:memory-latency">4.1.5↑</a> depend on whether the memory is DDR3 or DDR5? Does it depend on the number of arrays in a bank?
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-4.2">4.2</a> Optimizing memory access<a class="Label" name="sec:memory-optimizing"> </a>
</h2>
<div class="Unindented">
In this section, we look at optimization of memory access using three examples. The first example, in section <a class="Reference" href="#sub:memory-Bandwidth-to-DRAM">4.2.1↓</a>, is to simply access a long array of numbers to sum or to copy. With this simple example, we learn what may be the most important lesson related to memory access, which is to utilize each cache line as fully as possible.
</div>
<div class="Indented">
The examples in sections <a class="Reference" href="#sub:memory-Matrix-transpose">4.2.2↓</a> and <a class="Reference" href="#sub:memory-Optimized-matrix-multiplication">4.2.3↓</a> are more involved. Although cache lines are the units of transfer of data between DRAM and the caches, cache organization involves multiple levels and sets. In every memory access, a virtual address must be translated to a physical address. This translation using the TLB and possibly the page tables can  be a source of considerable overhead.
</div>
<div class="Indented">
The chief technique in optimizing for multiple cache levels and the TLB is the same, namely, blocking. In section <a class="Reference" href="#sub:memory-Matrix-transpose">4.2.2↓</a>, we study blocking using matrix transposition as an example. Section <a class="Reference" href="#sub:memory-Optimized-matrix-multiplication">4.2.3↓</a> also illustrates blocking in addition to the technique of streaming data from cache to reduce cache and TLB misses. 
</div>
<div class="Indented">
Fortunately, much of the time we do not need to program in assembly when optimizing memory access. This is partly because the memory system is so complicated that overly refined optimizations do not make sense. Another reason is that the memory system is more amenable to optimization than the instruction pipeline. For the vast majority of nontrivial programs, speed is limited by memory access. Thus, the memory system’s greater amenability to optimization is probably by design. Although it would be incorrect to assume that compilers generate optimal instruction streams, the penalty for suboptimality is not as great. 
</div>
<div class="Indented">
The design of the memory system consisting of DRAM, memory controllers, and caches is relatively stable across platforms. Therefore, the techniques of optimization may be expected to be the same on graphics devices, non-x86 platforms, and mobile devices.
</div>
<div class="Indented">
Our discussion of the memory system in the previous section began with aspects of hardware design and virtual memory and concluded with a measurement of latency. In contrast, through much of this section, the emphasis is on bandwidth to memory. In all the examples discussed in this section, the large latency to memory can be hidden with little effort.
</div>
<div class="Indented">
Examples in which latency to memory can be hidden completely are characterized by parallelism in the instruction stream. For example, if a program adds an array of numbers by accessing entries in sequence, the processor can issue multiple loads from memory in parallel. If the memory accesses can be overlapped, the effective latency to memory becomes manageable.
</div>
<div class="Indented">
However, if memory accesses cannot be overlapped, the program is exposed to latency to DRAM memory. Our program to measure latency, given in the previous section, is an example where memory accesses cannot be overlapped. In general, memory accesses cannot be overlapped if the location of the next memory access depends on the result of the previous memory access.
</div>
<div class="Indented">
Situations in which memory accesses cannot be overlapped are exceedingly common. Such scenarios arise whenever linked lists, trees, or graphs are used to handle dynamic data. Even in these situations, some of the techniques of memory optimization we discuss may still be relevant. For example, if successive items of a linked list are packed closely in memory, some advantage may be derived from caching. Techniques for dynamic data structures are deferred to the exercises. In general, when the use of dynamic data structures cannot be avoided, it is likely that exposure to DRAM latency also cannot be completely avoided.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.2.1">4.2.1</a> Bandwidth to DRAM<a class="Label" name="sub:memory-Bandwidth-to-DRAM"> </a>
</h3>
<div class="Unindented">
The most predictable and common pattern of memory access is to access a long line of data in sequence. Every memory controller is likely to be optimized to handle that pattern efficiently. Thus, to determine bandwidth to memory, we will access a long array in sequence.
</div>
<div class="Indented">
The following simple function returns the sum of an array of <tt>double</tt>s: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double sum(double *restrict a, long int n){
	double s = 0;
	for(long int i=0; i &lt; n; i++)
		s += a[i];
	return s;
}
</pre>
</div>

</div>
<div class="Indented">
The loop body uses XMM/YMM registers, as we may verify by inspecting the assembly code. The single statement in the loop has a loop-carried RAW dependency. Nevertheless, the simple structure of the loop helps the compiler unroll the loop and introduce parallelism in the loop body. The entries of the array <tt>a[]</tt> are read from memory in parallel. The additions do not introduce an overhead above the time it takes to read the array from memory. 
</div>
<div class="Indented">
The program was called with an array <tt>a[]</tt> that was <span class="formula">8</span> GB. The array was initialized in the same sequence it is summed. It is important to initialize the array before the function <tt>sum()</tt> is called. Pages of virtual memory are mapped to page frames of physical memory only at first access. If the array is not initialized at all, the mapping of virtual memory to physical memory takes place when <tt>sum()</tt> executes. 
</div>
<div class="Indented">
The cache is too small to hold <span class="formula">8</span> GB data. So none of the load instructions will hit the cache. Every byte accessed by <tt>sum()</tt> must be loaded from memory. 
</div>
<div class="Indented">
Because each <tt>double</tt> is <span class="formula">8</span> bytes, if the function <tt>sum()</tt> takes <span class="formula"><i>c</i></span> cycles to execute, we may take the bandwidth to memory to be <span class="formula">8<i>n</i> ⁄ <i>c</i></span> bytes per cycle. The bandwidth to memory on a <span class="formula">2.66</span>  GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines) was measured to be <span class="formula">4.36</span> bytes per cycle or, equivalently, <span class="formula">11.6</span> GB/s. On a more recent <span class="formula">2.20</span>  GHz AVX machine, the bandwidth was <span class="formula">5.40</span> bytes per cycle or <span class="formula">11.9</span> GB/s. On a yet more recent <span class="formula">3.6</span>  GHz AVX2 machine, the bandwidth was <span class="formula">4.85</span> bytes per cycle or <span class="formula">17.46</span> GB/s. 
</div>
<div class="Indented">
We use strided memory accesses to lead up to what is perhaps the single most important item a programmer should know about accessing memory. The following function sums entries of the array <tt>a[]</tt>, beginning with the zeroth entry and in steps of length <tt>stride</tt>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double sumstride(double *restrict a, long int n, 
		int stride){
	double s = 0;
	for(long int i=0; i &lt; n; i+=stride)
		s += a[i];
	return s;
}
</pre>
</div>

</div>
<div class="Indented">
The bandwidth to memory realized in strided access is <div class="formula">
<span class="fraction"><span class="ignored">(</span><span class="numerator">8<i>n</i></span><span class="ignored">)/(</span><span class="denominator"><i>c</i> × <span class="text">stride</span></span><span class="ignored">)</span></span>
</div>
assuming the function takes <span class="formula"><i>c</i></span> cycles. 
</div>
<div class="Indented">
Before we begin making measurements with non-unit strides, it is a good idea to try <tt>stride=1</tt>. It turns out that <tt>sumstride()</tt> realizes a bandwidth of only <span class="formula">2.4</span> bytes cycle on the SSE2 machine for unit stride, which is well short of the <span class="formula">4.36</span> bytes per cycle realized by <tt>sum()</tt>. Evidently, the compiler is not optimizing as well when stride is passed as a parameter.
</div>
<div class="Indented">
We make the stride a <tt>const int</tt> to coax the compiler to optimize better. For unit stride, we use the definition 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">const int STR=1;
</pre>
</div>

</div>
<div class="Indented">
To make the stride equal to <span class="formula">8</span>, we modify the definition to 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">const int STR=8;
</pre>
</div>

</div>
<div class="Indented">
The function <tt>sumconststride()</tt> given below uses <tt>STR</tt> as its striding parameter.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double sumconststride(double *restrict a, long int n){
	double s = 0;
	for(long int i=0; i &lt; n; i+=STR)
		s += a[i];
	return s;
}
</pre>
</div>

</div>
<div class="Indented">
Because the compiler knows the numerical value of the stride, it can optimize the loop much better.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-bandwidth"> </a><div class="table">
<div class="PlainVisible">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Stride
</td>
<td align="center" valign="top">
Read
</td>
<td align="center" valign="top">
Copy
</td>

</tr>
<tr>
<td align="center" valign="top">
1
</td>
<td align="center" valign="top">
4.36
</td>
<td align="center" valign="top">
3.53
</td>

</tr>
<tr>
<td align="center" valign="top">
2
</td>
<td align="center" valign="top">
2.25
</td>
<td align="center" valign="top">
1.83
</td>

</tr>
<tr>
<td align="center" valign="top">
4
</td>
<td align="center" valign="top">
1.21
</td>
<td align="center" valign="top">
0.92
</td>

</tr>
<tr>
<td align="center" valign="top">
8
</td>
<td align="center" valign="top">
0.80
</td>
<td align="center" valign="top">
0.60
</td>

</tr>

</table>

</div>
<br/>
<div class="center">
<div class="caption">
Table 4.4 Bandwidth to memory in bytes per cycle on a <span class="formula">2.6</span>  GHz SSE2 machine. The measured read and copy bandwidths are <span class="formula">5.3</span> bytes per cycle and <span class="formula">2.4</span> bytes per cycle on a <span class="formula">2.2</span>  GHz AVX machine. The read and copy bandwidths are both <span class="formula">4.85</span> bytes per cycle on a <span class="formula">3.6</span>  GHz AVX2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines).
</div>

</div>

</div>

</div>

</div>
Table <a class="Reference" href="#tab:memory-bandwidth">4.4↑</a> gives the measured bandwidth for strides equal to <span class="formula">1</span>, <span class="formula">2</span>, <span class="formula">4</span>, and <span class="formula">8</span>. The bandwidth is nearly halved every time the stride is doubled. This behavior is easily explained. A cache line is equal to <span class="formula">64</span> bytes or <span class="formula">8</span> <tt>double</tt>s in size. All traffic between DRAM and the processor packages is cache line by cache line. When we stride by <span class="formula">2</span>, <span class="formula">4</span>, or <span class="formula">8</span>, we utilize only a half, a quarter, or an eighth of every cache line that is brought in. The single most important memory optimization is to ensure that a cache line is utilized as fully as possible. 
</div>
<div class="Indented">
Bandwidth to memory depends on the type of access. Typically, reads are faster than writes to memory. The functions <tt>copy()</tt> and <tt>copyconststride()</tt> are used to measure bandwidth to memory when one array is copied into another array. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void copy(double *restrict a, double *restrict b, 
	  long int n){
	for(long int i=0; i &lt; n; i++)
		b[i] = a[i];
}
​
void copyconststride(double *restrict a, 
		     double *restrict b, long int n){
	for(long int i=0; i &lt; n; i+=STR)
		b[i] = a[i];
}
</pre>
</div>

</div>
<div class="Indented">
If <tt>copyconststride()</tt> takes <span class="formula"><i>c</i></span> cycles, the bandwidth realized is <div class="formula">
<span class="fraction"><span class="ignored">(</span><span class="numerator">16<i>n</i></span><span class="ignored">)/(</span><span class="denominator"><i>c</i> × <span class="text">stride</span></span><span class="ignored">)</span></span>.
</div>
The factor <span class="formula">16</span> in the numerator accounts for copying an <span class="formula">8</span>-byte-long <tt>double</tt> to another <tt>double</tt>. The bandwidth realized when one array is copied to another is listed in table <a class="Reference" href="#tab:memory-bandwidth">4.4↑</a>.
</div>
<div class="Indented">
Typically, bandwidths for copying and writing are lower than that of reading. In table <a class="Reference" href="#tab:memory-bandwidth">4.4↑</a>, the copying bandwidth (with stride 1) is 80% of the read bandwidth. In simple situations, there are ways to approach the read bandwidth more closely, however. One may invoke special instructions to get around the write-back cache policy. In fact, on a <span class="formula">3.6</span>  GHz AVX2 machine, the compiler invokes a runtime library function for copying, and the copy and read bandwidths are both <span class="formula">4.85</span> bytes per cycle.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.2.2">4.2.2</a> Matrix transpose<a class="Label" name="sub:memory-Matrix-transpose"> </a>
</h3>
<div class="Indented">
The only cache parameter of significance for strided accesses studied in the last section was cache line size. The sizes of the caches, their organization into sets, as well as the size of the TLB influence the performance of the example studied in this section.
</div>
<div class="Indented">
The example we study here is out-of-place matrix transpose. It is always best to access data sequentially with unit stride, but when a matrix is transposed to another matrix stored in the same column-major format, there is no way to access both matrices with unit stride. In many problems of this type, it is useful to break up the data into blocks. 
</div>
<div class="Indented">
When a single array is accessed with a constant stride, a cache line brought into cache is used in a single go. Once the array moves past a cache line, we do not return to it. In matrix transpose with blocking, we rely on a cache line remaining in cache as we return to it repeatedly after working on other columns of the matrix block. This type of cache usage is more delicate. We find that the performance of the matrix transpose depends in a nonmonotonic manner on the size of the blocks. The performance degrades abruptly if the leading dimension of the matrix is divisible by a high power of <span class="formula">2</span>. Such effects, though disconcerting to the programmer, cannot be eliminated. 
</div>
<div class="Indented">
The cache and TLB parameters of SSE2/AVX/AVX2 machines we use are given in tables <a class="Reference" href="#tab:memory-cache-parameters">4.1↑</a> and <a class="Reference" href="#tab:memory-TLB-parameters">4.2↑</a>. On all the machines, the L1 data cache is big enough to hold 4,000 <tt>double</tt>s and the L3 cache can hold more than a million double-precision numbers. 
</div>
<h? class="Subsubsection">
<b><u>Blocking </u></b>
</h?>
<div class="Unindented">
The function <tt>easytrans()</tt> listed below uses a simple doubly nested loop to transpose the matrix <tt>a[]</tt> to the matrix b<tt>[]</tt>. The matrices are of dimension <span class="formula"><i>m</i> × <i>n</i></span> and <span class="formula"><i>n</i> × <i>m</i></span>. Both of them are assumed to be stored in column-major order with leading dimension (see section <a class="Reference" href="#sub:libmake-blas-leadingdim">2.2.1↑</a>) equal to the number of rows.<a class="Label" name="function-easytrans"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void easytrans(double *restrict a, double *restrict b,
	       int m, int n){
	for(int i=0; i &lt; m; i++)
		for(int j=0; j &lt; n; j++)
			b[j+i*n] = a[i+j*m];
}
</pre>
</div>

</div>
<div class="Indented">
This function is easy to write and easy for the compiler to analyze. The array references use indices that are linear combinations of the loop variables, and the loop variables are incremented in steps of <span class="formula">1</span>. The assembly code generated by the compiler is far more complicated than the code presented to it. The total number of double-precision numbers accessed by this program is <span class="formula">2<i>mn</i></span>. Because each <tt>double</tt> is <span class="formula">8</span> bytes, the bandwidth to memory realized is <div class="formula">
<span class="fraction"><span class="ignored">(</span><span class="numerator">8<i>mn</i></span><span class="ignored">)/(</span><span class="denominator"><span class="text">cycles for a single transpose</span></span><span class="ignored">)</span></span>
</div>
bytes per cycle. The compiler-optimized <tt>easytrans()</tt> realizes a bandwidth of <span class="formula">1.48</span> bytes per cycle when <span class="formula"><i>m</i> = </span>20,000 and <span class="formula"><i>n</i> = </span>30,000. The best bandwidth we could have hoped for is <span class="formula">3.53</span> bytes per cycle, which is the bandwidth realized when an array is copied to another array with unit stride (see table <a class="Reference" href="#tab:memory-bandwidth">4.4↑</a>). Despite compiler optimization, the realized bandwidth falls well short of that mark.
</div>
<div class="Indented">
The function <tt>blocktransx()</tt> listed below implements matrix transpose block by block. The block size is <span class="formula"><i>B</i> × <i>B</i></span>, and <tt><span class="formula"><i>B</i></span></tt> is defined as a <tt>const int</tt> (definition is not shown). The matrix dimensions are assumed to be divisible by <span class="formula"><i>B</i></span>. The function <tt>blocktransx()</tt> uses a nest of four loops.<a class="Label" name="function-blocktransx"> </a>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void blocktransx(double *restrict a, double *restrict b, 
<span class="number-left">2</span>                 int m, int n){
<span class="number-left">3</span>  assert((m%B==0)&amp;&amp;(n%B==0));
<span class="number-left">4</span>  for(int i=0; i &lt; m; i+=B)
<span class="number-left">5</span>    for(int j=0; j &lt; n; j+=B)
<span class="number-left">6</span>      for(int ii=0; ii &lt; B; ii++)
<span class="number-left">7</span>        for(int jj=0; jj &lt; B; jj++)
<span class="number-left">8</span>          b[j+jj+(i+ii)*n] = a[i+ii+(j+jj)*m];
<span class="number-left">9</span>}
</pre>
</div>

</div>
<div class="Indented">
When writing functions such as <tt>blocktransx()</tt>, it helps to think directly in terms of the for-loop construct of C/C++. 
</div>
<div class="Indented">
The loop variable <tt>i</tt> defined on line 4 steps through the rows of the <span class="formula"><i>m</i> × <i>n</i></span> matrix stored in the array <tt>a[] </tt>in steps equal to the block size B. The loop variable <tt>ii</tt> defined on line 6 steps through the <tt>B</tt> rows of a single block of rows. Thus, <tt>i+ii</tt> is the index--- relative to the <span class="formula"><i>m</i> × <i>n</i></span> matrix---of row <tt>ii</tt> within the block of B rows from <tt>i</tt> to <tt>i+B-1</tt>. Similarly, <tt>j+jj</tt> is the index---relative to the matrix <tt>a[]</tt> as a whole---of column <tt>jj</tt> within the block of <tt>B</tt> columns from <tt>j</tt> to <tt>j+B-1</tt>. The loop body of <tt>blocktransx()</tt>, which is comprised of the single statement
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">b[j+jj+(i+ii)*n] = a[i+ii+(j+jj)*m];
</pre>
</div>

</div>
<div class="Indented">
corresponds to the statement <tt>b[j+i*n]=a[i+j*m];</tt>, which comprises the loop body of <tt>easytrans()</tt>.
</div>
<div class="Indented">
It is significant that the block size parameter <tt>B</tt> is a <tt>const int</tt>. Knowledge of the numerical value of <tt>B</tt> allows the compiler to optimize better.
</div>
<div class="Indented">
The bandwidth to memory realized by <tt>blocktransx()</tt> is given in table <a class="Reference" href="#tab:memory-transpose-1">4.5↓</a> for block sizes from <span class="formula"><i>B</i> = 8</span> to <span class="formula"><i>B</i> = 1000</span>. Once again, <span class="formula"><i>m</i> = </span>20,000 and <span class="formula"><i>n</i> = </span>30,000. From the columns headlined &ldquo;nest,&rdquo; we may observe that the realized bandwidth is worse when <span class="formula"><i>B</i> ≤ 25</span> than it is for <tt>easytrans()</tt>. The highest bandwidth of <span class="formula">2.60</span> bytes per cycle is realized when <span class="formula"><i>B</i> = 125</span>. The bandwidth to memory begins to degrade as <span class="formula"><i>B</i></span> is increased.
</div>
<div class="Indented">
While the optimal block size from the table is <span class="formula"><i>B</i> = 125</span>, the degradation is more severe for small block sizes than for larger ones. Block sizes <span class="formula"><i>B</i> ≤ 25</span> realize lower bandwidth to memory than <tt>easytrans()</tt>. Small block sizes inhibit the compiler from optimizing the inner loops, lowering the realized bandwidth. When the block size is large, the compiler optimizes the inner loops and generates code that is considerably different from what is presented to it. 
</div>
<div class="Indented">
Why does blocking improve memory bandwidth? First, it helps with reuse of cache lines. Suppose the blocks are small enough to fit into cache. Then every cache line in a block is fetched from memory only once if we ignore cache conflicts and misalignment. 
</div>
<div class="Indented">
Second, blocking helps reduce TLB misses. A single page is 4,096 bytes. Therefore, if <span class="formula"><i>m</i> ≥ 512</span>, each entry of a row will be in a different page. Thus, if the size of a row is greater than the number of TLB entries, row-by-row traversal of the entire matrix will trigger TLB misses for every entry. Blocking can eliminate TLB misses by limiting the number of entries in a row. 
</div>
<div class="Indented">
The function of <tt>blocktransx()</tt> uses only one level of blocking. Because the caches and the TLB are organized hierarchically in multiple levels, it is natural to wonder whether multiple levels of blocking may bring some advantage. In this setting, recursive blocking, which is described in the exercises and which uses multiple levels of blocking, appears to be no better (see table <a class="Reference" href="#tab:memory-transpose-1">4.5↓</a>) and does not make it easier to find an optimal block size. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-transpose-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
B
</td>
<td align="center" valign="top">
Nest
</td>
<td align="center" valign="top">
Recurse
</td>
<td align="center" valign="top">
B
</td>
<td align="center" valign="top">
Nest
</td>
<td align="center" valign="top">
Recurse
</td>

</tr>
<tr>
<td align="center" valign="top">
8
</td>
<td align="center" valign="top">
0.90
</td>
<td align="center" valign="top">
0.80
</td>
<td align="center" valign="top">
80
</td>
<td align="center" valign="top">
2.37
</td>
<td align="center" valign="top">
2.44
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
0.96
</td>
<td align="center" valign="top">
0.98
</td>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
2.37
</td>
<td align="center" valign="top">
2.46
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
1.33
</td>
<td align="center" valign="top">
1.56
</td>
<td align="center" valign="top">
125
</td>
<td align="center" valign="top">
2.60
</td>
<td align="center" valign="top">
2.51
</td>

</tr>
<tr>
<td align="center" valign="top">
25
</td>
<td align="center" valign="top">
1.47
</td>
<td align="center" valign="top">
1.55
</td>
<td align="center" valign="top">
200
</td>
<td align="center" valign="top">
2.12
</td>
<td align="center" valign="top">
2.13
</td>

</tr>
<tr>
<td align="center" valign="top">
40
</td>
<td align="center" valign="top">
1.86
</td>
<td align="center" valign="top">
1.97
</td>
<td align="center" valign="top">
500
</td>
<td align="center" valign="top">
2.02
</td>
<td align="center" valign="top">
2.02
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
2.06
</td>
<td align="center" valign="top">
1.93
</td>
<td align="center" valign="top">
1000
</td>
<td align="center" valign="top">
1.88
</td>
<td align="center" valign="top">
1.88
</td>

</tr>

</table>

</div>
<div class="caption">
Table 4.5 Bandwidth to memory in bytes per cycle for matrix transpose using <span class="formula"><i>B</i> × <i>B</i></span> blocks on a <span class="formula">2.6</span> GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full name of the machine). One-level blocking using a loop nest is compared with recursive blocking (recursive blocking is described in the exercises). With no blocking, the bandwidth realized is <span class="formula">1.48</span> bytes per cycle. The matrix had <span class="formula">20, 000</span> rows and <span class="formula">30, 000</span> columns.
</div>

</div>

</div>

</div>
<div class="Indented">
The best bandwidth for transposing in table <a class="Reference" href="#tab:memory-transpose-1">4.5↑</a> is <span class="formula">2.60</span> bytes per cycle. On the <span class="formula">2.6</span>  GHz SSE2 machine, that is 60% of the peak read bandwidth. In contrast, on a <span class="formula">3.6</span>  GHz AVX2 machine, the best bandwidth for transposing was <span class="formula">1.96</span> bytes per cycle, which is about the same as SSE2 in GB/s and which is only 40% of the read bandwidth. 
</div>
<div class="Indented">
The reason for poorer performance on the modern AVX2 machine is not entirely clear. Generating good code for the inner block is crucial in this example, and the compiler does not appear to do anything special beyond translating the code as it is written. If the compiler were doing a good job, one should see <tt>movpd</tt> instructions (<tt>pd</tt> stands for packed double), which use the entire capacity of the YMM registers, in the innermost loop. Instead, one sees <tt>movsd</tt> instructions (<tt>sd</tt> for single double), which use only a quarter of the YMM registers. Thus, the poorer performance could be a consequence of suboptimal compilation, a phenomenon we encountered several times in the previous chapter and whose probability increases on more recent hardware.
</div>
<h? class="Subsubsection">
<b><u>Leading dimension divisible by a high power of <span class="formula">2</span></u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:memory-transpose-2"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
B
</td>
<td align="center" valign="top">
nest
</td>
<td align="center" valign="top">
recurse
</td>
<td align="center" valign="top">
recurse/loop
</td>

</tr>
<tr>
<td align="center" valign="top">
8
</td>
<td align="center" valign="top">
0.84
</td>
<td align="center" valign="top">
0.92
</td>
<td align="center" valign="top">
0.95
</td>

</tr>
<tr>
<td align="center" valign="top">
16
</td>
<td align="center" valign="top">
1.30
</td>
<td align="center" valign="top">
1.32
</td>
<td align="center" valign="top">
1.34
</td>

</tr>
<tr>
<td align="center" valign="top">
32
</td>
<td align="center" valign="top">
1.17
</td>
<td align="center" valign="top">
1.22
</td>
<td align="center" valign="top">
1.22
</td>

</tr>
<tr>
<td align="center" valign="top">
64
</td>
<td align="center" valign="top">
1.17
</td>
<td align="center" valign="top">
1.18
</td>
<td align="center" valign="top">
1.18
</td>

</tr>
<tr>
<td align="center" valign="top">
128
</td>
<td align="center" valign="top">
0.58
</td>
<td align="center" valign="top">
0.58
</td>
<td align="center" valign="top">
0.58
</td>

</tr>

</table>

</div>
<div class="caption">
Table 4.6 Bandwidth in bytes per cycle (on a <span class="formula">2.6</span>  GHz SSE2 machine). The transposed matrix was square and of dimension <span class="formula">2<sup>14</sup> = 16, 384</span>. One level of blocking using a loop nest is compared against recursive blocking implemented using explicit recursion or loops (recursive blocking is discussed in the exercises). Transpose with no blocking had a bandwidth of only <span class="formula">0.43</span> bytes per cycle. Compare with table <a class="Reference" href="#tab:memory-transpose-1">4.5↑</a>.
</div>

</div>

</div>
If two locations in memory are separated by a high power of <span class="formula">2</span>, they are likely to map to the same set in cache and TLB. Table <a class="Reference" href="#tab:memory-transpose-2">4.6↑</a> shows the bandwidth realized when a matrix of dimension <span class="formula">2<sup>14</sup> × 2<sup>14</sup></span> is transposed. Successive entries in the same row of this matrix are separated by <span class="formula">2<sup>11</sup></span> cache lines. All entries of a row map to the same set in L1 and L2 caches as well as level 1 and level 2 TLB. 
</div>
<div class="Indented">
For a matrix of dimension <span class="formula">2<sup>14</sup> × 2<sup>14</sup></span>, <tt>easytrans()</tt> realizes a bandwidth of <span class="formula">0.43</span> bytes per cycle, which is less than a third of the bandwidth it realizes for a matrix of dimension <span class="formula">20, 000 × 30, 000</span>. With blocking, the best bandwidth observed is <span class="formula">1.34</span> bytes per cycle and the optimal block size is <span class="formula">16 × 16</span>. TLB and cache misses occur more frequently for the matrix of table <a class="Reference" href="#tab:memory-transpose-2">4.6↑</a> because the leading dimension is divisible by a high power of <span class="formula">2</span>. As a result, the observed bandwidth is <span class="formula">35</span>% of the best possible instead of <span class="formula">60</span>%, as in the case where the matrix dimensions are not divisible by high powers of <span class="formula">2</span>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.2.3">4.2.3</a> Optimized matrix multiplication<a class="Label" name="sub:memory-Optimized-matrix-multiplication"> </a>
</h3>
<div class="Unindented">
Some of the most nettlesome issues in implementing matrix multiplication arise at the level of the processor pipeline. In section <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↑</a>, we wrote an assembly program for <span class="formula">4 × 200 × 4</span> matrix multiplication, which reached <span class="formula">3.5</span> flops per cycle, against the theoretical limit of <span class="formula">4.0</span> flops per cycle, assuming all the matrices to be in cache. Here we assume the matrices to be in DRAM memory and not in cache, and we show how to optimize matrix multiplication.
</div>
<div class="Indented">
Suppose <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span> are matrices of dimensions <span class="formula">ℓ × <i>m</i></span>, <span class="formula"><i>m</i> × <i>n</i></span>, and <span class="formula">ℓ × <i>n</i></span>, respectively. The matrices are assumed to be in DRAM memory. The cost of the operation <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span> is <span class="formula">2ℓ<i>mn</i></span> arithmetic operations, half of which are additions and half of which are multiplications. If the cache were large enough, each of the matrices can be loaded into cache and kept there as the matrix multiplication is performed. Loading the matrices into cache would take <span class="formula">ℓ<i>m</i> + <i>mn</i> + ℓ<i>n</i></span> DRAM memory accesses. If <span class="formula">ℓ</span>, <span class="formula"><i>m</i></span>, and <span class="formula"><i>n</i></span> are large, the number of arithmetic operations is much greater than the number of memory accesses. We may expect the cost of the computation to be dominated by the arithmetic operations, allowing us to approach the peak bandwidth of <span class="formula">4</span> flops per cycle on a single core of an SSE2 machine (the figures are <span class="formula">8</span> flops per cycle and <span class="formula">16</span> flops per cycles for AVX and AVX2 machines, respectively).
</div>
<div class="Indented">
We want the matrices to be big so that the arithmetic operations are far more numerous than memory accesses, but the catch is that the matrices will not fit into cache when they are too big. One way to overcome this dilemma is to use block matrix multiplication. We can pick the block sizes to be small enough to fit into cache but large enough that the cost of loading from memory is outweighed by the cost of arithmetic operations. Careful blocking would indeed improve the simple programs of section <a class="Reference" href="#subsec:proc-compileropt-mmult">3.2.5↑</a> but not enough to get anywhere close to peak bandwidth. A much more powerful set of ideas<span class="FootOuter"><span class="SupFootMarker"> [79] </span><span class="HoverFoot"><span class="SupFootMarker"> [79] </span><span class="bibcites">[<a class="bibliocite" name="cite-40" href="#biblio-40"><span class="bib-index">40</span></a>]</span>.</span></span> shows how a program for multiplying matrices can approach the peak bandwidth for floating point arithmetic. 
</div>
<div class="Indented">
In outline, the basic idea remains to multiply in blocks, but intermediate blocks are stored in scratch space and in convenient formats to minimize TLB and cache misses. As far as possible, data is stored in a format that enables sequential access with unit stride. The actual execution of this idea can make it look more complicated than it is, but the idea is elegant as well as possibly applicable to many other problems.
</div>
<div class="Indented">
As shown in table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↓</a>, our implementation progresses systematically from the <span class="formula">4 × 200 × 4</span> microkernel described in section <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↑</a> to the multiplication of square matrices of dimension 9,000. We code a hierarchy of matrix multiplication functions with a function corresponding to each row of the table. Square matrices of dimension 9,000, which occur in the last row, are too large to fit into cache memory. Limiting ourselves to matrices of specific dimensions keeps the exposition tractable. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:matrix-mult-optimized-dimension-vs-bw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Matrix Dimensions
</td>
<td align="center" valign="top">
b/w
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">4 × 200 × 4</span>
</td>
<td align="center" valign="top">
3.48
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">4 × 200 × 12</span>
</td>
<td align="center" valign="top">
3.32
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">600 × 200 × 12</span>
</td>
<td align="center" valign="top">
3.22
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">600 × 200 × 3000</span>
</td>
<td align="center" valign="top">
3.21
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3000 × 200 × 3000</span>
</td>
<td align="center" valign="top">
3.19
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">9000 × 9000 × 9000</span>
</td>
<td align="center" valign="top">
3.19
</td>

</tr>

</table>

</div>
<div class="caption">
Table 4.7 Bandwidth in flops per cycle on a <span class="formula">2.6</span> GHz SSE2 machine for the multiplication of an <span class="formula">ℓ × <i>m</i></span> matrix with an <span class="formula"><i>m</i> × <i>n</i></span> matrix. Matrix dimensions are reported as <span class="formula">ℓ × <i>m</i> × <i>n</i></span>.
</div>

</div>

</div>

</div>
<div class="Indented">
The square matrices of dimension 9,000 at the bottom of table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↑</a> are stored in column-major order. However, the storage format of the matrices in every other row is different. The storage format for the <span class="formula">4 × 200 × 4</span> microkernel was assumed to be such as to lead to a high throughput of arithmetic operations. For the following rows of the table, the storage formats are chosen to allow for convenient and efficient invocation of the function that corresponds to the preceding row of the table. 
</div>
<div class="Indented">
The matrices are denoted using capitalized letters when they are in column-major order with a leading dimension that may exceed the number of rows. Lowercase letters are used for other storage formats. 
</div>
<div class="Indented">
So, for example, the arrays that store the arguments to the <span class="formula">4 × 200 × 4</span> microkernel are <tt>a[]</tt>, <tt>b[]</tt>, and <tt>c[]</tt>. The microkernel interprets <span class="formula">4 × 200 × 4</span> matrix multiplication as the sum of <span class="formula">200</span> outer products of <span class="formula">4 × 1</span> and <span class="formula">1 × 4</span> matrices. Accordingly, the array <tt>a[]</tt> is assumed to store a <span class="formula">4 × 200</span> matrix in column-major order. The array <tt>b[]</tt> is assumed to store a <span class="formula">200 × 4</span> matrix in row-major order. In both cases, the leading dimension is equal to <span class="formula">4</span>. The <span class="formula"><i>i</i></span>th outer product accesses the entries<div class="formula">
<i>a</i>[4<i>i</i>, …, 4<i>i</i> + 3] <span class="text"> and</span> <i>b</i>[4<i>i</i>, …, 4<i>i</i> + 3].
</div>
The array <tt>c[]</tt> is of length <span class="formula">16</span> and stores a <span class="formula">4 × 4</span> matrix. In section <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↑</a>, we assumed that the storage format of <tt>c[] </tt>was column-major with &ldquo;skewing.&rdquo; The skewing can be undone using a single step of unskewing, which introduces only an insignificant overhead at the end. Therefore, we ignore it, although skewing percolates down the rows of table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↑</a>, implying that the matrix <span class="formula"><i>C</i></span> of dimension 9,000 corresponding to the last row of the table needs to be unskewed. Thus, the array <tt>c[]</tt> is assumed to store a <span class="formula">4 × 4</span> matrix in column-major format (with skewing, which we ignore here) with leading dimension equal to <span class="formula">4</span>.
</div>
<div class="Indented">
As we progress down the rows of table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↑</a>, the implementation of each row reuses the lower case letters <span class="formula"><i>a</i>, <i>b</i>, <i>c</i></span>, assuming whatever format is most suitable for its purposes. 
</div>
<div class="Indented">
Closely related to storage formats is the use of work space. To allow for changes in storage format, the matrix multiplication functions use extra memory stored in an array called <tt>scratch[]</tt>. The size of this array in its original incarnation is <div class="formula">
600 × 12 + 600 × 200 + 200 × 3000
</div>
double-precision numbers. The three terms correspond to <tt>c[]</tt>, <tt>a[]</tt>, and <tt>b[]</tt>, respectively. The multiplication of square matrices of dimension <span class="formula">9000</span> is partitioned repeatedly into multiplications of lower dimensional matrices. The array <tt>scratch[]</tt> changes at every level in this hierarchy.
</div>
<div class="Indented">
As we step through the design, it is helpful to keep a few numbers in mind. The L1 cache is <span class="formula">32</span> KB and big enough to hold 4,000 double-precision numbers. The L3 cache is <span class="formula">12</span> MB and big enough to hold <span class="formula">1.5</span> million double-precision numbers. The second-level TLB has <span class="formula">512</span> entries (see tables <a class="Reference" href="#tab:memory-cache-parameters">4.1↑</a> and <a class="Reference" href="#tab:memory-TLB-parameters">4.2↑</a>). To keep the design simple, we ignore the L2 cache and the first level TLB. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:memory-Bandwidth-to-DRAM">4.2.1↑</a>, we found that it takes about two cycles to load a single <tt>double</tt> into memory at peak bandwidth. The changes in storage formats that occur, when we begin with the microkernel and increase the dimensions of the matrices that are multiplied, are too complicated to permit access to memory at peak bandwidth. It is perhaps reasonable to take the cost of accessing a <tt>double</tt> from DRAM memory as <span class="formula">10</span> cycles. At peak floating point bandwidth, the cost of a single arithmetic operation is <span class="formula">1 ⁄ 4</span> cycle on an SSE2 machine. To approach peak floating point bandwidth, we should perhaps expect to perform a few hundred arithmetic operations for every <tt>double</tt> loaded from DRAM memory.
</div>
<h? class="Subsubsection">
<b><u><span class="formula">4 × 200 × 12</span></u></b>
</h?>
<div class="Unindented">
The microkernel of section <a class="Reference" href="#subsec:proc-optim-microkernel">3.3.4↑</a> performs a <span class="formula">4 × 200 × 4</span> multiplication and reaches <span class="formula">3.48</span> flops per cycle, assuming all matrices to be in L1. The <span class="formula">4 × 200 × 12</span> multiplication function that follows is written under the assumption that <tt>a[]</tt> will need to be loaded from L3 cache. To pay for the cost of loading <tt>a[] </tt>from L3<tt>, </tt>it reuses<tt> </tt>a[] three times.<tt> </tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void mult4x200x12(double *a, double *b, double *c){
	asm4x200x4(a, b, c);
	asm4x200x4(a,b+800,c+16);
	asm4x200x4(a,b+1600,c+32);
}
</pre>
</div>

</div>
<div class="Indented">
In the <tt>mult4x200x12()</tt> function, the arrays <tt>a[]</tt>, <tt>b[]</tt>, and <tt>c[]</tt> are of lengths <span class="formula">800</span>, 2,400, and <span class="formula">48</span>, respectively. The L1 cache is big enough to hold all three of them. In the next step, this function is called repeatedly with the same <tt>b[]</tt> but with <tt>a[]</tt> and <tt>c[]</tt>, which change with iteration. Therefore, we assume that <tt>b[]</tt> is in L1 cache. 
</div>
<div class="Indented">
The function <tt>mult4x200x12()</tt> assumes <tt>a[]</tt> to be in column-major format with leading dimension equal to <span class="formula">4</span>---the same as in the microkernel. But <tt>b[]</tt> has a pretty strange format. The first four columns of the <span class="formula">200 × 12</span> matrix are stored in <tt>b[0..799]</tt> in row-major order, the next four columns are similarly stored in <tt>b[800..1599]</tt>, and the final four columns are in<tt> </tt>b[1600..2399]. The array <tt>c[]</tt> is in column-major format (except for skewing, which we are ignoring).
</div>
<div class="Indented">
How much is lost when <tt>a[]</tt> and <tt>c[]</tt> are loaded from L2 or L3 cache? Table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↑</a> shows that the floating point bandwidth drops from <span class="formula">3.48</span> to <span class="formula">3.32</span>. The following function <tt>run4x200x12()</tt> was used to figure out that answer. A number of points about timing functions such as <tt>mult4x200x12()</tt> are made in the discussion after the listing.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void run4x200x12(){
<span class="number-left">2</span>  __declspec(align(16)) double a[800*150];
<span class="number-left">3</span>  __declspec(align(16)) double b[2400];
<span class="number-left">4</span>  __declspec(align(16)) double c[48*150];
<span class="number-left">5</span>  TimeStamp clk;
<span class="number-left">6</span>  double cycles;
<span class="number-left">7</span>  for(int i=0; i &lt; 2400; i++)
<span class="number-left">8</span>    b[i] = rand()*1.0/RAND_MAX;
<span class="number-left">9</span>  for(int i=0; i &lt; 48*150; i++)
<span class="number-left">10</span>    c[i] = 0;
<span class="number-left">11</span>  for(int i=0; i &lt; 800*150; i++)
<span class="number-left">12</span>    a[i] = rand()*1.0/RAND_MAX;
<span class="number-left">13</span>  clk.tic();
<span class="number-left">14</span>  for(int i=0; i &lt; 1000*1000*10; i++)
<span class="number-left">15</span>    mult4x200x12(a+800*(i%150), b, c+48*(i%150));
<span class="number-left">16</span>  cycles = clk.toc();
<span class="number-left">17</span>  cycles = cycles/1e7;
<span class="number-left">18</span>  cout&lt;&lt;"flops per cycle = "&lt;&lt;2.0*16*200*3/cycles&lt;&lt;endl;
<span class="number-left">19</span>}
</pre>
</div>

</div>
<div class="Indented">
The arrays <tt>a[]</tt>, <tt>b[]</tt>, and <tt>c[]</tt> defined on lines 2, 3, and 4, respectively, are <span class="formula">16</span>-byte aligned. The <span class="formula">4 × 200 × 4</span> microkernel requires that its arguments be <span class="formula">16</span>-byte aligned, and we must be careful to respect that requirement. 
</div>
<div class="Indented">
The array <tt>a[]</tt> is too large to fit into L1 or L2 cache but fits comfortably into L3 cache. It is big enough to hold <span class="formula">150</span> matrices of size <span class="formula">4 × 200</span>. The array <tt>c[]</tt>, which holds <span class="formula">150</span> matrices of size <span class="formula">4 × 4</span>, fits into L2 cache but not L1.
</div>
<div class="Indented">
On lines 8 and 12, the arrays <tt>b[]</tt> and <tt>a[]</tt> are initialized with random numbers. Initializing with simple numbers such as <span class="formula">0</span> or <span class="formula">1</span> gives timing numbers that are misleading and not reproducible. 
</div>
<div class="Indented">
The <tt>mult4x200x12()</tt> function is called <span class="formula">10<sup>7</sup></span> times on line 15. The function calls cycle through the <span class="formula">150</span> possibilities for the first argument so that the function has to load <tt>a[]</tt> and <tt>c[]</tt> from L2 or L3 cache after each call. This pattern of function calls imitates the manner in which the function is called in the next stage.
</div>
<h? class="Subsubsection">
<b><u><span class="formula">600 × 200 × 12</span></u></b>
</h?>
<div class="Unindented">
The <tt>mult600x200x12()</tt> function requires scratch space of <span class="formula">600 × 12</span> to store <tt>c[]</tt>, which is then unpacked to the array <tt>C[]</tt> in column-major format with the right leading dimension.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void mult600x200x12(double *a, double *b, 
		    double *C, int ldC,
		    double *scratch){
	double *c = scratch;
	for(int i=0; i &lt; 7200; i++)
		c[i] = 0;
	for(int  i=0; i &lt; 150; i++)
		mult4x200x12(a+i*800, b, c+i*48);
	unpackC(c, C, ldC);
}
</pre>
</div>

</div>
<div class="Indented">
The array <tt>a[]</tt> stores a <span class="formula">600 × 200</span> matrix. The <span class="formula">600 × 200</span> matrix is thought of as <span class="formula">150</span> matrices of dimension <span class="formula">4 × 200</span> one below the other. These <span class="formula">150</span> submatrices must be lined up in <tt>a[]</tt>, with each submatrix stored in the column-major format required by <tt>mult4x200x12()</tt>.
</div>
<div class="Indented">
The array <tt>c[]</tt> stores a <span class="formula">600 × 12</span> matrix. The <span class="formula">600 × 12</span> matrix is thought of as <span class="formula">150</span> matrices of dimension <span class="formula">4 × 12</span> one below the other, and these submatrices are lined up in <tt>c[]</tt> with each submatrix in the format required by <tt>mult4x200x12()</tt>. 
</div>
<div class="Indented">
The function <tt>mult600x200x12()</tt> makes <span class="formula">150</span> calls to <tt>mult4x200x12()</tt>. At the end it unpacks <tt>c[]</tt>. The function <tt>unpackC()</tt> listed below converts <tt>c[]</tt> to the column-major format of <tt>C[]</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void unpackC(double *c, double *C, int ldC){
	for(int i=0; i &lt; 600; i+= 4)
		for(int ii=0; ii &lt; 12; ii++)
			for(int iii=0; iii&lt;4; iii++)
				C[i+iii+ii*ldC] += c[12*i+4*ii+iii];
}
</pre>
</div>

</div>
<div class="Indented">
Deciphering this triply nested loop is an exercise we omit. The main point is that the cost of unpacking, whatever it may be, is far less than the savings obtained in calling <tt>mult4x200x12()</tt> 150 times with <tt>c[]</tt> in a format that permits sequential access.
</div>
<h? class="Subsubsection">
<b><u><span class="formula">600 × 200 × 3000</span></u></b>
</h?>
<div class="Unindented">
The earlier function for <span class="formula">600 × 200 × 12</span> multiplication assumes <tt>a[]</tt> to be in packed format. The <span class="formula">600 × 200</span> entries of <tt>a[]</tt> store <span class="formula">150</span> submatrices of dimension <span class="formula">4 × 200</span>. Each submatrix occupies <span class="formula">800</span> contiguous entries and is in column-major order. The function <tt>packA()</tt> packs <tt>A[]</tt>, which is in column-major format, into this format. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void packA(double *A, int ldA, double *a){
	for(int j=0; j &lt; 200; j++)
		for(int i=0; i &lt; 150; i++)
			for(int ii=0; ii &lt; 4; ii++)
				a[i*800+ii+j*4] = A[4*i+ii+j*ldA];
}
</pre>
</div>

</div>
<div class="Indented">
Deciphering this triply nested loop is another exercise we omit.
</div>
<div class="Indented">
The function <tt>mult600x200x3000()</tt> reuses <tt>a[]</tt> as soon as it is packed by making <span class="formula">250</span> calls to the function at the previous stage in our design, thus more than making up for the cost of packing. It claims <span class="formula">600 × 200</span> entries of <tt>scratch[]</tt> to store the packed array.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void mult600x200x3000(double *A, int ldA,
		      double *b,
		      double *C, int ldC,
		      double *scratch){
	double *a = scratch;
	scratch += 600*200;
	packA(A, ldA, a);
	for(int i=0; i &lt; 250; i++)
		mult600x200x12(a, b+i*2400, C+i*12*ldC, ldC, 
			       scratch);
}
</pre>
</div>

</div>
<div class="Indented">
For every <tt>double</tt> entry in the packed array <tt>a[]</tt> of size <span class="formula">600 × 200</span>, the <span class="formula">600 × 200 × 3000</span> multiplication function performs 6,000 arithmetic operations. We may expect the cost of the arithmetic to greatly outweigh the cost of packing <tt>a[]</tt>. The multiplication function has floating point performance of <span class="formula">3.21</span> flops per cycle (see table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↑</a>). As expected, the decline from <span class="formula">3.22</span> flops per cycle yielded by the previous stage is marginal.
</div>
<h? class="Subsubsection">
<b><u><span class="formula">3000 × 200 × 3000</span></u></b>
</h?>
<div class="Unindented">
The earlier function <tt>mult600x200x3000()</tt> assumes that the array <tt>b[] </tt>stores a<tt> <span class="formula">200 × 3000</span> </tt>matrix in packed format.<tt> </tt>The matrix is thought of as <span class="formula">250</span> matrices of dimension <span class="formula">200 × 12</span> next to each other. Each of these <span class="formula">200 × 12</span> matrices is stored in the packed format required by the <span class="formula">4 × 200 × 12</span> multiplication function.
</div>
<div class="Indented">
The function <tt>packB()</tt> packs a submatrix of <tt>B[]</tt>, which is in column-major format, into <tt>b[]</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void packB(double *B, int ldB, double *b){
	for(int j=0; j &lt; 750; j++)
		for(int i=0; i &lt; 200; i++)
			for(int jj=0; jj &lt; 4; jj++){
				b[jj+i*4+j*800] = B[i+(4*j+jj)*ldB];
			}
}
</pre>
</div>

</div>
<div class="Indented">
Deciphering this triply nested loop is yet another exercise we omit. 
</div>
<div class="Indented">
The function <tt>mult3000x200x3000()</tt> resuses <tt>b[]</tt> five times as soon as it is packed, thus making up for the cost of packing. It claims <span class="formula">200 × 3000</span> entries from <tt>scratch[]</tt> for the packed array. The packed array fits comfortably in L3 cache.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void mult3000x200x3000(double *A, int ldA,
                       double *B, int ldB,
                       double *C, int ldC,
                       double *scratch){
  double *b = scratch;
  scratch += 200*3000;
  packB(B, ldB, b);
  for(int i=0; i &lt; 5; i++)
    mult600x200x3000(A+i*600, ldA, b, C+i*600, ldC, 
								scratch);
}
</pre>
</div>

</div>
<div class="Indented">
For each of the entries in the packed array, this function performs 6,000 arithmetic operations. Its floating point performance is <span class="formula">3.19</span> flops per cycle, a marginal decrease from that of the previous stage (see table <a class="Reference" href="#tab:matrix-mult-optimized-dimension-vs-bw">4.7↑</a>).
</div>
<h? class="Subsubsection">
<b><u>Block multiplication</u></b>
</h?>
<div class="Unindented">
The function <tt>blockmult()</tt> listed below carries out an <span class="formula">ℓ × <i>m</i> × <i>n</i></span> multiplication if <span class="formula">ℓ</span>, <span class="formula"><i>m</i></span>, and <span class="formula"><i>n</i></span> are multiples of 3,000, <span class="formula">200</span>, and 3,000, respectively.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void blockmult(double *A, double *B, double *C,
               int l, int m, int n,
               double *scratch){
  assert(l%3000==0);
  assert(m%200==0);
  assert(n%3000==0);
  int ldA = l;
  int ldB = m;
  int ldC = l;
  l = l/3000;
  m = m/200;
  n = n/3000;
  for(int i=0; i &lt; l; i++)
    for(int j=0; j &lt; m; j++)
      for(int k=0; k &lt; n; k++){
        double *AA = A + (i*3000)+(j*200)*ldA;
        double *BB = B + (j*200)+(k*3000)*ldB;
        double *CC = C + (i*3000)+(k*3000)*ldC;
        mult3000x200x3000(AA, ldA, BB, ldB, CC, ldC,
                          scratch);
      }
}
</pre>
</div>

</div>
<div class="Indented">
This function implicitly partitions <span class="formula"><i>A</i></span> into <span class="formula">3, 000 × 200</span> blocks, <span class="formula"><i>B</i></span> into <span class="formula">200 × 3, 000</span> blocks, and <span class="formula"><i>C</i></span> into <span class="formula">3, 000 × 3, 000</span> blocks. The function <tt>mult3000x200x3000()</tt>, defined at the previous stage, is used to multiply a single block of <span class="formula"><i>A</i></span> into a single block of <span class="formula"><i>B</i></span> to get a single block of <span class="formula"><i>C</i></span>. 
</div>
<div class="Indented">
The floating point bandwidth realized is <span class="formula">3.19</span> flops per cycle for the multiplication of square matrices of dimension 9,000 on a <span class="formula">2.6</span>  GHz SSE2 machine. For the same problem, the MKL library realizes more than <span class="formula">3.8</span> flops per cycle. Why is our design worse? Much of the blame falls on the microkernel. To begin with, the microkernel yields only <span class="formula">3.48</span> flops per cycle, assuming all its arguments are in L1 cache. On a <span class="formula">3.6</span>  GHz AVX2 machine, on which the microkernel yields <span class="formula">3.98</span> flops per cycle, the bandwidth realized for the multiplication of square matrices is <span class="formula">3.75</span> flops per cycle.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The function <tt>sum()</tt> of section <a class="Reference" href="#sub:memory-Bandwidth-to-DRAM">4.2.1↑</a> adds an array of numbers in sequence. Assuming the array to be several gigabytes long, explain why the number of cycles consumed by the function depends solely on memory access, with the additions not introducing any extra overhead.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Initialize a long array. Sum its entries with a stride that ensures only one entry is read from a page. What is the peak bandwidth to memory realized? That is the peak bandwidth to DRAM in the presence of TLB misses.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program to measure the bandwidth of writing to DRAM memory. Compare write bandwidth to read and copy bandwidths given in table <a class="Reference" href="#tab:memory-bandwidth">4.4↑</a>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why the copy bandwidth in table <a class="Reference" href="#tab:memory-bandwidth">4.4↑</a> is lower than the read bandwidth.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program that maps an <span class="formula"><i>n</i> × <i>n</i></span> array of numbers to another <span class="formula"><i>n</i> × <i>n</i></span> array, with each entry replaced by the average of north, south, east, and west entries. The arrays are assumed to be in column-major order. Compare the speed of a program that traverses the arrays columnwise with a program that traverses the arrays rowwise. Why do you expect columnwise traversal to be faster? Implement the same computation using blocking. Explain why blocking is likely to help make the program faster.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In recursive transpose,<span class="FootOuter"><span class="SupFootMarker"> [80] </span><span class="HoverFoot"><span class="SupFootMarker"> [80] </span><span class="bibcites">[<a class="bibliocite" name="cite-39" href="#biblio-39"><span class="bib-index">39</span></a>]</span>.</span></span> if a matrix <span class="formula"><i>A</i></span> to be transposed to a matrix <span class="formula"><i>B</i></span> has more rows than columns, the matrices are split as follows:<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>A</i><sub>1</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>A</i><sub>2</sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span> <span class="text"> and</span> <span class="symbol">(</span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>B</i><sub>1</sub>
</span>
<span class="arraycell align-c">
<i>B</i><sub>2</sub>
</span>

</span>
</span><span class="symbol">)</span>.
</div>
Recursion is used to transpose <span class="formula"><i>A</i><sub><i>i</i></sub></span> to <span class="formula"><i>B</i><sub><i>i</i></sub></span>. The case with more columns than rows is handled similarly. Write a program that implements this algorithm.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Rewrite the matrix multiplication programs of section <a class="Reference" href="#subsec:proc-compileropt-mmult">3.2.5↑</a> to use blocking. Compare program speeds with and without blocking.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Rewrite matrix multiplication using one level of blocking as in the previous exercise. But this time use scratch space to copy matrix subblocks with the blocks stored in scratch space having leading dimension equal to block dimension. The storage format of the blocks must be row-major or column-major to permit sequential access of entries. Time the program and compare to matrix multiplication with more straightforward blocking. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program to transpose a square matrix in place. Compare program speeds with and without blocking.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program that stores a sequence of double-precision numbers in a linked list. Write a program to sum all the numbers. Find the number of cycles per entry assuming that the linked list has more than <span class="formula">10<sup>9</sup></span> entries. Compare program speed in the following two situations: the entries of the linked list are in random locations in memory, and the entries of the linked list are next to each other. Explain what you observe.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  A balanced binary tree can be represented in two ways. The first way is to use an array with the convention that the two children of item <span class="formula"><i>i</i></span> are items <span class="formula">2<i>i</i> + 1</span> and <span class="formula">2<i>i</i> + 2</span>, with <span class="formula"><i>i</i> = 0</span> being the root. The second way is to use a <tt>struct</tt> with a pointer to the left child, the right child, and the parent. Write a program that replaces each entry of a node by the sum of all its descendants and itself. Compare program speed between the two implementations.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-4.3">4.3</a> Reading from and writing to disk<a class="Label" name="sec:memory-diskio"> </a>
</h2>
<div class="Unindented">
The data in registers and DRAM memory disappears when the computer is powered off. In contrast, hard disk storage is permanent. The hard disk is a collection of platters. Bits stored on circular tracks on either side of the platters are turned on and off using a magnetic field. More than 100 billion bits can be packed into a single square inch of a platter.
</div>
<div class="Indented">
File systems, implemented inside the operating system kernel, impose a logical structure on hard disk storage and facilitate its use. Files and directories are stored on the hard disk. In everyday usage, files are read from and written to with the understanding that the data is stored on a hard disk. Between the file as viewed inside a C/C++ program and the hard disk, there are several layers of software. These layers of software provide modularity, enabling the operating system kernel to handle several different file systems with a uniform interface and greatly improve speed of access.
</div>
<div class="Indented">
The Linux kernel implements a number of optimizations to speed up access to the disk. The most important optimization is to store a page cache. The page cache is a list of page frames that corresponds to data in the disk. When a file is read, Linux will service the read using the page cache if possible. A read or write file operation has to fall through a number of software layers before it reaches the disk. It begins as a system call. The <tt>read()</tt> and <tt>write()</tt> system calls are issued by the C library functions <tt>fread()</tt> and <tt>fwrite()</tt>. The C library function may do some buffering of its own. The kernel will service the system calls using a page cache if possible. If not, it invokes the file system to which the file belongs. There are software layers for combining, queuing, and scheduling requests to read or write to the hard disk. The request is finally issued using a device driver.<span class="FootOuter"><span class="SupFootMarker"> [81] </span><span class="HoverFoot"><span class="SupFootMarker"> [81] </span>When the file access falls through to the driver, the driver issues a DMA request and puts the calling process to sleep. The process is woken up by an interrupt handler when the request is complete. The Linux kernel will page cache the data before returning control to the user program.</span></span>
</div>
<div class="Indented">
The Linux command <tt>lspci -v</tt> may be used to find out the type of hard disk as well as the device driver that is in use. Although it is useful to understand that every disk access falls through layers of the file system within the operating system kernel, knowing specific details about the type of hard disk and the device driver is of little use in actual programming. There can be considerable variation in capacity as well as bandwidth of different hard disk systems, but that does not affect programming technique.
</div>
<div class="Indented">
The C versus C++ discussion in section <a class="Reference" href="#sub:memory-diskio-C-vs-C++">4.3.1↓</a> is on programming technique. The C++ language provides a convenient interface to file input/output using <tt>ifstream</tt> and <tt>ofstream</tt> objects. Although less convenient, the no-nonsense <tt>fread()</tt>, <tt>fwrite()</tt> interface in C can be much faster, by as much as a factor of <span class="formula">100</span>. In section <a class="Reference" href="#sub:memory-diskio-C-vs-C++">4.3.1↓</a>, we explain why there can be such a big difference in speed.
</div>
<div class="Indented">
In sections <a class="Reference" href="#sub:memory-diskio-latency">4.3.2↓</a> and <a class="Reference" href="#sub:memory-diskio-bw">4.3.3↓</a>, we investigate latency and bandwidth to hard disk. In both of these sections, the page cache maintained by the Linux kernel plays a big role. The page cache is a cache of the hard disk maintained in main memory. DRAM memory can be tens of GB in size, and the page cache can occupy a considerable portion of that memory. A lot of file input/output is serviced by the operating system kernel using the page cache. To get a real picture of latency and bandwidth to hard disk, one needs to get beyond the page cache, and that implies file sizes that are in the hundreds of GB.
</div>
<div class="Indented">
Latency to hard disk is of the order of milliseconds and can therefore be <span class="formula">10<sup>5</sup></span> times the latency to DRAM memory. Bandwidth to hard disk is of the order of hundreds of MB/s, which is only a thousandth of the bandwidth to DRAM on typical computers. However, hard disk capacity can be 100 or even 1,000 times DRAM capacity on typical computers. With respect to hard disk capacity, many supercomputing systems (and high-end workstations) are not well balanced. The need to provide a common file system across many computers for many users often implies that the hard disk capacity is not as high as it should be.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.3.1">4.3.1</a> C versus C++<a class="Label" name="sub:memory-diskio-C-vs-C++"> </a>
</h3>
<div class="Unindented">
With regard to the programming technique for reading and writing files, the simplest lesson is also the most valuable. The C interface can be much faster than the C++ interface as we show and for reasons we explain.
</div>
<div class="Indented">
The following functions use C++ <tt>ifstream</tt> and <tt>ofstream</tt> objects to read and write a <tt>double</tt> array <tt>v[]</tt> from or to a file of name <tt>fname</tt>.  
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void write_easy(double *v, long len, 
		const char *fname){
	ofstream ofile(fname);
	ofile&lt;&lt;scientific;
	ofile.precision(16);
	for(long i=0; i &lt; len; i++)
		ofile&lt;&lt;v[i]&lt;&lt;endl;
	ofile.close();
}
​
void read_easy(double *v, long len, const char *fname){
	ifstream ifile(fname);
	for(long i=0; i &lt; len; i++)
		ifile&gt;&gt;v[i];
}
</pre>
</div>

</div>
<div class="Indented">
The C interface below can be more than <span class="formula">100</span> times faster.  
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#include &lt;cstdio&gt;
<span class="number-left">2</span>void write_direct(double *v, long len, 
<span class="number-left">3</span>		  const char *fname){
<span class="number-left">4</span>	FILE *fptr;
<span class="number-left">5</span>	fptr = fopen(fname, "w");
<span class="number-left">6</span>	fwrite((void *)v, len, sizeof(double), fptr);
<span class="number-left">7</span>	fclose(fptr);
<span class="number-left">8</span>}
<span class="number-left">9</span>​
<span class="number-left">10</span>void read_direct(double *v, long len, 
<span class="number-left">11</span>		 const char *fname){
<span class="number-left">12</span>	FILE *fptr;
<span class="number-left">13</span>	fptr = fopen(fname, "r");
<span class="number-left">14</span>	fread((void *)v, len, sizeof(double), fptr);
<span class="number-left">15</span>	fclose(fptr);
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The <tt>FILE</tt> type as well as the functions <tt>fopen()</tt>, <tt>fwrite()</tt>, <tt>fread()</tt>, and <tt>fclose()</tt> are declared in the <tt>stdio.h</tt> header file included on line 1. On line 5, a file is opened for writing and on line 13 for reading.    
</div>
<div class="Indented">
The library function <tt>fwrite()</tt> (line 6) has a quite simple interface. Its first argument is a pointer to a location in memory. The second argument is the number of items to be written, and the third argument is the size of each item in bytes. The final argument is a pointer to a file. 
</div>
<div class="Indented">
The library function <tt>fread()</tt> (line 14) has an identical interface. It returns the number of items read, which may be less than the number requested if there is an error. Following our usual practice, we do not check for error conditions. Open files are closed using <tt>fclose()</tt> (lines 7 and 15).
</div>
<div class="Indented">
Why is the C++ interface more than 100 times slower? There are three reasons.
</div>
<div class="Indented">
First, while the C library functions <tt>fread()</tt> and <tt>fwrite()</tt> can read and write objects of any type or class, they do not waste a single byte of storage. Each <tt>double</tt> is <span class="formula">64</span> bits and gets stored as exactly <span class="formula">8</span> bytes. If file streams are used and each <tt>double</tt> is stored in ascii, as in the earlier program, each <tt>double</tt> uses <span class="formula">23</span> bytes (<span class="formula"> + 1</span> if the number is negative). 
</div>
<div class="Indented">
Second, the C++ interface incurs an overhead in converting every <tt>double</tt> from binary to ascii.
</div>
<div class="Indented">
Third, the C interface does the entire reading and writing using a single call to the C library. In contrast, the C++ interface reads or writes item by item, with the C++ <tt>fstream</tt> library being invoked for every item. The overhead of calling the library so frequently can build up. What is worse, the <tt>fstream</tt> library probably invokes the file system inside the operating system kernel quite frequently, although it presumably does some buffering. Every system call is like another function call, which can lead to many more function calls inside the file system. This overhead  can build up. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.3.2">4.3.2</a> Latency to disk<a class="Label" name="sub:memory-diskio-latency"> </a>
</h3>
<div class="Unindented">
The measurement of latency to hard disk brings up issues not unlike the ones encountered in measuring the latency to DRAM memory. Hard disk is so slow relative to memory that the operating system kernel goes to great lengths to cache file data in DRAM memory. The hard disk  maintains a cache and attempts to predict future accesses. The true latency to hard disk is not easily visible to simple computer programs. 
</div>
<div class="Indented">
The plan for measuring latency is to create a number of files and access several different positions in several different files to gather latency statistics. The plan does not work so easily. If <span class="formula">100</span> files each of <span class="formula">100</span> MB are used in the measurement, the latencies for a 1 TB hard disk come out quite low and of the order of microseconds, not milliseconds. That is because all of <span class="formula">10</span> GB can fit comfortably inside the page cache of a system with <span class="formula">16</span> GB of memory. 
</div>
<div class="Indented">
The size of page cache can run into several GB. The size of the page cache may be seen in <tt>/proc/meminfo</tt> against the item labeled &ldquo;Cached.&rdquo; If we write to a file of size <span class="formula">1</span> GB on a computer with several GB of memory, the entire file ends up in the page cache. To clear the page cache, one may use the GNU/Linux command <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">echo 1 &gt; /proc/sys/vm/drop_caches
</pre>
</div>

</div>
<div class="Indented">
One may look at <tt>/proc/meminfo</tt> after this command to verify that the page cache has indeed been cleared.
</div>
<div class="Indented">
The <tt>latency2disk()</tt> function defined below reads a single <tt>double</tt> from a given position in a given file and returns it. The idea is to use <span class="formula">100</span> files each of <span class="formula">1</span> GB to make the total file size in play of the order of <span class="formula">100</span> GB, and then read a double-precision number at a random location in a random file.<span class="FootOuter"><span class="SupFootMarker"> [82] </span><span class="HoverFoot"><span class="SupFootMarker"> [82] </span>On some computer systems, there is so much memory that even <span class="formula">100</span> GB may fit comfortably inside a page cache and be too small for latency measurement. Thus, the file count as well as the file size may need to be changed.</span></span> As a result, the actual measurement of latency makes a large number of calls to <tt>latency2disk()</tt> and the code for which is now shown. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>struct disk_latency{
<span class="number-left">2</span>	double fopen_cycles;
<span class="number-left">3</span>	double fseek_cycles;
<span class="number-left">4</span>	double fread_cycles;
<span class="number-left">5</span>	double fclose_cycles;
<span class="number-left">6</span>};
<span class="number-left">7</span>double latency2disk(const char *dir, int filenum, 
<span class="number-left">8</span>		    long posn,struct disk_latency&amp; lat){
<span class="number-left">9</span>	char fname[200];
<span class="number-left">10</span>	sprintf(fname, "%s/file%d.dat", dir, filenum);
<span class="number-left">11</span>	TimeStamp clk;
<span class="number-left">12</span>	FILE *fptr;
<span class="number-left">13</span>	clk.tic();
<span class="number-left">14</span>	fptr = fopen(fname,"r");
<span class="number-left">15</span>	lat.fopen_cycles = clk.toc();
<span class="number-left">16</span>	clk.tic();
<span class="number-left">17</span>	fseek(fptr,8l*posn,SEEK_SET);
<span class="number-left">18</span>	lat.fseek_cycles=clk.toc();
<span class="number-left">19</span>	double x;
<span class="number-left">20</span>	clk.tic();
<span class="number-left">21</span>	fread(&amp;x,sizeof(double),1,fptr);
<span class="number-left">22</span>	lat.fread_cycles=clk.toc();
<span class="number-left">23</span>	clk.tic();
<span class="number-left">24</span>	fclose(fptr);
<span class="number-left">25</span>	lat.fclose_cycles = clk.toc();
<span class="number-left">26</span>	return x;
<span class="number-left">27</span>}
</pre>
</div>

</div>
<div class="Indented">
The arguments to <tt>latency2disk()</tt> specify the directory and the file number (line 7). The file name is composed on line 10. The file is thought of as an array of <tt>double</tt>s, and <tt>posn</tt> (line 8) gives the position of the number to be retrieved in that array. 
</div>
<div class="Indented">
The function opens the file (line 14), seeks to the specified position (line 17), reads a <tt>double</tt> from the file (line 21), and closes the file (line 24). 
</div>
<div class="Indented">
Each of the four function calls is timed, and the number of cycles is saved in a <tt>struct </tt>(lines 1 to 6), which is passed by reference (line 8). The only new syntax here is <tt>fseek()</tt> for seeking to a new position in the file (line 17). 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:memory-diskio-latency">4.8↓</a> was obtained on two different computers, differing by 3 to 4 years in age. On both computers, opening and closing a file is fast, taking only a few microseconds. On both computers, the total latency is of the order of <span class="formula">10</span> milliseconds. Latency changes very little from computer to computer or with time.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-diskio-latency"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Computer
</td>
<td align="center" valign="top">
fsize
</td>
<td align="center" valign="top">
nfiles
</td>
<td align="center" valign="top">
open
</td>
<td align="center" valign="top">
seek
</td>
<td align="center" valign="top">
read
</td>
<td align="center" valign="top">
close
</td>

</tr>
<tr>
<td align="center" valign="top">
HP-Z220
</td>
<td align="center" valign="top">
1 GB
</td>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
.007
</td>
<td align="center" valign="top">
6.84
</td>
<td align="center" valign="top">
2.68
</td>
<td align="center" valign="top">
.010
</td>

</tr>
<tr>
<td align="center" valign="top">
ThinkStation-P300
</td>
<td align="center" valign="top">
1GB
</td>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
.001
</td>
<td align="center" valign="top">
9.29
</td>
<td align="center" valign="top">
.681
</td>
<td align="center" valign="top">
.002
</td>

</tr>

</table>
<div class="caption">
Table 4.8 Latency to hard disk in milliseconds. The measurements are made using <i>nfiles</i> each of size <i>fsize</i>. 
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
The total latency is limited by the seek time of the head assembly. Seek time of a hard disk refers to the time it takes for the head assembly to move between tracks of varying radii. This latency is of the order of milliseconds, and it takes of the order of <span class="formula">10</span> milliseconds for the hard disk platter to complete a rotation. Thus, it is clear that these are the parameters limiting latency to hard disk.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.3.3">4.3.3</a> Bandwidth to disk<a class="Label" name="sub:memory-diskio-bw"> </a>
</h3>
<div class="Unindented">
The functions <tt>write_direct()</tt> and <tt>read_direct()</tt> defined earlier in this section illustrated the use of <tt>fwrite()</tt> and <tt>fread()</tt>. These functions read or write a single array of <tt>double</tt>s. A single array cannot exceed the available DRAM, although it is desirable to work with files much larger than the size of DRAM memory when estimating bandwidth to disk. 
</div>
<div class="Indented">
The function <tt>write_direct()</tt> is modified below to write to files that are much bigger than available memory.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void write_direct(double *v, long len, 
		  const char *fname){
	FILE *fptr;
	fptr = fopen(fname, "w");
	fwrite((void *)v, len, sizeof(double), fptr);
	for(int i=1; i &lt; FLUSH_COUNT; i++)
		fwrite((void *)v, len, sizeof(double), 
		     fptr);
	fclose(fptr);
}
</pre>
</div>

</div>
<div class="Indented">
By setting <tt>FLUSH_COUNT</tt>, the array <tt>v[]</tt> is written to the same file multiple times. The purpose of using a large count is to flush the page cache. Linux provides the system call <tt>fsync()</tt> to flush the page cache, but we do not use it here. 
</div>
<div class="Indented">
As noted already, the page cache can run into several GB. If the system crashes before the dirty pages in the page cache are written back to disk, the file system will be left in an inconsistent state. Journaling file systems, such as ext4, record information about dirty pages on the disk. Journaling file systems can be restored to a consistent state much faster after a system crash. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:memory-diskio-bw">4.9↓</a> reports the read and write bandwidths for the same two computers as in table <a class="Reference" href="#tab:memory-diskio-latency">4.8↑</a>. The newer computer has nearly twice the bandwidth, although the latency is the same. Bandwidth to hard disk can be increased through greater storage density and parallelism. Bandwidth will increase with time in proportion to storage density and other parameters of the computer system.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:memory-diskio-bw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Device
</td>
<td align="center" valign="top">
fsize
</td>
<td align="center" valign="top">
Write
</td>
<td align="center" valign="top">
Read
</td>

</tr>
<tr>
<td align="center" valign="top">
HP-Z220
</td>
<td align="center" valign="top">
100GB
</td>
<td align="center" valign="top">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top">
<span class="formula">0.10</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
ThinkStation-P300
</td>
<td align="center" valign="top">
25 GB
</td>
<td align="center" valign="top">
<span class="formula">0.19</span>
</td>
<td align="center" valign="top">
<span class="formula">0.19</span>
</td>

</tr>

</table>
<div class="caption">
Table 4.9 Bandwidth to hard disk in GB/s. The bandwidths are measured by writing to and then reading from a single file of size <i>fsize</i>. 
</div>

</div>

</div>

</div>

</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program to determine the maximum size of the page cache on your system.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program to demonstrate that the page cache that comes into existence when one program accesses a file speeds up reading of the same file by any other program.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-4.4">4.4</a> Page tables and virtual memory<a class="Label" name="sec:memory-page-tables-virtual-memory"> </a>
</h2>
<div class="Unindented">
In this section, we make our first foray into the operating system kernel.<span class="FootOuter"><span class="SupFootMarker"> [83] </span><span class="HoverFoot"><span class="SupFootMarker"> [83] </span><span class="bibcites">[<a class="bibliocite" name="cite-44" href="#biblio-44"><span class="bib-index">44</span></a>]</span> is a superb account of the workings of the Linux kernel and the chief source for this section.</span></span> The cost of a single write to memory by an instruction such as <tt>movq %rax, (%rsi) </tt>brings in many layers of complexity. It could be a cache hit or a cache miss. If the DRAM memory is accessed, the cost of the access depends on preceding and following memory accesses. It depends on the manner in which the memory controllers operate the DRAM devices. It depends on the parallelism in the instruction stream. It depends on the pressure on dispatch ports in the instruction pipeline among other factors. On top of the layers of complexity engineered into hardware, the operating system introduces yet more complexity.
</div>
<div class="Indented">
The operating system is a process manager and has the responsibility of laying down and enforcing the rules that govern nearly all activity on the computer. It creates the environment under which all processes run. It goes about its job so surreptitiously that its presence is ignored in most programming.
</div>
<div class="Indented">
However, the activities of the operating system introduce a cost. The map from virtual memory addresses (generated by running programs) to physical DRAM memory is maintained by the operating system. A page is typically 4,096 bytes of virtual memory, as may be verified using the GNU/Linux command <tt>getconf PAGESIZE</tt>. The operating system creates page tables to map pages to page frames. A page frame is a 4,096-byte-long region of DRAM memory. As explained in section <a class="Reference" href="#sub:memory-virtual">4.1.3↑</a>, before accessing a word in DRAM memory, the processor uses page tables to convert a virtual address to a physical address. The operating system is invoked if there is a page fault or if the address is illegal. 
</div>
<div class="Indented">
Even ignoring page faults, having to look up the page tables every time a memory location is accessed introduces an overhead. At worst, as explained in section <a class="Reference" href="#sub:memory-virtual">4.1.3↑</a>, each memory access can turn into two or more memory accesses. The processor cores use a Translation Lookaside Buffer (TLB) to eliminate this overhead. Programs that transpose and multiply matrices must be mindful of the TLB, as evident from earlier sections. Managing the TLB is one of the kernel’s functions. 
</div>
<div class="Indented">
An account of program performance will be incomplete without discussion of the kernel’s activities. The kernel plays a big role in managing memory, creating processes and threads, and scheduling threads. The kernel mediates between the processes and the network when processes running on remote computers take on a task in parallel. 
</div>
<div class="Indented">
A knowledge of the kernel’s activities gives the programmer a better appreciation of the complex environment in which programs run. The occasional deterioration in performance will not seem so bizarre. The variation in performance from run to run will not seem inexplicable and erratic.
</div>
<div class="Indented">
There is a self-referential quality to the operating system. The memory management unit is part of the kernel and resides in the memory that it manages. The kernel associates a memory descriptor with each process or thread to encapsulate its usage of memory. The memory descriptors are stored in memory accessible only to the kernel. The page tables map virtual memory to physical memory. The page tables themselves reside in memory. The virtual addresses of the region of memory holding the page tables are mapped to physical addresses by those very page tables.
</div>
<div class="Indented">
Although it is a mere process manager, the operating system is far more complicated than almost any process or program. The operating system kernel is perhaps as complicated as any engineered system. Some of this complexity originates from the hardware environment the kernel manages. DRAM memory, network cards, hard disc, flash memory, graphics cards, monitors, and keyboards, along with many other parts of the computer, differ from each other as much as an airplane differs from a horse carriage. The kernel takes on the job of blending this cacophony of devices into a seamless computer system. Many of the parts, such as DRAM memory and the network, are quite complicated to operate. Some of the complexity arises from the need to guarantee a secure environment to processes and users. Some of the complexity is engineered. For example, the Linux kernel is monolithic. Every unit of the kernel and every device driver use the same virtual address space. Microkernel-based operating systems are more modular, but the coordination of and communication between microkernels incurs a penalty, which Linux is unwilling to pay. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:memory-virtual-user">4.4.1↓</a> uses a simple program to show how a user program as well as the data it uses is laid out in virtual memory. This information can be useful, not least when interpreting error messages. Section <a class="Reference" href="#sub:memory-virtual-kernel">4.4.2↓</a> continues that discussion and shows how the kernel establishes itself in physical memory and then in virtual memory. Section <a class="Reference" href="#sub:memory-virtual-kernel">4.4.2↓</a> gives an overview of the paging system implemented jointly by the operating system and the hardware. Although that topic may appear a little advanced, its relevance to programming is beyond debate. Demand paging plays a big role in multithreaded programming and networking. TLB flushes, necessitated during some context switches, can be a source of considerable overhead. In addition, having a full overview of the virtual memory system helps understand the manner in which multiple threads coexist and communicate using shared memory---an important point, inasmuch as multithreaded programming appears set to be the major paradigm for the next few decades. 
</div>
<div class="Indented">
It has been stated that &ldquo;memory management is by far the most complex activity&rdquo; in the Linux kernel.<span class="FootOuter"><span class="SupFootMarker"> [84] </span><span class="HoverFoot"><span class="SupFootMarker"> [84] </span><span class="bibcites">[<a class="bibliocite" name="cite-44" href="#biblio-44"><span class="bib-index">44</span></a>]</span>. </span></span> Our account of memory management will focus on the interaction between the hardware and  operating system kernel. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.4.1">4.4.1</a> Partitioning the virtual address space<a class="Label" name="sub:memory-virtual-user"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:page-tables-memory-protection-1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/vmem_view.png" alt="figure FIGS/chapter3/vmem_view.png" style="max-width: 312px; max-height: 237px;"/>

</div>
<div class="caption">
Figure 4.8 Schematic view of the virtual address space of a user process.
</div>

</div>

</div>

</div>
<div class="Indented">
Because a virtual address is <span class="formula">48</span> bits, a user process can in principle address <span class="formula">2<sup>48</sup></span> or more than 64,000 GB of memory. The physical memory can be less than a thousandth of that figure. Much of virtual address space is an unclaimed wilderness. Figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a> is an incomplete schematic view of the partitioning of the virtual address space of a typical user process. Much of the virtual address space is taken up by the unutilized regions shown as empty gaps.
</div>
<div class="Indented">
The boundary shown as <tt>PAGE_OFFSET</tt> separates user code and data from the kernel. User code and data map to virtual addresses lower than <tt>PAGE_OFFSET</tt>. The kernel code and data are beyond it. Every process uses exactly the same value for <tt>PAGE_OFFSET</tt>, which is defined as a constant by the kernel. Therefore, the kernel occupies exactly the same portion of virtual memory in every process. We will first look at the way the virtual memory addresses that precede <tt>PAGE_OFFSET</tt> are organized.
</div>
<div class="Indented">
A simple C program can help us understand the way functions, local variables, global variables, and dynamically allocated memory are mapped to virtual address space. The complete listing of such a program follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#include &lt;stdio.h&gt;
<span class="number-left">2</span>#include &lt;stdlib.h&gt;
<span class="number-left">3</span>int global;
<span class="number-left">4</span>void f(){
<span class="number-left">5</span>	double farray[512];
<span class="number-left">6</span>	printf(" farray = %p\n", farray);
<span class="number-left">7</span>}
<span class="number-left">8</span>int main(){
<span class="number-left">9</span>	double marray[512];
<span class="number-left">10</span>	printf(" marray = %p\n", marray);
<span class="number-left">11</span>	f();
<span class="number-left">12</span>	printf("\n");
<span class="number-left">13</span>	double *ptr;
<span class="number-left">14</span>	ptr = (double *)malloc(1000);
<span class="number-left">15</span>	printf("    ptr = %p\n", ptr);    
<span class="number-left">16</span>	printf("\n");
<span class="number-left">17</span>	printf("&amp;global = %p\n", &amp;global);
<span class="number-left">18</span>	printf("      f = %p\n", &amp;f);
<span class="number-left">19</span>	printf("   main = %p\n", &amp;main);
<span class="number-left">20</span>	free(ptr);
<span class="number-left">21</span>}
</pre>
</div>

</div>
<div class="Indented">
The program prints the pointer to <tt>global</tt>,  defined globally on line 3, the pointer to the function <tt>f()</tt> defined on line 4, the pointer <tt>farray[]</tt> defined locally within <tt>f()</tt> on line 5, the pointer to <tt>main()</tt>, the pointer <tt>marray[] </tt>defined locally within <tt>main()</tt> on line 9, and <tt>ptr</tt>, which points to memory allocated dynamically on line 14. The printing is in order from highest address to the least. When compiling this function, one must remember to use the <tt>-fno-inline-functions</tt> option. If not, the compiler will likely eliminate the call to <tt>f()</tt> on line 11.
</div>
<div class="Indented">
The following is output from a single run of this program.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"> marray = 0x7fffb6be1380
 farray = 0x7fffb6be0370
​
    ptr = 0xc60010
​
&amp;global = 0x604730
      f = 0x400ad0
   main = 0x400a00
</pre>
</div>

</div>
<div class="Indented">
Because <tt>marray[]</tt> is allocated on the stack, it has a high address as indicated in figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a>. The stack grows downward in virtual memory, and when a call is made to <tt>f()</tt>, the pointer <tt>farray</tt> has a lower value because of the downward growth of the stack. We will have more to say about stacks in the next chapter.
</div>
<div class="Indented">
The lowest addresses belong to the locations that store the set of instructions compiled from <tt>main()</tt> and <tt>f()</tt>. The global variable defined on line 3 is allocated in the data region. As indicated in figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a>, the global data map to higher addresses than code. 
</div>
<div class="Indented">
The dynamically allocated memory is in between. The principle way to allocate memory dynamically is using <tt>malloc()</tt> in C. In C++, we may also use <tt>new[]</tt>. In some of our earlier examples, we used <tt>_mm_malloc()</tt> to force alignment of allocated memory at <span class="formula">16</span>-byte boundaries, cache line boundaries, or page boundaries. 
</div>
<div class="Indented">
The sketch in figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a> has omitted a few things, namely, shared libraries and variables local to threads. Like dynamically allocated memory, these are also mapped to virtual addresses higher than code and data regions and lower than the stack region. 
</div>
<div class="Indented">
Kernel code and data are mapped to addresses beyond <tt>PAGE_OFFSET</tt>. We will look at that mapping shortly. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-4.4.2">4.4.2</a> Physical address space and page tables<a class="Label" name="sub:memory-virtual-kernel"> </a>
</h3>
<div class="Unindented">
Program speed is influenced by the paging system in several ways. Typically, 40% of the instructions are loads and stores, and each memory access involves the paging system. The TLB (translation look-aside buffer) is a shortcut past the huge overheads of the paging system. If a memory word is found in L1 cache and its virtual address is found in TLB, the latency of the memory access would be just <span class="formula">4</span> cycles. However, if there is a TLB miss, the latency can go up to several hundred cycles. If there is a page fault, the latency of a single memory access can go up to millions, even billions, of cycles. It is also true that the first access of a page in virtual memory always leads to a page fault, and the page fault handler is responsible for allocating a page frame in physical memory. 
</div>
<div class="Indented">
The paging system is responsible for protecting a program’s memory from other programs and preventing programs from making illegal memory accesses. During a context switch, when the kernel evicts one process and schedules another, the kernel may need to flush the TLB (using a privileged instruction) to protect the address space of the processes. The TLB flush can be expensive. We will look at how the kernel and hardware work together to implement the paging system to get a sense of the overheads of the paging system.
</div>
<h? class="Subsubsection">
<b><u>Physical address space</u></b>
</h?>
<div class="Unindented">
Soon after the computer is powered on, the kernel starts running in real mode. In real mode, the kernel generates physical addresses directly. Its first job is to load itself completely into physical memory. The kernel can call the BIOS using the <tt>int</tt> (interrupt)  instruction to find out about other devices in the system, read information from tables stored in specific areas of memory, or talk to the devices directly using special instructions. Any code that gets control of the computer in real mode can do almost anything it likes with the computer. 
</div>
<div class="Indented">
As shown in figure <a class="Reference" href="#fig:page-tables-memory-2">4.9↓</a>, the kernel is not allowed to use the lowest physical addresses for itself. That region of memory is used by BIOS and the hardware. The kernel loads itself in a low address region of physical memory. In addition to the physical memory used by initialized kernel data and code at startup, the kernel will need to dynamically allocate memory as more threads and processes are created. The memory claimed dynamically by the kernel is marked &ldquo;dynamic kernel data&rdquo; in figure <a class="Reference" href="#fig:page-tables-memory-2">4.9↓</a>. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:page-tables-memory-2"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/pmem_view.png" alt="figure FIGS/chapter3/pmem_view.png" style="max-width: 217px; max-height: 145px;"/>

</div>
<div class="caption">
Figure 4.9 Simplified sketch of the kernel in physical memory. 
</div>

</div>

</div>

</div>
<div class="Indented">
The virtual memory region marked &ldquo;linearly mapped kernel memory&rdquo; in figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a>, just beyond <tt>PAGE_OFFSET</tt>, is mapped to the chunk of physical memory the kernel reserves for its own exclusive usage. This mapping is the same for all processes and does not change as long as the computer is powered on. Furthermore, this mapping is linear. The virtual address is converted to a physical address by subtracting a constant.
</div>
<div class="Indented">
The areas marked &ldquo;kernel code,&rdquo; &ldquo;kernel data,&rdquo; and &ldquo;dynamic kernel data&rdquo; in the schematic sketch of figure <a class="Reference" href="#fig:page-tables-memory-2">4.9↑</a> hold page frames that are linearly mapped. The linearly mapped page frames extend monotonically in physical address space (except for memory regions reserved for BIOS, hardware, and other purposes). When the kernel attempts to claim a page frame dynamically and extend the linearly mapped region of address space, it may come into conflict with a user process that has already claimed the page frame the kernel needs. Resolving this conflict will be expensive as the kernel has to map the user’s page to some other page frame or swap it to disc before claiming the page frame for itself. The kernel keeps a few free page frames handy so that it can service urgent requests quickly.
</div>
<div class="Indented">
After the kernel has established itself in memory and created some of the data structures, such as an initial set of page tables, it will enter &ldquo;protected&rdquo; mode by loading a specific value into the <tt>cr0</tt> register. The paging system is turned on in protected mode, and all references to memory are now interpreted as virtual addresses. The virtual addresses are converted to physical addresses by the paging system.
</div>
<div class="Indented">
When the kernel needs more memory for its own use, it can get the memory in three different ways. The first method is to request more pages using <tt>alloc_pages()</tt> or another equivalent facility. The pages allocated in this way will be in the linearly mapped region just beyond <tt>PAGE_OFFSET</tt> in virtual memory.<i> </i>The kernel associates a kernel stack with each process that allows it to execute system calls and handle interrupts. The kernel stack is typically two pages or 8,096 bytes and it is allocated using <tt>alloc_pages()</tt>. The memory claimed by <tt>alloc_pages()</tt> is linearly mapped.
</div>
<div class="Indented">
The second method the kernel uses for claiming memory is <tt>kmalloc()</tt>. The memory claimed using <tt>kmalloc()</tt> is also linearly mapped. Many a time, the kernel needs memory in much smaller units than a page, for example, when creating process descriptors, memory region descriptors, or page descriptors. The kernel uses <tt>kmalloc()</tt> in such circumstances.
</div>
<div class="Indented">
A third method is used for loading modules and related activities. In Linux, device drivers are loaded as modules. The kernel programmer has no way of estimating in advance how many devices there could be on the system or how big the device drivers could be in size. Some of the network card and graphics card drivers can be quite complicated. The kernel claims space for such modules and their activities using <tt>vmalloc()</tt>. The page frames (physical memory) claimed by <tt>vmalloc()</tt> come from the general pool, and the virtual addresses are in the &ldquo;vmalloc&rdquo; area shown in figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a>. The map to page frames is not linear. The page tables must be changed after calling <tt>vmalloc()</tt>.
</div>
<div class="Indented">
The monolithic Linux kernel uses the same map to physical memory for every virtual address beyond <tt>PAGE_OFFSET</tt>. Every kernel function and module uses the same virtual address space. 
</div>
<h? class="Subsubsection">
<b><u>Page tables</u></b>
</h?>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:page-tables-memory-3"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/virtual2linear.png" alt="figure FIGS/chapter3/virtual2linear.png" style="max-width: 309px; max-height: 177px;"/>

</div>
<div class="caption">
Figure 4.10 Conversion of a virtual address/offset to a linear address. Here <tt>cs</tt> (code segment) and <tt>gdtr</tt> are system registers, and <tt>GDT</tt> is the global descriptor table. The code segment register also has information about the privilege level of the process. 
</div>

</div>

</div>
Figure <a class="Reference" href="#fig:page-tables-memory-3">4.10↑</a> shows how the linear addresses are generated. In Linux, linear and virtual addresses are practically identical, and the conversion of a virtual address to a linear address is heavily optimized so as to be essentially cost-free. We do not distinguish between the two.
</div>
<div class="Indented">
 The global descriptor table of figure <a class="Reference" href="#fig:page-tables-memory-3">4.10↑</a> is set up by the operating system kernel. It comes into play whenever the processor switches from a user program to the operating system kernel. A process may issue a system call using either the <tt>int 0x80</tt> or <tt>sysenter</tt> instructions. Any external device can cause an interrupt. Internal events such as page faults trigger exceptions. When any of these events happen, the processor uses the <tt>%tr</tt> task register (also set up by the kernel) to index into the global descriptor table and locate the address of the kernel stack. The hardware switches <tt>%rsp</tt> to the kernel stack before handing control to the kernel.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:page-tables-memory-4"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter3/virtual2physical.png" alt="figure FIGS/chapter3/virtual2physical.png" style="max-width: 290px; max-height: 181px;"/>

</div>
<div class="caption">
Figure 4.11 Page table look up using a virtual address. The <tt>cr3</tt> register holds a pointer to the base of the PGD table.
</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:page-tables-memory-4">4.11↑</a> shows the way page tables are organized. It is the kernel’s job to set up the PGD, PUD, PMD, and page table shown in that figure for each process. The sheer complexity of storing a multilevel table in memory shows why a TLB that bypasses page table lookup is essential. 
</div>
<div class="Indented">
Every process has its own page tables. When a process is created, the operating system has to set up page tables for it. The page tables of each process map valid virtual addresses below as well as above <tt>PAGE_OFFSET</tt> to physical memory. When the process makes a system call or if the kernel takes over to handle an interrupt or exception, it uses the page tables of the preempted process. It is illegal for a user process to generate a virtual address above <tt>PAGE_OFFSET</tt>. However, the kernel generates such addresses and uses the page tables of the user process to map them to physical memory.
</div>
<div class="Indented">
The kernel has a reference set of page tables mapping virtual addresses above <tt>PAGE_OFFSET</tt>. When a process is created, that information is incorporated into the process’s page tables.
</div>
<div class="Indented">
If a process defines an array locally or if it claims memory dynamically, it is allocated a chunk of memory in the virtual address space. Thus, the extent of the virtual address space a process may legally address changes as it runs. The virtual addresses are not mapped to page frames in physical memory as soon as they are allocated. The kernel waits until the first memory reference, which results in page fault because no page frame has been assigned. The page fault handler finds a free page frame, assigns it to the page, and updates the page tables. This is known as demand paging.
</div>
<div class="Indented">
Demand paging has its uses. For example, when several threads are using the same virtual address space, the kernel or, more specifically, the page fault handler can map a page to the memory channel that is as close as possible to the processor core that generates the first access. This &ldquo;first touch policy&rdquo; can be exploited in a program to improve bandwidth to memory, as we show in the next chapter.
</div>
<div class="Indented">
A consequence of demand paging is that a set of contiguous pages in virtual memory can be mapped to page frames scattered all over physical memory. Neighboring cache lines in virtual memory may be quite far away from each other in physical address space if the cache lines cross a page boundary.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why all processes cannot use the same page tables for virtual addresses above <tt>PAGE_OFFSET</tt>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why certain parts of the kernel should never be swapped out to hard disc.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Is <tt>%cr3</tt> (see figure <a class="Reference" href="#fig:page-tables-memory-4">4.11↑</a>) the virtual or physical address of the base of the global page directory? 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-4.5">4.5</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-35"><span class="bib-index">35</span></a>] </span> <span class="bib-authors">B. Jacob, S. Ng, D. Wang</span>: <i><span class="bib-title">Memory Systems: Cache, DRAM, Disk</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">2008</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-36"><span class="bib-index">36</span></a>] </span> <span class="bib-authors">D. Molka, D. Hackenberg, R. Shöne, M.S. Müller</span>: “<span class="bib-title">Memory performance and cache coherency effects on an Intel Nehalem multiprocessor system</span>”, <i><span class="bib-journal">18th International Conference on Parallel Architectures and Compilation Techniques</span></i>, pp. <span class="bib-pages">261-270</span>, <span class="bib-year">2009</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-37"><span class="bib-index">37</span></a>] </span> <span class="bib-authors">D.P. Bovet, M. Cesati</span>: <i><span class="bib-title">Understanding the Linux Kernel</span></i>. <span class="bib-publisher">O'Reilly</span>, <span class="bib-year">2005</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-38"><span class="bib-index">38</span></a>] </span> <span class="bib-authors">J.L. Hennessy, D.A. Patterson</span>: <i><span class="bib-title">Computer Architecture: A Quantitative Approach</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">1990-2011</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-39"><span class="bib-index">39</span></a>] </span> <span class="bib-authors">M. Frigo, C.E. Leiserson, H. Prokop, S. Ramachandran</span>: “<span class="bib-title">Cache oblivious algorithms</span>”, <i><span class="bib-journal">Foundations of Computer Science, 40th Annual Symposium</span></i>, pp. <span class="bib-pages">285-297</span>, <span class="bib-year">1999</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-40"><span class="bib-index">40</span></a>] </span> <span class="bib-authors">S. Goto, R. A. van de Geijn</span>: “<span class="bib-title">Anatomy of high performance matrix multiplication</span>”, <i><span class="bib-journal">ACM TOMS</span></i>, pp. <span class="bib-pages">art:12</span>, <span class="bib-year">2008</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-41"><span class="bib-index">41</span></a>] </span> <span class="bib-authors">V. Bubka, P. Tuma</span>: <i><span class="bib-title">Investigating cache parameters of x86 processors</span></i> in <i><span class="bib-booktitle">SPEC Benchmark Workshop 2009</span></i> (<span class="bib-editor">D. Kaeli and K. Sachs</span>, ed.). <span class="bib-publisher">Springer-Verlag</span>, <span class="bib-year">2009</span>.
</p>

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-5">5</a> Threads and Shared Memory<a class="Label" name="chap:Threads-and-shared"> </a>
</h1>
<div class="Unindented">
Programming with threads is a paradigm of great range and utility that encompasses everything from cell phones to web servers to supercomputers. The processor cores of today are clocked at around the same rate as the processor cores of 2005. However, the number of processor cores on the same package continues to increase dramatically. In addition, multiple processor packages can be connected using a fast interconnect. All the processors in the interconnected processor packages share the same DRAM memory. Nodes with multiple processor cores are so powerful that problems with a billion grid points can be tackled on a single node. Multithreaded programming is set to remain a leading programming paradigm.
</div>
<div class="Indented">
This chapter is organized into four sections. Section <a class="Reference" href="#sec:threads-omp-intro">5.1↓</a> is an introduction to OpenMP. OpenMP is an industry standard that enables programming with threads. It is implemented by <tt>icpc</tt>, <tt>gcc</tt>, <tt>pgcc</tt>,<tt> </tt>and other compilers. One only needs to add an option such as<tt> </tt>-<tt>openmp</tt> and <tt>-fopenmp</tt> during compilation and linking if OpenMP syntax is used. The syntax is so simple that the transition to threaded programming is easy and barely noticeable. All of OpenMP syntax used much of the time can be made to fit into half a page or less.
</div>
<div class="Indented">
OpenMP programs are written as if they are sequential. To begin with, there is a single thread of execution. Whenever the program enters a parallel region, multiple threads come alive. Because memory is shared between threads, the different threads can split the work between them. They can either operate on the same locations in memory or split the memory between themselves. 
</div>
<div class="Indented">
Although the OpenMP syntax is simple, sharing memory between threads has many pitfalls that can trip up the novice as well as the experienced programmer. Suppose we define a variable <tt>var</tt> to be an <tt>int</tt>. If this is a shared variable and one of the threads does <tt>var+=1</tt>, it is natural to assume that the update in the value of <tt>var</tt> is seen by all the threads. That assumption can be wrong, however. The variable <tt>var</tt> may be stored in a register, and the update on one thread may <i>never</i> be propagated to the other threads. 
</div>
<div class="Indented">
The memory issues that arise in threaded programming are exceedingly intricate, subtle, and deceptive. Even the simplest OpenMP program that uses multiple threads to add a list of numbers relies on the OpenMP memory model in ways that are not always appreciated. In section <a class="Reference" href="#sec:threads-omp-intro">5.1↓</a>, we explain OpenMP’s memory model thoroughly. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:threads-optimizing-omp">5.2↓</a> introduces techniques to optimize OpenMP programs. Most of these techniques are minor modifications of techniques for memory access already seen in the last chapter. There is only one substantially new point that arises. Although all processor cores on a computing node have access to all the memory (DRAM) on that node, some of the memory is closer to some processor cores and far away from others. As may be expected, accesses of near memory are faster. It is important to make sure that memory mainly accessed by a certain processor core is close to it.
</div>
<div class="Indented">
The nonuniformity of access between near and far memory can be dealt with during memory initialization, as explained in section <a class="Reference" href="#sec:threads-optimizing-omp">5.2↓</a>. However, it is a definite negative as far as program modularity is concerned. Program modularity is promoted by uniformity of access, where the cost of memory access is independent of which processor is accessing which region of memory. If memory access is uniform, the subdivision of a task between multiple processors can stay closer to logic intrinsic to the problem domain. Nonuniform memory access burdens memory initialization with an interpretation that is completely extrinsic to the problem domain and dependent only on the conveniences and constraints of hardware design.
</div>
<div class="Indented">
The unwary are apt to assume that a computer with <span class="formula">12</span> cores is <span class="formula">12</span> times faster than a computer with one core. Bandwidth to memory does not increase linearly with the number of processor cores, and most significant programs are limited by bandwidth to memory, as explained in section <a class="Reference" href="#sec:threads-optimizing-omp">5.2↓</a>.
</div>
<div class="Indented">
Although OpenMP is of much utility in scientific computing, it is a limited framework. It applies mainly in those situations where the data layout is static and access patterns are regular. In these situations, one can think of a parallel program as a sequential program that splits naturally between threads every time a parallel region is entered. This model is totally inapplicable to web servers, for example, where every thread does its own thing, according to the demands placed on it by its client, and interacts with other threads in complex ways. 
</div>
<div class="Indented">
When scientific computing was dominated by classical physics, the types of problems that arose fit reasonably well into the OpenMP framework. However, newer and growing areas of scientific computing, such as genomics and data science, have some of the qualities of computer science applications. Problems in such areas may not be as complex and dynamic as web servers or Internet applications, but a trend in that direction is undeniable. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sec:threads-pthreads">5.3↓</a>, we look at Pthreads, which are a far more fundamental way to program with threads than OpenMP. Pthreads can be used for everything from web servers to scientific applications. In fact, Pthreads help us understand OpenMP better. 
</div>
<div class="Indented">
The treatment of Pthreads in section <a class="Reference" href="#sec:threads-pthreads">5.3↓</a> exhibits interactions between threads and computer architecture. Cache coherence is essential for threaded programming. However, if several threads share the same cache line, there can be considerable overhead in propagating writes from one cache to another. TLB flushes can incur overheads when threads of different applications share the same core. In addition, section <a class="Reference" href="#sec:threads-pthreads">5.3↓</a> explores the overhead of thread creation and briefly introduces memory fences.
</div>
<div class="Indented">
Much of the complexity of threaded programming is the complexity of sharing memory between concurrent threads. In section <a class="Reference" href="#sec:threads-program-memory">5.4↓</a>, we take a look at the organization of program memory into stacks and heaps. We explain how recursion works. Segmentation faults are the bane of C/C++ programming, and dealing with them occupies much of the programmer’s time. In section <a class="Reference" href="#sec:threads-program-memory">5.4↓</a>, we make another excursion into the Linux kernel and explain exactly how these segmentation faults are triggered.
</div>
<div class="Indented">
As in the previous chapters, we run programs on SSE2, AVX, and AVX2 platforms (see tables <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a> and <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). As far as what we say about program optimization is concerned, there is no great difference between any of these machines, or with AVX-512 machines that are set to appear in the future.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-5.1">5.1</a> Introduction to OpenMP<a class="Label" name="sec:threads-omp-intro"> </a>
</h2>
<div class="Unindented">
Section <a class="Reference" href="#sub:threads-omp-syntax">5.1.1↓</a> introduces OpenMP.<span class="FootOuter"><span class="SupFootMarker"> [85] </span><span class="HoverFoot"><span class="SupFootMarker"> [85] </span>The website <i>www.openmp.org</i> has a document titled &ldquo;OpenMP Application Program Interface&rdquo; describing the OpenMP standard. <span class="bibcites">[<a class="bibliocite" name="cite-42" href="#biblio-42"><span class="bib-index">42</span></a>]</span> is an easily paced introduction to OpenMP.</span></span> Nearly all the syntax that most programmers will ever need is brought out by parallelizing the summation of the Leibniz series in two different ways.
</div>
<div class="Indented">
OpenMP programs look much like sequential programs, and the syntax is easy to learn. The simplicity is mostly illusory, however. Whenever concurrent programs share memory, as OpenMP programs do, the programming model inevitably becomes very subtle. OpenMP syntax conceals much of this subtlety behind a sophisticated memory model. Every OpenMP programmer would be well advised to understand this memory model. The correctness of even the simplest OpenMP program relies on it. The memory model is discussed in section <a class="Reference" href="#sub:threads-omp-memory-model">5.1.2↓</a>.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:threads-omp-overheads">5.1.3↓</a> is about the overheads associated with OpenMP parallel regions and other constructs. When a task is divided between threads using OpenMP, the division itself will incur overhead. If this overhead is too great relative to the size of the task, the task may not be worth parallelizing. OpenMP parallelism works best at a coarse level and for outer regions of the program. One may think of a program as being essentially sequential and split certain tasks within that sequential flow between threads using OpenMP constructs. Where and whether such a split makes sense or not is entirely determined by OpenMP overheads.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:openmp-x86-spinlock">5.1.3↓</a>, we describe the implementation of mutual exclusion on x86 machines. Mutual exclusion is so fundamental to threaded programming that the x86 instruction set has supported it for many decades. Much of the overhead of OpenMP constructs is incurred through instructions related to mutual exclusion.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.1.1">5.1.1</a> OpenMP syntax<a class="Label" name="sub:threads-omp-syntax"> </a>
</h3>
<div class="Unindented">
OpenMP syntax is beguilingly simple. A simple function for computing the <span class="formula"><i>n</i></span>th partial sum of the Leibniz series follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double leibniz(long int n){
	long int i;
	double ans=4.0;
	for(i=1; i &lt; n; i=i+2){
		ans -= 4.0/(2.0*i+1);
		ans += 4.0/(2.0*i+3);
	}
	return ans;
}
</pre>
</div>

</div>
<div class="Indented">
As it is written, this function runs on a single processor core. We will rewrite it to run on multiple cores using OpenMP. By writing the OpenMP program in different ways, we expose much of the basic syntax of OpenMP. There is a lot more syntax to OpenMP than we will discuss here, but much of it is hardly ever needed. OpenMP constructs are embedded into C or C++ code. The GNU, Intel, and PGI compilers support OpenMP. With <tt>icpc</tt>, the option <tt>-openmp</tt> must be used during compilation and linking. For <tt>gcc/g++</tt>, the corresponding option is <tt>-fopenmp</tt>. 
</div>
<div class="Indented">
A single important OpenMP construct does not arise (explicitly) in the two ways of summing the Leibniz series we explore. This is the 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#pragma omp barrier
</pre>
</div>

</div>
<div class="Indented">
construct. OpenMP allows the programmer to embed statements such as this in C or C++ programs. In such constructs, <tt>omp</tt> is an abbreviation of OpenMP and <tt>#pragma</tt> is a compiler directive. 
</div>
<div class="Indented">
If a running thread encounters a <tt>barrier</tt>, it must stop until all other threads in the same team have also arrived at the same barrier. The threads get in sync at a <tt>barrier</tt> construct. If the program is parallelized in phases, and each phase must wait for the preceding phase to complete before it begins, we must use <tt>barrier</tt> constructs in between phases. For example, if one phase of the program assembles a stiffness matrix in parallel and another phase solves the stiffness matrix, the two phases must be separated by a <tt>barrier</tt> construct in principle. In practice, however, there are implicit barriers  in a number of OpenMP constructs such as <tt>parallel for</tt> and <tt>parallel</tt>, as we will see, and only rarely is the <tt>barrier</tt> construct needed. 
</div>
<h? class="Subsubsection">
<b><u>omp parallel for</u></b>
</h?>
<div class="Unindented">
The simplest way to sum the Leibniz series in OpenMP is to use the <tt>parallel for </tt>directive. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	double ans=0;
<span class="number-left">2</span>#pragma omp parallel for				\
<span class="number-left">3</span>	reduction(+:ans)
<span class="number-left">4</span>	for(long int i=0; i &lt; n; i = i+2)
<span class="number-left">5</span>		{
<span class="number-left">6</span>			ans += 4.0/(2*i+1);
<span class="number-left">7</span>			ans -= 4.0/(2*i+3);
<span class="number-left">8</span>		}
</pre>
</div>

</div>
<div class="Indented">
Except for lines 2 and 3, this could be a sequential program to sum the Leibniz series. Line 2 is the <tt>parallel for</tt> construct, which specifies that the for-block extending from lines 4 through 8 must be parallelized. When the program encounters this statement, it splits into multiple threads. Each thread will execute a chunk of the iterations of the for-loop.
</div>
<div class="Indented">
The <tt>reduction</tt> clause on line 3 is of much importance. It states that <tt>ans</tt> is a reduction variable that is tagged <span class="formula"> + </span>. The compiler understands that each thread should compute its own <tt>ans</tt>, and the result from all the threads should be reduced into a single <tt>ans</tt> using addition.
</div>
<div class="Indented">
An attractive feature of OpenMP is that parallel programs can be written using syntax that is  close to that of sequential programs. The programmer is asked to make only minimal adjustments to write parallel programs, although these adjustments can be quite subtle and their simplicity a major pitfall. 
</div>
<div class="Indented">
Not every for-loop can be parallelized using the OpenMP <tt>parallel for</tt> construct. The compiler has to be able to look at the for-loop and generate the code for the for-loop that is to be executed by each thread. The for-loops that can be parallelized are said to be in canonical form. The complete definition of for-loops in canonical form is too complicated to be useful for anyone except compiler writers. It is usually possible to tell whether a for-loop is in canonical form by looking. 
</div>
<div class="Indented">
Even when a for-loop is in canonical form, one has to make sure that there are no dependencies between successive iterations of the loop. If the later iterations depend on the execution of an earlier iteration, the loop cannot be parallelized correctly. An example of a for-loop with a dependency across iterations is the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">factorial[0] = 1;
for(int i=1; i &lt; n; i++)
  factorial[i] = i*factorial[i-1];
</pre>
</div>

</div>
<div class="Indented">
This loop is in canonical form. If the OpenMP <tt>for</tt> construct is used, the compiler will be able to break it up syntactically into a separate loop for every thread. However, the result will be incorrect. To respect the loop-carried RAW dependency, every iteration has to wait for the previous iteration to be complete. That does not happen when the for-loop is split and the threads start executing their share of the iterations in parallel. Applying the <tt>parallel for</tt> construct to this for-loop results in an incorrect program.
</div>
<div class="Indented">
There is an implicit barrier at the end of the OpenMP <tt>parallel for</tt> construct. Due to the implicit barrier, we may think of the for-loop as part of a sequential program that is executed in parallel by multiple threads.
</div>
<div class="Indented">
In much of OpenMP programming, one does not need to go beyond <tt>parallel for</tt>. However, the <tt>parallel for</tt> as shown makes a number of implicit assumptions. Those assumptions are made explicit below using less concise syntax:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double parallelfor(long int n, int nthreads){
<span class="number-left">2</span>	assert(nthreads%2==0);
<span class="number-left">3</span>	double ans=0;
<span class="number-left">4</span>#pragma omp parallel for				\
<span class="number-left">5</span>	num_threads(nthreads)			\
<span class="number-left">6</span>	schedule(static)				\
<span class="number-left">7</span>	default(none)					\
<span class="number-left">8</span>	shared(n)						\
<span class="number-left">9</span>	reduction(+:ans)
<span class="number-left">10</span>	for(long int i=0; i &lt; n; i = i+2)
<span class="number-left">11</span>		{
<span class="number-left">12</span>			ans += 4.0/(2*i+1);
<span class="number-left">13</span>			ans -= 4.0/(2*i+3);
<span class="number-left">14</span>		}
<span class="number-left">15</span>	return ans;
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The <tt>parallel for</tt> on line 4 and the <tt>reduction</tt> clause on line 9 are the same as before. The <tt>parallel for</tt> construct (or compiler directive) applies to the entire for-loop from lines 10 through 14.
</div>
<div class="Indented">
The <tt>num_threads()</tt> clause on line 5 specifies the number of threads to be created. Here we have required <tt>nthreads</tt>, which is an argument to <tt>parallelfor()</tt>, to be even.
</div>
<div class="Indented">
If the <tt>num_threads</tt> clause is omitted, by default, the number of threads is usually the number of processor cores. It may be set to some other value using the shell variable <tt>OMP_NUM_THREADS</tt>. Relying on the default behavior is simpler and better most of the time.
</div>
<div class="Indented">
The <tt>schedule()</tt> clause on line 6 tells the compiler how to split the for-loop across threads. The scheduling option is given as <tt>static</tt>. Static scheduling, which we describe presently, is the most useful in practice and the default. Assuming <span class="formula"><i>n</i></span> to be even, the loop variable <tt><span class="formula"><i>i</i></span></tt> of the for-loop on line 10 steps through the list of values <div class="formula">
0, 2, …, <i>n</i> − 2.
</div>
Because the schedule is given as <tt>static</tt> on line 6, each thread will get a contiguous set of iterations of roughly the same size. For example, if there are <span class="formula"><i>N</i></span> threads, each thread gets approximately <span class="formula"><i>n</i> ⁄ <i>N</i></span> iterations, and the iterations of thread number <span class="formula"><i>t</i></span> extend from <span class="formula"><i>i</i> ≈ <i>t</i> × <i>n</i> ⁄ <i>N</i></span> to <span class="formula"><i>i</i> ≈ (<i>t</i> + 1) × <i>n</i> ⁄ <i>N</i></span>. Other schedules are possible. The iterations can be assigned in a round robin fashion or dynamically. 
</div>
<div class="Indented">
Variables declared within the parallel block are private to each thread. Therefore, the variable <tt>i</tt>, which is declared on line 10, is unique to each thread. 
</div>
<div class="Indented">
The question arises whether variable names such as <tt>ans</tt> and <tt>n</tt> that occur within the parallel block, but are declared and defined outside the block, are names for the same location in memory. Such variables may be shared or unused (or even private---but it is hard to think of a meaningful use of this feature) in the parallel block. One can make all the variables defined outside the parallel block into shared variables using the clause <tt>default(shared)</tt>. In this example, the clause <tt>default(none)</tt> on line 7 is used to say that the variables declared outside are not visible inside the parallel block by default. The clause on line 8 declares that the variable <tt>n</tt> is visible inside the block and shared by all the threads. The variable <tt>ans</tt> is not marked as shared because it is a reduction variable.
</div>
<h? class="Subsubsection">
<b><u><tt>omp critical</tt></u></b>
</h?>
<div class="Unindented">
The <tt>omp critical </tt>construct is similar to <tt>omp barrier</tt> in the following way. Both are rarely used constructs yet important to understand. The <tt>barrier</tt> construct is important to understand because it occurs implicitly at the end of parallel regions, enabling us to think of OpenMP programs as sequential programs with parallel regions. Critical regions are employed implicitly whenever reduction variables are used and the <tt>critical</tt> construct makes critical regions explicit.
</div>
<div class="Indented">
The <tt>ompfor()</tt> function listed below introduces the eponymous <tt>omp for</tt> construct and the <tt>omp parallel</tt> directive in addition to <tt>omp critical</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double ompfor(long int n){
<span class="number-left">2</span>	double ans=0;
<span class="number-left">3</span>#pragma omp parallel				\
<span class="number-left">4</span>	default(none)					\
<span class="number-left">5</span>	shared(n, ans)				
<span class="number-left">6</span>	{
<span class="number-left">7</span>		double sum=0;
<span class="number-left">8</span>#pragma omp for					
<span class="number-left">9</span>		for(long int i=0; i &lt; n; i = i+2)
<span class="number-left">10</span>			{
<span class="number-left">11</span>				sum += 4.0/(2*i+1);
<span class="number-left">12</span>				sum -= 4.0/(2*i+3);
<span class="number-left">13</span>			}
<span class="number-left">14</span>#pragma omp critical
<span class="number-left">15</span>		ans += sum;
<span class="number-left">16</span>	}
<span class="number-left">17</span>	return ans;
<span class="number-left">18</span>}
</pre>
</div>

</div>
<div class="Indented">
The parallel directive on line 3 applies to the block (or region) that begins on line 6 and ends on line 16. This block will be executed in parallel by each of the threads.
</div>
<div class="Indented">
The <tt>default(none)</tt> clause on line 4 states that no variable is shared by default. In this parallel region, both <tt>n</tt> and <tt>ans</tt> are declared as shared by the <tt>shared()</tt> clause on line 5. We have made all that explicit to give an example of syntax in which multiple variables are declared as shared.
</div>
<div class="Indented">
The variable <tt>sum</tt> defined on line 7 is local to each thread. The <tt>omp for</tt> construct on line 8 splits the for-loop extending from lines 9 through 13 between the threads. The threads are already in a parallel region when the for-loop is encountered. Therefore, the <tt>parallel for</tt> construct must not be used. The <tt>omp for</tt> compiler directive on line 8, which splits the for-loop between threads without creating new threads, is appropriate here. Each thread adds its total into <tt>sum</tt>, which is local to itself.
</div>
<div class="Indented">
The <tt>critical</tt> construct on line 14 applies to the block immediately following, which in this case is simply line 15, which is <tt>ans += sum</tt>. On this line, every thread adds its local <tt>sum</tt> to <tt>ans</tt>. Because <tt>ans</tt> is a shared variable, the threads may overwrite each other if they access it simultaneously. The <tt>critical</tt> construct ensures that the threads take turns in executing line 15. Whenever a reduction variable is used, critical regions are employed implicitly. 
</div>
<div class="Indented">
By definition, critical regions are mutually exclusive, and only one thread can occupy a critical region at any one time. This is true even if the critical regions encountered by the threads are not the same syntactically.<span class="FootOuter"><span class="SupFootMarker"> [86] </span><span class="HoverFoot"><span class="SupFootMarker"> [86] </span>OpenMP allows finer control of mutual exclusion when the critical regions are named. The critical region that follows <tt>#pragma omp critical</tt> on line 14 is unnamed.</span></span>
</div>
<div class="Indented">
There can be no doubt that the earlier <tt>parallel for</tt> syntax is simpler. Indeed, that should be the preferred syntax in most situations. However, the more complex <tt>ompfor()</tt> is twice as fast as <tt>parallelfor()</tt> on the <span class="formula">3.6</span>  GHz AVX2 machine, although they are equally fast on the <span class="formula">2.2</span>  GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). Here we are seeing the same point for the umpteenth time: although the hardware on newer machines is faster, the compilers are yet to catch up.
</div>
<div class="Indented">
There is an implicit barrier at the end of each parallel region and at the end of an <tt>omp for</tt> block. Thus, in <tt>ompfor()</tt>, there are implicit barriers on lines 13 and 16. The parallel region comes to end on line 13, and beyond that point there is only one thread (the master thread). In summary, implicit barriers are found at the end of parallel regions and <tt>omp for</tt> constructs, but no implicit barrier occurs at the beginning of a parallel region or in a <tt>critical</tt> construct.
</div>
<h? class="Subsubsection">
<b><u>Some more syntax</u></b>
</h?>
<div class="Unindented">
The OpenMP library defines the function
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int omp_get_thread_num();
</pre>
</div>

</div>
<div class="Indented">
which returns the thread number of the calling thread. The thread number is <span class="formula">0</span> for the master thread. If the number of threads is <span class="formula"><i>N</i></span>, the threads are numbered <span class="formula">0, 1, …, <i>N</i> − 1</span>. The library also defines the function
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int omp_get_num_threads();
</pre>
</div>

</div>
<div class="Indented">
which returns the total number of threads in a parallel region when called by any thread. Calls of these functions are legal only inside parallel regions. They may not be called from functions that are called from parallel regions. However, the <tt>critical</tt> construct can be used even in functions called from parallel regions.
</div>
<div class="Indented">
In addition to the two functions above, constructs such as <tt>omp master</tt>, <tt>omp section</tt>, and <tt>omp ordered</tt> are  occasionally useful. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.1.2">5.1.2</a> Shared variables and OpenMP’s memory model<a class="Label" name="sub:threads-omp-memory-model"> </a>
</h3>
<div class="Unindented">
Let us look again at the <tt>critical</tt> construct.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#pragma omp critical
	ans += sum;
</pre>
</div>

</div>
<div class="Indented">
As stated earlier, the <tt>critical</tt> construct ensures that the statement <tt>ans += sum</tt> is executed by only one thread at a time. That might seem sufficient to ensure the correctness of the program because each thread will separately add its local sum to the final answer, but it is not.
</div>
<div class="Indented">
The compiler may decide to store the variable <tt>ans</tt> in a register and add its local sum to a register when <tt>ans += sum</tt> executes. The code for copying the register to the memory location of <tt>ans</tt> may be inserted <i>after</i> the critical block. In that case, we are back to the situation where the threads interfere with each other.
</div>
<div class="Indented">
Why is that not a problem? In answering this question, we arrive at the <tt>flush</tt> feature, which is a vital part of the OpenMP memory model. When a thread has a shared variable such as <tt>ans</tt>, it is allowed to keep a copy of that variable in a register. Indeed, when the compiler generates assembly instructions, it may keep multiple copies of the same variable in memory and in the register file for its convenience. When an OpenMP <tt>flush</tt> operation is executed, a thread must make all its writes visible to other threads even if the writes have only modified locally stored copies of a shared variable. The temporary view of all shared variables must be synced with the global view in shared memory.
</div>
<div class="Indented">
The OpenMP standard requires that a thread should implicitly carry out a <tt>flush</tt> operation when it enters and  leaves a critical or parallel construct, and at each barrier it encounters. Even the basic program for summing the Leibniz series relies on this rule for its correctness. Each thread syncs its local copy of <tt>ans</tt> with the global copy in shared memory when it enters the critical region, updates its local copy in the critical region, and syncs the updated value with the global copy of <tt>ans</tt> in shared memory before it exits the critical region. 
</div>
<h? class="Subsubsection">
<b><u>A more complex example</u></b>
</h?>
<div class="Unindented">
The OpenMP memory model is in play every time threads use a shared variable to communicate directly or indirectly. For finding the partial sum of the Leibniz series, the threads communicate indirectly using a shared variable to which each thread adds its part of the sum. The correctness of almost every OpenMP program depends on <tt>flush</tt> operations that are implicitly carried out at the beginning and end of parallel and critical constructs and at every barrier.
</div>
<div class="Indented">
In general, OpenMP allows the compiler to generate assembly for a parallel region or for chunks of a for-loop assigned to a thread, as if the code is sequential---except for implicit <tt>flush</tt> statements. Of course, the compiler must also handle reduction variables correctly.
</div>
<div class="Indented">
The peculiar implications of the OpenMP memory model can be difficult to grasp with just a single example. Therefore, we give another more complex example here. The variable <tt>x</tt> is the only variable shared by the two threads created by the function <tt>printstuff()</tt>, whose listing follows. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void printstuff(){
<span class="number-left">2</span>	int x;
<span class="number-left">3</span>#pragma omp parallel			\
<span class="number-left">4</span>	num_threads(2)			\
<span class="number-left">5</span>	default(none)			\
<span class="number-left">6</span>	shared(x)
<span class="number-left">7</span>	{
<span class="number-left">8</span>		int tid = omp_get_thread_num();
<span class="number-left">9</span>		if(tid==0){
<span class="number-left">10</span>			x = 0;
<span class="number-left">11</span>		}
<span class="number-left">12</span>		else if(tid==1){
<span class="number-left">13</span>			x = 1;
<span class="number-left">14</span>			printf("x = %d\t", x);
<span class="number-left">15</span>			printf("x = %d\n", x);
<span class="number-left">16</span>		}
<span class="number-left">17</span>	}
<span class="number-left">18</span>}
</pre>
</div>

</div>
<div class="Indented">
The parallel region extends from lines 7 to 17. The thread with <tt>tid</tt> (abbreviation of thread identifier) equal to 1 prints the shared variable <tt>x</tt> on lines 14 and 15. 
</div>
<div class="Indented">
The function <tt>printstuff()</tt> may print <span class="formula"><i>x</i> = 0,  <i>x</i> = 0</span> if thread 0 executes line 10 after thread 1 executes line 13 but before it prints on lines 14 and 15. If thread 1 executes line 13 after thread 0 executes line 10, the function prints <span class="formula"><i>x</i> = 1,  <i>x</i> = 1</span>. A little thought will show that <span class="formula"><i>x</i> = 1,  <i>x</i> = 0</span> is another possibility.
</div>
<div class="Indented">
A legal compilation of <tt>printstuff()</tt> may produce behavior that is different from picking one of these three possibilities, but that is illustrative of the implications of OpenMP’s memory model. 
</div>
<div class="Indented">
The function may even print <span class="formula"><i>x</i> = 0,  <i>x</i> = 1</span>. Because the compiler is allowed to treat the parallel region as if it were sequential, it may store <span class="formula"><i>x</i></span> in a register R in addition to the memory location <tt>x</tt>. The assignment on line 13 may update both the register R and the memory location. The first print statement on line 14 may access <span class="formula"><i>x</i></span> using the shared memory location <tt>x</tt>, and the print statement that follows may access <span class="formula"><i>x</i></span> using the register R. It may seem idiosyncratic for the compiler to generate such code. But the point here is that generating such code is legal if the parallel region is treated as sequential code. If the compiler generates such code, the function may print <span class="formula"><i>x</i> = 0,  <i>x</i> = 1</span>. 
</div>
<div class="Indented">
Alternatively, both the print statements may access <span class="formula"><i>x</i></span> using the register R. If so, the function will always print <span class="formula"><i>x</i> = 1,  <i>x</i> = 1</span>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.1.3">5.1.3</a> Overheads of OpenMP constructs<a class="Label" name="sub:threads-omp-overheads"> </a>
</h3>
<div class="Unindented">
OpenMP constructs are embedded into C/C++ programs to create teams of threads that work in parallel. The parallel, barrier, and for constructs introduce overheads. Work may need to be assigned to threads, threads may need to be created and destroyed, or synchronization and serialization may need to be implemented using system calls to the operating system kernel. These activities consume cycles. If the parallelized task is too small, the benefits of parallelization will be overwhelmed by the overheads. Effective programming requires knowledge of the overheads of OpenMP constructs.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:threads-omp-overhead-parallel"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Computer
</td>
<td align="center" valign="top">
Number of Cores/Threads
</td>
<td align="center" valign="top">
min
</td>
<td align="center" valign="top">
median
</td>
<td align="center" valign="top">
max
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">3.6</span>  GHz AVX2 
</td>
<td align="center" valign="top">
2
</td>
<td align="center" valign="top">
<span class="formula">1176</span>
</td>
<td align="center" valign="top">
<span class="formula">1332</span>
</td>
<td align="center" valign="top">
<span class="formula">3.4 × 10<sup>6</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.6</span>  GHz SSE2 
</td>
<td align="center" valign="top">
12
</td>
<td align="center" valign="top">
<span class="formula">4888</span>
</td>
<td align="center" valign="top">
<span class="formula">6188</span>
</td>
<td align="center" valign="top">
<span class="formula">3.2 × 10<sup>6</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.2</span>  GHz AVX 
</td>
<td align="center" valign="top">
16
</td>
<td align="center" valign="top">
<span class="formula">5025</span>
</td>
<td align="center" valign="top">
<span class="formula">6573</span>
</td>
<td align="center" valign="top">
<span class="formula">1.8 × 10<sup>7</sup></span>
</td>

</tr>

</table>
<div class="caption">
Table 5.1 Overhead (in cycles) of the parallel construct on three different computers. For the full names of the machines, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> of the appendix. 
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:threads-omp-overhead-parallel">5.1↑</a> reports the overhead of entering and exiting from a parallel region. The entries of the table are based on many invocations of the following program:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void parallelA(int nthreads, StatVector&amp; stats){
<span class="number-left">2</span>	TimeStamp clk;
<span class="number-left">3</span>	clk.tic();
<span class="number-left">4</span>#pragma omp parallel					\
<span class="number-left">5</span>	num_threads(nthreads)				\
<span class="number-left">6</span>	default(none)
<span class="number-left">7</span>	{
<span class="number-left">8</span>		dummy();
<span class="number-left">9</span>	}
<span class="number-left">10</span>	double cycles = clk.toc();
<span class="number-left">11</span>	stats.insert(cycles);
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
Every call of <tt>parallelA()</tt> enters and exits from a parallel region. The master thread times the entire parallel region and inserts the cycle count into the stats object on line 11. The parallel regions consists of a single call to <tt>dummy()</tt> on line 8. As its name suggests, <tt>dummy() </tt>does nothing<tt>. </tt>Its definition<tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void dummy(){}
</pre>
</div>

</div>
<div class="Indented">
is stashed in a separate compilation unit to force the compiler to generate a function call in the parallel region. The statistics of entering and exiting the parallel region are accumulated in the stats object after a large number of calls to <tt>parallelA()</tt>. 
</div>
<div class="Indented">
Several points emerge from table <a class="Reference" href="#tab:threads-omp-overhead-parallel">5.1↑</a>. The worst case is much worse than the median cost (or the mean cost---median and mean are approximately equal). The median or mean cost of a parallel region is in the thousands of cycles. The worst cost is of the order of millions of cycles. The worst case occurs at the first entry to parallel region and only occasionally later on, for reasons that will be explained later. It is important not to change the number of threads between parallel regions. Doing so will trigger the worst-case cost. The reason for that will also become clear later.
</div>
<div class="Indented">
Thus, assuming all parallel regions use the same number of threads, the cost we have to contend with is the median or mean cost, which is in the thousands of cycles. The median cost is much lower for the two-core machine than for the <span class="formula">12</span>-core or the <span class="formula">16</span>-core machines (see table <a class="Reference" href="#tab:threads-omp-overhead-parallel">5.1↑</a>). The two core machine has only one processor package for both cores. In the other two machines, the cores are split between two processor packages (as in figures <a class="Reference" href="#fig:proc-Block-diagram-of-5500P">3.6↑</a> and <a class="Reference" href="#fig:memory-dram-channel-array">4.2↑</a>b). More processor packages implies greater cost.
</div>
<div class="Indented">
The typical cost of around 5,000 cycles for a parallel region is not prohibitive. Even a for-loop that iterates <span class="formula">10<sup>5</sup></span> or <span class="formula">10<sup>6</sup></span> times may be parallelized effectively. However, an inner loop that iterates only <span class="formula">100</span> or 1,000 times cannot be parallelized effectively. The overhead of entering and leaving the parallel region will overwhelm any benefit from parallelization. Thus, OpenMP parallelism is not as fine in scale as the parallelism involved in XMM/YMM/ZMM registers (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>) or the instruction pipeline.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:threads-omp-overhead-barrier-for"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Number of Threads
</td>
<td align="center" valign="top">
Barrier
</td>
<td align="center" valign="top">
for
</td>
<td align="center" valign="top">
for with nowait
</td>

</tr>
<tr>
<td align="center" valign="top">
1
</td>
<td align="center" valign="top">
26
</td>
<td align="center" valign="top">
66
</td>
<td align="center" valign="top">
36
</td>

</tr>
<tr>
<td align="center" valign="top">
2
</td>
<td align="center" valign="top">
1178
</td>
<td align="center" valign="top">
1401
</td>
<td align="center" valign="top">
93
</td>

</tr>
<tr>
<td align="center" valign="top">
3
</td>
<td align="center" valign="top">
1250
</td>
<td align="center" valign="top">
1451
</td>
<td align="center" valign="top">
101
</td>

</tr>
<tr>
<td align="center" valign="top">
4
</td>
<td align="center" valign="top">
1671
</td>
<td align="center" valign="top">
1758
</td>
<td align="center" valign="top">
96
</td>

</tr>
<tr>
<td align="center" valign="top">
6
</td>
<td align="center" valign="top">
2427
</td>
<td align="center" valign="top">
2555
</td>
<td align="center" valign="top">
100
</td>

</tr>
<tr>
<td align="center" valign="top">
8
</td>
<td align="center" valign="top">
2710
</td>
<td align="center" valign="top">
2731
</td>
<td align="center" valign="top">
99
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
2346
</td>
<td align="center" valign="top">
2458
</td>
<td align="center" valign="top">
99
</td>

</tr>
<tr>
<td align="center" valign="top">
12
</td>
<td align="center" valign="top">
2516
</td>
<td align="center" valign="top">
2415
</td>
<td align="center" valign="top">
101
</td>

</tr>
<tr>
<td align="center" valign="top">
16 (2.2  GHz AVX)
</td>
<td align="center" valign="top">
3262
</td>
<td align="center" valign="top">
3743
</td>
<td align="center" valign="top">
371
</td>

</tr>

</table>

</div>
<div class="caption">
Table 5.2 Overheads of OpenMP constructs reported as number of cycles. All the entries of this table are for a <span class="formula">2.6</span>  GHz SSE2 machine, except the last, which is for a <span class="formula">2.2</span>  GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full names of the machines).
</div>

</div>

</div>
Table <a class="Reference" href="#tab:threads-omp-overhead-barrier-for">5.2↑</a> reports the overheads of the <tt>barrier</tt> and <tt>for</tt> constructs. The programs for calculating these overheads are similar to the one we just saw. Therefore, the programs are not listed. 
</div>
<div class="Indented">
The most striking inference from table <a class="Reference" href="#tab:threads-omp-overhead-barrier-for">5.2↑</a> is that most of the cost of the <tt>omp for</tt> construct seems to be due to the implicit barrier at the end of the construct. To confirm that is indeed so, we modified the for construct slightly. In particular, the <tt>nowait</tt> clause was added to the <tt>omp for</tt> construct. The <tt>nowait</tt> clause removes the implicit barrier at the end of the for construct. The last column of table <a class="Reference" href="#tab:threads-omp-overhead-barrier-for">5.2↑</a> shows that of the <tt>omp for</tt> construct with the <tt>nowait</tt> clause has an overhead of around <span class="formula">100</span> cycles and no more. The <tt>omp for</tt> construct is virtually cost free if the implicit barrier at the end of the construct is removed.
</div>
<div class="Indented">
Comparing tables <a class="Reference" href="#tab:threads-omp-overhead-parallel">5.1↑</a> and <a class="Reference" href="#tab:threads-omp-overhead-barrier-for">5.2↑</a>, it becomes clear that the cost of entering and leaving a parallel region too is due to a considerable extent to the implicit barrier at exit. The implicit memory <tt>flush</tt> occurs during entry/exit of parallel regions but does not apply to the <tt>omp for</tt> construct. Therefore, parallel regions are somewhat more expensive. 
</div>
<div class="Indented">
As far as understanding the cost of parallel regions and other OpenMP constructs is concerned, we are left with two questions. The first is regarding the high cost in millions of cycles that occurs during the first entry into a parallel region, or whether the number of threads in a parallel region is changed, and rarely otherwise. The other is regarding the cost of a barrier. We will address the first question later, when we study Pthreads. The cost of a barrier is due to the way mutual exclusion is enforced, and we turn to it right now.
</div>
<h? class="Subsubsection">
<b><u>Mutual exclusion on x86 computers<a class="Label" name="sub:openmp-x86-spinlock"> </a></u></b>
</h?>
<div class="Unindented">
The x86 instruction set provides instructions to simplify the implementation of mutual exclusion and critical regions. One of these instructions is <tt>XCHG</tt>. The <tt>XCHG</tt> instruction swaps a register with a memory location and is guaranteed to be atomic. Suppose the memory location is a lock variable named <tt>lockvar</tt>. If the lock variable is <span class="formula">0</span>, it means no process is in the critical region. If the lock variable is <span class="formula">1</span>, it means some process is in the critical region. To enter the critical region, a process will set some register, say <tt>EAX</tt>, to <span class="formula">1</span>. Next it exchanges <tt>EAX</tt> and <tt>lockvar</tt> using <tt>XCHG</tt>. If the value that is exchanged into <tt>EAX</tt> is <span class="formula">0</span>, the process enters the critical region with the assurance that it set <tt>lockvar</tt> to be <span class="formula">1</span> and the critical region is now locked. In contrast, if the value that is exchanged into <tt>EAX</tt> is <span class="formula">1</span>, it means some other process has locked the critical region. The recommended way (from Intel) to enter the critical region is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">SpinLock:
     CMP $0, lockvar
     JE GetLock
     PAUSE
     JMP SpinLock
GetLock:
     MOV $1, EAX
     XCHG lockvar, EAX
     CMP $0, EAX
     JNE SpinLock
</pre>
</div>

</div>
<div class="Indented">
This code avoids executing <tt>XCHG</tt> too frequently and uses <tt>PAUSE</tt> inside the loop. The protocol for exiting the critical region is much simpler: <tt>MOV $0, lockvar</tt>. This solution works with any number of competing processes or threads. 
</div>
<div class="Indented">
This little segment of code gives  insight into the cost of a barrier. A single access of <tt>lockvar</tt> will involve cost that is of the order of latency to memory. Thus, the cost of a barrier is equal to a multiple of the latency to memory. The multiple depends on how many threads need to be synchronized. One method of implementing a barrier would be for each thread to enter a critical region and increment a shared variable to notify others of its presence. A more efficient implementation may pair off the threads in a binary tree and synchronize by walking up to the root of the tree. 
</div>
<div class="Indented">
There are two distinct families of solutions to the mutual exclusion problem. One family uses busy waiting. The use of <tt>XCHG</tt> is of that type. Another family of solutions is wait-free. Classical semaphores and mutexes are of that type. In wait-free synchronization, a process or thread gives up the processor core and goes to sleep when it is waiting. It is woken up again when some condition flag has changed. Because wait-free synchronization involves putting processes or threads to sleep and then waking them up, it necessarily involves the scheduling function of the operating system kernel. Device drivers typically use wait-free synchronization to access shared resources. In contrast, interrupt handlers are not allowed to go to sleep and must use spinlocks if they need to synchronize or serialize.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write an OpenMP program in which threads print their identifiers in reverse order.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write an OpenMP program that reverses an array in place. How long should the array be before OpenMP parallelism proves advantageous?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write an OpenMP program that calls <tt>qsort()</tt> defined in the C library from each thread. The sections of the array that are sorted by each thread are merged to complete sorting the entire array. The easy way to do a merge is to leave it all to the master thread. A more sophisticated way is to merge pairwise while assigning the pairs to different threads. Time and compare the two sorting routines, with two different ways of doing a merge, with a single threaded sort. 
</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  If the merge is implemented pairwise, it could be important to ensure that the number of threads in the parallel regions remains the same, even if some threads remain inactive. Explain why that could be the case.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  An array, or equivalently a pointer to its first entry, is a shared variable. Array entries are modified independently by different threads using the same pointer. Is an OpenMP <tt>flush</tt> essential to make such changes made by one thread visible to the other threads?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write an OpenMP program to find the minimum entry of an array using the <tt>parallel for</tt> construct with an appropriate reduction clause.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-5.2">5.2</a> Optimizing OpenMP programs<a class="Label" name="sec:threads-optimizing-omp"> </a>
</h2>
<div class="Unindented">
In OpenMP, program memory is often naturally split between threads. In an example such as matrix transpose, for instance, certain blocks of the matrix are mainly handled by certain threads. When multiple threads occupy processor cores across multiple packages, there is much to be gained by allocating memory for each thread in a memory channel that is close to it. 
</div>
<div class="Indented">
The way to do this is explained in section <a class="Reference" href="#sub:threads-omp-near-far">5.2.1↓</a>. At first one may suspect that memory is allocated during a call to <tt>malloc()</tt> or its equivalent. In fact, during <tt>malloc()</tt>, the memory that gets allocated is typically virtual and not physical. It is only during first access, and the resulting page fault, that a page of virtual memory is mapped to a page of physical memory. Therefore, the allocation of page frames in near memory depends on the initialization protocol. Briefly, each page in virtual memory must first be accessed by the thread most likely to use it. 
</div>
<div class="Indented">
It is certainly possible that a page of virtual memory is heavily used by a certain thread in a certain phase of the program and by another thread in another phase of the program. In such situations, the best option is to try to map the threads that use the same pages of memory to nearby cores. This technique is also explained in section <a class="Reference" href="#sub:threads-omp-near-far">5.2.1↓</a>.
</div>
<div class="Indented">
It is commonly believed that a program running on <span class="formula">16</span> cores is <span class="formula">16</span> times faster than a program that runs on <span class="formula">1</span> core, ignoring communication costs. This belief is completely mistaken. Bandwidth to memory does not scale linearly with the number of cores. Although linear speedup is often claimed in scientific computing research, such claims are a consequence of the program not going out of cache, a surprisingly common occurrence. Algorithms that use memory in a nontrivial manner and achieve linear speedup are rare. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:threads-omp-bw-DRAM">5.2.2↓</a>, we find that the bandwidth to memory increases by only a factor of <span class="formula">3</span> or <span class="formula">4</span>, when we go from <span class="formula">1</span> thread to <span class="formula">12</span> threads on a <span class="formula">12</span>-core machine. Similarly, the speedup is sublinear and between <span class="formula">4</span> and <span class="formula">8</span> on a <span class="formula">16</span>-core machine.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:threads-omp-Matrix-transpose">5.2.3↓</a>, we find the improvement in the realized bandwidth in effecting a matrix transpose is a factor of four on a <span class="formula">12</span>-core machine, and seven on a <span class="formula">16</span>-core machine. The Fast Fourier Transform (FFT) is one of the more arithmetic-intensive algorithms. As explained earlier, the cost of memory accesses cannot be completely hidden in the FFT. In section <a class="Reference" href="#sub:threads-omp-Fast-Fourier-transform">5.2.4↓</a>, we find the speedups for the FFT to be nine and <span class="formula">12</span> on <span class="formula">12</span>- and <span class="formula">16</span>-core machines, respectively. The speedups are impressive but still sublinear. The only commonly used algorithms that would result in linear speedups, assuming nontrivial utilization of memory resources, seem to be dense matrix algorithms such as LU factorization and matrix multiplication.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.2.1">5.2.1</a> Near memory and far memory<a class="Label" name="sub:threads-omp-near-far"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:threads-omp-dram-numa"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter4/compact_scattered.png" alt="figure FIGS/chapter4/compact_scattered.png" style="max-width: 346px; max-height: 135px;"/>

</div>
<div class="caption">
Figure 5.1 Assignment of threads numbered <span class="formula">0</span> through <span class="formula">11</span> to processor cores with <tt>compact</tt> and <tt>scatter</tt> affinities.
</div>

</div>

</div>

</div>
<div class="Indented">
To explain the distinction between near and far memory, we turn to figure <a class="Reference" href="#fig:threads-omp-dram-numa">5.1↑</a>. That figure shows a <span class="formula">12</span>-core machine, with the processors divided into packages. All memory references to far memory, which is those page frames that reside in a memory channel connected to the other processor package, have to go through an interconnect. This interconnect is similar to a fast network channel and its use makes references to far memory more expensive. For example, on a <span class="formula">2.6</span>  GHz <span class="formula">12</span>-core SSE2 machine, the latency to near memory is <span class="formula">180</span> cycles, whereas the latency to far memory is <span class="formula">300</span> cycles. On another machine (<span class="formula">2.2</span>  GHz <span class="formula">16</span>-core AVX) with two processor packages, the latency to near memory is again around <span class="formula">180</span> cycles and the latency to far memory is again much greater, being more than <span class="formula">350</span> cycles. 
</div>
<div class="Indented">
Thus, it is advantageous if a page frame that is mostly used by a thread resides in near memory. If page frames are in far memory, the speed of the program can degrade by more than a factor of two.
</div>
<div class="Indented">
The assignment of pages to page frames can be enforced through some kind of an initialization routine. An example follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void init_manycore(double *list, long len, int nthreads){
#pragma omp parallel for			\
	num_threads(nthreads)			\
	default(none)				\
	shared(list, len, nthreads)
	for(long i=0; i &lt; len; i++)
		list[i] = 0.0;
}
</pre>
</div>

</div>
<div class="Indented">
This initialization routine must be called soon after the array <tt>list[]</tt> is allocated and before any access of any entry of the array. The initialization may not be doing anything useful. The point is that the array is split between the threads, and each thread initializes the memory it is mostly likely to use. 
</div>
<div class="Indented">
The first time an entry of <tt>list[]</tt> that begins a page (the address at page beginning is <span class="formula">0</span> module the page size, which is typically <span class="formula">4096</span>), there is a page fault. The page fault handler typically finds a page frame in memory that is near the thread that triggered the fault.<span class="FootOuter"><span class="SupFootMarker"> [87] </span><span class="HoverFoot"><span class="SupFootMarker"> [87] </span>In principle, first-touch allocation depends on assumptions about C/C++ memory management. The key assumption is that the memory that is returned by <tt>malloc()</tt> exists in virtual memory but has no associated page frames.</span></span> 
</div>
<div class="Indented">
In this example, the allocation of page frames in near memory is accomplished using an explicit initialization routine and as a consequence of demand paging. In other programs, it may be more natural to make an invocation of the most intensive part of the program. For example, in a certain program, the most cycle-intensive segment may be computing the FFT using multiple threads. In such a program, the allocation of pages in near memory may be effected by making a dummy call of the FFT routines. The results of this dummy call will be useless. In fact, it is  important not to initialize memory prior to the call. Its only purpose is to allocate page frames in near memory.
</div>
<div class="Indented">
In a program with many phases, such as an FFT phase, a transpose phase, and a solver phase, it may be impossible to ensure that a page frame is in near memory during each phase. We can still pick one of the phases, perhaps the most cycle intensive among the phases, and ensure that page frames are in near memory during that phase.
</div>
<div class="Indented">
Another useful technique is to control the assignment of threads to processor cores. In the <tt>icpc</tt> compiler, there are two ways of doing that. The assignment of threads to cores can be specified on the Linux command line as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">export KMP_AFFINITY=scatter
</pre>
</div>

</div>
<div class="Indented">
or
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">export KMP_AFFINITY=compact
</pre>
</div>

</div>
<div class="Indented">
The default assignment is to scatter the threads. The <tt>icpc</tt> runtime library looks up the environment variable <tt>KMP_AFFINITY</tt> and uses Linux system calls to control the binding of threads to processor cores (for the distinction between <tt>compact</tt> and <tt>scatter</tt>, see figure <a class="Reference" href="#fig:threads-omp-dram-numa">5.1↑</a>). Another way to control thread binding to processor cores with <tt>icc/icpc</tt> is to use the <tt>kmp_set_defaults()</tt> function. 
</div>
<div class="Indented">
For programs that assign a thread to each processor core, <tt>compact</tt> is often better, although <tt>scatter</tt> is the default. It may be a better idea to keep nearby threads in nearby processor cores because threads with nearby identifiers are more likely to be correlated and access nearby pages. All our timings assume the <tt>compact</tt> thread affinity.
</div>
<div class="Indented">
In general, one can try to fix the assignment of threads to processor cores completely. Although this sounds simple in principle, it is cumbersome to get it to work in nontrivial programs.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.2.2">5.2.2</a> Bandwidth to DRAM memory<a class="Label" name="sub:threads-omp-bw-DRAM"> </a>
</h3>
<div class="Unindented">
The <tt>sum_onecore()</tt> function listed below is the engine for measuring read bandwidth to memory.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double sum_onecore(double *list, int n){
	double ans = 0;
#pragme vector always
	for(int i=0; i &lt; n; i++)
		ans += list[i];
	return ans;
}
</pre>
</div>

</div>
<div class="Indented">
The <tt>icpc</tt> compiler generates excellent code for this function. It unrolls the loop and generates packed double instructions so that the cost of adding is irrelevant. The <tt>pragma</tt> directive gives a strong suggestion to <tt>icpc</tt> to use XMM/YMM/ZMM registers and packed double instructions. Other compilers would ignore this directive.
</div>
<div class="Indented">
The <tt>sum_onecore()</tt> function is called by each thread created by the function <tt>sum_manycore()</tt>, whose listing follows. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double sum_manycore(double *list, long len, 
							int nthreads){
	double ans = 0;	
#pragma omp parallel				\
	num_threads(nthreads)			\
	default(none)				\
	shared(ans, list, len, nthreads)
	{
		int tid = omp_get_thread_num();
		long first = len*tid/nthreads;
		long next = len*(tid+1)/nthreads;
		double s = sum_onecore(list+first,next-first);
#pragma omp critical
		ans += s;
	}
	return ans;
}
</pre>
</div>

</div>
<div class="Indented">
This program splits the entries of <tt>list[]</tt> between the threads in a certain way. That is the same split as in <tt>init_manycore()</tt>, assuming <tt>nthreads</tt> to be the same. Therefore, we can send page frames to near memory by making either a call to <tt>init_manycore()</tt> or a dummy call to <tt>sum_manycore()</tt> <i>even before</i> <tt>list[]</tt> is initialized. 
</div>
<div class="Indented">
The following functions were called from each thread to determine the write and copy bandwidths: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void write_onecore(double *list, int n){
#pragma vector always nontemporal
	for(int i=0; i &lt; n; i++)
		list[i] = i;
}
​
void copy_onecore(double *list, int n){
#pragma vector always nontemporal
	for(int i=0; i &lt; n/2; i++)
		list[i] = list[n/2+i];
}
</pre>
</div>

</div>
<div class="Indented">
Here the <tt>pragma</tt> directive, specific to <tt>icpc</tt> and ignored by other compilers, asks for <tt>nontemporal</tt> writes. Nontemporal writes are streamed directly to DRAM, bypassing the write-back cache, using the <tt>movntpd</tt> instruction. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:threads-omp-dram-bw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Computer
</td>
<td align="center" valign="top">
# of Threads/Cores
</td>
<td align="center" valign="top">
read
</td>
<td align="center" valign="top">
write
</td>
<td align="center" valign="top">
copy
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.6</span> GHz SSE2 
</td>
<td align="center" valign="top">
12
</td>
<td align="center" valign="top">
<span class="formula">15.0</span> (3.3)
</td>
<td align="center" valign="top">
<span class="formula">9.51</span> (3.2)
</td>
<td align="center" valign="top">
<span class="formula">9.48</span>(2.5)
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.2</span> GHz AVX
</td>
<td align="center" valign="top">
16
</td>
<td align="center" valign="top">
<span class="formula">36.7</span> (7.5)
</td>
<td align="center" valign="top">
<span class="formula">34.7</span>(16)
</td>
<td align="center" valign="top">
<span class="formula">22.5</span> (4.4)
</td>

</tr>

</table>

</div>
<div class="caption">
Table 5.3 Bandwidth to memory in bytes per cycle (for reading, writing, and copying) on two different computers. The parenthesized numbers are the speedups relative to bandwidth from a single core (for the full names of the computers, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:threads-omp-dram-bw">5.3↑</a> lists the read, write, and copy bandwidths for two different computers. In neither case do we get anything close to linear speedup for reading and copying. The speedup for the write bandwidth on the AVX machine is linear as a consequence of nontemporal writes. However, the speedup is only slightly more than a quarter as much for copying despite the same <tt>pragma</tt> directive, suggesting that the magic of the nontemporal <tt>pragma</tt> may be limited to the occasional toy example. The bandwidth for copying remains the same even when the <tt>pragma</tt> is omitted.
</div>
<div class="Indented">
The peak read bandwidth on the SSE2 machine is <span class="formula">40</span> GB/s. The peak read bandwidth on the AVX machine is nearly twice as great. The peak copy bandwidths are <span class="formula">25</span> GB/s and <span class="formula">50</span> GB/s, respectively.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.2.3">5.2.3</a> Matrix transpose<a class="Label" name="sub:threads-omp-Matrix-transpose"> </a>
</h3>
<div class="Indented">
The function <tt>blocktransx()</tt> defined below is similar to the one defined in chapter <a class="Reference" href="#chap:Memory">4↑</a>. This definition differs by allowing the matrix stored in the array <tt>b[]</tt> to have a leading dimension greater than the number of rows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void blocktransx(double *restrict a, double *restrict b, 
                 int ldb, int m, int n){
  assert((m%B==0)&amp;&amp;(n%B==0));
  for(int i=0; i &lt; m; i+=B)
    for(int j=0; j &lt; n; j+=B)
      for(int ii=0; ii &lt; B; ii++)
        for(int jj=0; jj &lt; B; jj++)
          b[j+jj+(i+ii)*ldb] = a[i+ii+(j+jj)*m];
}
</pre>
</div>

</div>
<div class="Indented">
The function listed below is a multithreaded implementation of the matrix transpose, which uses <tt>blocktransx()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void blocktrans(double *restrict a, double *restrict b,
<span class="number-left">2</span>		int m, int n, int nthreads){
<span class="number-left">3</span>	assert(m%B==0);
<span class="number-left">4</span>	assert(n%(nthreads*B)==0);
<span class="number-left">5</span>#pragma omp parallel					\
<span class="number-left">6</span>	num_threads(nthreads)				\
<span class="number-left">7</span>	default(none)					\
<span class="number-left">8</span>	shared(a, b, m, n, nthreads)			
<span class="number-left">9</span>	{
<span class="number-left">10</span>		int tid = omp_get_thread_num();
<span class="number-left">11</span>		int nn = n/nthreads;
<span class="number-left">12</span>		int nfst = tid*nn;
<span class="number-left">13</span>		int ldb = n;
<span class="number-left">14</span>		blocktransx(a+m*nfst, b+nfst, ldb, m, nn); 
<span class="number-left">15</span>	}
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>blocktrans()</tt> assumes that the array <tt>a[]</tt> stores an <span class="formula"><i>m</i> × <i>n</i></span> matrix (with leading dimension <span class="formula"><i>m</i></span>) and saves its transpose in <tt>b[]</tt>. The transposed <span class="formula"><i>n</i> × <i>m</i></span> matrix is stored in <tt>b[]</tt> using leading dimension <span class="formula"><i>n</i></span>. The threads split the columns of <tt>a[]</tt> equally between themselves on lines 11 and 12. The matrix transposed by each thread has dimension <span class="formula"><i>m</i> × <i>nn</i></span>, where <tt>nn</tt> is defined on line 11. The function call on line 14 shifts the first entry of <tt>a[]</tt> by <tt>nfst</tt> columns and the first entry of <tt>b[]</tt> by <tt>nfst</tt> to isolate the submatrix that the thread works on. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:threads-omp-transpose">5.4↓</a> shows that the bandwidth realized for varying number of threads and varying block sizes <span class="formula"><i>B</i></span>. The matrix transposed was square with dimension as close to <span class="formula">40, 000</span> as possible subject to the divisibility conditions assumed on lines 3 and 4. On both the <span class="formula">12</span>-core SSE2 machine and the <span class="formula">16</span>-core AVX machine, the bandwidth realized in transposing is nearly <span class="formula">80</span>% of the bandwidth for copying, as may be verified by comparing with table <a class="Reference" href="#tab:threads-omp-transpose">5.4↓</a>. Although we had complaints about the compiled code for a single processor with a single thread, such complaints may not be justified here. Getting to <span class="formula">80</span>% of the best possible in a matrix transpose is quite good. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:threads-omp-transpose"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Computer
</td>
<td align="center" valign="top">
# of Threads/Cores
</td>
<td align="center" valign="top">
<span class="formula"><i>B</i></span>
</td>
<td align="center" valign="top">
bw
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.6</span> GHz SSE2 
</td>
<td align="center" valign="top">
12
</td>
<td align="center" valign="top">
125
</td>
<td align="center" valign="top">
<span class="formula">7.60</span> (4)
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.2</span> GHz AVX 
</td>
<td align="center" valign="top">
16
</td>
<td align="center" valign="top">
75
</td>
<td align="center" valign="top">
<span class="formula">16.2</span> (7)
</td>

</tr>

</table>

</div>
<div class="caption">
Table 5.4 Bandwidth to memory (in bytes per cycle) realized in transposing a square matrix of dimension close to 40,000 on two different computers. The block size is <span class="formula"><i>B</i> × <i>B</i></span>. The parenthesized numbers are speedups relative to a single core (for the full names of the machines, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>

</div>
<div class="Indented">
The following code was used to ensure that page frames reside in near memory:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  double *a = new double[m*n];
  double *b = new double[m*n];
  blocktrans(a, b, m, n, nthreads);
</pre>
</div>

</div>
<div class="Indented">
The first call to <tt>blocktrans()</tt> is made even before the arrays are initialized. The memory accesses generated by <tt>blocktrans()</tt> induce a favorable mapping to page frames. The same page of memory in either <tt>a[]</tt> or <tt>b[]</tt> may be accessed by more than one thread. In that case, whichever thread races to the page first gets a page frame close to itself. If the two threads are on the same processor package, the page will be near both the threads.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.2.4">5.2.4</a> Fast Fourier transform<a class="Label" name="sub:threads-omp-Fast-Fourier-transform"> </a>
</h3>
<div class="Unindented">
For a purely arithmetic program, such as summing the Leibniz series, the speed of the program increases linearly with the number of cores, provided the number of terms summed is large enough to outweigh the overhead of entering and leaving an OpenMP parallel region. Other tasks, such as transposing a matrix, are limited by bandwidth to memory. The bandwidth to memory does not increase linearly with the number of cores employed. Many problems in scientific computing---finite differences or finite elements on a 3D mesh to give two examples---fall in the latter category.
</div>
<div class="Indented">
Dense linear algebra problems, such as matrix multiplication, are limited by bandwidth to memory if implemented in a straightforward way. In chapter <a class="Reference" href="#chap:The-processor">3↑</a>, we showed how to write a microkernel for matrix multiplication that takes advantage of parallelism in the instruction set. Chapter <a class="Reference" href="#chap:Memory">4↑</a> showed how to cleverly hide the cost of accessing memory. A multithreaded matrix multiplication must account for cache memory slightly differently because the L3 cache is common to all the processor cores on the same package. Yet linear speedup with increasing processor cores can be achieved without drastic rethinking.
</div>
<div class="Indented">
Hiding the cost of memory accesses and making efficient use of processor resources are essential for optimized FFT routines as well. The inverse Discrete Fourier Transform (DFT) of the sequence <span class="formula"><i>a</i><sub>0</sub>, …, <i>a</i><sub><i>N</i> − 1</sub></span> is given by <div class="formula">
<i>b</i><sub><i>j</i></sub> = <span class="limits"><sup class="limit"><i>N</i> − 1</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>k</i> = 0</sub></span><i>ω</i><sup><i>jk</i></sup><i>a</i><sub><i>k</i></sub>
</div>
where <span class="formula"><i>ω</i> = exp(2<i>π</i><i>i</i> ⁄ <i>N</i>)</span> is a primitive <span class="formula"><i>N</i></span>th root of unity. This transformation is sometimes called the DFT, but we prefer to call it the inverse DFT to maintain an analogy with Fourier analysis. If the <span class="formula"><i>a</i><sub><i>j</i></sub></span> are discrete Fourier coefficients, we may think of <span class="formula"><i>b</i><sub><i>j</i></sub></span> as equispaced samples in the time domain. The FFT is a fast algorithm for computing the DFT or its inverse.
</div>
<div class="Indented">
In chapter <a class="Reference" href="#chap:C/C++:-Libraries-and">2↑</a>, we discussed the FFTW and MKL libraries and their interfaces to the 1D inverse DFT. Here we return to that topic to make a few points about the FFT and its implementation. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:threads-omp-fourier">5.5↓</a> shows the number of cycles used by the FFT for various <span class="formula"><i>N</i></span> as a function of the number of threads. The speedups are quite good and reach <span class="formula">75</span>% of linear speedup for some of the values of <span class="formula"><i>N</i></span>. The numbers in the table were obtained by lining up many blocks of <span class="formula"><i>N</i></span> complex numbers (or <span class="formula">2<i>N</i></span> double-precision numbers) in <span class="formula">16</span> GB of memory. The number of blocks is approximately <span class="formula">10<sup>9</sup> ⁄ <i>N</i></span>, and the FFT of each block was computed. The blocks were split between threads using OpenMP. Each thread applied MKL’s 1D FFT to its share of the blocks. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:threads-omp-fourier"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
Computer
</td>
<td align="center" valign="top">
# of Threads/Cores
</td>
<td align="center" valign="top">
<span class="formula"><i>N</i></span>
</td>
<td align="center" valign="top">
bw
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
<span class="formula">64</span>
</td>
<td align="center" valign="top">
<span class="formula">0.35</span> (8)
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.6</span> GHz SSE2 
</td>
<td align="center" valign="top">
12
</td>
<td align="center" valign="top">
<span class="formula">1024</span>
</td>
<td align="center" valign="top">
<span class="formula">0.29</span> (8)
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
<span class="formula">8192</span>
</td>
<td align="center" valign="top">
<span class="formula">0.33</span> (9)
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
<span class="formula">64</span>
</td>
<td align="center" valign="top">
<span class="formula">0.16</span> (7)
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">2.2</span> GHz AVX 
</td>
<td align="center" valign="top">
16
</td>
<td align="center" valign="top">
<span class="formula">1024</span>
</td>
<td align="center" valign="top">
<span class="formula">0.15</span> (12)
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
<span class="formula">8192</span>
</td>
<td align="center" valign="top">
<span class="formula">0.14</span> (13)
</td>

</tr>

</table>

</div>
<div class="caption">
Table 5.5 Bandwidth (more precisely, the inverse bandwidth) given as the number of cycles consumed by an FFT of size <span class="formula"><i>N</i></span> divided by <span class="formula"><i>N</i>log<sub>2</sub><i>N</i></span>. The parenthesized numbers are speedups relative to a single core (for the full names of the machines, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). 
</div>

</div>

</div>

</div>
<h? class="Subsubsection">
<b><u>Notes on 1D versus 2D FFT</u></b>
</h?>
<div class="Unindented">
Let us suppose that <span class="formula"><i>N</i> = 2<i>n</i></span>. The basic idea of the radix-2 FFT is as follows.<span class="FootOuter"><span class="SupFootMarker"> [88] </span><span class="HoverFoot"><span class="SupFootMarker"> [88] </span>For the FFT algorithm see, <span class="bibcites">[<a class="bibliocite" name="cite-43" href="#biblio-43"><span class="bib-index">43</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-48" href="#biblio-48"><span class="bib-index">48</span></a>]</span>. The Wikipedia pages on the FFT and related topics have a wealth of information.</span></span> First rearrange the data <span class="formula"><i>a</i><sub>0</sub>, <i>a</i><sub>1</sub>, …, <i>a</i><sub>2<i>n</i> − 1</sub></span> as a <span class="formula">2 × <i>n</i></span> matrix:<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>0</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>2</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>a</i><sub>2<i>n</i> − 2</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>1</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>3</sub>
</span>
<span class="arraycell align-c">

</span>
<span class="arraycell align-c">
<i>a</i><sub>2<i>n</i> − 1</sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>.
</div>
The original vector is recovered by reading this matrix in column-major order. The first step is to take the <span class="formula"><i>n</i></span>-dimensional FFT of each row to get<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>ã</i><sub>0</sub>
</span>
<span class="arraycell align-c">
<i>ã</i><sub>2</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>ã</i><sub>2<i>n</i> − 2</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>ã</i><sub>1</sub>
</span>
<span class="arraycell align-c">
<i>ã</i><sub>3</sub>
</span>
<span class="arraycell align-c">

</span>
<span class="arraycell align-c">
<i>ã</i><sub>2<i>n</i> − 1</sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>.
</div>
The next step is to multiply by the twiddle factors to get<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>ã</i><sub>0</sub>
</span>
<span class="arraycell align-c">
<i>ã</i><sub>2</sub>
</span>
<span class="arraycell align-c">
⋯
</span>
<span class="arraycell align-c">
<i>ã</i><sub>2<i>n</i> − 2</sub>
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>ã</i><sub>1</sub>
</span>
<span class="arraycell align-c">
<i>ω</i><i>ã</i><sub>3</sub>
</span>
<span class="arraycell align-c">

</span>
<span class="arraycell align-c">
<i>ω</i><sup><i>n</i> − 1</sup><i>ã</i><sub>2<i>n</i> − 1</sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>.
</div>
Here <span class="formula"><i>ω</i> = exp(2<i>π</i><i>i</i> ⁄ <i>N</i>)</span>. If this matrix is multiplied by <div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
1
</span>
<span class="arraycell align-c">
1
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
1
</span>
<span class="arraycell align-c">
 − 1
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>, 
</div>
which is the 1D transform matrix with <span class="formula"><i>N</i> = 2</span>, we will have the inverse DFT of the original matrix organized as a matrix in column-major format. The mathematical justification of this claim is omitted. 
</div>
<div class="Indented">
The <span class="formula">5<i>N</i>log<sub>2</sub><i>N</i></span> operation count of the FFT is obtained by recursive applications of the same idea, which is possible if <span class="formula"><i>N</i></span> is a power of <span class="formula">2</span>. An implementation that follows the resulting scheme will be quite inefficient, however. The scheme as it is presented will lead to a lot of cache conflicts. 
</div>
<div class="Indented">
The first decomposition, which we described in detail, thinks of a vector <span class="formula"><i>a</i><sub><i>i</i></sub></span>, <span class="formula">0 ≤ <i>i</i> &lt; <i>N</i> − 1</span>, as two vectors of length <span class="formula"><i>N</i> ⁄ 2</span>. The two vectors are <div class="formula">
<i>a</i><sub><i>i</i></sub>, <i>a</i><sub><i>i</i> + 2</sub>, <i>a</i><sub><i>i</i> + 4</sub>, <i>a</i><sub><i>i</i> + 6</sub>, …
</div>
with <span class="formula"><i>i</i> = 0, 1</span>. These two vectors appeared as the rows of an <span class="formula">2 × <i>N</i> ⁄ 2</span> matrix in the first step. If we assume <span class="formula"><i>N</i></span> is a power of <span class="formula">2</span> and go to a depth of <span class="formula"><i>k</i></span> in the recursion, we find that the FFT is breaking the given data into <span class="formula">2<sup><i>k</i></sup></span> vectors of length <span class="formula"><i>N</i> ⁄ 2<sup><i>k</i></sup></span>. These <span class="formula">2<sup><i>k</i></sup></span> vectors are given by <div class="formula">
<i>a</i><sub><i>i</i></sub>, <i>a</i><sub><i>i</i> + 2<sup><i>k</i></sup></sub>, <i>a</i><sub><i>i</i> + 2.2<sup><i>k</i></sup></sub>, <i>a</i><sub><i>i</i> + 3.2<sup><i>k</i></sup></sub>, <i>a</i><sub><i>i</i> + 4.2<sup><i>k</i></sup></sub>, …
</div>
for <span class="formula"><i>i</i> = 0, 1, …, 2<sup><i>k</i></sup> − 1</span>. The successive entries of each of these vectors are separated by <span class="formula">2<sup><i>k</i></sup></span> in the original sequence. As we found in chapter <a class="Reference" href="#chap:Memory">4↑</a>, locations separated by powers of <span class="formula">2</span> are likely to map to the same sets in L1 and L2 cache and create a lot of cache conflicts. This problem may be tackled using the bit-reversed permutation as explained in chapter <a class="Reference" href="#chap:C/C++:-Libraries-and">2↑</a>.
</div>
<div class="Indented">
Suppose the data is 2D and arranged in an <span class="formula"><i>M</i> × <i>N</i></span> matrix:<div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>0, 0</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>0, 1</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>0, 2</sub>
</span>
<span class="arraycell align-c">
⋯
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>1, 0</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>1, 1</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>1, 2</sub>
</span>
<span class="arraycell align-c">
⋯
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>2.0</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>2, 1</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>2, 2</sub>
</span>
<span class="arraycell align-c">
⋯
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋱
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span><sub><i>M</i> × <i>N</i></sub>.
</div>
The 2D FFT of this matrix is obtained by applying an FFT of size <span class="formula"><i>M</i></span> to every column followed by an application of an FFT of size <span class="formula"><i>N</i></span> to every row. The structure of the algorithm is similar to the recursive decomposition of a 1D FFT of size <span class="formula"><i>MN</i></span> into FFTs of size <span class="formula"><i>M</i></span> and <span class="formula"><i>N</i></span>, but there are no twiddle factors this time. The 2D structure is imposed by the data not by the algorithm. 
</div>
<div class="Indented">
If the data is in column-major order, the application of 1D FFTs of size <span class="formula"><i>M</i></span> to columns involves no new point. However, the rows are different. If <span class="formula"><i>M</i></span> is a large power of <span class="formula">2</span>, for example, the successive entries in a row are separated by a large power of <span class="formula">2</span> in memory, increasing the risk of cache conflicts. Thus, it is not a good idea to apply 1D FFTs row by row. It is far better to directly invoke library functions for 2D FFT.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose a large array needs to be reversed. What is the best way to split the array between threads, so that most of the entries that are accessed are in near memory? Explain why the <tt>init_manycore()</tt> function would not be suitable for initializing entries.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose a large array needs to be sorted. Would the <tt>init_manycore()</tt> function be a good way to initialize the array to allocate pages in near memory?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Implement an in-place transpose of square matrices using OpenMP. Estimate the bandwidth to memory realized by your program. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Implement an out-of-place FFT for multiple threads (using library functions). Determine the resulting speedup on a machine with multiple cores.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Refer to table <a class="Reference" href="#tab:threads-omp-fourier">5.5↑</a> and determine that the fraction of the peak bandwidth realized is 35% and 28%, respectively, for the <span class="formula">12</span>-core <span class="formula">2.6</span>  GHz SSE2 machine and the <span class="formula">16</span>-core <span class="formula">2.2</span>  GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for information about these machines).
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-5.3">5.3</a> Introduction to Pthreads<a class="Label" name="sec:threads-pthreads"> </a>
</h2>
<div class="Unindented">
So far our discussion of threads has been limited to OpenMP. OpenMP is a limited programming model. It applies mainly to those situations where the data is static and the access patterns are regular. Pthreads are a more powerful programming model.
</div>
<div class="Indented">
The Pthread interface for creating and running threads is supported by the Linux kernel. In Linux, each thread is a separate process. Thread creation is the same as process creation. The distinguishing feature of a group of threads is that they all use the same page tables. The same virtual address generated by different threads in the same group maps to the same physical memory location. Thus, threads offer a seamless way for different processes to share memory. 
</div>
<div class="Indented">
Pthreads are a powerful programming model. The range of programs that can be written using Pthreads goes well beyond scientific computing. In this section, we introduce Pthreads, beginning with the elementary and progressing to more advanced topics. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:pthreads-basic">5.3.1↓</a>, we introduce Pthreads using a simple program to print messages. The two main techniques for achieving mutual exclusion, mutexes and spinlocks, are introduced. Summing the familiar Leibniz series is another example.
</div>
<div class="Indented">
Every thread needs a stack to save its state through a succession of function calls. The user mode stack (explained in the next section) can be used by only one thread (the master thread). In section <a class="Reference" href="#sub:pthreads-basic">5.3.1↓</a>, we explain how a stack is set up for each thread. Thread stacks are normally just a few MB and not large. Therefore, it is important not to abuse the stack in threaded programming. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:pthreads-overhead">5.3.2↓</a>, we determine that the overhead of thread creation is typically around <span class="formula">10<sup>5</sup></span> cycles and can be 10 or 100 times as large on occasion. This number explains the large overhead of the first parallel region an OpenMP program enters. Threads are created the first time an OpenMP program enters a parallel region, and thread creation overhead is a major cost. The cost can even go into the milliseconds.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:pthreads-parallel-regions">5.3.3↓</a> considers several advanced topics. The theme of that section is to implement OpenMP-type parallel regions using Pthreads. Several implementations are explored. If the number of threads in the parallel region remains constant, threads created for the first parallel region can be reused repeatedly. This explains the low cost of a typical OpenMP parallel region compared with the first parallel region.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:pthreads-parallel-regions">5.3.3↓</a> shows the central role played by memory fences and cache coherence in threaded programming. Although the role of cache coherence is invisible to the programmer, unless it is looked for, there can be no multithreaded programming without cache coherence. Propagating writes from cache to cache, when the same cache line is used by multiple threads, can be a considerable source of overhead. Section <a class="Reference" href="#sub:pthreads-parallel-regions">5.3.3↓</a> also shows the way in which TLB flushes can impact threaded programming.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.3.1">5.3.1</a> Pthreads<a class="Label" name="sub:pthreads-basic"> </a>
</h3>
<div class="Unindented">
We begin our discussion of Pthreads with a function that could be in a simple sequential program. The function <tt>print_message()</tt> uses basic C syntax and has no Pthread construct in it. Indeed, it is compiled into assembly instructions as if it were a single threaded program---an important point that has already come up in our discussion of OpenMP. Each Pthread will later take control by executing this program.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void *print_message(void *arg){
<span class="number-left">2</span>	char *s = (char *)arg;
<span class="number-left">3</span>	char ss[400];
<span class="number-left">4</span>	int l = strlen(s);
<span class="number-left">5</span>	for(int i=0; i &lt; l; i++){
<span class="number-left">6</span>		ss[2*i] = s[i];
<span class="number-left">7</span>		ss[2*i+1] = (s[i]==’ ’)?’ ’:’_’;
<span class="number-left">8</span>	}
<span class="number-left">9</span>	ss[2*l] =’\0’;
<span class="number-left">10</span>	printf("%s",s);
<span class="number-left">11</span>	printf("\n");
<span class="number-left">12</span>	printf("%s", ss);
<span class="number-left">13</span>	printf("\n");
<span class="number-left">14</span>	printf("ss = %p \n\n", (void *)ss);
<span class="number-left">15</span>	return NULL;
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The only sign that <tt>print_message()</tt> may have something to do with Pthreads occurs in its first line. It is declared to be a function that takes a single argument of type <tt>void *</tt> and returns a single value also of type <tt>void *</tt>. Whatever arguments we want to send to a Pthread must be packed into a region of memory and sent to the Pthread as a pointer of type <tt>void *</tt>. Any pointer can be cast to the type <tt>void *</tt>.
</div>
<div class="Indented">
The argument to this function is a message or character string. On line 2, this message is recovered by casting it back to type <tt>char *</tt>. 
</div>
<div class="Indented">
The statements that extend from line 3 through line 9 define another string <tt>ss</tt>, and copy the characters in <tt>s</tt> into its even positions. The odd positions are filled with the underscore character except when the preceding even position has a blank character. Line 9 ensures that the string is properly terminated with ’\0’ following C convention. 
</div>
<div class="Indented">
The program prints the string <tt>s</tt>, the string <tt>ss</tt>, and the pointer <tt>ss</tt> on lines 10, 12, and 14, respectively. 
</div>
<div class="Indented">
Presently, we will introduce Pthreads. For the ensuing discussion, the key items are the definition of <tt>ss</tt> on line 3 and the address that is printed on line 14 of <tt>print_message()</tt>.
</div>
<div class="Indented">
Pthreads are created inside <tt>call_pthreads()</tt> whose definition follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#include &lt;pthread.h&gt;
<span class="number-left">2</span>void call_pthreads(){
<span class="number-left">3</span>	pthread_t thread1, thread2, thread3;
<span class="number-left">4</span>	char s[600];
<span class="number-left">5</span>	sprintf(s, "Message for thread1");
<span class="number-left">6</span>	sprintf(s+200, "Message for thread2");
<span class="number-left">7</span>	sprintf(s+400, "Message for thread3");
<span class="number-left">8</span>	pthread_create(&amp;thread1, NULL, print_message, 
<span class="number-left">9</span>		 (void *)s);
<span class="number-left">10</span>	pthread_create(&amp;thread2, NULL, print_message, 
<span class="number-left">11</span>		       (void *)(s+200));
<span class="number-left">12</span>	pthread_create(&amp;thread3, NULL, print_message, 
<span class="number-left">13</span>		       (void *)(s+400));
<span class="number-left">14</span>	void *result;
<span class="number-left">15</span>	pthread_join(thread1, &amp;result);
<span class="number-left">16</span>	pthread_join(thread2, &amp;result);
<span class="number-left">17</span>	pthread_join(thread3, &amp;result);
<span class="number-left">18</span>}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>call_pthreads()</tt> creates three threads and sends a character string as the argument to each of them. The three threads print the message in the character string and return. There are four threads in this program, including the master thread. Each of the four threads will probably run on a different processor core on a processor with <span class="formula">4</span> or more cores.
</div>
<div class="Indented">
On line 3, the program defines variables of the type <tt>pthread_t</tt>. The Pthread library will access administrative information about the threads using these variables.
</div>
<div class="Indented">
The library function <tt>pthread_create()</tt> is used to create threads on lines 8, 10, and 12. The first argument is of type <tt>pthread_t *</tt>. During the call on lines 8 and 9, the pthread library will create a thread and make the variable <tt>thread1</tt> point to information about that thread. Later information about the created thread can be accessed using <tt>thread1</tt>. 
</div>
<div class="Indented">
The second argument to <tt>pthread_create()</tt> will always be <tt>NULL</tt> for us.
</div>
<div class="Indented">
The third argument to <tt>pthread_create()</tt> is <tt>print_message</tt>, which is a function pointer. The third argument is the function that the thread will start executing as soon as it is scheduled to run. The third argument must be a pointer to a function that takes an argument of type <tt>void *</tt> and returns an argument of type <tt>void *</tt>. In this program, all three threads that are created will run the same function.
</div>
<div class="Indented">
The fourth argument to <tt>pthread_create()</tt> must be of type  <tt>void *</tt>. When the thread starts running, it will get this pointer as an argument. In this program, each of the threads gets a different character string as its argument. 
</div>
<div class="Indented">
The library function <tt>pthread_join()</tt> may be used to retrieve the pointer returned by a thread. In this program, the pointer returned by each thread is saved in <tt>result</tt>. The variable <tt>result</tt> is defined on line 14 to be of type <tt>void *</tt>. The threads do not return anything meaningful here. If they did, the pointer <tt>result</tt> could be used to access the data returned by the threads.
</div>
<div class="Indented">
The Pthread library has a fairly transparent syntax. By going over the definition of <tt>call_pthreads()</tt>, it is easy to see where threads are being created and how. 
</div>
<div class="Indented">
C/C++ compilers include Pthread header files by themselves without any prompting. The Pthread library may be linked using the <tt>-lpthread</tt> option.
</div>
<h? class="Subsubsection">
<b><u>Thread stacks</u></b>
</h?>
<div class="Unindented">
Each thread gets its own stack. This stack is used for storing the local variables defined by functions the thread executes. When a thread makes a function call, the return address is pushed onto its stack.
</div>
<div class="Indented">
On line 14, the function <tt>print_message()</tt> prints the value of <tt>ss</tt>, which is a pointer to a location in the thread stack. The three threads print as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">ss = 0x4173bf90
ss = 0x4193cf90
ss = 0x41b9cf90
</pre>
</div>

</div>
<div class="Indented">
These do not look like stack addresses: they do not begin with <tt>0x7fff</tt>, as in section <a class="Reference" href="#sub:memory-virtual-user">4.4.1↑</a>. The addresses are in the region that belongs to dynamic memory. During thread creation, each thread is allocated a thread stack in dynamic memory.
</div>
<div class="Indented">
The difference between values of the address <tt>ss</tt> printed by the three threads is about <span class="formula">2</span> MB, and the maximum thread stack size on the system we used is about <span class="formula">2</span> MB. That is typical. 
</div>
<h? class="Subsubsection">
<b><u>Mutexes and spinlocks</u></b>
</h?>
<div class="Unindented">
The function <tt>print_message()</tt> prints to the terminal, which is shared by all the threads. The message may get garbled as in the following fragment:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Message for thread3Message for thread2
M_e_s_s_a_g_e_  f_o_r_  t_h_r_e_a_d_2_
</pre>
</div>

</div>
<div class="Indented">
Each thread must treat the block of print statements as a critical region. 
</div>
<div class="Indented">
Mutexes are one mechanism for implementing mutual exclusion in the Pthread library. The function below uses a single mutex to ensure that a block of print statements is executed by a single thread at a time.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">pthread_mutex_t mutex=PTHREAD_MUTEX_INITIALIZER;
void *print_message_mutex(void *arg){
	char *s = (char *)arg;
	char ss[400];
	int l = strlen(s);
	for(int i=0; i &lt; l; i++){
		ss[2*i] = s[i];
		ss[2*i+1] = (s[i]==’ ’)?’ ’:’_’;
	}
	ss[2*l] =’\0’;
	pthread_mutex_lock(&amp;mutex);
	printf("%s",s);
	printf("\n");
	printf("%s", ss);
	printf("\n");
	printf("ss = %p \n\n", (void *)ss);
	pthread_mutex_unlock(&amp;mutex);
	return NULL;
}
</pre>
</div>

</div>
<div class="Indented">
The new syntax used in this function is quite simple and hardly requires an explanation. On the first line, a variable <tt>mutex</tt> is defined and initialized. The mutex is locked before entering the block of print statements and unlocked at exit. 
</div>
<div class="Indented">
Another mechanism for mutual exclusion is the spinlock. The spinlock version of the function for printing messages follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">pthread_spinlock_t spinlock;
void *print_message_spinlock(void *arg){
	char *s = (char *)arg;
	char ss[400];
	int l = strlen(s);
	for(int i=0; i &lt; l; i++){
		ss[2*i] = s[i];
		ss[2*i+1] = (s[i]==’ ’)?’ ’:’_’;
	}
	ss[2*l] =’\0’;
	pthread_spin_lock(&amp;spinlock);
	printf("%s",s);
	printf("\n");
	printf("%s", ss);
	printf("\n");
	printf("ss = %p \n\n", (void *)ss);
	pthread_spin_unlock(&amp;spinlock);
	return NULL;
}
</pre>
</div>

</div>
<div class="Indented">
The first line defines a spinlock but does not initialize the spinlock. The spinlock is locked before entering the block of print statements and unlocked after the printing is done. The spinlock is initialized by the master thread as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">pthread_spin_init(&amp;spinlock, PTHREAD_PROCESS_PRIVATE);
</pre>
</div>

</div>
<div class="Indented">
Classical mutexes, which differ in behavior from Pthread mutexes, were introduced during the early days of computers, when most computers had a single processor. When a process attempts to lock a classical mutex but the mutex is already locked, the kernel will put the process to sleep and schedule some other process to run. The sleeping process will be reawakened and scheduled to run when the mutex is available to be locked. 
</div>
<div class="Indented">
Spinlocks may be a preferred way to enforce mutual exclusion if every thread may be assumed to have possession of a processor core. Spinlocks do not relinquish the processor but wait in a loop until the lock is available. Mutual exclusion is achieved by spinlocks using a shared locking variable accessed using atomic instructions. 
</div>
<div class="Indented">
In simple programs, the behavior of Pthread mutexes is closer to spinlocks than classical mutexes. In more complex programs, Pthread mutexes can behave like classical mutexes.
</div>
<h? class="Subsubsection">
<b><u>Partial sums of the Leibniz series</u></b>
</h?>
<div class="Unindented">
Our discussion of Pthreads has been confined to printing messages. We turn to the Leibniz series to illustrate how multiple arguments can be passed and how to retrieve the answer computed by a thread.
</div>
<div class="Indented">
In the code fragment that follows, the number of threads, including the master thread, is fixed at four, with the assumption that the program is run on a quad-core machine. The number may be set equal to the number of cores, whatever that may be, as long as it is even.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">const int NTHREADS = 4;
struct leib_stuff{
	int offset;
	int n;
	double sum;
};
</pre>
</div>

</div>
<div class="Indented">
Each thread is given a pointer to <tt>struct leib_stuff</tt>. The struct holds three items of information. The first item is the offset from which that particular thread must start summing the Leibniz series. The second item is how many terms of the Leibniz series must be summed. The third item is used by each thread to return its part of the partial sum of the Leibniz series. Each thread assumes all terms in its share of the partial sum to be of the same sign. Therefore, the number of threads must be even.
</div>
<div class="Indented">
In the earlier examples, the master thread does not participate with the workers. There is some inherent asymmetry between the master thread and other threads. For example, the master thread makes use of the user mode stack, which none of the other threads can. Yet the ideal is to make all the threads, including the master thread, do exactly the same amount of work. In the present example, the master thread will sum its share of the terms of the Leibniz series. Therefore, only three threads are created explicitly.
</div>
<div class="Indented">
Each of the three created threads gets control with the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void *leibniz(void *arg){
	int offset = ((struct leib_stuff *)arg)-&gt;offset;
	int n = ((struct leib_stuff *)arg)-&gt;n;
	double sum = 0;
	for(int i=offset; i &lt; n; i += NTHREADS)
		sum += 4.0/(2*i+1);
	((struct leib_stuff *)arg)-&gt;sum = sum;
	return arg;
}
</pre>
</div>

</div>
<div class="Indented">
We can read this the way we read a function meant for single-threaded execution. Its peculiarity is in the roundabout way in which arguments are passed and the computed sum is returned. The thread finds out its <tt>offset</tt> and the number of terms in the partial sum <tt>n</tt> from two of the items in <tt>struct leib_stuff</tt> and stuffs its share of the sum into the third item. The argument passed and the value returned are both pointers to that struct.
</div>
<div class="Indented">
The master thread computes its share of the Leibniz sum as well as accumulating the total from all the threads. It is defined below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double leibsum(int nterms){
	pthread_t t[NTHREADS-1];
	struct leib_stuff linfo[NTHREADS-1];
	for(int i=1; i &lt; NTHREADS; i++){
		linfo[i-1].offset = i;
		linfo[i-1].n = nterms;
		pthread_create(&amp;(t[i-1]), NULL, leibniz, 
			       (void *)(linfo+(i-1)));
	}
	double ans = 0;
	for(int i=0; i &lt; nterms; i+=NTHREADS)
		ans += 4.0/(2*i+1);
	void *res;
	pthread_join(t[0], &amp;res);
	ans -= ((struct leib_stuff *)res)-&gt;sum;
	for(int i=2; i &lt; NTHREADS; i+=2){
		pthread_join(t[i-1], &amp;res);
		ans += ((struct leib_stuff *)res)-&gt;sum;
		pthread_join(t[i], &amp;res);
		ans -= ((struct leib_stuff *)res)-&gt;sum;
	}
	return ans;
}
</pre>
</div>

</div>
<div class="Indented">
The master thread here takes on the job of accumulating the final sum. This burden can be more equally distributed by making the threads join with each other in pairs. To implement such a strategy, the array <tt>t[]</tt> of type <tt>pthread_t</tt> must be defined globally and made visible to all the threads.
</div>
<div class="Indented">
In this program, the master thread joins with the other threads to retrieve the results of their computations. Even if a thread returns nothing, the master thread or some other thread must join with each thread that terminates. Otherwise, the threads live on as zombies in the operating system kernel.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.3.2">5.3.2</a> Overhead of thread creation<a class="Label" name="sub:pthreads-overhead"> </a>
</h3>
<div class="Unindented">
To find the cost of creating and destroying Pthreads, we use the following simple function, which each thread will execute:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void *addone(void *arg){
	long *p = (long *)(arg);
	*p += 1;
	return NULL;
}
</pre>
</div>

</div>
<div class="Indented">
This function receives a pointer to <tt>long</tt> and adds <span class="formula">1</span> to the location its argument is pointing to. The function defined below is responsible for creating threads and asking each one of them to execute <tt>addone()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void addone_list(long *list, int nthreads){
	pthread_t *plist = new pthread_t[nthreads-1];
	*list += 1;
	for(int i=0; i &lt; nthreads-1; i++)
		pthread_create(plist+i, NULL, addone, 
		                  (void *)(list+i+1));
	for(int i=0; i &lt; nthreads-1; i++){
		void *result;
		pthread_join(plist[i], &amp;result);
	}
	delete[] plist;
}
</pre>
</div>

</div>
<div class="Indented">
This function was invoked many times with <tt>nthreads</tt> being three. On a <span class="formula">3.4</span>  GHz AVX machine with four cores (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> in the appendix), there are three processes, two for the created threads and one for the master thread, each of which gets a core. The work that each thread does is trivial. During each invocation, most of the cycles are consumed by the creation and destruction of threads.
</div>
<div class="Indented">
In <span class="formula">10<sup>6</sup></span> trials, the first five invocations of <tt>addone_list()</tt> used the following number of cycles:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">364488 
153068 
102000 
220218 
290844
</pre>
</div>

</div>
<div class="Indented">
The worst five invocations of <tt>addone_list()</tt> were as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">1.48340e+07 
1.28682e+07 
7.52992e+06 
5.57948e+06 
5.06503e+06
</pre>
</div>

</div>
<div class="Indented">
The median number of cycles was <span class="formula">78, 242</span>. 
</div>
<div class="Indented">
The typical cost of creating and destroying Pthreads appears to be somewhat less than <span class="formula">10<sup>5</sup></span> cycles. That number is not unreasonable given that each process descriptor used by the kernel is nearly <span class="formula">6</span> KB. The cost of creating threads will vary from system to system, but the numbers are qualitatively the same on many different systems. The <span class="formula">3.4</span>  GHz AVX computer with four cores used here has a single processor package. The cost of creating a thread per core may be expected to be higher on computers with multiple processor packages.
</div>
<div class="Indented">
However, the cost of creating three threads is much more likely to run into millions of cycles on the quad-core <span class="formula">3.4</span>  GHz AVX computer than on a machine with a dozen or more cores, even if the processor cores are split into two packages. A thread is more likely to have to wait for the time quantum of some other process to expire on a quad-core computer than on a computer with <span class="formula">12</span> cores.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.3.3">5.3.3</a> Parallel regions using Pthreads<a class="Label" name="sub:pthreads-parallel-regions"> </a>
</h3>
<div class="Unindented">
This section begins with a simple OpenMP program. The OpenMP program alternates between two parallel regions. In the first parallel region, every thread runs a function called <tt>addone()</tt>. In the second parallel region, every thread runs a function called <tt>addtwo()</tt>.
</div>
<div class="Indented">
Later in the section, the parallel regions are implemented using Pthreads. The first implementation is plain C, except for creating and launching Pthreads. The second and third implementations use spinlocks and mutexes, respectively. The final implementation uses conditional variables.
</div>
<div class="Indented">
The basic idea in all  four implementations is as follows. If the number of threads is <span class="formula"><i>n</i></span>, including the master thread, the master thread begins by creating <span class="formula"><i>n</i> − 1</span> workers. The worker threads do not exit when their job is done but keep waiting for the master to send them some more work. The threads exit only when the master tells them to. 
</div>
<div class="Indented">
Corresponding to the parallel regions of the OpenMP program, the master tells the workers to alternately execute <tt>addone()</tt> and <tt>addtwo()</tt>. The master thread itself alternately executes those two functions.
</div>
<div class="Indented">
A glance at table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↓</a> already throws up a number of questions. The spinlock and mutex implementations look quite similar. That is an artifact of the rather simple setting of our experiment. If the threads were executing a complex function inside the parallel region, the two implementations would look quite different.
</div>
<div class="Indented">
The plain C and the spinlock implementation do not yield the processor core voluntarily. In our setting, that is true for mutexes as well. As a result, all three implementations are highly wasteful when the number of threads is greater than the number of cores. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:pthreads-parallel-regions"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
nthreads
</td>
<td align="center" valign="top">
2
</td>
<td align="center" valign="top">
3
</td>
<td align="center" valign="top">
4
</td>
<td align="center" valign="top">
8
</td>

</tr>
<tr>
<td align="center" valign="top">
OpenMP (gcc/g++)
</td>
<td align="center" valign="top">
1,400
</td>
<td align="center" valign="top">
1,700
</td>
<td align="center" valign="top">
2,100
</td>
<td align="center" valign="top">
40,000
</td>

</tr>
<tr>
<td align="center" valign="top">
Plain C
</td>
<td align="center" valign="top">
720
</td>
<td align="center" valign="top">
975
</td>
<td align="center" valign="top">
1,200
</td>
<td align="center" valign="top">
<span class="formula">8.1 × 10<sup>7</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
Spinlocks (randomized)
</td>
<td align="center" valign="top">
760
</td>
<td align="center" valign="top">
2,000
</td>
<td align="center" valign="top">
4,200
</td>
<td align="center" valign="top">
*
</td>

</tr>
<tr>
<td align="center" valign="top">
Mutexes
</td>
<td align="center" valign="top">
900
</td>
<td align="center" valign="top">
2,200
</td>
<td align="center" valign="top">
6,500
</td>
<td align="center" valign="top">
<span class="formula">9.2 × 10<sup>7</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
Conditional variables
</td>
<td align="center" valign="top">
30,000
</td>
<td align="center" valign="top">
28,000
</td>
<td align="center" valign="top">
9,000
</td>
<td align="center" valign="top">
18,000
</td>

</tr>

</table>

</div>
<div class="caption">
Table 5.6 The cost of entering and exiting a parallel region in cycles. All measurements were made on a <span class="formula">3.4</span> GHz quad-core AVX computer (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for its full name).
</div>

</div>

</div>

</div>
<div class="Indented">
In the conditional variable implementation, the threads yield gracefully. As a result, it is the most efficient when the number of threads is greater than the number of cores. 
</div>
<div class="Indented">
Strangely, the conditional variable implementation is faster with four or eight threads on a quad-core machine than with two threads, which is faster than with one thread (see table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a>). The explanation of this conundrum is important to understand, as it can happen in almost any threaded program. The explanation (TLB flushes) is given later. 
</div>
<div class="Indented">
The gcc/g++ implementation of OpenMP also creates threads only at the point of first entry into the parallel regions. Overall, it looks better than all our implementations. The first OpenMP parallel region that is entered is expensive because threads are created. If the number of threads changes from parallel region to parallel region, the parallel regions will be constantly hit by the same overhead.
</div>
<div class="Indented">
The C implementation highlights the role of cache coherence, which is essential and fundamental to multithreaded programming. Propagating writes from cache to cache can cause significant overhead. The C implementation also introduces memory fences.
</div>
<h? class="Subsubsection">
<b><u>A simple OpenMP program</u></b>
</h?>
<div class="Unindented">
The two OpenMP parallel regions, in the program that will be listed shortly, alternately call the following two functions:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void addone(void *arg){
	long *p = (long *)(arg);
	*p += 1;
}
​
void addtwo(void *arg){
	long *p = (long *)(arg);
	*p += 2;
}
</pre>
</div>

</div>
<div class="Indented">
The functions receive a pointer to a <tt>long</tt> cast to <tt>void *</tt>, dereference it, and then add either <span class="formula">1</span> or <span class="formula">2</span> to the <tt>long</tt> location. It would of course be simpler to add <span class="formula">1</span> and <span class="formula">2</span> directly without having to call functions that work through pointers. The OpenMP parallel regions invoke these functions to preserve a close analogy to the Pthread implementations. The Pthread implementations also use these two functions.
</div>
<div class="Indented">
Here is the OpenMP program with two parallel regions.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void ompmaster(long *list, int nthreads, int count){
	for(int i=0; i &lt; count/2;i++){
#pragma omp parallel					\
	num_threads(nthreads)				\
	default(none)					\
	shared(list)
		{
			int j = omp_get_thread_num();
			addone((void *)(list+j));
		}
#pragma omp parallel					\
	num_threads(nthreads)				\
	default(none)					\
	shared(list)
		{
			int j = omp_get_thread_num();
			addtwo((void *)(list+j));
		}
	}
}
</pre>
</div>

</div>
<div class="Indented">
This program has two parallel regions, one of which calls <tt>addone()</tt> to add <span class="formula">1</span> to an entry of <tt>list[]</tt>. Each thread adds to the entry whose index is the same as its thread id. The thread id is returned by <tt>omp_get_thread_num()</tt>. The other parallel region adds <span class="formula">2</span> to the same entry using <tt>addtwo()</tt>. Because the parallel regions are in a for-loop that is iterated <tt>count/2</tt> times, the effect of a single call to <tt>ompmaster()</tt> is to add <tt>3*count/2</tt> to as many entries of <tt>list[]</tt> as there are threads.
</div>
<h? class="Subsubsection">
<b><u>Parallel regions in plain C</u></b>
</h?>
<div class="Unindented">
The first implementation of parallel regions we consider makes minimal use of Pthreads. Most of it is in plain C. 
</div>
<div class="Indented">
The following global definitions are the basis of the C implementation of parallel regions:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">typedef void (*fnlist_t[nthreads])(void *);
typedef void *arglist_t[nthreads];
volatile fnlist_t fnlist;
volatile arglist_t arglist;
volatile long work_count[nthreads];
volatile long done_count[nthreads];
</pre>
</div>

</div>
<div class="Indented">
This code segment defines two types, <tt>fnlist_t</tt> and <tt>arglist_t</tt>, and four variables. Here <tt>nthreads</tt>, whose definition is not shown, is a <tt>const int</tt> equal to the number of threads. For example, <tt>nthreads</tt> is <span class="formula">3</span> if the number of threads is <span class="formula">3</span>. 
</div>
<div class="Indented">
The type <tt>fnlist_t</tt> is an array (of size <tt>nthreads</tt>) of pointers to functions with a single argument of type <tt>void *</tt> and returning <tt>void</tt>. The syntax for such complex types can be gotten right with a little trial and error. The type <tt>arglist_t</tt> is an array (of size <tt>nthreads</tt>) pointers to <tt>void</tt>. 
</div>
<div class="Indented">
All four of the variables <tt>fnlist</tt>[], <tt>arglist</tt>[], <tt>work_count[]</tt>, and <tt>done_count[]</tt> are defined to be <tt>volatile</tt>. The <tt>volatile</tt> qualifier is a message to the compiler that a variable may change unexpectedly, and the usual dependency analysis may not be valid. It prevents the compiler from saving variables in registers and  carrying out other optimizations. In general, if a variable is declared <tt>volatile</tt>, the compiler assumes that the value of the variable can change unexpectedly because of some other thread, processor, or device. 
</div>
<div class="Indented">
These four <tt>volatile</tt> variables are used by the worker threads to communicate with the manager thread. The manager thread sets <tt>fnlist[tid]</tt> and <tt>arglist[tid]</tt> to tell the thread of identifier <tt>tid</tt> which function it should execute and what argument should be passed to it. For example, <tt>fnlist[tid]</tt> is set to <tt>addone</tt> to ask the worker thread with identifier <tt>tid</tt> to execute the <tt>addone()</tt> function defined earlier in this section. 
</div>
<div class="Indented">
The manager thread uses the array entry <tt>work_count[tid]</tt> to tell the worker thread of identifier <tt>tid</tt> how many units of work have been assigned to it from the beginning. The worker thread uses <tt>done_count[tid]</tt> to tell the manager how many units of work it has completed. 
</div>
<div class="Indented">
Making the four arrays used for communication between the manager thread and the workers <tt>volatile</tt> ensures that every single reference generates a load or store instruction. In particular, the compiler will not assign any of these items to a register, which would get in the way of communication between the manager and the workers.<span class="FootOuter"><span class="SupFootMarker"> [89] </span><span class="HoverFoot"><span class="SupFootMarker"> [89] </span>The <tt>volatile</tt> qualifier suppresses compiler optimizations. It does not ensure any kind of mutual exclusion. See <a class="FlexURL" href="https://www.kernel.org/doc/Documentation/volatile-considered-harmful.txt">https://www.kernel.org/doc/Documentation/volatile-considered-harmful.txt</a></span></span> 
</div>
<div class="Indented">
Every thread created using <tt>pthread_create()</tt> begins execution with a function that takes a pointer as an argument and returns a pointer. The worker threads begin execution with the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void *worker(void *arg){
<span class="number-left">2</span>	int tid = *((int *)arg);
<span class="number-left">3</span>	while(1){
<span class="number-left">4</span>		while(work_count[tid] == done_count[tid]);
<span class="number-left">5</span>		if(work_count[tid]==done_count[tid]+1){
<span class="number-left">6</span>			(*(fnlist[tid]))(arglist[tid]);
<span class="number-left">7</span>			asm volatile("mfence");
<span class="number-left">8</span>			done_count[tid] += 1;
<span class="number-left">9</span>		}
<span class="number-left">10</span>	}
<span class="number-left">11</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 2, the pointer <tt>arg</tt> is cast to <tt>int *</tt> and dereferenced to recover <tt>tid</tt>. In the Pthread library, the responsibility of assigning <tt>tid</tt> rests with the programmer. Lines 3 to 10 form a while-loop, which always tests positive. On line 4, the worker loops as long as the work count is equal to the count of items it has already completed. The compiler generates load instructions for <tt>work_count[tid]</tt> and <tt>done_count[tid]</tt> because they are volatile locations. The manager signals work to be done by incrementing <tt>work_count[tid]</tt>. Thus, on line 5, the worker enters an if-block to do work that has been assigned to it. 
</div>
<div class="Indented">
Line 6 is where the worker thread does the work assigned to it. On line 6, <tt>fnlist[tid]</tt> is the function pointer assigned to this worker. The worker dereferences that pointer as in 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">(*(fnlist[tid]))
</pre>
</div>

</div>
<div class="Indented">
and applies it to <tt>arglist[tid]</tt>, which is the <tt>void *</tt> argument assigned to it by the manager thread. 
</div>
<div class="Indented">
On line 8, the worker thread signals that the unit of work assigned to it is complete by incrementing <tt>done_count[tid]</tt>. The memory fence instruction <tt>MFENCE</tt>, which occurs on line 7, is essential to the correctness of this program. On line 6, the worker thread launches the function it is told to execute, and this function leads to a potentially long instruction stream. After returning from the function, the worker increments <tt>done_count[tid]</tt>, but the processor may look ahead in the instruction stream and increment even before the function returns. Such a thing would be valid in a single-threaded program but would corrupt the communication with the manager thread here. The memory fence on line 7 ensures that all load and store instructions that occur before it are complete before any loads and stores after it are issued. It is a serializing instruction for memory references. Here the memory fence makes sure that the processor does not jump ahead and signal completion of work too early.
</div>
<div class="Indented">
The worker threads spin in place (on line 4), do work (line 6), and go back to spinning in place to wait for work to be assigned. How do the threads terminate? To terminate a worker, the manager sets the function pointer <tt>fnlist[tid]</tt> to the <tt>exitfn</tt>. The exit function is defined below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void exitfn(void *arg){
	pthread_exit(NULL);
}
</pre>
</div>

</div>
<div class="Indented">
To keep things simple, the worker threads do not return anything. They could return a pointer at the point of exit, which is <tt>pthread_exit()</tt> in this case. If some other thread joins to this one, it can pick up the returned pointer when it joins (as in the <tt>leibsum()</tt> example).
</div>
<div class="Indented">
The manager creates threads, assigns work to worker threads, does its own share of the work, and shuts down the worker threads. It uses the following function to create threads:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void spawn_workers(){
<span class="number-left">2</span>	pthread_t plist[nthreads-1];
<span class="number-left">3</span>	static int tidlist[nthreads];
<span class="number-left">4</span>	for(int i=0; i &lt; nthreads; i++){
<span class="number-left">5</span>		work_count[i] = 0;
<span class="number-left">6</span>		done_count[i] = 0;
<span class="number-left">7</span>		tidlist[i] = i;
<span class="number-left">8</span>		if(i&gt;0){
<span class="number-left">9</span>			pthread_create(plist+i-1, NULL, worker, 
<span class="number-left">10</span>				       (void *)(tidlist+i));
<span class="number-left">11</span>			pthread_detach(plist[i-1]);
<span class="number-left">12</span>		}
<span class="number-left">13</span>	}
<span class="number-left">14</span>}
</pre>
</div>

</div>
<div class="Indented">
 The manager thread uses the array <tt>tidlist[]</tt> defined on line 3 and initialized on line 6 to pass the thread identifier to the workers. Notice that threads are created only if <tt>i&gt;0</tt> (lines 8 through 12) because the manager thread has identifier <span class="formula">0.</span> The initialization on lines 5, 6, and 7 is complete before the thread is created on lines 9 and 10, as it must be. 
</div>
<div class="Indented">
There are two new elements in this function that merit comment. Why is <tt>tidlist[] </tt>defined on line 3 specified to be <tt>static</tt>? Variables defined to be <tt>static</tt> persist in memory even after the function exits (and may be reused when the function returns). The function <tt>spawn_workers()</tt> may exit before the worker threads start executing, in which case the storage allocated to <tt>tidlist[]</tt> may disappear before the threads access it. The <tt>static</tt> specifier ensures that the storage persists in memory. 
</div>
<div class="Indented">
The other new bit of syntax is <tt>pthread_detach()</tt> on line 11. When a thread is detached after creation, it is no longer joinable. The operating system kernel discards detached threads after they return. If a thread is not detached, the threads are kept alive until the manager or some other thread joins with them. If a thread that is not detached returns but no other thread joins with it, it becomes a zombie. 
</div>
<div class="Indented">
The manager thread executes the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void manager(long *list, int count){
<span class="number-left">2</span>	spawn_workers();
<span class="number-left">3</span>	for(int i=0; i &lt; count; i++){
<span class="number-left">4</span>		for(int j=0; j &lt; nthreads; j++){
<span class="number-left">5</span>			fnlist[j] = (i%2==0)?addone:addtwo;
<span class="number-left">6</span>			arglist[j] = (void *)(list+j);
<span class="number-left">7</span>			asm volatile("mfence");
<span class="number-left">8</span>			work_count[j] += 1;
<span class="number-left">9</span>		}
<span class="number-left">10</span>		(*(fnlist[0]))(arglist[0]);
<span class="number-left">11</span>		done_count[0] += 1;
<span class="number-left">12</span>		for(int j=0; j &lt; nthreads; j++)
<span class="number-left">13</span>			while(work_count[j]&gt;done_count[j]);
<span class="number-left">14</span>	}
<span class="number-left">15</span>	shutdown_workers();
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The function assigned to the worker thread (with tid equal to <tt>j</tt>) on line 5 is either <tt>addone()</tt> or <tt>addtwo()</tt>, as in the OpenMP example. The memory fence on line 7 separates the assignment of work to a thread (lines 5 and 6) from the statement that signals assignment of work (line 8). 
</div>
<div class="Indented">
The manager does its own share of work on line 11. 
</div>
<div class="Indented">
The while-loop on line 13 spins in place as long as thread <span class="formula"><i>j</i></span> is busy. Each iteration of the for-loop from lines 3 to 14 counts as one parallel region.
</div>
<div class="Indented">
For completeness, we list the function for shutting down workers.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void shutdown_workers(){
	for(int i=1; i &lt; nthreads; i++){
		fnlist[i] = exitfn;
		asm volatile("mfence");
		work_count[i] += 1;
	}
}
</pre>
</div>

</div>
<div class="Indented">
The manager does not join with any of the worker threads after it tells them to shut down. The worker threads were detached earlier. If the worker threads were not detached, the manager must join with the worker threads to prevent them from turning into zombies.
</div>
<h? class="Subsubsection">
<b><u>Cache coherence and the cost of propagating writes</u></b>
</h?>
<div class="Unindented">
Table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a> shows that the plain C implementation we have described takes only <span class="formula">720</span> cycles per parallel region on an average when the number of threads is two. Why does each parallel region take <span class="formula">720</span> cycles? In answering that question, we run into a vital part of the hardware infrastructure for supporting threaded programming, namely, the cache coherence protocols.
</div>
<div class="Indented">
The worker threads and the manager are exchanging information using the arrays <tt>work_count[]</tt>, <tt>done_count[]</tt>, <tt>fnlist[]</tt>, and <tt>arglist[]</tt>. The locations of these arrays are specified to be volatile. So we may think of the array entries as residing in DRAM memory, but that is not really correct. In a simple program such as this, the array entries are certain to be in L1 cache of each thread. That brings to light a new issue. Suppose the manager increases the <tt>work_count[]</tt> of a worker thread. The increment will take place in its own L1 cache. But when the worker accesses the same entry, it will look up its own L1 cache. How do writes propagate from L1 cache to L1 cache?
</div>
<div class="Indented">
The manner in which writes propagate from cache to cache is vital for the validity of threaded programs. Much of the time we must try to make each thread work on its own portion of the memory and keep the threads as independent as possible. But threads cannot synchronize without shared memory. Because memory is mirrored in caches, any method of synchronization using shared memory is dependent on the manner in which writes propagate from cache to cache.
</div>
<div class="Indented">
Many computers, such as those listed in table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> of the appendix, handle writes to cache as follows. Suppose a processor wants to write to a shared cache line. Before the write is complete, the hardware sends a signal to other caches to invalidate their copy of the same cache line. If the other caches do not hold a copy of the same cache line, nothing needs to be done. Indeed, the hardware stores sharing information for each cache line. If a cache line is exclusive, the protocol for invalidating other copies of the cache line will be omitted. If other copies need to be invalidated, the write to cache is not complete until all duplicate copies have been invalidated. When another processor wants to read the same cache line, a cache-to-cache transfer is triggered.
</div>
<div class="Indented">
It is possible that two or more processors may attempt to write to a cache line that is duplicated in L1 caches belonging to each of them. If so, the hardware resolves the race condition so that one of the processors wins. The hardware implementation of cache coherence protocols has been stated to be a &ldquo;major complication.&rdquo; The degree of complication increases as the number of processors that share memory increases. Although the cache coherence protocols are complicated, they are indispensable for shared memory programming.
</div>
<div class="Indented">
The L1 cache to L1 cache transfers are a significant cost in our implementation of parallel regions using plain C. On the <span class="formula">3.4</span>  GHz quad-core AVX computer, the cost for three threads is <span class="formula">975</span> cycles and for four threads 1,200. The cost is so low thanks to excellent implementation of cache coherence by the hardware. When the number of threads is <span class="formula">8</span> on this quad-core computer, the cost jumps to more than <span class="formula">10<sup>7</sup></span> cycles, however (see table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a>). This high cost is due to dependence on context switches triggered by timer interrupts and reflects the time quantum assigned to each process by the Linux kernel.
</div>
<h? class="Subsubsection">
<b><u>Parallel regions using spinlocks</u></b>
</h?>
<div class="Unindented">
Here we use spinlocks in the Pthread library to implement parallel regions in which threads alternately add <span class="formula">1</span> or <span class="formula">2</span> to entries of a list.
</div>
<div class="Indented">
The following variable definitions are global:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void (*fnlist[nthreads-1])(void *);
void *arglist[nthreads-1];
int workflag[nthreads-1];
pthread_t pthrd[nthreads-1];
pthread_spinlock_t spin[nthreads-1];
</pre>
</div>

</div>
<div class="Indented">
As in the plain C implementation, <tt>fnlist[]</tt> and <tt>arglist[]</tt> are arrays used by the manager thread to tell the worker threads which functions they must execute on which argument. In the plain C implementation, the manager thread updated a count of units of work assigned to each thread, and each worker thread updated a count of the number of units of work it had completed. In this implementation, the manager and workers synchronize using the <tt>workflag[]</tt> array, each entry of which is <span class="formula">0</span> or <span class="formula">1</span>. The manager sets <tt>workflag[tid]</tt> to tell thread <tt>tid</tt> that it has work to do, and the worker sets the same flag to <span class="formula">0</span> after it has completed the work. 
</div>
<div class="Indented">
The array <tt>pthrd[]</tt> is used for creating the worker threads. The manager thread must not assign work when a worker is busy. The spinlocks used to enforce mutual exclusion between the manager assigning work and the workers are in the array <tt>spin[]</tt>.
</div>
<div class="Indented">
None of the global variables is specified to be volatile.
</div>
<div class="Indented">
The worker threads begin execution with the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void *worker(void *arg){
<span class="number-left">2</span>	int tid = *((int *) arg);
<span class="number-left">3</span>	while(1){
<span class="number-left">4</span>		pthread_spin_lock(spin+tid-1);
<span class="number-left">5</span>		if(workflag[tid-1]==1){
<span class="number-left">6</span>			(*(fnlist[tid-1]))(arglist[tid-1]);
<span class="number-left">7</span>			workflag[tid-1] = 0;
<span class="number-left">8</span>		}
<span class="number-left">9</span>		pthread_spin_unlock(spin+tid-1);
<span class="number-left">10</span>	}
<span class="number-left">11</span>	return NULL;
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
The spinlock <tt>spin[tid-1]</tt> is used for mutual exclusion between the worker thread <tt>tid</tt> and the manager whose <tt>tid</tt> is <span class="formula">0</span>. The worker repeatedly locks (line 4) and unlocks (line 9). Every access of <tt>workflag[tid-1]</tt> (lines 5 and 7) is protected inside the lock. 
</div>
<div class="Indented">
The exit function, whose pointer is passed by the manager to shut down a worker, has the following definition:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void exitfn(void *arg){
	int tid = *((int *) arg);
	pthread_spin_unlock(spin+tid-1);
	pthread_exit(NULL);
}
</pre>
</div>

</div>
<div class="Indented">
The exit function must unlock as it is invoked by the worker thread after gaining the lock.
</div>
<div class="Indented">
The manager uses the following function to assign a single unit of work to worker thread <tt>tid</tt>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void assignwork(int i, int tid, long *list){
<span class="number-left">2</span>	int j = tid-1;
<span class="number-left">3</span>	while(1){
<span class="number-left">4</span>		pthread_spin_lock(spin+j);
<span class="number-left">5</span>		if(workflag[j]==0){
<span class="number-left">6</span>			fnlist[j] = (i%2==0)?addone:addtwo;
<span class="number-left">7</span>			arglist[j] = (void *)(list+tid);
<span class="number-left">8</span>			workflag[j]=1;
<span class="number-left">9</span>			pthread_spin_unlock(spin+j);
<span class="number-left">10</span>			return;
<span class="number-left">11</span>		}
<span class="number-left">12</span>		pthread_spin_unlock(spin+j);
<span class="number-left">13</span>	}
<span class="number-left">14</span>}
</pre>
</div>

</div>
<div class="Indented">
Lines 5 through 11 are protected by a spinlock, and in this region the manager attempts to assign a single unit of work to the worker thread <tt>tid</tt>. If the worker is not busy (line 5), it assigns a single unit of work (lines 6, 7, and 8) and unlocks (line 9) before returning (line 10).
</div>
<div class="Indented">
None of the global variables is specified to be volatile. So we may wonder whether the worker thread will see the values assigned to the entries <tt>fnlist[j]</tt>, <tt>arglist[j]</tt>, and <tt>workflag[j]</tt> by the manager thread. The processor may execute the assignments out of order, yet we have not included a memory fence to force a consistent view of memory. Is this a correct program? It is in fact correct because of the memory model of the Pthread library. 
</div>
<div class="Indented">
The POSIX specification of memory synchronization between threads is a little vague. But the standard does require memory sync during locking and unlocking. The Pthread library must insert a memory fence or another equivalent instruction during locking and unlocking. The user is freed from that responsibility. Memory fences alone are not sufficient. The compiler may decide to store some of the variables in registers. In that case, the compiler must issue store instructions and write registers to memory before a memory sync. 
</div>
<div class="Indented">
For completeness, we give the definition of the function invoked by the manager thread to assign work to worker threads and do its own share of the work.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void manager(long *list, int count){
	spawn_workers();
	for(int i=0; i &lt; count; i++){
		for(int tid=1; tid &lt; nthreads; tid++)
			assignwork(i, tid, list);
		if(i%2==0)
			addone((void *)list);
		else
			addtwo((void *)list);
	}
	shutdown_workers();
}
</pre>
</div>

</div>
<div class="Indented">
The manager creates worker threads, assigns work, does its own share of the work, and finally shuts down the worker thread before exiting. The definitions of functions that spawn and shut down workers are omitted, as they involve no new point.
</div>
<h? class="Subsubsection">
<b><u>Fairness and the cost of spinlock implementation</u></b>
</h?>
<div class="Unindented">
What is the average cost of a parallel region in the spinlock implemenation on the <span class="formula">3.4</span>  GHz quad-core AVX machine? To get the answer given in table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a>, we have to dig deeper into spinlocks and modify our implementation.. The answer is much worse with our current implementation. On three different runs, we got the following three figures for the average cost in cycles:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">1641
120361
1190550
</pre>
</div>

</div>
<div class="Indented">
Each average was computed over at least a million trials. The cost of a spinlock varies so unpredictably because the spinlocks are not guaranteed to be fair. If one thread gains a lock and another thread fails to gain it, it is fair to expect that the second thread must have priority after the first thread releases the lock. But spinlocks in the Pthread library do not guarantee such fairness,<span class="FootOuter"><span class="SupFootMarker"> [90] </span><span class="HoverFoot"><span class="SupFootMarker"> [90] </span>For a rigorous discussion of fairness in locking, see <span class="bibcites">[<a class="bibliocite" name="cite-45" href="#biblio-45"><span class="bib-index">45</span></a>]</span>.</span></span> and the highly erratic performance we see is an undesirable consequence.
</div>
<div class="Indented">
When the manager thread wants to assign work, it repeatedly locks and unlocks until it can assign work. Once the master thread does its share of the work, it goes back to locking and unlocking repeatedly. The worker thread repeatedly locks and unlocks while performing work assigned to it. Yet the efficient performance of this implementation of parallel regions depends on alternation between the manager and the worker. If the spinlock is fair, we may expect them to alternate in a reasonable fashion, but that is not the case here. One of the processors may have faster access to the lock variable, for example, because it has a faster route to cache. In such a scenario, the spinlock will be unfair, and either the manager or the worker may monopolize the lock, resulting in the sort of erratic performance we are seeing.
</div>
<div class="Indented">
We can insert a little random wait time to obtain an imperfect kind of fairness, which is still enough to show how significant it can be. The definition of <tt>worker()</tt> is modified as follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void *worker(void *arg){
<span class="number-left">2</span>	int tid = *((int *) arg);
<span class="number-left">3</span>	srand((tid+1)*28887);
<span class="number-left">4</span>	while(1){
<span class="number-left">5</span>		pthread_spin_lock(spin+tid-1);
<span class="number-left">6</span>		if(workflag[tid-1]==1){
<span class="number-left">7</span>			(*(fnlist[tid-1]))(arglist[tid-1]);
<span class="number-left">8</span>			workflag[tid-1] = 0;
<span class="number-left">9</span>		}
<span class="number-left">10</span>		pthread_spin_unlock(spin+tid-1);
<span class="number-left">11</span>		int rcount = rand()%TIEBREAK;
<span class="number-left">12</span>		for(int i=0; i &lt; rcount; i++)
<span class="number-left">13</span>			dummy();
<span class="number-left">14</span>	}
<span class="number-left">15</span>	return NULL;
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
This definition of <tt>worker()</tt> is a randomized version of the definition given earlier. The random number generator on each thread is seeded differently (line 3). A random integer between <span class="formula">0</span> and <tt>TIEBREAK</tt> is generated (line 11). TIEBREAK is a constant whose definition is not shown. On lines 12 and 13, the worker thread makes a random number of calls to <tt>dummy()</tt>, which is a function defined in another compilation unit that does nothing. The objective of inserting a random number of calls is to introduce a random wait time between unlocking and the next attempt to lock.
</div>
<div class="Indented">
We do not show the modified version of <tt>assignwork()</tt> because the modifications are practically identical. Once again, the point is to insert a random wait between unlocking and the next attempt to lock to prevent one thread from monopolizing the lock.
</div>
<div class="Indented">
In table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a>, <tt>TIEBREAK</tt> was <span class="formula">10</span>, <span class="formula">400</span>, and <span class="formula">600</span> for <span class="formula">2</span>, <span class="formula">3</span>, and <span class="formula">4</span> threads, respectively. Spinlocking is less efficient than the plain C implementation, even after randomization. The performance is highly sensitive to the choice of <tt>TIEBREAK</tt>, which is one of the many imperfections in our attempt to ensure fairness.
</div>
<div class="Indented">
Our implementation of parallel regions has highlighted the unfairness of spinlocks. Fairness is of so much consequence here because the same threads repeatedly lock and unlock the same locking variable. It must be noted, however, that fairness is not always essential or of so much consequence. For example, when an interrupt handler gains control, it typically uses a spinlock to do a little bit of work while ensuring access to shared data structures is protected by mutual exclusion. It releases the lock soon after, and the use of the same spinlock is only occasional. 
</div>
<div class="Indented">
When spinlocks are used, it is always a good idea to ensure that the thread is likely to be running and not waiting for a processor. 
</div>
<h? class="Subsubsection">
<b><u>Parallel regions using mutexes</u></b>
</h?>
<div class="Unindented">
The implementation of parallel regions using mutexes is  similar to that using spinlocks. There is a close parallel in the Pthread library between functions for handling mutexes and functions for handling spinlocks, as we have already seen.
</div>
<div class="Indented">
The global variables in the mutex implementation of parallel regions are as follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void (*fnlist[nthreads-1])(void *);
void *arglist[nthreads-1];
int workflag[nthreads-1];
pthread_mutex_t mtx[nthreads-1];
pthread_t pthrd[nthreads-1];
</pre>
</div>

</div>
<div class="Indented">
The only difference here is that the array of locks <tt>mtx[]</tt> is of type <tt>pthread_mutex_t</tt> and not <tt>pthread_spin_t</tt>.
</div>
<div class="Indented">
Below we list the definitions of <tt>worker()</tt> and <tt>assignwork()</tt> in the mutex implementation. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void *worker(void *arg){
	int tid = *((int *) arg);
	while(1){
		pthread_mutex_lock(mtx+tid-1);
		if(workflag[tid-1]==1){
			(*(fnlist[tid-1]))(arglist[tid-1]);
			workflag[tid-1] = 0;
		}
		pthread_mutex_unlock(mtx+tid-1);
	}
	return NULL;
}
​
void assignwork(int i, int tid, long *list){
	int j = tid-1;
	while(1){
		pthread_mutex_lock(mtx+j);
		if(workflag[j]==0){
			fnlist[j] = (i%2==0)?addone:addtwo;
			arglist[j] = (void *)(list+tid);
			workflag[j]=1;
			pthread_mutex_unlock(mtx+j);
			return;
		}
		pthread_mutex_unlock(mtx+j);
	}
}
</pre>
</div>

</div>
<div class="Indented">
The Pthread library functions
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">pthread_spin_lock()  and pthread_spin_unlock()
</pre>
</div>

</div>
<div class="Indented">
have been replaced by
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">pthread_mutex_lock() and pthread_mutex_unlock()
</pre>
</div>

</div>
<div class="Indented">
and that is all. 
</div>
<div class="Indented">
During locking and unlocking of mutexes, the local view of memory is serialized and made visible to other threads using a memory fence, and the compiler takes care to commit variables stored in registers to memory. None of the global variables needs to be specified as volatile.
</div>
<div class="Indented">
We omit the definitions of <tt>spawn_workers()</tt> and <tt>shutdown_workers()</tt> as before.
</div>
<h? class="Subsubsection">
<b><u>Cost of mutex implementation</u></b>
</h?>
<div class="Unindented">
Table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a> shows that the average cost of a parallel region implemented using mutexes is <span class="formula">900</span> cycles with two threads, 2,200 cycles with three threads, and 6,500 cycles with four threads on the quad-core <span class="formula">3.4</span>  GHz AVX computer. The cost is comparable to and slightly worse than the spinlock implementation. The plain C implementation is much faster. 
</div>
<div class="Indented">
If mutexes were implemented in the classical manner, a process that attempts to gain a mutex lock that is not available is put to sleep by the operating system. When the lock is released, one of the sleeping processes in the queue for that mutex lock is woken up. With classical mutexes, the average cost of a parallel region would be more. 
</div>
<div class="Indented">
Mutexes in the Pthread library on Linux are not implemented in the classical manner. They are implemented using futexes, which are supported by Linux system calls. Mutexes implemented using futexes do not enter the kernel at all if there is not much contention. In our setting, each thread that gains the lock releases it so quickly that the contending thread is rarely put to sleep. If a thread holds onto a lock for a long time, a Pthread mutex will put the contending thread to sleep.
</div>
<h? class="Subsubsection">
<b><u>Parallel regions using conditional variables</u></b>
</h?>
<div class="Unindented">
Using Pthread conditional variables, a more predictable alternation between the manager thread assigning work and the worker threads performing that work can be obtained. When conditional variables are used, the manager thread sends a signal to a worker when work has been assigned to it. A worker thread does not repeatedly lock and unlock a mutex when it is looking for work. Instead, it waits for a signal. Correspondingly, the worker thread sends a signal to the manager when it has completed that work, which means that the manager thread does not repeatedly lock and unlock to check whether a worker thread is free.
</div>
<div class="Indented">
The global variables in the conditional variables implementation of parallel regions are the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void (*fnlist[nthreads])(void *);
void *arglist[nthreads];
int workflag[nthreads-1];
pthread_mutex_t wrklock[nthreads-1];
pthread_cond_t cv_work[nthreads-1];
pthread_cond_t cv_free[nthreads-1];
pthread_t pthrd[nthreads-1];
</pre>
</div>

</div>
<div class="Indented">
The variables <tt>fnlist[]</tt>, <tt>arglist[]</tt>, <tt>workflag[]</tt>, and <tt>pthrd[]</tt> have the same meaning as in the mutex implementation. As before, the manager sets <tt>fnlist[tid]</tt> and <tt>arglist[tid]</tt> to let worker thread <tt>tid</tt> know which function to execute on which argument. As in all implementations of parallel regions considered in this section, the workers are made to alternate between <tt>addone()</tt> and <tt>addtwo()</tt>. The exit function has the following definition:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void exitfn(void *arg){
	int tid = *((int *) arg);
	pthread_mutex_unlock(wrklock+tid-1);	
	pthread_exit(NULL);
}
</pre>
</div>

</div>
<div class="Indented">
Because a worker thread executes <tt>exitfn()</tt>, as well as other functions it is told to execute, after locking the mutex <tt>wrklock[tid]</tt>, it must unlock before it exits. The conditional variables <tt>cv_work[]</tt> are used by the manager to tell the workers that there is work to do. The conditional variables <tt>cv_free[]</tt> are used by the workers to tell the manager that they have completed the work assigned to them. Conditional variables are always used in coordination with mutexes. The conditional variables <tt>cv_work[]</tt> and <tt>cv_free[]</tt> coordinate with the mutexes <tt>wrklock[]</tt>. 
</div>
<div class="Indented">
The worker threads gain control with the following function: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void *worker(void *arg){
<span class="number-left">2</span>   int tid = *((int *)(arg));
<span class="number-left">3</span>   while(1){
<span class="number-left">4</span>      pthread_mutex_lock(wrklock+tid-1);
<span class="number-left">5</span>      if(workflag[tid-1]==0)
<span class="number-left">6</span>         pthread_cond_wait(cv_work+tid-1, wrklock+tid-1);
<span class="number-left">7</span>      (*(fnlist[tid-1]))(arglist[tid-1]);
<span class="number-left">8</span>      workflag[tid-1]=0;
<span class="number-left">9</span>      pthread_cond_signal(cv_free+tid-1);
<span class="number-left">10</span>      pthread_mutex_unlock(wrklock+tid-1);
<span class="number-left">11</span>   }
<span class="number-left">12</span>   return NULL;
<span class="number-left">13</span>}
</pre>
</div>

</div>
<div class="Indented">
The worker thread locks <tt>wrklock[tid-1]</tt>, the mutex corresponding to worker <tt>tid</tt>, on line 4. On line 5, it checks <tt>workflag[tid-1]</tt>, a region of shared memory accessed by both the worker and the manager, as soon as it gains the lock. If there is no work to be done, it waits on a conditional variable on line 6.
</div>
<div class="Indented">
The library function <tt>pthread_cond_wait()</tt> takes a pointer to a conditional variable as well as a pointer to a mutex as an argument. It <i>unlocks</i> the mutex and puts the thread to sleep---atomically with respect to any attempt to access either the mutex or the conditional variable by some other thread. When the conditional variable is signaled by some other thread, the sleeping thread is ready to be woken up, but it must lock the mutex before waking up.
</div>
<div class="Indented">
The unlocking of the mutex before putting the thread to sleep during <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">pthread_cond_wait()
</pre>
</div>

</div>
<div class="Indented">
implies that the worker thread does not contend for the lock when it is looking for work. This unlocking is the central element in the semantics of conditional variables. 
</div>
<div class="Indented">
On line 7, the worker thread performs the work assigned to it, and on line 8, it sets an entry of the <tt>workflag[]</tt> to indicate it is free. On line 9, a signal is sent indicating that the worker thread is free, and the mutex is freed on line 10. The Pthread memory model requires a memory sync at each of the library functions in the while block from lines 3 through 11. Thus, we may be sure that the work is indeed complete and the <tt>workflag[]</tt> entry is appropriately modified in memory before the <tt>cv_free</tt> signal and unlocking of the associated mutex.
</div>
<div class="Indented">
The corresponding function executed by the manager thread is as defined below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void manager(long *list, int count){
   for(int i=0; i &lt; count;i++){
      for(int j=0; j &lt; nthreads-1; j++){
         pthread_mutex_lock(wrklock+j);
         if(workflag[j]==1)
            pthread_cond_wait(cv_free+j, wrklock+j);
         fnlist[j] = (i%2==0)?addone:addtwo;
         arglist[j] = (void *)(list+j+1);
         workflag[j]=1;
         pthread_cond_signal(cv_work+j);
         pthread_mutex_unlock(wrklock+j);
      }
      if(i%2==0)
         addone((void *)list);
      else
         addtwo((void *)list);
   }
}
</pre>
</div>

</div>
<div class="Indented">
The manager does its own share of the work outside the locked region, but the signal <tt>cv_work</tt> indicating that the work has been assigned must be sent inside the locked region. If the signal is sent outside the locked region, it may arrive after the worker thread has gained the lock and verified <tt>workflag[tid-1]</tt> to be <span class="formula">0</span> and before it calls <tt>pthread_cond_wait()</tt> to wait on <tt>cv_work[tid-1]</tt>. If such a thing happens, the signal will be lost and the program will deadlock. The manager does its share of the work, invoking either <tt>addone()</tt> or <tt>addtwo()</tt> outside the parallel region.
</div>
<div class="Indented">
A little more syntax is needed for using conditional variables. To exhibit that, we give a listing of the function used to create the worker threads.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void spawn_workers(){
	static  int tidlist[nthreads-1];
	for(int i=0; i &lt; nthreads-1; i++){
		workflag[i] = 0;
		tidlist[i] = i+1;
		pthread_mutex_init(wrklock+i,NULL);
		pthread_cond_init(cv_work+i, NULL);
		pthread_cond_init(cv_free+i, NULL);
		pthread_create(pthrd+i,NULL,worker, tidlist+i); 
	}
}
</pre>
</div>

</div>
<div class="Indented">
The reason for specifying the definition of <tt>tidlist[]</tt> as static has been explained before. The only new syntactic element here is the function for creating and initializing condition variables. The conditional variables, as well as the mutexes and the threads, are created with the <tt>NULL</tt> or default attribute.
</div>
<div class="Indented">
The listing of the function for destroying threads is given in full. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void shutdown(){
	static int tidlist[nthreads-1];
	for(int i=0; i &lt; nthreads-1; i++){
		pthread_mutex_lock(wrklock+i);
		if(workflag[i]==1)
			pthread_cond_wait(cv_free+i, wrklock+i);
		tidlist[i] = i+1;
		fnlist[i] = exitfn;
		arglist[i] = tidlist+i;
		workflag[i]=1;
		pthread_cond_signal(cv_work+i);
		pthread_mutex_unlock(wrklock+i);
	}
	for(int i=0; i &lt; nthreads-1; i++){
		pthread_join(pthrd[i], NULL);
		pthread_mutex_destroy(wrklock+i);
		pthread_cond_destroy(cv_work+i);
		pthread_cond_destroy(cv_free+i);
	}
}
</pre>
</div>

</div>
<div class="Indented">
The workers are terminated by asking them to execute <tt>exitfn()</tt>. The new syntax here is the function for destroying a conditional variable. The manager thread must join with a worker thread before destroying the conditional variables and the mutex corresponding to that worker thread.
</div>
<div class="Indented">
When parallel regions are implemented using conditional variables, the manager thread puts itself to sleep if it wants to assign work but the worker is busy. The worker threads put themselves to sleep if they are free to work but find that no work has been assigned. The spinlock-like behavior of mutexes does not occur here. There is regular and predictable alternation between assignment of work by the manager and its completion by the worker threads.
</div>
<h? class="Subsubsection">
<b><u>TLB flushes and cost of conditional variables implementation</u></b>
</h?>
<div class="Unindented">
Table <a class="Reference" href="#tab:pthreads-parallel-regions">5.6↑</a> shows that the average cost of a parallel region implemented using conditional variables is 30,000<span class="default"> cycles with two threads. On a quad-core <span class="formula">3.4</span>  GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for its full name), the cost is much lower and only 9,000 cycles with four threads. Even with eight threads, the average cost per parallel region is </span>18,000<span class="default"> cycles and much lower than the cost for two threads. </span>
</div>
<div class="Indented">
The explanation of this conundrum appears to be as follows. With two, four, or eight threads, there will be context switches during the alternation between assignment of work and its completion by the worker threads. With two threads, the threads get switched around between the four cores of the <span class="formula">3.4</span>  GHz AVX machine. When either the worker or manager thread is switched out, an editor, a web browser, or a kernel thread may get the processor. With eight threads, there are many more context switches per parallel region on an average, but nearly every context switch is between two threads in the same group of eight. Context switches between two threads in the same thread group are much less expensive. The kernel does not need to switch page tables or flush the TLB. The TLB flush is essential to preserve the integrity and correctness of the memory system when two processes with different virtual address spaces are switched. The greater expense of context switches and related factors appear to explain the reason the conditional variables implementation is so much more expensive with just two threads. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program to reverse a long array using Pthreads. Does your program need mutexes or spinlocks? Do you expect your program to be faster or slower than an OpenMP program for the same task? Explain.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why the <tt>volatile</tt> qualifier is used in the plain C implementation of parallel regions but not in the implementations using mutexes, spinlocks, or conditional variables.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-5.4">5.4</a> Program memory<a class="Label" name="sec:threads-program-memory"> </a>
</h2>
<div class="Unindented">
In the virtual memory setup described in section <a class="Reference" href="#sec:memory-page-tables-virtual-memory">4.4↑</a>, every process has its own page tables to map virtual memory to physical memory, and every instruction with a memory reference invokes this map from virtual to physical memory. The virtual memory system allows many distinct processes to coexist on the same computer. Threads are a group of processes that share the same map from virtual to physical memory. In addition, each thread may define its own local variables in an area that is ordinarily invisible to other threads. Thus, the concept of threads is tied to the memory system, and understanding program memory facilitates understanding threads.
</div>
<div class="Indented">
In this section, we limit ourselves to single-threaded programs but take a deeper look into the way program memory is set up and organized. Section <a class="Reference" href="#sub:threads-easy-sys-call">5.4.1↓</a> shows how to define a new system call in Linux. The operating system is a program, like any other program, and system calls are the set of functions used by user programs to invoke it. Thus, there are system calls to read and write files and so on. Writing one’s own system call should almost never be done. Linux provides an extensive set of system calls,<span class="FootOuter"><span class="SupFootMarker"> [91] </span><span class="HoverFoot"><span class="SupFootMarker"> [91] </span>For Linux system calls, see <span class="bibcites">[<a class="bibliocite" name="cite-46" href="#biblio-46"><span class="bib-index">46</span></a>]</span>.</span></span> so extensive that it is difficult to get a good grip on the many facilities it provides.
</div>
<div class="Indented">
Despite this caveat, the system call defined in section <a class="Reference" href="#sub:threads-easy-sys-call">5.4.1↓</a> serves usual pedagogical purposes. Later, we will insert print statements into the Linux kernel to examine the memory system and thread creation. The system call is used to turn those print statements on or off. 
</div>
<div class="Indented">
The first lesson from implementing a system call is to simply understand that the operating system is just another program, although an exceedingly complicated one. Because the operating system manages distinct and simultaneous processes as well as multifarious devices, handling concurrency has always been an issue with operating systems. Many of the topics of threaded programming, such as mutual exclusion, came up long ago in the context of operating systems.
</div>
<div class="Indented">
Programs rely on operating systems in many more ways than most programmers realize. When we try to understand program speed or some other characteristic in depth, we are inevitably led inside the operating system kernel. The little forays we make into the Linux kernel will help us understand segmentation faults, memory errors, and thread creation.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:threads-stack">5.4.2↓</a> introduces stacks. The stack is one of the simplest and most important data structures. All programming models are based on function calls, and the stack is the data structure used to save program state at the point of call and to restore it at the point of return. Understanding how program state is saved and restored is useful for all kinds of programming. Another advantage is that it helps clarify programming abstractions that may look artificial in their abstract setting. In section <a class="Reference" href="#sub:threads-stack">5.4.2↓</a>, we explain how recursion works using stacks. Without knowledge of stacks, the idea of recursion can look vaguely abstract, although in fact it is quite straightforward.
</div>
<div class="Indented">
Much of a C/C++ programmer’s time is spent dealing with segmentation faults and memory errors. Section <a class="Reference" href="#sub:threads-seg-faults">5.4.3↓</a> explains how these errors arise and are caught. Segmentation faults are caught deep inside the operating system kernel, and memory errors can escape detection for a long while. The memory system is set up for speed and efficiency, not to make the C programmer’s life easier. It is often true that memory errors are hard to detect and crash the program at a location far away from the bug, making memory errors difficult to deal with. Section <a class="Reference" href="#sub:threads-seg-faults">5.4.3↓</a> explains why this is so.
</div>
<div class="Indented">
Many scientific programmers have the habit of allocating large amounts of space statically on the stack. This habit is a holdover from Fortran. Section <a class="Reference" href="#sub:threads-seg-faults">5.4.3↓</a> shows why this is a bad idea.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.4.1">5.4.1</a> An easy system call<a class="Label" name="sub:threads-easy-sys-call"> </a>
</h3>
<div class="Unindented">
The operating system kernel offers its services to user programs through system calls. There are system calls for dealing with every single part of the computing system. There are system calls related to the file system, networks, memory management, and process creation and scheduling. Every <tt>printf()</tt> or <tt>malloc()</tt> begins life in the C library but finds its way into the operating system kernel through a system call.
</div>
<h? class="Subsubsection">
<b><u>System call definition and invocation</u></b>
</h?>
<div class="Unindented">
The system call defined in this section gives us a peek into the Linux kernel. The system call sets a global flag. The global flag is used to turn print statements on and off elsewhere in the kernel. 
</div>
<div class="Indented">
The definition of the system call is placed at the end of the file <tt>kernel/sys.c</tt> in the Linux source tree (Version 3.6).<span class="FootOuter"><span class="SupFootMarker"> [92] </span><span class="HoverFoot"><span class="SupFootMarker"> [92] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-47" href="#biblio-47"><span class="bib-index">47</span></a>]</span>.</span></span> 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int dv_print_flag=0;
<span class="number-left">2</span>EXPORT_SYMBOL(dv_print_flag);
<span class="number-left">3</span>​
<span class="number-left">4</span>asmlinkage long sys_set_dvflag(int flag){
<span class="number-left">5</span>	dv_print_flag = flag;
<span class="number-left">6</span>	return 77;
<span class="number-left">7</span>}
</pre>
</div>

</div>
<div class="Indented">
Line 1 defines a global variable of type <tt>int</tt>. This global variable is used to control the level of printing in other parts of the kernel. For example, if the value of the variable is in the range <span class="formula">[100, 200)</span>, kernel functions related to thread/process creation will print messages. For some other range, functions related to file I/O will print messages. The purpose of these messages is to help us understand what is happening inside the kernel. Printing these messages requires modifications to other parts of the kernel.
</div>
<div class="Indented">
Line 2 exports the global variable. The Linux kernel is built by linking thousands of compilation units, and <tt>sys.c</tt> is only one of them. Some parts of the kernel, such as device drivers, are loaded dynamically using <tt>vmalloc()</tt>. The global variable is exported  to make it visible to the dynamically linked modules. Some of our print messages will be inside device drivers and other modules.
</div>
<div class="Indented">
The <tt>asmlinkage</tt> qualifier on line 4 is a new element in the definition of this system call. It is the first hint here that a system call is not like any function. If an ordinary function has a single argument of type <tt>int</tt>, by convention that argument is passed using the <tt>%edi</tt> register in 64-bit Linux.<span class="FootOuter"><span class="SupFootMarker"> [93] </span><span class="HoverFoot"><span class="SupFootMarker"> [93] </span>For function call conventions pertaining to registers on GNU/Linux, see the gcc manuals or part 5 of Agner Fog’s optimization document posted on his web page:  <a class="FlexURL" href="http://www.agner.org/optimize/">http://www.agner.org/optimize/</a>.</span></span> No such convention is in effect when a system call is made. All the arguments to a system call are passed on the kernel stack (kernel stacks are discussed in the next section). The <tt>asmlinkage</tt> qualifier is telling the compiler that the function will receive all its arguments on the stack.
</div>
<div class="Indented">
Exiting a system call is special, just like entry. The <tt>SYSEXIT</tt> instruction is used to exit from system calls.
</div>
<div class="Indented">
To make the global variable <tt>dv_print_flag</tt> visible to all the compilation units, the declaration
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">extern int dv_print_flag;
</pre>
</div>

</div>
<div class="Indented">
is placed in <tt>include/linux/printk.h</tt> in the source tree. The kernel is not linked against the C libraries. So the kernel cannot call <tt>printf()</tt>. But the Linux kernel has <tt>printk()</tt>, which is similar. 
</div>
<div class="Indented">
Unlike ordinary functions, system calls are not called using their names. Instead there is a system call table. We need to make an entry in the table and make the declaration of the system call visible in a header file. The table entry goes into the file
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">arch/x86/syscalls/syscall_64.tbl
</pre>
</div>

</div>
<div class="Indented">
in Linux 3.6 and takes the form
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">313 	common	set_dvflag		sys_set_dvflag
</pre>
</div>

</div>
<div class="Indented">
The system call number is <span class="formula">313</span>. There are <span class="formula">312</span> system calls already. The name of the function is given at the end. Although system calls are called by number and not by name, it is conventional to wrap the call inside a function or macro. For this system call, the name of the wrapper function or macro would be <tt>set_dvflag</tt>. It is typical to derive the name by dropping the <tt>sys</tt> prefix.
</div>
<div class="Indented">
The system call table is a table of function pointers prepared by the kernel and made available to the processors. The function that implements the system call must be declared in <tt>include/linux/syscalls.h</tt>. For system call number 313, the declaration is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">asmlinkage long sys_set_dvflag(int flag);
</pre>
</div>

</div>
<div class="Indented">
The kernel is now in a position to set up the table of function pointers.
</div>
<div class="Indented">
If the kernel is built and loaded,<span class="FootOuter"><span class="SupFootMarker"> [94] </span><span class="HoverFoot"><span class="SupFootMarker"> [94] </span>For a guide to building and loading the Linux kernel, see <a class="FlexURL" href="http://kernelnewbies.org/KernelBuild">http://kernelnewbies.org/KernelBuild</a>. The directions worked smoothly on Xubuntu Linux but not Fedora Linux.</span></span> we may invoke system call number <span class="formula">313</span> as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;unistd.h&gt;
#include &lt;asm/unistd.h&gt;
.
.
​
long y=syscall(__NR_set_dvflag,100);
</pre>
</div>

</div>
<div class="Indented">
When the kernel is rebuilt, it uses the information we entered into <tt>syscall_64.tbl</tt> and defines <tt>__NR_set_dvflag</tt> as <span class="formula">313</span>. So we don’t need to remember the system call number explicitly. The system call is generated using <tt>syscall()</tt>. The first argument is the number of the system call. Later arguments are passed to the system call on the kernel stack. The system call is issued using the <tt>SYSENTER</tt> instruction. The <tt>syscall()</tt> shown here passes <span class="formula">100</span> as an argument. The global print flag gets set to <span class="formula">100</span>. We may discard the returned value <tt>y</tt> or verify that it is <span class="formula">77</span> as it should be.
</div>
<h? class="Subsubsection">
<b><u>Cost of making a system call</u></b>
</h?>
<div class="Unindented">
By timing a large number of calls to <tt>set_dvflag()</tt>, we find that the average call takes approximately <span class="formula">190</span> cycles. In contrast, the cost of calling a function that does something similar is just <span class="formula">5.2</span> cycles. The timing was done on a <span class="formula">3.4</span>  GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> of the appendix).
</div>
<div class="Indented">
Why is it so much more expensive to call and return from a system call? Every time a system call is made, the processor automatically saves all the general-purpose registers<span class="FootOuter"><span class="SupFootMarker"> [95] </span><span class="HoverFoot"><span class="SupFootMarker"> [95] </span>For a full list of registers saved during a system call, see <span class="bibcites">[<a class="bibliocite" name="cite-44" href="#biblio-44"><span class="bib-index">44</span></a>]</span>.</span></span> on the kernel stack and restores all the registers when the system call returns. Saving and restoring the registers must be a big part of the overhead of a system call.
</div>
<div class="Indented">
The system call we have considered here is particularly simple. More complicated system calls will have an overhead greater than <span class="formula">190</span> cycles. A few system calls have lengthy argument lists, and it takes time to save the arguments on the kernel stack. The same is true for more complicated function calls.
</div>
<div class="Indented">
Whenever a function or system call is timed inside a loop, there will be many fewer cache misses than will be the case in a more realistic scenario. The function pointer will be in cache, and so will be the instructions in the body of the function. When the  function is called as part of a more complex activity, the function definition is much less likely to be in cache.
</div>
<div class="Indented">
An overhead of around <span class="formula">200</span> cycles is not cheap but is not too bad. Kernel calls perhaps get a little too much blame for being expensive. Some kernel calls may be expensive because they invoke complex algorithms, but user space implementations of the same algorithms would incur the same expenses. If a single kernel call is made to transfer <span class="formula">10</span> MB of data over a network, the overhead of calling the kernel is negligible and immeasurably small. However, if a kernel call is made for every single byte that is written to a file of size <span class="formula">10</span> GB, the overhead may freeze the system.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.4.2">5.4.2</a> Stacks<a class="Label" name="sub:threads-stack"> </a>
</h3>
<div class="Unindented">
The stack is a collection of objects that supports two operations: push and pop. If an object is pushed on the stack, it lands right at the top. If another object is pushed, it goes on top above the last object that was pushed. The objects of a stack are ordered with the most recently pushed object on top. The pop operation removes the topmost object from the stack.
</div>
<div class="Indented">
In terms of its representation in memory, the stack is a simple data structure. To represent a stack of <tt>int</tt>s, for example, we may simply use an array of type <tt>int</tt> and another variable to keep track of the size of the stack. Every pop operation decreases the size of the stack by <span class="formula">1</span>, and every push operation increases the size of the stack by <span class="formula">1</span>.
</div>
<div class="Indented">
In terms of the way data is represented or accessed, there is nothing new to stacks. The novelty is in the abstract view of a collection of objects that constitute a stack. As with a stack of plates, we can insert and remove only at the top. 
</div>
<h? class="Subsubsection">
<b><u>Towers of Hanoi</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:openmp-stack-1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter4/hanoi.png" alt="figure FIGS/chapter4/hanoi.png" style="max-width: 194px; max-height: 67px;"/>

</div>
<div class="caption">
Figure 5.2 Towers of Hanoi. The discs must be moved from peg 0 to peg 1 without ever place a bigger disc on top of a smaller disc.
</div>

</div>

</div>

</div>
<div class="Indented">
Stacks are useful for implementing recursion and function calls. The Towers of Hanoi, a classic example, illustrates the usefulness of stacks.
</div>
<div class="Indented">
The problem is to move <span class="formula"><i>n</i></span> disks from peg <span class="formula">0</span> to peg <span class="formula">1</span>. In a single move, we may move the topmost disc of one peg to another peg, but a move is not allowed to place a bigger disc over a smaller disc. To begin with, the <span class="formula"><i>n</i></span> discs on peg 0 are ordered with the biggest disc at the bottom of the smallest at the top---to form a tower (see figure <a class="Reference" href="#fig:openmp-stack-1">5.2↑</a>).
</div>
<div class="Indented">
The notation <span class="formula"><span class="text">hanoi</span>(<i>A</i>, <i>B</i>, <i>n</i>)</span> stands for the problem of moving <span class="formula"><i>n</i></span> discs from peg <span class="formula"><i>A</i></span> to peg <span class="formula"><i>B</i></span>. We begin by pushing <span class="formula"><span class="text">hanoi</span>(0, 1, <i>n</i>)</span> on top of the stack:<div class="vspace" style="height: -0.2cm;">

</div>
<div class="formula">
<span class="text">hanoi</span>(0, 1, <i>n</i>).
</div>
<div class="vspace" style="height: -0.2cm;">

</div>

</div>
<div class="Indented">
If the problem instance on top of the stack is <span class="formula"><span class="text">hanoi</span>(<i>A</i>, <i>B</i>, <i>n</i>)</span> and <span class="formula"><i>n</i> = 1</span>, the lone disc is moved from <span class="formula"><i>A</i></span> to <span class="formula"><i>B</i></span> and the problem instance is popped to reduce the size of the stack. If <span class="formula"><i>n</i> &gt; 1</span>, the strategy is to move <span class="formula"><i>n</i> − 1</span> discs from <span class="formula"><i>A</i></span> to <span class="formula"><i>C</i></span>, where <span class="formula"><i>C</i></span> is the third peg, move the last disc from <span class="formula"><i>A</i></span> to <span class="formula"><i>B</i></span>, and then move <span class="formula"><i>n</i> − 1</span> discs from <span class="formula"><i>C</i></span> to <span class="formula"><i>A</i></span>. The topmost problem instance is popped from the top of the stack and three other problem instances are pushed on the stack. If <span class="formula"><i>n</i> &gt; 1</span>, we get:
</div>
<div class="Indented">
<div class="center">
<div class="vspace" style="height: -0.2cm;">

</div>
<table>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(0, 2, <i>n</i> − 1)</span>
</td>

</tr>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(0, 1, 1)</span>
</td>

</tr>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(2, 1, <i>n</i> − 1)</span>
</td>

</tr>

</table>
<div class="vspace" style="height: -0.2cm;">

</div>

</div>

</div>
<div class="Indented">
If <span class="formula"><i>n</i> &gt; 2</span>, the topmost problem instance is popped and <span class="formula"><span class="text">hanoi</span>(0, 2, <i>n</i> − 1)</span> is replaced by three other problem instances to get the following stack:
</div>
<div class="Indented">
<div class="center">
<div class="vspace" style="height: -0.2cm;">

</div>
<table>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(0, 1, <i>n</i> − 2)</span>
</td>

</tr>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(0, 2, 1)</span>
</td>

</tr>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(1, 2, <i>n</i> − 2)</span>
</td>

</tr>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(0, 1, 1)</span>
</td>

</tr>
<tr>
<td align="left" valign="top">
<span class="formula"><span class="text">hanoi</span>(2, 1, <i>n</i> − 1)</span>
</td>

</tr>

</table>
<div class="vspace" style="height: -0.2cm;">

</div>

</div>

</div>
<div class="Indented">
The process is repeated. The topmost problem instance is popped and either removed or replaced with three other problem instances until the stack is empty. If the number of discs is equal to <span class="formula">1</span> and the problem instance is <span class="formula"><span class="text">hanoi</span>(<i>A</i>, <i>B</i>, 1)</span>, the message &ldquo;move from A to B&rdquo; is printed out when the problem instance is popped and removed. 
</div>
<div class="Indented">
If we write a recursive function in C/C++ for solving Towers of Hanoi, the function will make two recursive calls if <span class="formula"><i>n</i> &gt; 1</span> and print a message if <span class="formula"><i>n</i> = 1</span>. The stacking of problem instances occurs during nested recursive function calls. Alternatively, the problem may be solved by using a stack explicitly. The stack is the natural mechanism for nesting function calls with or without recursion.
</div>
<h? class="Subsubsection">
<b><u>The user mode stack</u></b>
</h?>
<div class="Unindented">
Stacks are useful for maintaining the state of running processes. This application is so important that the stack is hardwired into the x86 instruction set as well as most other instruction sets.
</div>
<div class="Indented">
In a running process, certain variables are allocated using definitions such as <tt>int x</tt>. The state of a running process includes the values of all its variables, the values stored in all memory locations that may be referenced using those variables, and the next instruction the processor will execute, whose address is stored in <tt>%rip</tt>.
</div>
<div class="Indented">
However, these three items of information are not enough to fully capture the state of a running process. The next instruction to be executed may be deeply nested inside function calls. Each of these functions must return to the instruction that immediately follows the point where the functions were called. The return address of each function in a nested series of calls is essential information for capturing the state of a running process. This is where stacks come in.
</div>
<div class="Indented">
Conceptually, the local variables of the function that is currently running are topmost on the stack. The stack pointer (the pointer to the top of the stack) is stored in the <tt>%rsp</tt> register. During a function call, the return address is pushed on top of the stack by the <tt>call</tt> instruction. The newly called function may allocate other variables on the stack. Variables declared locally using declarations such as <tt>int x</tt> or <tt>int y[100]</tt> are allocated on the stack. If the newly called function makes another function call, it does so after pushing its return address on the stack and so on. 
</div>
<div class="Indented">
When a function returns, it first cleans up its stack by restoring the stack pointer <tt>%rsp</tt> to its original value. The stack pointer is restored by performing simple arithmetic or copying back its original value saved in some other register. After cleaning up its stack, the returning function executes the <tt>RET</tt> instruction, which pops the stack to find the return address.
</div>
<div class="Indented">
This conceptual picture hits the main ideas, but the actual picture is more complicated. The complications arise because of the way registers are used to store certain variables, differing conventions with regard to different registers, and conventions for passing arguments to and returning values from functions.
</div>
<div class="Indented">
The user mode stack grows every time the process makes a function call and shrinks every time a function returns. It is natural to ask what happens when a program makes a system call. A system call transfers control to a function defined in the operating system’s kernel. The kernel functions use another stack called the kernel stack. Every process has a kernel stack in addition to its user space stack. 
</div>
<h? class="Subsubsection">
<b><u>Kernel mode stack</u></b>
</h?>
<div class="Unindented">
In Linux, <tt>PAGE_OFFSET</tt> divides the virtual address space into two parts, as shown in figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a>. The lower part is for user processes and the upper part is for the kernel. The user area in virtual memory is occupied by the program, its global data, user mode stack, and dynamically allocated memory. The program is in a low region of the address space and the stack is in a high region, as we explained in an earlier chapter (see figure <a class="Reference" href="#fig:page-tables-memory-protection-1">4.8↑</a>).
</div>
<div class="Indented">
Nested function calls are stacked one above the other on the user mode stack. What happens if there is a system call? Every program requests services from the kernel directly or indirectly. Print statements, requests for dynamic memory, and reading and writing to a file end up as system calls. Unlike ordinary function calls, system calls and the kernel functions they call are not pushed onto the user mode stack.
</div>
<div class="Indented">
In Linux, every process gets a kernel mode stack, in addition to its user mode stack. The user mode stack occupies a high region in the user area, and the kernel mode stack is in the kernel area of virtual memory.<span class="FootOuter"><span class="SupFootMarker"> [96] </span><span class="HoverFoot"><span class="SupFootMarker"> [96] </span>The virtual address space immediately above <tt>PAGE_OFFSET</tt> is linearly mapped to page frames using <tt>alloc_pages()</tt> and <tt>kmalloc()</tt>. A part of the kernel’s virtual address space is mapped nonlinearly to page frames using <tt>vmalloc()</tt>. The kernel mode stack of each process is allocated in the linearly mapped region immediately above <tt>PAGE_OFFSET</tt>.</span></span> The kernel mode stack is much smaller than the user mode stack. The maximum size of the user mode stack is configurable and is typically several megabytes (this number may be found using the GNU/Linux command <tt>ulimit -a</tt>). The kernel mode stack has a fixed size that is typically equal to two pages or 8,192 bytes. The two pages that constitute the kernel mode stack of a process are next to each other and are allocated in the linearly mapped region of kernel memory.
</div>
<div class="Indented">
When a process is running in user mode, the <tt>%rsp</tt> register points to the top of the user mode stack. After a system call is made and the process switches to kernel mode, the <tt>%rsp</tt> register points to the top of the kernel mode stack. When the system call returns, the stack pointer reverts to the user mode stack. When the kernel mode stack is created, a pointer to the process descriptor is stored in the kernel stack of that process. The process descriptor is a kernel data structure that holds all kinds of information about the process. The Linux kernel defines a macro called <tt>current</tt> that extracts the process descriptor from the kernel stack.
</div>
<div class="Indented">
System calls, kernel functions that implement system calls, and page fault handlers execute in process context. Kernel functions that execute in process context may use <tt>current</tt> to obtain information about the process on whose behalf they are executing. Interrupt handlers, however, live on borrowed time and do not execute in process context.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-5.4.3">5.4.3</a> Segmentation faults and memory errors<a class="Label" name="sub:threads-seg-faults"> </a>
</h3>
<div class="Unindented">
The use of pointers exposes the programmer to errors that corrupt memory. Memory errors can befuddle even expert programmers. An erroneous program with memory errors may work on some systems but crash on others. The point where the program crashes can be far away from the point where the error occurs. The program may work for some inputs but not for others. In multithreaded programs, memory errors may be triggered by race conditions that are hard to reproduce, making debugging  difficult.
</div>
<div class="Indented">
In this section, we take a detailed look at segmentation faults and memory errors. Throughout this section, we assume that the maximum size of the stack is at least <span class="formula">2<sup>23</sup></span> bytes or <span class="formula">8.389</span> MB. The examples can be easily modified for smaller stacks. The size of the stack may be determined using the GNU/Linux command <tt>ulimit -a</tt>.
</div>
<div class="Indented">
Most of the segmentation faults and memory errors are caught by the paging system. Suppose a user process issues an instruction such as <tt>movq $80, (%rsi)</tt> to load the number <span class="formula">80</span> into the <span class="formula">64</span>-bit location to which the register <tt>%rsi</tt> is pointing. The processor first looks up the TLB to map the virtual address stored in <tt>%rsi</tt> to a physical address. If there is no TLB entry for that address, it uses the <tt>%cr3</tt> register to access the page tables. If there is no page table entry, there is a page fault and the page fault handler is invoked. Many memory errors are caught by the page fault handler.
</div>
<div class="Indented">
The mere fact that the page fault handler has been invoked does not imply a memory error. The page might have been swapped out to disc. More likely, the memory may have been allocated dynamically using <tt>malloc()</tt> or statically on the stack using a definition such as <tt>int x[1000]</tt> but uninitialized. A page is mapped to a page frame only at the point of first use (demand paging). 
</div>
<h? class="Subsubsection">
<b><u>VM areas in the Linux kernel</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:threads-vmarea"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter4/vmarea.png" alt="figure FIGS/chapter4/vmarea.png" style="max-width: 261px; max-height: 157px;"/>

</div>
<div class="caption">
Figure 5.3 Illustration of how vm areas are accessed.
</div>

</div>

</div>

</div>
<div class="Indented">
VM (virtual memory) areas<span class="FootOuter"><span class="SupFootMarker"> [97] </span><span class="HoverFoot"><span class="SupFootMarker"> [97] </span>See <span class="bibcites">[<a class="bibliocite" name="cite-44" href="#biblio-44"><span class="bib-index">44</span></a>]</span>.</span></span> are data structures maintained by the kernel for each process that specify which virtual addresses the program may legally generate.
</div>
<div class="Indented">
The definition of <tt>sys_getpid()</tt>, which implements the <tt>getpid()</tt> system call, is found in <tt>kernel/timer.c</tt> in the source tree of Linux version 3.6. This system call is modified to access the vm area that contains the user mode stack. The following code is inserted into <tt>sys_getpid()</tt>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">if((300&lt;=dv_print_flag)&amp;&amp;(dv_print_flag&lt;400)){
  struct vm_area_struct *dv_vmptr 
    = find_vma(current-&gt;mm,current-&gt;mm-&gt;start_stack);
  printk(KERN_ALERT "vm_start = %lx \n",
	 dv_vmptr-&gt;vm_start);
  printk(KERN_ALERT "vm_end = %lx \n",
	 dv_vmptr-&gt;vm_end);
}
</pre>
</div>

</div>
<div class="Indented">
The print flag, which is set using the system call of section <a class="Reference" href="#sub:threads-easy-sys-call">5.4.1↑</a>, is used to turn the print statements on or off. 
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:threads-vmarea">5.3↑</a> is a schematic depiction of how vm areas are accessed. The vm areas are stored in a linked list. Each vm area specifies the beginning and end of a valid area of virtual memory.
</div>
<div class="Indented">
For every process, the kernel creates a process descriptor, and the process descriptor contains a pointer to an <tt>mm_struct</tt> that contains a pointer to the list of vm areas that the program may legally access. There is a single vm area that corresponds to the stack of the process. There can be multiple vm areas corresponding to dynamically allocated memory. 
</div>
<div class="Indented">
If the generated address page faults but belongs to one of the vm areas, there is no memory error. The page fault handler will assign a page frame to the page (assuming one is available) that contains the address and update the page tables. There is a memory error (segmentation fault) only if the address does not belong to any of the vm areas of the process.
</div>
<h? class="Subsubsection">
<b><u>Functions <tt>ff0()</tt>, <tt>f0(),</tt> and<tt> f1()</tt></u></b>
</h?>
<div class="Unindented">
We shall write a few simple functions to exercise the user mode stack and trigger segmentation faults in a controlled manner. 
</div>
<div class="Indented">
The function <tt>sum_arr()</tt> prints the sum of an array of <tt>long</tt>s. Its definition is listed for completeness.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void sum_arr(long *p,long len){
	long ans=0;
	for(long i=0; i &lt; len; i++)
		ans += p[i];
	printf("sum = %ld\n",ans);
}
</pre>
</div>

</div>
<div class="Indented">
The heart of our attempts to exercise the user mode stack is the function <tt>ff0()</tt>. Its definition follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void ff0(long *a0, long *list, long n){
	for(int i=0; i &lt; 1000; i++)
		a0[i] = 0;
	for(long i=0; i &lt; n; i++)
		a0[list[i]] = list[i]*list[i]; 
	printf("in ff0: ");
	sum_arr(a0, 1000);
}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>ff0()</tt> is always called by <tt>f0()</tt>, and <tt>f0()</tt> defines <tt>a0</tt> using <tt>long a0[1000]</tt>. To begin with, the entries of the array <tt>a0[]</tt> are set to zero. The pointer <tt>list</tt> is allocated dynamically, and the entries of <tt>list[]</tt> are not stored on the stack. 
</div>
<div class="Indented">
The usefulness of <tt>list[]</tt> lies in being able to generate controlled illegal accesses into <tt>a0[]</tt>. For example, if <tt>list[]</tt> is the sequence <span class="formula">0,  − 1, …,  − 1000</span>, locations of lower addresses than <tt>a0</tt> are accessed. If <tt>list[]</tt> is the sequence <span class="formula"> − 500, …,  − 1000</span>, locations with addresses lower than <tt>a0</tt> are accessed after skipping <span class="formula">500</span> locations. If <tt>list[]</tt> is the sequence <span class="formula">1000, …, 1999</span>, then a thousand locations with addresses higher than <tt>a0</tt>, and just beyond the legal limit of the array <tt>a0[]</tt>, are accessed.
</div>
<div class="Indented">
The chief purpose of <tt>f0()</tt>, whose definition is given below, is to call <tt>ff0()</tt>, which it does at the end.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void f0(long *list, long n){
	long a0[1000];
	printf("a0 (in f0)= %p\n", a0);
	printf("pid (in f0) = %ld\n", getpid());
	ff0(a0, list, n);
}
</pre>
</div>

</div>
<div class="Indented">
The array <tt>a0[]</tt> is allocated on the stack and takes up 8,000 bytes. Because of the modifications made to the Linux kernel, <tt>getpid()</tt> will print the start and end of the vm area of the stack. The array <tt>list[0..n-1]</tt> specifies entries of <tt>a0[]</tt> that will be modified by the call to <tt>ff0()</tt>.
</div>
<div class="Indented">
The function <tt>f1()</tt> defined below calls <tt>f0()</tt>, which as we have seen calls <tt>ff0()</tt>. It defines an <span class="formula">8</span> MB array on the stack.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void f1(long *list, long n){
	long a1[1000*1000];
	for(int i=0; i &lt; 1000*1000; i++)
		a1[i] = 0;
	f0(list, n);
	printf("in f1: ");
	sum_arr(a1, 1000*1000);
}
</pre>
</div>

</div>
<div class="Indented">
This function defines the array <tt>a1[]</tt> and claims <span class="formula">8</span> MB on the stack. 
</div>
<div class="Indented">
The key thing to remember is that the arrays <tt>a0[]</tt> and <tt>a1[]</tt> are both allocated on the user mode stack---<tt>a0[]</tt> by the function <tt>f0()</tt> and <tt>a1[]</tt> by <tt>f1()</tt>. The arrays <tt>a0[]</tt> and <tt>a1[]</tt> are defined to have <span class="formula">10<sup>3</sup></span> and <span class="formula">10<sup>6</sup></span> entries, respectively.
</div>
<h? class="Subsubsection">
<b><u>Segmentation fault caused by illegal virtual address</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:threads-segf-run1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter4/run1.png" alt="figure FIGS/chapter4/run1.png" style="max-width: 345px; max-height: 116px;"/>

</div>
<div class="caption">
Figure 5.4 The stack resulting from an invocation of <tt>run1()</tt> from <tt>main()</tt>, just after call to <tt>ff0()</tt>. Notice that the stack grows leftward beginning with <tt>main()</tt>, the first function to be called and which calls <tt>run1()</tt>,  which calls <tt>f0()</tt>. 
</div>

</div>

</div>
We begin exercising the stack with the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void run1(){
	long *list = new long[5000];
	for(int i=0; i &lt; 5000; i++)
		list[i] = i;
	f0(list, 5000);
	delete[] list;
}
</pre>
</div>

</div>
<div class="Indented">
The stacking of function calls is shown in figure <a class="Reference" href="#fig:threads-segf-run1">5.4↑</a>. Figure <a class="Reference" href="#fig:threads-segf-run1">5.4↑</a> shows <tt>a0</tt> between <tt>f0</tt> and <tt>run1</tt>, with <tt>f0</tt> to its left, because the array <tt>a0[]</tt> is defined after <tt>run1()</tt> calls <tt>f0()</tt> but before <tt>f0()</tt> calls <tt>ff0()</tt>. 
</div>
<div class="Indented">
What is <tt>run1()</tt> doing? It is in effect forcing <tt>ff0()</tt> to generate accesses to entries numbered <span class="formula">0…4999</span> of the array <tt>a0[]</tt>. Of course, <tt>a0[]</tt> defined by <tt>f0()</tt> has only <span class="formula">10<sup>3</sup></span> entries (see figure <a class="Reference" href="#fig:threads-segf-run1">5.4↑</a>). We will understand exactly how the segmentation fault occurs.
</div>
<div class="Indented">
Just before <tt>f0()</tt> calls <tt>ff0()</tt>, which is the situation depicted in figure <a class="Reference" href="#fig:threads-segf-run1">5.4↑</a>, the vm area corresponding to the stack looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">  vm_end = 0x7fffc64a5000
      a0 = vm_end - 13312
vm_start = vm_end - 139264
</pre>
</div>

</div>
<div class="Indented">
Here the address <tt>vm_end</tt> is given in hex, but <tt>a0</tt> and <tt>vm_start</tt> are given as offsets from <tt>vm_end</tt> in decimal for convenience. The pointer <tt>a0</tt> is <span class="formula">13, 312</span> bytes below <tt>vm_end</tt> even though the array <tt>a0[]</tt> has only <span class="formula">8, 000</span> bytes. The system appears to use up around <span class="formula">5, 000</span> bytes near the top of the stack. There is also a lot of padding between <tt>a0</tt> and <tt>vm_start</tt>.
</div>
<div class="Indented">
Because <tt>a0[]</tt> is an array of type <tt>long</tt> and <span class="formula">4, 999 × 8 &gt; 13, 312</span>, the locations accessed go well beyond <tt>vm_end</tt>. When the address generated crosses <tt>vm_end</tt>, there is a page fault. The page fault handler will find that there is no valid vm area containing the address and trigger a segmentation fault. With a little trouble, we can calculate exactly which iteration of the for-loop in <tt>ff0()</tt> triggers a segmentation fault. That iteration is <span class="formula"><i>i</i> = 13, 312 ⁄ 8 = 1, 664</span>. 
</div>
<h? class="Subsubsection">
<b><u>Segmentation fault caused by overwriting return address</u></b>
</h?>
<div class="Unindented">
The second run we look at also seg faults, but in a quite different manner, as we will see. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void run2(){
	long *list = new long[2000];
	for(int i=0; i &lt; 2000; i++)
		list[i] = i;
	f1(list, 2000);
	delete[] list;
}
</pre>
</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:threads-segf-run2">5.5↓</a> shows the order in which pointers are pushed on the stack: <tt>main, </tt>run2, <tt>f1</tt>, and <tt>f0</tt>. Each of these is a pointer to the point of return. The array <tt>a1[]</tt> is defined by <tt>f1()</tt> to be large enough for <span class="formula">10<sup>6</sup></span> <tt>long</tt>s or 8 MB. The vm area of the stack and the value of <tt>a0</tt> just before the call to <tt>ff0()</tt> inside <tt>f0()</tt> looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">   vm_end = 0x7fff325e2000
       a0 = vm_end - 8019136
 vm_start = vm_end - 8028160
</pre>
</div>

</div>
<div class="Indented">
Notice that there is more than <span class="formula">8</span> MB of room in the stack above <tt>a0</tt>. Much of that is memory allocated for <tt>a1[]</tt> by the function <tt>f1()</tt>. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:threads-segf-run2"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter4/run2.png" alt="figure FIGS/chapter4/run2.png" style="max-width: 345px; max-height: 116px;"/>

</div>
<div class="caption">
Figure 5.5 The stack resulting from an invocation of <tt>run2()</tt> from <tt>main()</tt>, just after call to <tt>ff0()</tt>. Notice that the stack grows leftward. 
</div>

</div>

</div>

</div>
<div class="Indented">
When control reaches <tt>ff0()</tt>,<tt> </tt>the initialization of <tt>list[]</tt> in <tt>run2() </tt>triggers the execution of the statement<tt> </tt>a0[j]=j*j for <span class="formula"><i>j</i> = 0, …, 1, 999</span> in the for-loop on lines 4 and 5 in <tt>ff0()</tt>. There is so much room above <tt>a0</tt> in the vm area that includes the stack that none of the iterations of the loop will seg fault. Indeed, the print statement at the end of <tt>ff0()</tt> prints 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">in ff0: sum = 332833500
</pre>
</div>

</div>
<div class="Indented">
It may be verified that <span class="formula"><span class="limits"><span class="limit">∑</span></span><span class="scripts"><sup class="script">999</sup><sub class="script"><i>i</i> = 0</sub></span><i>i</i><sup>2</sup> = 332, 833, 500</span>.
</div>
<div class="Indented">
Despite the memory error, not only does <tt>ff0()</tt> complete its loop, it also prints the sum correctly. Why then is there a segmentation fault? To answer this question, we must go back to figure <a class="Reference" href="#fig:threads-segf-run2">5.5↑</a> and look at it closely. The pointer <tt>a0</tt> is pointing to a location between <tt>f1</tt> and <tt>f0</tt>. The gap between <tt>f1</tt> and <tt>f0</tt> on the stack is large enough for <span class="formula">1, 000</span> <tt>long</tt> entries and is approximately <span class="formula">8, 000</span> bytes---<span class="formula">8</span> KB being the memory allocated to the array <tt>a0[]</tt>. When <tt>ff0()</tt> indexes into the array <tt>a0[]</tt> and goes from <tt>a0[0]</tt> to <tt>a0[1999]</tt>, it writes over the address <tt>f1</tt> that has been carefully saved to allow <tt>f0()</tt> to complete its return. That is where the problem arises.
</div>
<div class="Indented">
So <tt>ff0()</tt> is able to complete successfully. During return, it finds the return address <tt>f0</tt> on the stack, and it is able to return to <tt>f0()</tt> correctly. However, when <tt>f0()</tt> tries to return, its return address <tt>f1</tt> has been overwritten by something. There is a segmentation fault when <tt>f0()</tt> tries to return to <tt>f1()</tt>. 
</div>
<h? class="Subsubsection">
<b><u>Memory error without segmentation fault</u></b>
</h?>
<div class="Unindented">
In fact, we can modify <tt>run2()</tt> slightly so that there is no segmentation fault, although the program is erroneous.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void run3(){
	long *list = new long[2000];
	for(int i=0; i &lt; 2000; i++)
		list[i] = i+10000;
	f1(list, 2000);
	delete[] list;
}
</pre>
</div>

</div>
<div class="Indented">
The stacking of functions with <tt>run3()</tt> is identical to that with <tt>run2()</tt>. The difference is in the way the <span class="formula">2, 000</span> entries of <tt>list[]</tt> have been set up. The for-loop on lines 4 and 5 of <tt>ff0()</tt> executes the statement <tt>a0[j]=j*j</tt> for <span class="formula"><i>j</i> = 10<sup>4</sup>, …, 10<sup>4</sup> + 1, 999</span>. So it skips over the first <span class="formula">1, 000</span> entries of <tt>a0[]</tt> as well as <tt>f1</tt>---the address that <tt>f0()</tt> must return to---and modifies certain entries of <tt>a1[]</tt>. The program prints the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">in ff0: sum = 0 
in f1: sum = 242644667000
</pre>
</div>

</div>
<div class="Indented">
We may verify that <span class="formula"><span class="limits"><span class="limit">∑</span></span><span class="scripts"><sup class="script">1999</sup><sub class="script"><i>i</i> = 0</sub></span><span class="symbol">(</span><i>i</i> + 10<sup>4</sup><span class="symbol">)</span><sup>2</sup> = 242, 644, 667, 000</span> to validate our explanation. 
</div>
<div class="Indented">
In <tt>run1()</tt>, <tt>run2()</tt>, and <tt>run3()</tt>, all the entries of <tt>list[]</tt> were <span class="formula"> ≥ 0</span>. We may fill <tt>list[]</tt> with negative entries to examine scenarios that break the stack in the opposite direction, which is the direction the stack grows. If the entries of <tt>list[]</tt> are all negative and <span class="formula"> ≤  − 1, 000</span>, for example, the local variables of <tt>ff0()</tt> and the return addresses are not overwritten. The program will run to completion if the maximum limit on the size of the stack is not exceeded. In a more complex program, the point where the program fails can be far away from the error.
</div>
<div class="Indented">
The kernel does not shrink the stack, even when a function that uses a lot of memory on the stack returns. Once the kernel adopts the policy that it will let the stack expand up to its maximum limit, whenever the process generates memory references in the stack area, there is no reasonable way for the kernel to shrink the stack. Functions return using the <tt>RET</tt> instruction. No kernel function is called during every function return---it would be a huge waste to do so. The kernel has no way of knowing when a process scales back its use of the stack. 
</div>
<div class="Indented">
Many computing clusters set the maximum stack size to be as large as <span class="formula">8</span> GB. Such a large limit on the stack size is not a good thing, although it might be helpful to inexperienced programmers. Memory acquired on the stack is never released and the performance of programs that abuse the stack may degrade.
</div>
<h? class="Subsubsection">
<b><u>Corruption of dynamically allocated memory</u></b>
</h?>
<div class="Unindented">
Dynamic memory allocation using <tt>malloc()</tt>, <tt>_mm_malloc()</tt>, or <tt>new[]</tt> can invoke algorithms of considerable complexity. Functions such as <tt>malloc()</tt> and <tt>free()</tt> invoked by user programs are defined in runtime libraries in user space. 
</div>
<div class="Indented">
Our discussion of dynamic memory errors is confined to the following simple function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void mreg(long *a, long n){
	for(long i=0; i &lt; 5; i++)
		a[i] = i;
	a[n] = n;
} 
</pre>
</div>

</div>
<div class="Indented">
This function initializes the first five entries of the array <tt>a[]</tt>. Each entry is a <tt>long</tt>. Finally, <tt>mreg()</tt> stores <span class="formula"><i>n</i></span> in the <span class="formula"><i>n</i> + 1</span>st entry <tt>a[n]</tt>. We will attempt to cause memory corruption by passing invalid values of <span class="formula"><i>n</i></span> and understand how the system responds. 
</div>
<div class="Indented">
The function <tt>mreg()</tt> is called by <tt>run()</tt>, whose definition follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void run(long n){
	long *a = (long *)malloc(5*sizeof(long));
	mreg(a, n);
	free(a);
}
</pre>
</div>

</div>
<div class="Indented">
It is evident that <tt>run()</tt> allocates <tt>a[]</tt> to point to <span class="formula">5</span> entries of type <tt>long</tt>. It then calls <tt>mreg[]</tt> to assign to <tt>a[n]</tt>. 
</div>
<div class="Indented">
The point to be noted is that the vm area where the array <tt>a[]</tt> is allocated typically has a lot of padding in either direction.
</div>
<div class="Indented">
If we make the call <tt>run(5)</tt>, we may expect that there is no segmentation fault because the vm area is most probably larger than <span class="formula">5</span> entries of the type <tt>long</tt>. There is in fact no segmentation fault, but the runtime library detects an error when we try to free the pointer. How could that be? It turns out that the runtime library is storing a magic number in the location <tt>a[5]</tt>. When freeing the pointer, it checks whether that magic number has been overwritten. The runtime <tt>glibc</tt> library catches this error and produces a lengthy error report, which includes the memory map of the process---as if that were just the tonic to cheer up the programmer. In a more complex program, the point where the error is caught can be far away from where it occurs.
</div>
<div class="Indented">
Let us try <tt>run(6)</tt> or <tt>run(7)</tt> or <tt>run(10000)</tt>. None of these function calls produces an error message. They all run smoothly. All of them access illegal locations, but it turns out the kernel has created a pretty large vm area expecting more memory to be allocated dynamically. The runtime <tt>glibc</tt> library fails to catch the error. The magic number is stuffed into the first entry past the legal boundary but not ones after that. Stuffing a magic number in the entries just after the legal boundaries is a low overhead way to catch memory corruption errors. Unfortunately, the point where the error is caught can be far away from the place where the error is committed, and in some cases, the error may not be caught.
</div>
<div class="Indented">
There is one last point we make about dynamic memory. Freeing a pointer can be a high overhead activity. When a pointer is freed, the runtime library may decide to give up a vm area. When the vm area of a running process is deleted, the kernel must change the page tables. When the page tables are changed, the entire TLB must be flushed to prevent the process from accessing memory illegally. 
</div>
<div class="Indented">
After our discussion of stack and dynamic memory, the very idea of what is a legal memory access may seem fuzzy. In fact, the notion is fuzzy. There may be a notion of what is a legal memory reference with respect to program semantics. Neither the runtime library nor the Linux kernel enforce that notion too stringently. As a result, memory errors can be quite hard to catch, especially in multithreaded programs. The point where the program crashes can be quite far away from the point where the error is committed. The programmer’s best defense is to know what is going on. 
</div>
<div class="Indented">
Dynamic memory is sometimes referred to as heap. For example, <tt>malloc()</tt> is said to allocate memory on the heap. In this usage, &ldquo;heap&rdquo; is a reference to the data structure that was used to keep track of dynamically allocated memory at some point in history. It may well be that <tt>glibc</tt> uses the heap. But Linux does not use the heap to organize vm areas. It uses linked lists and red-black trees.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Look up <tt>linux/syscalls.h</tt> on your Linux system and find out the number of system calls available.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In section <a class="Reference" href="#sub:threads-easy-sys-call">5.4.1↑</a>, the cost of a system call is compared to the cost of a function call, both of which have short argument lists and an insignificant body. In addition, the function and system call definitions were in L1 instruction cache during timing. Redo the timing in such a way that the function and system call definitions are out of cache during each timing. Compare the out-of-cache timing figures to the in-cache figures.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why it is a good idea to have a separate kernel stack for every process to make system calls, which typically call other kernel functions, rather than use the user mode stack. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  If a large amount of memory is claimed on the stack, explain why the memory is not given back to the system until the program terminates.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  A program can commit memory errors in the following ways:
</div>
<ul>
<li class="nested">
<ul>
<li>
By writing to a virtual address that is illegal.
</li>
<li>
By incorrectly writing over a virtual address that may be legally accessed but indirectly triggers an illegal access later.
</li>

</ul>
<div class="--Separator--">

</div>

</li>

</ul>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Give examples of programs that commit these errors for the two cases where the virtual address in question is either in the stack area or is dynamically allocated. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose you allocate <span class="formula"><i>n</i></span> bytes using <tt>malloc()</tt>. The size of the vm area allocated will typically be larger than <span class="formula"><i>n</i></span>. Write a program that determines the size of the vm area (without going into the kernel). You may supply your own signal handlers to prevent the program from crashing when it generates illegal memory accesses.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-5.5">5.5</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-42"><span class="bib-index">42</span></a>] </span> <span class="bib-authors">B. Chapman, G. Jost, R. van der Pas</span>: <i><span class="bib-title">Using Open MP</span></i>. <span class="bib-publisher">MIT Press</span>, <span class="bib-year">2008</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-43"><span class="bib-index">43</span></a>] </span> <span class="bib-authors">C. van Loan</span>: <i><span class="bib-title">Computational Frameworks for the Fast Fourier Transform</span></i>. <span class="bib-publisher">SIAM</span>, <span class="bib-year">1992</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-44"><span class="bib-index">44</span></a>] </span> <span class="bib-authors">D.P. Bovet, M. Cesati</span>: <i><span class="bib-title">Understanding the Linux Kernel</span></i>. <span class="bib-publisher">O'Reilly</span>, <span class="bib-year">2005</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-45"><span class="bib-index">45</span></a>] </span> <span class="bib-authors">M. Herlihy, N. Shavit</span>: <i><span class="bib-title">The Art of Multiprocessor Programming</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">2008</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-46"><span class="bib-index">46</span></a>] </span> <span class="bib-authors">M. Kerrisk</span>: <i><span class="bib-title">The Linux Programming Interface</span></i>. <span class="bib-publisher">No Starch Press</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-47"><span class="bib-index">47</span></a>] </span> <span class="bib-authors">R. Love</span>: <i><span class="bib-title">Linux Kernel Development</span></i>. <span class="bib-publisher">Addison-Wesley</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-48"><span class="bib-index">48</span></a>] </span> <span class="bib-authors">T.H. Cormen, C.E. Lieserson, R.L. Rivest</span>: <i><span class="bib-title">Introduction to Algorithms</span></i>. <span class="bib-publisher">MIT Press</span>, <span class="bib-year">2001</span>.
</p>

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-6">6</a> Special Topic: Networks and Message Passing<a class="Label" name="chap:Networks-and-message"> </a>
</h1>
<div class="Unindented">
For many, computer networks are synonymous with the Internet. The Internet is a means for rapid communication, propagation of information and data, and delivery of services. Much of the activity is between clients and servers. Peer-to-peer activity is less common.
</div>
<div class="Indented">
In scientific computing, networks of thousands of computers are deployed to solve large-scale problems. The coordination between the computers is much tighter than in the Internet. The subproblem tackled by each computer typically has dependencies on other subproblems, which often implies that interprocess communication must be deeply integrated into the computation. Accordingly, the architecture of high-performance computing networks features much tighter integration than the Internet.
</div>
<div class="Indented">
The principal framework for programming high-performance networks is Message Passing Interface (MPI). MPI is a library that allows processes running concurrently on different nodes to communicate by sending and receiving messages. Each computing node will have a few dozen (this number is growing rapidly) or so processor cores and threads running on the same node share memory. However, threads on different nodes do not share memory but can use the MPI library to communicate by sending and receiving messages. Ideally, the sending and receiving of messages is done solely by the master thread on any given node.
</div>
<div class="Indented">
Many MPI programs were written years ago when each node had just a single core. Even now, when nodes have dozens of processor cores, one may pretend that each processor core is a separate node and put a single MPI process on each core. Then the MPI processes on the same node ignore that they share memory and communicate using the MPI library. Although this practice is common, it is not a good one. It ignores the powerful market forces that are putting more and more processor cores on the same node. Message passing is an inefficient way to communicate when processors share memory. 
</div>
<div class="Indented">
Normally, we compile sources into object files, build an executable, and then just run it at the command line. The process of running an executable is a little different with MPI. The executable must be simultaneously launched on multiple computers, and the MPI processes on the different computers must become aware of each other before they can send and receive messages. 
</div>
<div class="Indented">
Because the manner in which MPI processes start running is a little different, section <a class="Reference" href="#sec:ntwks-mpi-getting started">6.1↓</a> begins by showing how to initialize and run MPI. In section <a class="Reference" href="#sec:ntwks-infinib-architecture">6.2↓</a>, we describe the architecture of high-performance networks. The particular architecture we discuss is the one most common today, and other high-performance networks are built on similar ideas. The MPI standard and its wide adoption within scientific computing have provided powerful impetus to innovation in this area. For more than two decades, the biggest scientific computations have been performed using MPI.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:ntwks-mpi-examples">6.3↓</a> discusses a range of examples. Each of these is informed by the discussion of network architecture in the earlier section. In many examples, memory optimizations studied in previous chapters are of greater consequence than network optimizations. When optimizations such as overlapping processor activity with network activity do make a difference, they do so only after memory accesses have been carefully dealt with. Overlapping processor activity with network activity requires deep knowledge of the manner in which MPI library functions map to the network’s architecture. One of the examples in section <a class="Reference" href="#sec:ntwks-mpi-examples">6.3↓</a> is a discussion of bandwidth to disk from MPI programs. 
</div>
<div class="Indented">
It is likely that the largest scientific computations in the world will be performed using MPI for several years to come. The main competition to MPI is not from other networking libraries but from the increasing power of a single computing node. The sort of applications amenable to MPI are often ones that are also amenable to OpenMP. Programming such applications for a single node using OpenMP is far simpler. Already one can fit complex 3D computations with a billion grid points into a single node. That figure will grow rapidly as a consequence of market forces that are driving DRAM capacity and putting more and more processor cores on the same node. When problems of such size can be solved on a single node, the convenience of working on a single node makes MPI less attractive. 
</div>
<div class="Indented">
Although the sort of applications that are amenable to OpenMP and MPI tend to be the same, there is an important difference between the two programming models. In the OpenMP model, the program is written much like a sequential program. The parallel regions appear only in relatively inner parts of the program, although not the innermost. MPI parallelism intrudes on both outer (we have already noted that the act of launching the program must be specialized for MPI programs) and inner parts of the program. Although the basic MPI calls may appear as simple as OpenMP syntax, this makes MPI programming considerably more difficult. Even printing a simple variable can be a hassle, and on many supercomputing clusters, the program does not have ready access to a terminal.
</div>
<div class="Indented">
The Internet is a different kind of a network from the high-performance clusters targeted by MPI. It is far wider in extent and far more decentralized. If some market forces may make MPI less attractive, the Internet by itself is one of the powerful market forces. The Internet’s relevance to emerging areas of science is unquestionable. The huge volumes of investment that are and will flow into the Internet will imply that it is integrated more and more deeply into science and scientific computing. Conversely, one should not forget that the World Wide Web was invented by a physicist. Section <a class="Reference" href="#sec:ntwksmpi-Internet">6.4↓</a> gives an overview of the TCP/IP protocol, which powers the Internet, as well as clients and servers. The connection between Internet bandwidth and congestion windows is explained using actual programs. 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-6.1">6.1</a> MPI: Getting started<a class="Label" name="sec:ntwks-mpi-getting started"> </a>
</h2>
<div class="Unindented">
Section <a class="Reference" href="#sub:mpi-initializing">6.1.1↓</a> explains how to compile, link, and run an MPI program. MPICH, MVAPICH2, and Open MPI are the three major MPI implementations. The details of compilation, linking, and running vary between these implementations as well as between different sites. However, the general picture is the same. The Open MPI-specific discussion in section <a class="Reference" href="#sub:mpi-initializing">6.1.1↓</a> may be altered to other implementations in a straightforward manner. 
</div>
<div class="Indented">
It is typical to compile and link MPI programs using commands such as <tt>mpiCC</tt> or <tt>mpicxx</tt>. In such usage, MPI pretends to be an extension of C/C++. However, MPI is in fact a library, like many other libraries, although it influences program structure in a quite radical way. The compilation and linking syntax exhibited in section <a class="Reference" href="#sub:mpi-initializing">6.1.1↓</a> does not use wrappers such as <tt>mpiCC</tt> or <tt>mpicxx</tt>. Instead, both compilation and linking use the C/C++ compiler. This is a minor point, but our preference is to make a library look like a library.
</div>
<div class="Indented">
From the first MPI program in section <a class="Reference" href="#sub:mpi-initializing">6.1.1↓</a>, we run MPI with one process per node and not one process per core. The much more common one process per core MPI programs must be discouraged. Such usage is directly in opposition to powerful market forces that are increasing the number of cores on a single node. To make MPI processes on the same node communicate using messages is to completely ignore the shared memory architecture of each node. Because there is a single network card per node, the processes on the same node will contend for the network card when passing messages to other nodes. The penalty for such negligence is likely to increase. In our programs, the master thread on each node will use OpenMP to create a thread on each core. All the message passing is handled by the master thread. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↓</a> introduces <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt>. These two function calls, together with variants to be introduced later, are the backbone of the MPI library. Both function calls block. That means <tt>MPI_Send()</tt> does not return until the library verifies that whatever is sent has been received. Similarly, <tt>MPI_Recv()</tt> does not return until whatever is expected to be received is completely received.
</div>
<div class="Indented">
The blocking semantics of <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt> is natural from the point of view of a user. Indeed, these calls are widely used. However, the way they map to network architecture creates inefficiencies. The use of blocking calls wastes network bandwidth as we will see later. In section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↓</a>, we write a program called <tt>unsafe()</tt> that shows how blocking send and receive can easily deadlock a program.
</div>
<div class="Indented">
The <tt>unsafe()</tt> example of section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↓</a> is well known. The common explanation that the program works for short messages and fails for longer messages because of the size limitation of a secret internal buffer is not entirely correct, however. MPI implementations use different protocols for short and long messages, and a buffer is employed only for short messages, for reasons explained in later sections.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.1.1">6.1.1</a> Initializing MPI<a class="Label" name="sub:mpi-initializing"> </a>
</h3>
<div class="Unindented">
MPI<span class="FootOuter"><span class="SupFootMarker"> [98] </span><span class="HoverFoot"><span class="SupFootMarker"> [98] </span>The MPI standard is posted at <a class="FlexURL" href="http://www.mpi-forum.org/docs/docs.html">http://www.mpi-forum.org/docs/docs.html</a>. Also see <span class="bibcites">[<a class="bibliocite" name="cite-55" href="#biblio-55"><span class="bib-index">55</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-57" href="#biblio-57"><span class="bib-index">57</span></a>]</span>.  </span></span> is the most commonly used library for solving scientific problems on networked computers. MPI calls itself a fully featured library---with some justice. It has a lot of features. We will use only a thin sliver of MPI. As our goal is to write programs informed by the underlying network architecture, many of MPI’s features are of little concern to us. The features we use are the ones that are best optimized by MPI implementations. 
</div>
<div class="Indented">
When an MPI run is set up, each node will have a process on it. For example, if the run is with <span class="formula">8</span> processes, each of the <span class="formula">8</span> nodes in figure <a class="Reference" href="#fig:mpi-fat-tree-lei">6.1↓</a> may have one process on it. The processes communicate by sending and receiving messages. We begin by looking at how the processes are set up and turn to message passing later. 
</div>
<div class="Indented">
Our first MPI program follows. This program assumes that the processor name is shorter than <span class="formula">199</span> characters.<span class="FootOuter"><span class="SupFootMarker"> [99] </span><span class="HoverFoot"><span class="SupFootMarker"> [99] </span>In general, it would be better to use <tt>MPI_MAX_PROCESSOR_NAME</tt> instead of guessing the maximum limit on the length of the processor’s name. It is not employed here to prevent our first MPI program from getting too complicated.</span></span>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#include &lt;mpi.h&gt;
<span class="number-left">2</span>int main(int argc, char **argv){
<span class="number-left">3</span>	MPI_Init(NULL, NULL);
<span class="number-left">4</span>	int numprocs, rank;
<span class="number-left">5</span>	MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs);
<span class="number-left">6</span>	MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
<span class="number-left">7</span>	char procname[200];
<span class="number-left">8</span>	int procnamelen;
<span class="number-left">9</span>	MPI_Get_processor_name(procname, &amp;procnamelen);
<span class="number-left">10</span>	cout&lt;&lt;"proc name="&lt;&lt;procname&lt;&lt;" rank="&lt;&lt;rank&lt;&lt;endl;
<span class="number-left">11</span>	MPI_Finalize();
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 10, each process prints the name of the compute node it is running on. Thus, the purpose of this program is to print the names of the nodes that the processes run on. If the program is started up with <span class="formula">10</span> processes, there will be <span class="formula">10</span> names printed.
</div>
<div class="Indented">
Calls to the MPI library occur on lines 3, 5, 6, 9, and 11. Each MPI function begins with the prefix <tt>MPI</tt>. The declarations of the MPI library functions are found in <tt>mpi.h</tt>, the header file  included on line 1. 
</div>
<div class="Indented">
The <tt>MPI_Init()</tt> function on line 3 must be called before messages can be sent or received. Presumably, it initializes data structures essential to other MPI functions. Both its arguments are <tt>NULL</tt>.
</div>
<div class="Indented">
On line 5, the function <tt>MPI_Comm_size()</tt> is used to determine the size of an MPI communicator. MPI communicators are one of many MPI features we do not get into. The only communicator that occurs in this chapter is <tt>MPI_COMM_WORLD</tt>. All the MPI processes are members of this communicator. If the MPI run is started up with <span class="formula">10</span> processes, line 5 will set the value of the <tt>int</tt> variable <tt>numprocs</tt> to <span class="formula">10</span>.
</div>
<div class="Indented">
Each of the processes has a rank, much as each OpenMP thread has a thread identifier. If the number of processes is <span class="formula">10</span>, the process ranks will go from <span class="formula">0</span> through <span class="formula">9</span>. On line 6, the <tt>int </tt>variable <tt>rank</tt> is set to be equal to the process rank by calling <tt>MPI_Comm_rank()</tt>.
</div>
<div class="Indented">
MPI functions such as <tt>MPI_Comm_size()</tt> and <tt>MPI_Comm_rank()</tt> may be made to return error codes. Our aim being to understand the dependence of program performance on computer architecture, we follow our usual custom and ignore error handling. Eliminating error handling saves considerable clutter in the program. By default, MPI implementations abort when an error occurs. Although the default behavior can be changed, it is just fine by us.
</div>
<div class="Indented">
On line 9, the processor name is read into a character string. The function used on that line is self-explanatory.
</div>
<div class="Indented">
MPI programs must call <tt>MPI_Finalize()</tt> at the end, as this program does on line 11.
</div>
<div class="Indented">
The program is saved in the file <tt>procname.cpp</tt> and compiled using the following command in the implementation we employ:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc -c -O3 -prec-div -no-ftz -DNDEBUG \
‘mpiCC -showme:compile‘ procname.cpp
</pre>
</div>

</div>
<div class="Indented">
The compilation command can be quite different between MPI implementations. We are using Open MPI. All the <tt>icpc</tt> options have been discussed in earlier chapters. The only new option here is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">‘mpiCC -showme:compile‘
</pre>
</div>

</div>
<div class="Indented">
The effect of this syntax is to treat the back-quoted string as a shell command and splice in its output. The output of <tt>mpiCC -showme:compile</tt> is
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">-I/home1/00013/tg456871/openmpi-1.6.3/include -pthread
</pre>
</div>

</div>
<div class="Indented">
The <tt>-I</tt> option gives the directory where the header file <tt>mpi.h</tt> is found. A simpler command to compile this MPI program is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">mpiCC -c procname.cpp
</pre>
</div>

</div>
<div class="Indented">
Here we have omitted options to the C++ compiler for simplicity. Depending on the flavor of MPI, the compilation command may be <tt>mpicxx</tt> or <tt>mpic++</tt> or <tt>mpiCC</tt> or <tt>mpiicpc</tt>. The simpler form is the one that is used almost universally. All that the simpler form does is pass suitable <tt>-I</tt> options to the compiler.
</div>
<div class="Indented">
The linking command is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc ‘mpiCC -showme:link‘ -o procname.exe procname.o
</pre>
</div>

</div>
<div class="Indented">
Like the compiling command, this linking command too is specific to the Open MPI implementation. This time the string that is spliced in is
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">-pthread -L/home1/00013/tg456871/openmpi-1.6.3/lib\ 
-lmpi_cxx -lmpi -ldl -lm -lnuma -Wl,--export-dynamic\
-lrt -lnsl -lutil 
</pre>
</div>

</div>
<div class="Indented">
The first line here ends with the shell continuation character \ for convenient displaying. The libraries are linked dynamically. Therefore, the program must be able to find the libraries at runtime using <tt>LD_LIBRARY_PATH</tt> or some other mechanism. The simpler form of the linking command is
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">mpiCC -o procname.exe procname.o
</pre>
</div>

</div>
<div class="Indented">
All that the simpler form does is link a few libraries automatically.
</div>
<div class="Indented">
The compilation and linking of MPI programs is no different from that of any sequential program. The declarations of the library functions must be visible at compile time, and the executable is linked against libraries. However, the manner in which the program is run is quite different. An MPI program must be started up simultaneously on several nodes. During startup, the processes on different nodes have to set up data structures to recognize each other across the network. A typical command<span class="FootOuter"><span class="SupFootMarker"> [100] </span><span class="HoverFoot"><span class="SupFootMarker"> [100] </span>The command for running an MPI program varies between installations. The command <tt>mpiexec</tt> recommended by the MPI standard is unavailable on any implementation this author has accessed.</span></span> for running the program is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">mpirun -np 5 -bynode procname.exe
</pre>
</div>

</div>
<div class="Indented">
Like the <tt>mpiCC</tt> linking and compilation commands, this command too is installation-specific. The <tt>-np</tt> option indicates the number of processes, given here as <span class="formula">5</span>. The <tt>bynode</tt> option says that every process must be on a different node. The command to run MPI can vary considerably from installation to installation.
</div>
<div class="Indented">
A program run with the <tt>-np 5</tt> option produced the following output:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">proc name = c341-111.ls4.tacc.utexas.edu rank = 3
proc name = c340-114.ls4.tacc.utexas.edu rank = 0
proc name = c341-105.ls4.tacc.utexas.edu rank = 2
proc name = c340-107.ls4.tacc.utexas.edu rank = 4
proc name = c341-313.ls4.tacc.utexas.edu rank = 1
</pre>
</div>

</div>
<div class="Indented">
The manner in which MPI jobs are submitted varies from cluster to cluster. The <tt>ls</tt> in the processor names probably stands for Lonestar, the name of the cluster we used. We have no control over the order in which the processes print, although we could use <tt>MPI_Barrier()</tt> to make them print in the order of their rank. The technique of making the processes print to standard output does not scale well with job size but is good enough for this simple program.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.1.2">6.1.2</a> Unsafe communication in MPI<a class="Label" name="sub:mpi-unsafe"> </a>
</h3>
<div class="Unindented">
All MPI processes execute the same program. If the <tt>-bynode</tt> (or an equivalent) option is used, each process runs on a different node. The processes communicate by sending each other messages across the network. MPI syntax allows a process to use either blocking or nonblocking function calls to send and receive. A blocking send or receive waits until the operation is complete. Blocking communication is more vulnerable to deadlocks. Because two processes have to participate in message passing, the processes may get stuck waiting for each other. Even relatively simple programs can deadlock when blocking calls are used, as we show in this section. 
</div>
<div class="Indented">
At the beginning, each MPI process has to determine its rank and the total number of processes. The following function is called for doing so:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void mpi_initialize(int&amp; rank, int&amp; nprocs){
	MPI_Init(NULL, NULL);
	MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
	MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs);
}
</pre>
</div>

</div>
<div class="Indented">
Using <tt>mpi_initialize()</tt> saves us the trouble of remembering the syntax of three MPI library functions. Before making any calls to the MPI library, a program may execute the following code fragment:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	int rank;
	int nprocs;
	mpi_initialize(rank, nprocs);
</pre>
</div>

</div>
<div class="Indented">
After this fragment, each process knows its rank and the total number of processes. Before exiting, each process must call <tt>MPI_Finalize()</tt>. Each MPI process uses process rank to identify the process to which it sends a message or from which it receives a message.
</div>
<div class="Indented">
Our first example of sending and receiving messages is the function <tt>unsafe()</tt> listed below.<a class="Label" name="page:mpi-unsafe-comm-unsafe()"> </a> 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void unsafe(int numOFdoubles, int rank, int nprocs,
<span class="number-left">2</span>	    double *sendbuf, double *recvbuf)
<span class="number-left">3</span>{
<span class="number-left">4</span>	int tag = 0;
<span class="number-left">5</span>	MPI_Status status;
<span class="number-left">6</span>	MPI_Barrier(MPI_COMM_WORLD);
<span class="number-left">7</span>	if(rank==0){
<span class="number-left">8</span>		MPI_Send(sendbuf, numOFdoubles, MPI_DOUBLE, 
<span class="number-left">9</span>		         nprocs-1, tag, MPI_COMM_WORLD);
<span class="number-left">10</span>		MPI_Recv(recvbuf, numOFdoubles, MPI_DOUBLE, 
<span class="number-left">11</span>		         nprocs-1, tag, MPI_COMM_WORLD,
<span class="number-left">12</span>		         &amp;status);
<span class="number-left">13</span>	}
<span class="number-left">14</span>	else if(rank==nprocs-1){
<span class="number-left">15</span>		MPI_Send(sendbuf, numOFdoubles, MPI_DOUBLE, 
<span class="number-left">16</span>		         0, tag, MPI_COMM_WORLD);
<span class="number-left">17</span>		MPI_Recv(recvbuf, numOFdoubles, MPI_DOUBLE, 
<span class="number-left">18</span>		         0, tag, MPI_COMM_WORLD, &amp;status);
<span class="number-left">19</span>	}
<span class="number-left">20</span>}
</pre>
</div>

</div>
<div class="Indented">
We will assume that the MPI program is started up with exactly two processes and that each process calls <tt>unsafe()</tt>. The two processes send a certain number of <tt>double</tt>s to each other. The number of <tt>double</tt>s transmitted is given by the first argument. It is assumed that <tt>rank</tt> and <tt>nprocs</tt> have been appropriately initialized by both processes after calling <tt>mpi_initialize()</tt>. These are passed to <tt>unsafe()</tt> as arguments on line 1. The buffers for sending and receiving appear as arguments <tt>sendbuf</tt> and <tt>recvbuf</tt> on line 2. These buffers are allocated by each process before the call to <tt>unsafe()</tt>. 
</div>
<div class="Indented">
The MPI syntax that appears in <tt>unsafe()</tt>, supplemented by variants of send and receive described in a later section, is nearly all the MPI syntax we need. Line 5 defines <tt>status</tt> to be of type <tt>MPI_Status</tt>. The <tt>MPI_Recv()</tt> function returns information in <tt>status</tt> on lines 12 and 18. Other MPI variants for message passing and message probing also return status information.
</div>
<div class="Indented">
The <tt>MPI_Barrier()</tt> on line 6 stalls until all processes have entered the barrier. Its argument is a communicator, and the only MPI communicator we ever use, <tt>MPI_COMM_WORLD</tt>,<tt> </tt>includes all processes. The barrier plays no essential role in the function <tt>unsafe()</tt>. MPI processes get in sync while sending and receiving messages. The situation is quite different from shared memory where threads may conflict while reading and writing from the same location. Processes that communicate or coordinate using shared memory must use spinlocks, mutexes, critical regions, or barriers. Barriers are useful in MPI as well, but they are less frequently used. There is a degree of synchronization inherent in message passing. 
</div>
<div class="Indented">
The if-block of statements from lines 8 through 12 is executed only by the process with rank <span class="formula">0</span>. The else-if block of statements from lines 15 through 18 is executed only by the process with rank <tt>nprocs-1</tt>. Because we are assuming exactly two processes, the else-if block is executed by the process with rank <span class="formula">1</span>. This manner of using process rank to distinguish between processes is typical of MPI in the same way that the use of thread identifiers is typical of OpenMP. 
</div>
<div class="Indented">
The crucial bits of syntax in <tt>unsafe()</tt> are <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt>. On lines 8 and 9, the process with rank <span class="formula">0</span> sends a message to process with rank <span class="formula">1</span>. The first argument to <tt>MPI_Send()</tt> is a pointer to the send buffer. This argument is declared to be of type <tt>void *</tt>. On line 8, the argument is of type <tt>double *</tt>, which is automatically cast to the right type. Any pointer may be type cast to <tt>void *</tt>. The second argument is the number of items in the buffer, and the third argument is the data type of each item. The data type is given as <tt>MPI_DOUBLE</tt>. MPI has  elaborate constructions for data types. MPI data types are difficult to comprehend in full generality, and it is difficult to imagine how they could be useful. Most programs can get by with <tt>MPI_CHAR</tt>, <tt>MPI_INT</tt>, and <tt>MPI_DOUBLE</tt>. By giving the data type as <tt>MPI_DOUBLE</tt>, we are indicating that each item is a <tt>double</tt> and is <span class="formula">8</span> bytes wide as in C/C++ .
</div>
<div class="Indented">
On line 9, the fourth argument to <tt>MPI_Send()</tt> is given as <tt>nprocs-1</tt>. This argument is the destination of the message and is indicated here to be the process with rank <tt>nprocs-1</tt> or <span class="formula">1</span>. The fifth argument is the tag. The tags must agree when receives are matched with sends. The last argument must be a communicator.
</div>
<div class="Indented">
<tt>MPI_Recv()</tt> has seven arguments, one more than <tt>MPI_Send()</tt>. The first three arguments given on line 11 are the receive buffer, the number of items to be received, and the data type of each item. The fourth argument, which is given on line 12, is the source of the message---the sender specifies the destination and the receiver specifies the source. The source is specified to be <tt>nprocs-1</tt> or the process with rank <span class="formula">1</span>. The fifth argument is the tag and the sixth argument must be a communicator. 
</div>
<div class="Indented">
The MPI receive must match the corresponding MPI send. In the case of <tt>unsafe()</tt>, the send on line 8 matches the receive on line 17. The send on line 15 matches the receive on line 10. In MPI jargon, a receive and send must have the same message envelope to match. The message envelope consists of the source, destination, tag, and communicator. 
</div>
<div class="Indented">
In <tt>unsafe()</tt>, the number of items received is equal to the number of items sent. In general, the number of items may be less than or greater than the number of items sent. The last and seventh argument to <tt>MPI_Recv()</tt> is a pointer to <tt>MPI_Status</tt> (pointer to <tt>status</tt> on line 12). The status may be passed to <tt>MPI_Get_count()</tt> to discover the number of items received. 
</div>
<div class="Indented">
The tag argument in a receive may be given as <tt>MPI_ANY_TAG</tt>. The source argument may be given as <tt>MPI_ANY_SOURCE</tt>. The ability of a message to have an envelope that matches any tag and any source is not of the greatest use in scientific computing. When the same problem is being solved on multiple compute nodes, the processes must discriminate between messages depending on their source and possibly tag. However, an analogous facility supported by TCP/IP sockets is part of the basis of the Internet. In a typical transaction on the Internet, a client connects with a server to initiate a search, retrieve a document, or make a purchase. The client must know which server it wants to connect to, but the server has no way to know which client may want a connection. The server uses syntax similar to <tt>MPI_ANY_SOURCE</tt> to allow clients to make a connection to it.
</div>
<div class="Indented">
The else-if block of statements from lines 15 through 18 mirrors the if-block. In the if-block, the process with rank <span class="formula">0</span> sends its send buffer to the process with rank <tt>nprocs-1</tt> or <span class="formula">1</span>. In the else-if block, the process with rank <span class="formula">1</span> (we are assuming <tt>nprocs</tt> to be <span class="formula">2</span>) receives that message into its receive buffer. Similarly, the send buffer of the process with rank <span class="formula">1</span> is copied to the receive buffer of the process with rank <span class="formula">0</span>. As with the process of rank <span class="formula">0</span>, the send on line 15 precedes the receive on line 17.
</div>
<div class="Indented">
Both <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt> are blocking calls. They do not return until the transaction is complete. The deeper discussion of network architecture in the next section is essential to understanding what it means for a transaction to complete. The function <tt>unsafe()</tt> runs as expected if the number of <tt>double</tt> items in the messages is <span class="formula">1, 500</span> or fewer. It deadlocks if the number of items is <span class="formula">1, 600</span> or more. Both processes execute send before receive. But a send must match with receive before it can complete. The deadlock occurs when both the sends stall waiting for the corresponding receive. But then why do messages with fewer than <span class="formula">1, 500</span> items not deadlock? Open MPI uses a different protocol for transmitting messages that are smaller than <span class="formula">12</span> KB for reasons related to network architecture. The absence of deadlocks is an unintended side effect of Open MPI’s short message protocol, which is discussed later.
</div>
<div class="Indented">
The use of blocking calls is unsafe unless the program is careful not to deadlock. In a later section, we find that blocking sends and receives waste network bandwidth. <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt>, although familiar to many users, must be avoided in favor of safer and more efficient variants.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write an MPI program that makes the processors print their names in the reverse order of their ranks.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose there are three processes <span class="formula">0, 1, 2</span>, and the <span class="formula"><i>i</i></span>th process sends data to <span class="formula"><i>i</i> + 1<span class="mathrm"> mod</span>3</span> and receives data from <span class="formula"><i>i</i> − 1<span class="mathrm"> mod</span>3</span>. Devise a communication protocol that uses blocking send and receive yet avoids deadlocks.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-6.2">6.2</a> High-performance network architecture<a class="Label" name="sec:ntwks-infinib-architecture"> </a>
</h2>
<div class="Unindented">
A network connects many computers together. Each computer is an independent entity. Thus, every communication between two computers requires coordination. The coordination is much looser in the vast and decentralized Internet than it is in high-performance networks used in scientific computing.
</div>
<div class="Indented">
Infiniband, which is an open standard, is a leading technology in high-performance networking. The technology has stabilized over the past decade or so, and almost all large and even small computing clusters use Infiniband or a proprietary variation. In this section, we review three principal features of Infiniband: network topology, kernel-free message passing, and one-sided communication. These aspects of the network have an influence on the way MPI programs are written, as shown in the next section.
</div>
<div class="Indented">
Network topology is the graph used to connect computers together. On the Internet, the graph is not regular or structured, although it is far from being random (the Internet is a heavily designed network). In high-performance networks, the graphs are far more regular and structured.
</div>
<div class="Indented">
Every time information is transferred over the Internet, the message passes through the operating system kernel at both the sender and the receiver. For example, when a send button is pressed on an email client, the email is first copied to kernel buffers. It is transferred to the network card at the client site before traveling to the server. The network card on the server again copies the email to kernel buffers before passing it along to some other server or the receiving client. 
</div>
<div class="Indented">
Kernel copies create considerable overhead and the Linux kernel goes to great lengths to avoid excessive copying. In Infiniband, kernel copies can be avoided altogether. Information is copied directly between the Infiniband network card and the DRAM memory owned by the client program.
</div>
<div class="Indented">
Communication over the Internet is two sided, although that would be invisible to most users. When a web link is followed to download a page, the web server executes <tt>send()</tt> calls (these are defined inside the operating system kernel) to transmit the page and the browser executes matching <tt>recv()</tt> calls (these too are in the kernel) to receive it. 
</div>
<div class="Indented">
In a one-sided network such as Infiniband, the entire transaction may be carried out by making a single call at either the sender or the receiver. There is no reason to match sending with receiving.
</div>
<div class="Indented">
To be clear, one-sided communication does not mean that all the network activity is at either the sender or  receiver but not both. In any communication, there will be network activity involving the Infiniband adapters at both the sender and the receiver. One-sidedness means that a function call to initiate and complete the data transfer is made at either the sender or the receiver but not necessarily both. 
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:ntwks-Fat-tree-network">6.2.1↓</a>, we describe the fat-tree network topology. In a large network, a typical user gets control of only a small fraction of the nodes. An advantage of the fat-tree is that the bandwidth realized is relatively independent of how MPI processes are mapped to nodes of the fat-tree. The fat-tree topology is common among high-performance networks, although actual networks are approximations and not perfect realizations of the fat-tree.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:ntwks-Infiniband-network-architecture">6.2.2↓</a>, we turn to a deeper study of the Infiniband network. The discussion of MPI in the next section is informed by the many architectural issues that arise in this study.
</div>
<div class="Indented">
Like almost all modern networks, Infiniband is packetized. Data is broken up into packets before it travels over the network. Each packet has an envelope in addition to the data, and the layout of the envelope together with the data constitutes the packet format. Every packet needs an envelope for the same reason that every parcel mailed needs an envelope carrying the addresses of the sender and the receiver. The parcel would be undeliverable otherwise.
</div>
<div class="Indented">
Packetization and the consequent change in data format means that copying to buffers is inevitable. In Infiniband transmission, the buffers exist on the Infiniband network card (the so-called host channel adapter), and packetization does not require kernel intervention. Error correction, flow control, and congestion control are also handled by the network card.
</div>
<div class="Indented">
Read Direct Memory Access (RDMA) is the distinctive feature of Infiniband. RDMA allows either the sender or  the receiver to make a single function call to transmit data. The function call is made from a user program. The kernel does not participate in any way in executing that function call. Data transfer is entirely the responsibility of the Infiniband adapters and network.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:ntwks-Infiniband-network-architecture">6.2.2↓</a> includes two applications to actual programming, with many more to follow in the next section. The first application is a discussion of the latency of Infiniband networks. The second application is an explanation of what makes the program <tt>unsafe()</tt> of the previous section deadlock for long messages but not for short messages.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.2.1">6.2.1</a> Fat-tree network<a class="Label" name="sub:ntwks-Fat-tree-network"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:mpi-fat-tree-lei"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/fat_tree_lei.png" alt="figure FIGS/chapter5/fat_tree_lei.png" style="max-width: 296px; max-height: 88px;"/>

</div>
<div class="caption">
Figure 6.1 Fat-tree with <span class="formula">8</span> nodes. Switches are shown as rectangles. The ideal fat-tree is a perfect binary tree with the number of links between levels being approximately constant. The fat-tree is particularly easy to lay out on the plane, as shown on the right-hand side. 
</div>

</div>

</div>

</div>
<div class="Indented">
In many high-performance clusters, nodes are networked using fat-trees.<span class="FootOuter"><span class="SupFootMarker"> [101] </span><span class="HoverFoot"><span class="SupFootMarker"> [101] </span>Fat-trees were discovered and made popular by <span class="bibcites">[<a class="bibliocite" name="cite-51" href="#biblio-51"><span class="bib-index">51</span></a>]</span>.</span></span> A defining property of fat-trees, shown in figure <a class="Reference" href="#fig:mpi-fat-tree-lei">6.1↑</a>, is that the number of links between levels is roughly a constant. More links join switches at higher levels than at lower levels. With a suitable switching algorithm and assuming that any two nodes are equally likely to communicate, the load on each link is roughly the same. Of course, the switches at higher levels will use more hardware. Yet a fat-tree is particularly easy to lay out in the plane, as shown in the figure on the right.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-fat-tree-switches"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/fat_tree.png" alt="figure FIGS/chapter5/fat_tree.png" style="max-width: 275px; max-height: 145px;"/>

</div>
<div class="caption">
Figure 6.2 A fat-tree with <span class="formula">8</span> nodes implemented using switches, each with <span class="formula">4</span> ports.
</div>

</div>

</div>

</div>
<div class="Indented">
An entire fat-tree can be implemented within a single box with external ports for connecting nodes. A fat-tree can also be built using switches. The fat-tree shown in figure <a class="Reference" href="#fig:mpi-fat-tree-switches">6.2↑</a> is built using <span class="formula">10</span> switches, each of which has <span class="formula">4</span> ports. Each of the four switches at the lowest level is connected to <span class="formula">2</span> computing nodes, and the remaining ports are connected to switches at the next higher level. The number of compute nodes in this fat-tree is <span class="formula">8</span>. In figure <a class="Reference" href="#fig:mpi-fat-tree-switches">6.2↑</a>, there are <span class="formula">8</span> links from the compute nodes to switches at the lowest level. There are <span class="formula">8</span> links between the four switches at the lowest level and the four switches at the intermediate level. The number of links from the intermediate level to the two switches at the topmost level is also <span class="formula">8</span>.
</div>
<div class="Indented">
The network we study in this chapter is Infiniband. Infiniband technology is widely deployed in high-performance computing. The Infiniband switches are designed to deliver the same bandwidth at every port. The fat-tree configuration ensures that there is no bottleneck at any level of the network.<span class="FootOuter"><span class="SupFootMarker"> [102] </span><span class="HoverFoot"><span class="SupFootMarker"> [102] </span>The Infiniband network is oversubscribed in many installations, which may create bottlenecks.</span></span> Thanks to such a layout, the bandwidth between two nodes of the network is the same for every possible pairing. In theory, the bandwidth does not diminish if there is traffic between other pairs in the network. 
</div>
<div class="Indented">
The switches shown in figure <a class="Reference" href="#fig:mpi-fat-tree-lei">6.1↑</a> have only four ports. A more realistic count for the number of ports of an Infiniband switch is <span class="formula">36</span>. If each computer and link in the figure is multiplied ninefold, we get a fat-tree with <span class="formula">72</span> compute nodes built using <span class="formula">10</span> Infiniband switches, each with <span class="formula">36</span> ports.
</div>
<div class="Indented">
When large networks are built, it is typical to subsume the higher levels of the fat-tree within gigantic director switches. Indeed, the ability to do so is one of the advantages of the fat-tree. As shown in figure <a class="Reference" href="#fig:mpi-fat-tree-lei">6.1↑</a>, the switches at higher levels can be neatly laid down on a plane. Director switches with as many as <span class="formula">648</span> ports are available on the market.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.2.2">6.2.2</a> Infiniband network architecture<a class="Label" name="sub:ntwks-Infiniband-network-architecture"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:mpi-infini-net-arch-1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/multiaccess_ntwk.png" alt="figure FIGS/chapter5/multiaccess_ntwk.png" style="max-width: 256px; max-height: 50px;"/>

</div>
<div class="--Separator--">
<div class="caption">
Figure 6.3 Multiple access network.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-infini-net-arch-1">6.3↑</a> is a sketch of a multiple access network. The host computers connected to the shared cable can communicate directly with each other. In a switched network such as figure <a class="Reference" href="#fig:mpi-fat-tree-switches">6.2↑</a>, the communication must pass through a switch. Ethernet Local Area Networks (LANs) are in part multiple access networks. 
</div>
<div class="Indented">
In both kinds of networks, data must change format when it travels from one computer to another. To begin with, the transmitted data resides in a set of pages in the DRAM memory of the source computer. However, the physical link does not handle data the way the memory system does. In modern networks, data travels across the physical link in packets or frames. 
</div>
<div class="Indented">
Each packet is made up of headers with information about the source and destination in addition to the payload, which is the data to be transmitted. In packet switched networks, such as Infiniband or the Ethernet, data in DRAM memory is converted to a sequence of packets by the source. The packets are transmitted across the network. The packets are reassembled at the destination and stored once again in DRAM memory.
</div>
<div class="Indented">
The change in data format is fundamental to packet switched networks. Although the amount of data to be transmitted can be several gigabytes, packets are typically a few kilobytes. Other aspects of network architecture are either related to the change in data format or follow as consequences. We highlight three features of network architecture: buffering, error correction, and flow/congestion control. Each of these features occurs at multiple levels of the network partly because the data format changes at each level.
</div>
<div class="Indented">
Some amount of buffering is inevitable when the data format changes. When a long stream of data is transmitted as a sequence of packets, some of the packets may need to be resent if packets are dropped by the network. The packets must be buffered by the sender in some form. Similarly, the receiver must buffer the packets if it is to have the ability to assemble packets that arrive out of order. Additional buffering occurs at the level of the physical link. 
</div>
<div class="Indented">
A network without error control is like a leaky freight train. It is probably true that every modern network has some form of error control. In packet switched networks, every packet carries checksums and other error control information. The checksums may apply to the payload or parts of the header. When a long stream of data is broken up into packets, the header usually carries information about the sequence number of each packet. Using the sequence number, the receiver can reassemble the data even if the packets arrive out of order, and it can detect missing packets.
</div>
<div class="Indented">
A computer that is sending data may need to slow down because the receiver is not able to consume packets fast enough. Or the sender may have room to speed up because the receiver can consume faster. In flow control, the sender and receiver exchange information to ensure that the receive buffers do not overflow repeatedly. A typical mechanism is for the receiver to advertise the size of the unfilled part of its receive buffer to the sender. The sender adjusts its behavior based on that information. In Infiniband, as in many other networks, there is flow control at the level of packets as well as at the level of the physical link.
</div>
<div class="Indented">
Congestion control  is related to but distinct from flow control. The network is a resource shared by several hosts. Excessive traffic between two hosts can impede traffic between two other hosts. If the hosts are unmindful of the traffic in the network as a whole, a large network will become congested quickly because of overly aggressive usage by the hosts. It is the sender’s job to monitor not only the available room in the receiver’s buffer but also the level of traffic in the network as a whole. If the sender finds that the network is congested, it must adjust its behavior to reduce congestion. However, if the network is lightly loaded, the sender may be able to increase the rate of data transmission. Congestion control algorithms make the host computers into good citizens with respect to their network usage. 
</div>
<div class="Indented">
Flow control and error handling are typically hidden from user programs, and that is certainly true of MPI programs. However, user applications are not spared from having to buffer messages. Buffering can add significantly to the overhead of exchanging messages, especially over fast networks such as Infiniband. When a large matrix stored on several computers is transposed, for example, the program on each computer must decide what part of its data must be transmitted to another computer on the network. If the network expects the user program to hand it contiguous data, the sender has no option but to use buffers to pack data headed for each destination. The receiver will get chunks of its part of the transposed matrix from several senders. Receive buffers will be needed to gather incoming data before the transposed matrix is assembled.
</div>
<div class="Indented">
When Internet protocols are implemented over the Ethernet, data is broken up into a sequence of packets and frames by software. A significant innovation in the Infiniband network architecture is that the host channel adapter is responsible for breaking up a message into packets. Intelligent use of MPI is aided greatly by knowledge of network architecture. At the end of this section, we explain why the function <tt>unsafe()</tt> defined in section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↑</a> deadlocks if messages are more than <span class="formula">12</span> KB but not if messages are smaller than <span class="formula">12</span> KB. Discussion of MPI syntax and usage in later sections is closely tied to Infiniband network architecture. 
</div>
<h? class="Subsubsection">
<b><u>Infiniband data packet format</u></b>
</h?>
<div class="Unindented">
Telephone networks of yesteryears established point-to-point connections between communicating agents. The network bandwidth was shared using either time  or frequency domain multiplexing. Modern computer networks, such as Infiniband, are packet switched. Packet switched networks are more flexible, are easier to configure, and make more efficient use of network resources.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-infini-packet-2"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/ibpacket.png" alt="figure FIGS/chapter5/ibpacket.png" style="max-width: 292px; max-height: 40px;"/>

</div>
<div class="caption">
Figure 6.4 Infiniband data packet format.
</div>

</div>

</div>

</div>
<div class="Indented">
The data packet is the fundamental unit of data transmission in a packet switched network. Figure <a class="Reference" href="#fig:mpi-infini-packet-2">6.4↑</a> shows the layout of a typical Infiniband packet. Infiniband data packets serve many different purposes, and packets that serve different purposes have slightly different layouts.<span class="FootOuter"><span class="SupFootMarker"> [103] </span><span class="HoverFoot"><span class="SupFootMarker"> [103] </span>The Infiniband architecture is described in <i>Infiniband Architecture Specification<span class="default"></span></i>, vols. 1 and 2, Infiniband Trade Association, 2007. (Paul Grun, <i>Introduction to Infiniband for End Users, </i>Infiniband Trade Association, 2010) is another reference. (Ashok Raj, <i>Infiniband Host Channel Adapter</i> <i>Verb Implementer’s Guide</i>, 2003) is helpful for understanding the verb layer. The verb interface is documented in (<i>RDMA Aware Networks Programming User Manual</i>, Mellanox Technologies, 2009). The verb interface is not part of the Infiniband standard but is implemented with the same interface by several vendors.</span></span>
</div>
<div class="Indented">
The local routing header (LRH) is <span class="formula">8</span> bytes. This field occurs right at the top of the packet. It is used by the switches to route the packet from source to destination. Within the header, <span class="formula">2</span> bytes are reserved for the source address and <span class="formula">2</span> bytes are reserved for destination address.
</div>
<div class="Indented">
The base transport header (BTH), which occurs right after LRH, is <span class="formula">12</span> bytes. When a packet arrives at its destination, the host channel adapter at the destination uses information in the base transport header to handle the packet. For example, if a message must be assembled from a stream of packets, the information for message assembly is found in the base transport header. Every base transport header has a <span class="formula">24</span>-bit field that gives the packet sequence number.
</div>
<div class="Indented">
The payload is either the message or part of the message that must be delivered at the destination. How large can the payload be? The Linux command <tt>ibv_devinfo</tt> gives the Maximum Transmission Unit (MTU) as <span class="formula">2, 048</span> bytes on our system.   Each Infiniband packet must be 2 KB or smaller. When a large message is transmitted, the typical packet has a size that is nearly <span class="formula">2</span> KB.
</div>
<div class="Indented">
The two fields of the Infiniband data packet that follow the payload are invariant Cyclic Redundancy Checksum (CRC) and variant CRC. CRC error detection/correction information is calculated using polynomial arithmetic. The invariant CRC is <span class="formula">32</span> bits and the variant CRC is <span class="formula">16</span> bits. Certain fields of the data packet may change when it travels from source to destination. The invariant CRC refers only to those fields that do not change. The variant CRC refers to the whole packet. The CRC fields are generated and verified at the link layer. If a packet has an error that cannot be fixed, it is silently dropped at the destination.
</div>
<div class="Indented">
Infiniband packets may have additional fields. If a network is so large that it uses routers, in addition to switches, each packet includes a global route header (GRH) right after the local route header and before the transport header. Most high-performance computing networks (or maybe all) in use today are built without routers, and the global route header is absent from their Infiniband data packets.
</div>
<div class="Indented">
Remote Direct Memory Access (RDMA) is a notable feature of Infiniband technology. RDMA may either write into or read from the memory of a remote computer on the network. RDMA read request packets, which initiate an RDMA read, and the very first RDMA write packet include an extended RDMA transport header (RETH). This header occurs right after the base transport header. It is <span class="formula">16</span> bytes long. It has three fields---a <span class="formula">64</span>-bit virtual address, a 32-bit remote key, and a 32-bit field that gives the size in bytes of the remote DMA transfer.
</div>
<div class="Indented">
Acknowledgment (ACK) packets include the <span class="formula">4</span>-byte acknowledgment extended transport header (AETH). The same packet may carry both an ACK header and payload, and thus serves a double purpose. For example, during an RDMA read, the very first response packet carries an ACK header acknowledging the request and some of the remotely read data in its payload. The very last response packet too doubles up in the same manner. The first and last packets in a sequence are marked in the base transport header.
</div>
<div class="Indented">
High-performance computing networks are built using switches and are currently not large enough to need routers. One consequence, we have already mentioned, is the absence of the global routing header from the data packets. Another consequence is that an Infiniband data packet always follows the same path through the switches from a given source to a given destination.
</div>
<h? class="Subsubsection">
<b><u>Protection domains and queue pairs</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:mpi-infini-qps-3"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/ibntwk.png" alt="figure FIGS/chapter5/ibntwk.png" style="max-width: 241px; max-height: 181px;"/>

</div>
<div class="caption">
Figure 6.5 This depiction shows processor nodes, each with <span class="formula">4</span> processor cores, and data modules connected to a single Infiniband switch. In general, multiple switches can be connected to each other. HCA stands for Host Channel Adapter and TCA for Target Channel Adapter. TCAs do not support the Verb layer. The connection of processor cores to HCA via the PCIe bus is not shown here.
</div>

</div>

</div>

</div>
<div class="Indented">
The Infiniband network has several unusual features. These unusual features cannot be detected solely from the data packet format. Some of the features particular to Infiniband become visible when we look inside host channel adapters. Host channel adapters are hardware devices used on host computers to interface to the Infiniband network.
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-infini-qps-3">6.5↑</a> shows an infiniband network with several processors and data modules connected to a big switch, which presumably implements some version of fat-tree internally. The processor is connected to the host channel adapter via the PCIe bus. The Infiniband host channel adapters take over most of the networking activity. Not much is left to the software layers.
</div>
<div class="Indented">
Suppose a message is sent over the Internet from a source computer that is on an Ethernet LAN. Once a connection is established, the user program executes the <tt>send()</tt> function on a socket to send the message. The send function is a kernel system call. It is the first of a cascade of function calls within the kernel. The message is broken up into packets and frames by the kernel, and the kernel eventually invokes the network device driver. The device driver places the frames on the Ethernet cable connected to the source computer. 
</div>
<div class="Indented">
In Infiniband, the user program executes a single function call to initiate a message transfer. This function call goes <i>directly</i> to the host channel adapter. Information initiating the message transfer is directly written into the host channel adapter. At that point, the host channel adapter takes over, and there is nothing more for the processor to do to transmit the message. 
</div>
<div class="Indented">
The host channel adapter can use direct memory access to retrieve data in DRAM without intervention of the processor. In the Infiniband network architecture, the host channel adapter at the source can read data to be sent from source DRAM, and the host channel adapter at the destination can write that data to its DRAM without intervention of the processor. Thus, the entire transmission of the message can be completed without processor intervention.
</div>
<div class="Indented">
Infiniband vendors provide a Verb library that is used by user programs to access Infiniband facilities. Some of the Infiniband Verbs are functions that call the kernel and the Infiniband driver. Other Verbs are entirely in user space. Important examples of Infiniband Verbs that are entirely in user space are <tt>ib_post_send()</tt> and <tt>ib_post_recv()</tt>.  These are used by user programs to post send and receive requests directly to the host channel adapter.
</div>
<div class="Indented">
The key concept that enables user programs to talk to the host channel adapter directly is that of protection domains. The host channel adapters have their own memory and their own buffers. A part of the memory on the host channel adapter may be mapped to the virtual memory of the user process to create a protection domain. A user process can use virtual addresses within a protection domain to write to the host channel adapter and read from it.
</div>
<div class="Indented">
It is not such a simple matter for user processes to share a resource and access it without the intervention of the kernel. User processes can interfere with each other and render the system insecure. We have seen the elaborate and complicated paging system infrastructure that is set up to allow user process to share DRAM memory and access it directly. A similar infrastructure is implemented by the host channel adapter to enforce protection domains. The Verbs to create and release protection domains---<tt>ib_allocate_pd()</tt> and <tt>ib_deallocate_pd()</tt>---call the kernel (probably using the <tt>ioctl()</tt> system call). 
</div>
<div class="Indented">
It is unusual, but not unheard of, to map device memory into the virtual address space of a user process. An example where that is done is the X window system in Linux, which uses a client-server architecture. The server is a privileged user space program that maps video memory into its virtual address space. An unusual feature of Infiniband is that any user process, even an unprivileged one, can talk to the host channel adapter directly.
</div>
<div class="Indented">
User processes use &ldquo;queue pairs&rdquo;  to post send and receive requests.  The queue pair is a data structure created within a protection domain using a Verb. 
</div>
<div class="Indented">
The queue pair is a C structure of type <tt>struct ibv_qp</tt>. It resides on the host channel adapter and not in DRAM memory. The queue pair has a send queue and a receive queue. A single user process can create thousands of queue pairs to communicate with other processes on the network. A single host channel adapter may hold tens of thousands of queue pairs corresponding to several user processes on the same computer. 
</div>
<div class="Indented">
Send requests are posted to queue pairs using the Verb <tt>ibv_post_send()</tt>.  The request is posted to the send queue. The function <tt>ibv_post_recv()</tt> is used to post requests to the receive queue. Its declaration is similar. Neither function calls the kernel. Work requests go directly to the host channel adapter, and the kernel has no idea that data is being transferred over the network.
</div>
<div class="Indented">
When a work request is completed, the host channel adapter places a completion work element on the completion queue. Like queue pairs, completion queues reside on the host channel adapter and are tied to protection domains. A user process finds out whether a work request has been completed by polling the completion queue. 
</div>
<div class="Indented">
The completion queue is polled using the Verb <tt>ibv_poll_cq()</tt>. This Verb also bypasses the kernel completely. Using these Verbs, processes can send messages to each other and check completion of their work requests with no help from the kernel once protection domains and other data structures have been set up. 
</div>
<div class="Indented">
Infiniband also provides a facility to trigger events when work requests are completed. However, interrupt-driven event handling scales poorly with network size and traffic. Even Ethernet drivers switch off interrupt mode and resort to polling when there is heavy traffic on the network card.
</div>
<h? class="Subsubsection">
<b><u>RDMA write and RDMA read</u></b>
</h?>
<div class="Unindented">
Much of the action in Infiniband networking is on the host channel adapters. We have seen that protection domains reside on the host channel adapters and that queue pairs and completion queues are tied to protection domains. Our purpose here is to explain RDMA  reads and writes, but we will begin by making one more point about host channel adapters.
</div>
<div class="Indented">
Infiniband host channel adapters maintain virtual memory tables. A little reflection will show that such a facility is essential. Work requests for sending and receiving messages are posted directly to queue pairs that reside on host channel adapters. The memory address for accessing data resides within the work request. Because the work request is created by a user process, the only kind of address that a work request can pass on to the host channel adapter is the virtual address.
</div>
<div class="Indented">
If the host channel adapter is to access DRAM memory directly, it must be able to convert the virtual address it receives into physical addresses. The kernel maintains page tables to map virtual addresses to physical addresses, but the host channel adapter cannot and does not access the page tables. Indeed, the whole point of Infiniband design is to bypass the kernel and concentrate all the layers of networking within the host channel adapter. To enable the host channel adapter to map virtual addresses to physical addresses, Infiniband requires the user process to register memory before posting a work request. 
</div>
<div class="Indented">
The Verb that registers memory is <tt>ibv_reg_mr()</tt>.  This Verb uses a system call to map a region of virtual memory, supplied to it via arguments, to physical addresses. The map is saved on the host channel adapter using a protection domain. All commands that transfer data across the network are required to register memory. A memory region is unregistered (or deregistered) using the verb <tt>ibv_dereg_mr()</tt>. 
</div>
<div class="Indented">
Protection domains, queue pairs, completion queues, and memory registration are the mechanisms that enable messages to be sent and received while completely bypassing the kernel. Having discussed all these mechanisms, we are at last ready to talk about how messages are sent and received across Infiniband networks.
</div>
<div class="Indented">
Infiniband provides conventional channel semantics as well as RDMA read and write. We will first discuss conventional channel semantics. In conventional channel semantics, a queue pair at the sender is bound to a queue pair at the receiver to create a channel. A send work request is posted on the send queue of the queue pair at the sending side. A receive work request is posted on the receive queue of the receiver. The send work request must be matched with a receive work request to complete a data transfer across the channel.
</div>
<div class="Indented">
Channel-based communication is not what Infiniband technology is about. Channels are unsuitable for large messages and have lower bandwidth for even small messages. Infiniband allows a user process on one computer to directly transfer data into the virtual memory region of a remote process using RDMA read or RDMA write. Such RDMA transfers have low latency and excellent bandwidth. RDMA data transfers are the heart of Infiniband technology. A queue pair at the source is bound to a queue pair at the destination in reliable RDMA communication as it is in reliable channel-based communication.
</div>
<div class="Indented">
To understand RDMA, we must look a little more deeply into the memory registration facility. When memory is registered using <tt>ibv_reg_mr()</tt>, a local key and a remote key to the memory region are created. These keys are stored in separate fields of <tt>struct ibv_mr</tt>. If a process has a remote key to the memory region created on a remote computer on the network, it can use that key to initiate memory transfers between the remote host channel adapter and the remote memory region.
</div>
<div class="Indented">
RDMA write works as follows. The requester obtains a remote address key from the responder. The Infiniband standard does not specify how remote address keys are sent from the responder to the requester. An implementation may use queue pairs and channel semantics to transmit this information or it may use datagrams. It may even use TCP/IP sockets. The requester prepares a work request using the remote address key and a local memory region. The work request asks that data in the local memory region must be written to the remote memory region. This work request is posted to the send queue of a queue pair on the requester using <tt>ibv_post_send()</tt>.
</div>
<div class="Indented">
RDMA transfers are one sided. The requester posts an RDMA write to a send queue on its host channel adapter. But no user process posts a receive request at the responder. Send and receive requests are not matched. An RDMA write takes place as follows. The requester accesses its local memory and generates a sequence of Infiniband data packets. The RDMA extended transport header, which is present in the first packet, has a field to store a <span class="formula">32</span>-bit remote key. When the sequence of packets arrives at the responder, the responder uses the remote key to write data in the packets it receives to one of its local memory regions. The responder generates an ACK when the data transfer is complete. The RDMA write is completely one sided because the responder, which is the receiver, transfers data from its host channel adapter to its DRAM memory without intervention of a processor or the operating system.
</div>
<div class="Indented">
In an RDMA write, the requester is the sender and the responder is the receiver. In contrast, in an RDMA read, the requester is the receiver and the responder is the sender. An RDMA read begins with an RDMA request packet from the receiver to the sender. When the sender receives the request packet, it responds by initiating the data transfer. Like RDMA write, RDMA read is completely one sided. Once the receiver acquires a remote address key, it posts a work request, but no work request is posted at the sender.<span class="FootOuter"><span class="SupFootMarker"> [104] </span><span class="HoverFoot"><span class="SupFootMarker"> [104] </span>RDMA read requests may be posted to the send queue using <tt>ibv_post_send()</tt>, just like RDMA write requests.</span></span> The responder, which in this case is the sender, transfers data from its DRAM memory to its host channel adapter without intervention of a processor or the operating system.
</div>
<h? class="Subsubsection">
<b><u>Infiniband latency and bandwidth</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:mpi-infini-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="right" valign="top">
RDMA write latency
</td>
<td align="left" valign="top">
1.83 <span class="formula"><i>μ</i></span>secs
</td>

</tr>
<tr>
<td align="right" valign="top">
RDMA read latency
</td>
<td align="left" valign="top">
3.42 <span class="formula"><i>μ</i></span>secs
</td>

</tr>
<tr>
<td align="right" valign="top">
RDMA write bandwidth
</td>
<td align="left" valign="top">
6.24 GB/s
</td>

</tr>
<tr>
<td align="right" valign="top">
RDMA read bandwidth
</td>
<td align="left" valign="top">
6.16 GB/s
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.1 Latency and bandwidth of bidirectional data transfer between two hosts on a QDR Infiniband network.
</div>

</div>

</div>
Linux Infiniband installation provides commands for measuring network latency and bandwidth. Measurements carried out using the commands <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">ib_write_lat, ib_read_lat, ib_write_bw, ib_read_bw
</pre>
</div>

</div>
<div class="Indented">
are reported in table <a class="Reference" href="#tab:mpi-infini-1">6.1↑</a>. The measurements are between two nodes. The latencies depend on the distance between the two hosts on the Infiniband network. 
</div>
<div class="Indented">
Both RDMA read and write have excellent point-to-point bandwidth of <span class="formula">6.2</span> GB/s. The realized bandwidth is well in excess of the advertised maximum of <span class="formula">40</span> Gbps or <span class="formula">5</span> GB/s for QDR infiniband. Interestingly, bandwidths above <span class="formula">6</span> GB/s are realized between MPI processes as well.
</div>
<div class="Indented">
The RDMA read latency is nearly twice the RDMA write latency. We will look more closely at RDMA write and RDMA read to attempt to explain this phenomenon. The microsecond latencies of table <a class="Reference" href="#tab:mpi-infini-1">6.1↑</a> are impressive. The overhead of a simple spinlock can be a few microseconds. Infiniband network latencies are only about a factor of <span class="formula">10</span> greater than latencies to DRAM.
</div>
<div class="Indented">
To measure RDMA write latency, one of the two hosts will do as follows. An RDMA write is posted using <tt>ibv_post_send()</tt>. The same host that posts the request immediately begins to poll the completion queue. The transfer is complete the moment a completion element is detected on the completion queue. RDMA write latency is the time elapsed from the moment before the request is posted to the moment after the completion is detected.
</div>
<div class="Indented">
The RDMA write latency measurement comprises the following events:
</div>
<ol>
<li>
The host channel adapter of the sender (or requester) initiates a DMA transfer. It prepares a single Infiniband data packet after the DMA transfer is complete. In addition to the local routing header and the base transport header, this data packet will have a <span class="formula">16</span>-byte RDMA extended transport header. The RDMA header has a field for the remote key. Because this is a latency test, the amount of data transferred must be small enough to be the payload of a single packet.
</li>
<li>
This packet travels to the receiver (or responder). The receiver reads the headers. It uses the RDMA header and its own virtual memory table to figure out the physical address it must write to. It initiates a DMA transfer to write the payload into its local DRAM memory.
</li>
<li>
The host channel adapter of the receiver (or responder) generates an ACK or acknowledgment packet and sends it to the requester.
</li>
<li>
The host channel adapter of the sender (or requester) generates a completion element and places it on the completion queue as soon as it receives ACK.
</li>

</ol>
<div class="Unindented">
RDMA write latency is approximately equal to the time taken by a round trip from source to destination and back.
</div>
<div class="Indented">
RDMA read latency comprises the following events:
</div>
<ol>
<li>
The receiver (or requester) generates an RDMA request packet.
</li>
<li>
The sender (or responder) initiates a DMA transfer using information in the RDMA request packet and its own virtual tables. As soon as the DMA transfer is complete, the sender prepares a single Infiniband packet with the appropriate payload and sends it to the receiver. This first response packet must include an ACK header as required by the Infiniband standard.
</li>
<li>
The host channel adapter of the receiver receives the first response packet and initiates a DMA transfer of its payload to local memory.
</li>

</ol>
<div class="Unindented">
If the RDMA read is considered to be complete at this point, it too would comprise one round trip from the source to destination. Why then is the RDMA read latency nearly twice the RDMA write latency? Our best surmise is that the RDMA read cannot be considered to be complete at this point.
</div>
<div class="Indented">
To implement a reliable connection, the sender has to wait for acknowledgment of receipt of packets from the receiver. If it does not receive an acknowledgment within a specified time frame, it must resend the packet. Resending of dropped packets is the basis of a reliable network. 
</div>
<div class="Indented">
In the case of RDMA read latency test over a <i>reliable</i> connection, the receiver cannot conclude that the transaction is complete as soon as it gets a data packet. The sender will repeatedly resend the same data packet if it does not receive a message that the transaction is over---such resending being the basis of reliable connections. Therefore, the receiver must generate another packet and send it to the sender to tell it not to resend. In addition, the receiver must wait to make sure that the message has reached the sender before shutting down the transaction and placing a completion element on the completion queue. Therefore, the RDMA read latency test must comprise two round trips, thus explaining why the RDMA read latency is nearly twice the RDMA write latency. 
</div>
<div class="Indented">
Generating ACK packets and resending data packets can get quite complicated at the level of the physical link. For example, an ACK packet itself may get lost. The agent that generated the ACK packet has to detect this loss based on repeated resends by the opposite agent. A second ACK packet must be generated and sent. At the other end, the network has to allow for the possibility that the first ACK packet was not really lost but only slow to arrive. Failure to handle such events correctly can leave the network in an inconsistent state.
</div>
<div class="Indented">
A typical RDMA write or read transaction will not involve packet loss. However, the protocol used to send and receive packets must plan for all eventualities. 
</div>
<div class="Indented">
Infiniband technology is notable for  low latency as well as excellent bandwidth. Bypassing the kernel is advantageous for both latency and bandwidth. With regard to latency, bypassing the kernel saves at least one system call and at least one copy to kernel buffers at each end. With regard to bandwidth, it is quite likely that the conversion of data in program memory to Infiniband data packets is pipelined on the host channel adapter. 
</div>
<h? class="Subsubsection">
<b><u>MPI protocol for short messages</u></b>
</h?>
<div class="Unindented">
MPI libraries use RDMA to transmit large messages over Infiniband networks. The responder must send its remote key to the requester before an RDMA operation is initiated. There are other overheads that arise when MPI library calls are mapped to Infiniband Verbs. Latency measurements do not include these overheads. For large messages, the overheads are a small fraction of the transmission time and therefore not significant. However, the overhead of setting up RDMA transfers can prove to be too great for small messages.
</div>
<div class="Indented">
The other alternative is to use channel semantics. When two communicating processes use a channel, the sender posts a send work request to the send queue of a queue pair, and the receiver posts a receive request to the receive queue of another queue pair that is bound to the sender’s queue pair. When the sent message arrives at the receiver, it has no information about the region of memory to which its payload must be copied. The send must be matched with a receive to complete the data transfer. Having to match sends with receives implies that the MPI library has to implement flow control to the extent it uses channel semantics. The need for flow control, which brings with it the need to retry and resend messages, makes channel semantics unattractive.
</div>
<div class="Indented">
For long messages, the overhead of arranging a rendezvous between the sender and receiver before initiating an RDMA transfer is not significant. For short messages, this overhead can be avoided if there are preassigned buffers to which the messages are copied.<span class="FootOuter"><span class="SupFootMarker"> [105] </span><span class="HoverFoot"><span class="SupFootMarker"> [105] </span>The transmission of short messages using preassigned buffers and RDMA is described by <span class="bibcites">[<a class="bibliocite" name="cite-52" href="#biblio-52"><span class="bib-index">52</span></a>]</span>. </span></span> Open MPI and other MPI implementations do just that. They use preassigned buffers and RDMA transfer for short messages. The buffers at the sender and the receiver are matched in advance. The sender will copy its short message to a send buffer as soon as a send function call is issued in the MPI library. The buffer is transmitted using RDMA write. The receiver polls the corresponding buffer on the receive side to check whether the receive is complete. A special flag is written into the receive buffer to indicate that the data transfer is complete. 
</div>
<div class="Indented">
Using preassigned buffers and RDMA transfers means that short messages must be copied to and from buffers. MPI implementations may use a combination of RDMA with preassigned buffers and channel semantics for short messages.
</div>
<div class="Indented">
In this chapter, we attempt to tie discussion of the MPI library to Infiniband architecture as far as possible. An advantage is that some phenomena that otherwise look arbitrary will begin to look more reasonable. Knowledge of how the network actually works helps us make better use of the MPI library. An example is the connection of the short message protocol to the behavior of <tt>unsafe()</tt> defined in section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↑</a>. The function <tt>unsafe()</tt> deadlocks if the message is longer than <span class="formula">12</span> KB but not for shorter messages. What is special about messages smaller than <span class="formula">12</span> KB? The command <tt>ompi_info</tt> reveals that by default Open MPI treats messages as short if they are shorter than <span class="formula">12</span> KB. For short messages, <tt>MPI_Send()</tt> copies the message to a local buffer and returns immediately. The buffer will be transmitted using RDMA.
</div>
<div class="Indented">
However, for longer messages, the MPI library registers the buffer in <tt>MPI_Send()</tt> and sends the remote key to the destination. The destination is expected to initiate an RDMA read when it comes across a matching <tt>MPI_Recv()</tt>. The <tt>MPI_Send()</tt> returns after the RDMA operation is complete. For longer messages, the processes of rank <span class="formula">0</span> and <span class="formula">1</span> each execute <tt>MPI_Send()</tt> in <tt>unsafe()</tt> and wait for an RDMA read operation to be initiated by the opposite party. They deadlock in that state because neither of them gets to execute the matching <tt>MPI_Recv()</tt> and initiate an RDMA read operation.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Consider a perfect binary tree with <span class="formula">2<sup><i>n</i></sup></span> leaves at level <span class="formula"><i>n</i></span>. Assume that messages pass between each pair of leaves in either direction with equal probability. Calculate the expected number of messages passing between level <span class="formula"><i>k</i></span> and level <span class="formula"><i>k</i> + 1</span> for <span class="formula"><i>k</i> = 0, …, <i>n</i> − 1</span>. The number of edges between levels <span class="formula"><i>k</i></span> and <span class="formula"><i>k</i> + 1</span> is <span class="formula">2<sup><i>k</i> + 1</sup></span>. By what factor does the number of messages per edge vary?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Figure <a class="Reference" href="#fig:mpi-fat-tree-lei">6.1↑</a> shows how a fat-tree with <span class="formula">8</span> leaves may be laid out on a plane surface. Describe the layout for fat-trees with <span class="formula">16</span>, <span class="formula">32</span>, and <span class="formula">64</span> leaves.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  In the Infiniband architecture, what is the maximum number of bytes that can be broken up into a single train of packets?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Find out the format of TCP/IP packets as well as Ethernet frames. Are the Infiniband packets more similar to TCP/IP packets or to Ethernet frames?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Is it possible to create protection domains without kernel intervention? Is it possible to set up page tables without kernel intervention?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  To initiate an RDMA write, the initiating node or the requester needs to know the remote address into which the data should be written. Should this address be a virtual address or a physical address or can it be either?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  When an RDMA read or write function call is made, the requester needs to know the memory location at the responder that will be involved in the data transfer. Therefore, there must be a prior exchange of information between the requester and the responder before the RDMA transaction is initiated. This prior exchange involves an overhead. Explain why RDMA reads and writes can be efficient despite this overhead.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-6.3">6.3</a> MPI examples<a class="Label" name="sec:ntwks-mpi-examples"> </a>
</h2>
<div class="Unindented">
In this section, we get more deeply into MPI syntax. Our account of MPI syntax is not extensive. However, every bit of syntax introduced is related to computer architecture. MPI can make programming a lot more challenging. Yet users take the trouble to write MPI programs  to make programs faster and solve bigger problems. MPI syntax can look a bit monochromatic, and by looking at MPI syntax (or semantics), one cannot gain any sense of how effective MPI facilities may be in speeding up programs. The discussion of Infiniband network architecture in the previous section will help us learn MPI with a double focus on syntax and its effectiveness.
</div>
<div class="Indented">
We introduce standard optimizations such as load balancing and overlapping processor activity with network activity. At a conceptual level, these optimizations are no more than common sense. At a practical level, implementing such optimizations effectively can be quite challenging.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:mpi-send-recv">6.3.1↓</a> is about sending and receiving of messages. Three versions of send and receive are discussed. In persistent send/receive, the sender and the receiver preallocate memory using the MPI library. All the sends and receives are from or to the same preallocated memory. In nonblocking send/receive, the send/receive function returns quickly, and it is up to the user to use functions such as <tt>MPI_Wait()</tt> and <tt>MPI_Test()</tt> to confirm that the sending and receiving has indeed completed. Persistent send/receives are nonblocking. We also discuss blocking send/receive and explain why they are slower than the nonblocking versions.
</div>
<div class="Indented">
On Infiniband clusters, all MPI sends and receives map to RDMA transactions. MPI send and receive are not one sided. MPI requires that receives must be matched against sends. However, RDMA transactions are one sided. Typically, the effect of an MPI send/receive is an exchange of message envelopes to enable RDMA transactions to be set up. The actual RDMA transaction is initiated later after the envelopes have been matched. MPI send/receive transactions can be mapped to either RDMA send or RDMA receive. For reasons explained in section <a class="Reference" href="#sub:mpi-send-recv">6.3.1↓</a>, RDMA receive is slightly more natural, and Open MPI opts for it. 
</div>
<div class="Indented">
Our systems/architecture perspective brings out some other aspects of sending and receiving in MPI. Persistent MPI send/receive may seem the best because MPI is given advance notice of the memory that will be used in subsequent sends and receives. In fact, giving advance notice of memory to be used is not a good idea in Open MPI. When MPI receives advance notice of memory to be used, it allocates page frames for all the memory. That has the undesirable effect of making it impossible to use demand paging to allocate pages in near memory, when the MPI process is multithreaded.
</div>
<div class="Indented">
Much of the time, the nonblocking <tt>MPI_Isend()</tt> and <tt>MPI_Irecv()</tt> variants are the best options. In modern networks such as Infiniband, the bandwidth of data transfer out of a single node can be around 10% to 20% of the peak bandwidth to DRAM memory from a single node. Such remarkably high bandwidths have raised the potential of network computing. A great range of scientific problems, from optimization and data mining to numerical PDE, are amenable to parallelization across high-performance networks.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:mpi-Jacobi-iteration">6.3.2↓</a>, we turn to the Jacobi iteration, a standard example in the MPI world. There is a single MPI process on each computing node, which is true in all our examples, and each MPI process uses OpenMP threads to carry out its share of the Jacobi iteration. In the Jacobi example, the cost of MPI message passing is negligible. The optimizations that make a difference are the sort of memory optimizations we have already studied. This situation is not uncommon.
</div>
<div class="Indented">
The matrix transpose example of section <a class="Reference" href="#sub:mpi-transpose">6.3.3↓</a> gives a good picture of how MPI optimizations work. In this example, each process communicates with every other process. We use the matrix transpose example to illustrate both load balancing and the overlapping of processor activity with network activity.
</div>
<div class="Indented">
In matrix transposition, the data format at the receiver differs from the data format at the sender. Having to cope with changes in data format comes up in all kinds of network programming. The change in data format implies a need for buffers and copying into and out of buffers. The challenge in matrix transposition is to hide the cost of copying into and out of buffers by overlapping with network activity.
</div>
<div class="Indented">
Prior to that, one has to ensure that copying is done in an efficient manner. In section <a class="Reference" href="#sub:mpi-transpose">6.3.3↓</a>, we find that a more efficient and multithreaded copy improves the effective network bandwidth from less than <span class="formula">15</span>% of the peak to about <span class="formula">50</span>%.
</div>
<div class="Indented">
Overlapping network activity with processor activity requires knowledge of exactly how RDMA calls are issued by the MPI library. MPI calls can be mapped to RDMA in several ways. In Open MPI, the RDMA calls are typically issued, not during send or receive, but when calls such as <tt>MPI_Test()</tt> are used to verify whether nonblocking send/receive have completed. Other implementations of MPI may differ in this respect. Once the processor activity is overlapped with network activity, the effective network bandwidth rises to between <span class="formula">75</span>% and <span class="formula">90</span>% of the peak capability of the network. Further optimizations may be possible as discussed in section <a class="Reference" href="#sub:mpi-transpose">6.3.3↓</a>.
</div>
<div class="Indented">
In sending and receiving, an MPI call made by the receiver is typically matched against an MPI call made by the sender. In collective communication, all MPI processes participate. For example, in a minimizing reduction, all the process cooperate to find the minimum of data that is uniformly split between the processes. Section <a class="Reference" href="#sub:mpi-coll">6.3.4↓</a> is an introduction to collective communication in MPI.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:mpi-diskio">6.3.5↓</a> introduces complex MPI syntax for reading and writing files in parallel. The Lustre file system is also introduced. While the bandwidth to disk on a typical file system is of the order of <span class="formula">100</span> MB/s (see section <a class="Reference" href="#sec:memory-diskio">4.3↑</a>), Lustre bandwidths can be a hundred times as much. The downside is the complexity of the interface. 
</div>
<div class="Indented">
Much of the MPI syntax in this section may look simple. Yet even simple MPI programs can become quite difficult to maintain. Data input and output to MPI programs can be cumbersome to manage. Because the data is laid out across multiple nodes, even basic tasks can require coordination between MPI processes. In general, MPI programs are difficult to interact with.
</div>
<div class="Indented">
A programmer will do well to first ask whether the task at hand can be dealt with on a single node. OpenMP-type multithreaded programs are much easier to handle, and the range and size of problems that can be tackled on a single computer is growing rapidly.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.3.1">6.3.1</a> Variants of MPI send and receive<a class="Label" name="sub:mpi-send-recv"> </a>
</h3>
<div class="Unindented">
Function calls that send and receive messages are the heart of the MPI library. An MPI user who masters sending and receive messages, but not much more, can get by quite well. 
</div>
<div class="Indented">
Although the syntax for sending and receiving messages is simple (relative to other MPI syntax), the MPI function calls do not map to RDMA directly. MPI send is issued by a process without explicitly accounting for the matching receive on the remote process. Indeed, the remote process may use multiple receives to match a single send. If MPI function calls are mapped directly to the network, the network will have to use channel semantics. Channel semantics implies the need for wasteful buffering and flow control inside the MPI library. RDMA calls completely bypass the kernel and do not require the MPI library to implement flow control. However, MPI send and receive do not map to RDMA function calls directly.
</div>
<div class="Indented">
In this section, we describe several variants of send and receive in the MPI library. We begin with the variant that maps to RDMA in the best possible manner. The blocking variants are in wide use, but they waste nearly half the network bandwidth, as we will see.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-exchange-1"> </a><div class="figure">
<div class="PlainVisible">
<div class="center">
<img class="embedded" src="FIGS/chapter5/exchange.png" alt="figure FIGS/chapter5/exchange.png" style="max-width: 227px; max-height: 49px;"/>

</div>
<br/>
<div class="center">
<div class="caption">
Figure 6.6 Data exchange between two nodes.
</div>

</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-exchange-1">6.6↑</a> shows the communication pattern we will implement in this section. We consider the situation with just two hosts. Each host has a send and a receive buffer. Each host sends its send buffer to the other host. The other host copies the received message to its receive buffer. 
</div>
<div class="Indented">
This type of exchange communication between two hosts is a good place to start. Even if the network has many active hosts, the communication between any two hosts is likely to fit this pattern. On the Internet, TCP/IP connections between a client and a server involves a pattern of communication similar to that shown in figure <a class="Reference" href="#fig:mpi-exchange-1">6.6↑</a>.
</div>
<div class="Indented">
The network bandwidth is measured at each host as the number of bytes that are transferred into or out of that host in a single cycle. This manner of measuring network bandwidth is in line with the fat-tree architecture of the Infiniband network. The fat-tree is designed to realize good bidirectional bandwidth at each port of each switch. 
</div>
<h? class="Subsubsection">
<b><u>Persistent exchange</u></b>
</h?>
<div class="Unindented">
In persistent communication as described here, user processes secure memory using the MPI library (or even a facility such as <tt>malloc()</tt> or <tt>new[]</tt>---but that is not recommended). Send and receive request objects are initialized using memory secured from the MPI library. Sends and receives are initiated using the same request objects. The termination of the data transfer is also verified using the request objects.
</div>
<div class="Indented">
The <tt>Exchg</tt> C++ class, which will be described, is an interface for persistent exchange of data between two hosts. The usage of C++ confirms to earlier practice in this book. The C++ class is mainly a convenient interface that reduces burden on the programmer’s memory. Class constructors and destructors are convenient for setting up and tearing down persistent communication between MPI processes.
</div>
<div class="Indented">
The <tt>Exchg</tt> class is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>class Exchg{
<span class="number-left">2</span>private:
<span class="number-left">3</span>     double *sendbuf;
<span class="number-left">4</span>     double *recvbuf;
<span class="number-left">5</span>     int bufsize;
<span class="number-left">6</span>     MPI_Request req1;
<span class="number-left">7</span>     MPI_Request req2;
<span class="number-left">8</span>     TimeStamp clk;
<span class="number-left">9</span>public:
<span class="number-left">10</span>     Exchg(int rank, int nprocs, int bufsize);
<span class="number-left">11</span>     ~Exchg();
<span class="number-left">12</span>     void post();
<span class="number-left">13</span>     double wait();
<span class="number-left">14</span>     int getbufsize(){return bufsize;}
<span class="number-left">15</span>     double *getsbuf(){return sendbuf;}
<span class="number-left">16</span>     double *getrbuf(){return recvbuf;}
<span class="number-left">17</span>};
</pre>
</div>

</div>
<div class="Indented">
The class constructor (line 10) takes the MPI rank and the total number of MPI processes as its first two arguments. Its third and last argument is the buffer size (line 10). Each process gets an <tt>Exchg</tt> object of its own. If we have two processes, the send and receive buffers of lines 3 and 4 exchange data as shown in figure <a class="Reference" href="#fig:mpi-exchange-1">6.6↑</a>. The constructor uses the process rank and the total process count to set up persistent communication. 
</div>
<div class="Indented">
The MPI request objects,  which are used to initialize and initiate message passing, are defined on lines 6 and 7. The message passing transactions are initiated by the member function<tt> </tt>post() (line 12). The member function <tt>wait()</tt> (line 13) returns when the transactions are complete. The clock defined on line 8 starts ticking when the transactions are posted. The member function <tt>wait()</tt> returns the time taken for the exchange transaction.
</div>
<div class="Indented">
The member functions defined on lines 14, 15, and 16 access the buffer size, the send buffer, and the receive buffer, respectively.
</div>
<div class="Indented">
The class constructor is defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>Exchg::Exchg(int rank, int nprocs, int bsize){
<span class="number-left">2</span>	assrt(nprocs==2);
<span class="number-left">3</span>	bufsize = bsize;
<span class="number-left">4</span>	MPI_Alloc_mem(bufsize*8, MPI_INFO_NULL, 
<span class="number-left">5</span>		      (void *)(&amp;sendbuf));
<span class="number-left">6</span>	MPI_Alloc_mem(bufsize*8, MPI_INFO_NULL, 
<span class="number-left">7</span>		      (void *)(&amp;recvbuf));
<span class="number-left">8</span>	int tag = 0;
<span class="number-left">9</span>	if(rank==0){
<span class="number-left">10</span>		MPI_Send_init(sendbuf, bufsize, MPI_DOUBLE, 
<span class="number-left">11</span>			  nprocs-1, tag, MPI_COMM_WORLD, &amp;req1);
<span class="number-left">12</span>		MPI_Recv_init(recvbuf, bufsize, MPI_DOUBLE, 
<span class="number-left">13</span>			  nprocs-1, tag, MPI_COMM_WORLD, &amp;req2);
<span class="number-left">14</span>	}
<span class="number-left">15</span>	else if(rank==nprocs-1){
<span class="number-left">16</span>		MPI_Send_init(sendbuf, bufsize, MPI_DOUBLE, 
<span class="number-left">17</span>			      0, tag, MPI_COMM_WORLD, &amp;req1);
<span class="number-left">18</span>		MPI_Recv_init(recvbuf, bufsize, MPI_DOUBLE, 
<span class="number-left">19</span>			      0, tag, MPI_COMM_WORLD, &amp;req2);
<span class="number-left">20</span>	}
<span class="number-left">21</span>}
</pre>
</div>

</div>
<div class="Indented">
This constructor works correctly even if the total number of MPI processes is greater than <span class="formula">2</span>. However, other member functions assume the number to be <span class="formula">2</span>. Thus, the assertion that <tt>nprocs</tt> must be exactly <span class="formula">2</span> on line 2. 
</div>
<div class="Indented">
The constructor sets up persistent communication between processes of ranks <span class="formula">0</span> and <span class="formula">1</span> (or <tt>nprocs-1</tt>). The send and receive buffers (<tt>senbuf</tt> and <tt>recvbuf</tt>) are allocated on lines 4 and 6. The function <tt>MPI_Alloc_mem()</tt> takes three arguments. The first argument is the size of the buffer in bytes. The second argument has a null value as shown on lines 4 and 5. It is another of MPI’s features we prefer to stay away from. The third argument is used to return the allocated memory. This function as well as most other MPI functions return error codes. The error codes are ignored by this constructor.
</div>
<div class="Indented">
The library function <tt>MPI_Alloc_mem()</tt>  will register memory. Memory allocation using <tt>malloc()</tt> or <tt>new[]</tt> will allocate a memory region in virtual address space. The actual mapping to physical pages is done by the page fault handler during first access of the allocated memory. Allocation using <tt>MPI_Alloc_mem()</tt> has a different effect. Not only are physical page frames allocated, but an Infiniband Verb is invoked and the mapping from virtual address space to physical pages is registered on the host channel adapter. 
</div>
<div class="Indented">
The syntax for setting up send and receive requests appears in the if-block on lines 9 through 14 as well as else-if block on lines 15 through 20. The first block is for the process of rank <span class="formula">0</span> and the latter block for the process of rank <span class="formula">1</span>. The first three arguments to <tt>MPI_Send_init()</tt> are the buffer, buffer size, and the type of items in the buffer. The next three arguments are the destination rank, message tag, and the communicator. These six arguments have exactly the same order and meaning as the six arguments of <tt>MPI_Send()</tt> discussed in section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↑</a>. The seventh argument is new here, however. The seventh argument is a pointer to an MPI request object. The MPI library will allocate and initialize the request object. 
</div>
<div class="Indented">
The syntax of <tt>MPI_Recv_init()</tt> closely matches that of <tt>MPI_Recv()</tt> discussed in section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↑</a>. The first six arguments have the same order and meaning---buffer, buffer size, type of items in the buffer, message source, message tag, and communicator. The seventh argument differs, however. The seventh argument of <tt>MPI_Recv_init()</tt> is a pointer to a request object. The library allocates and initializes the request object.
</div>
<div class="Indented">
The buffers used by <tt>MPI_Send_init()</tt> and <tt>MPI_Recv_init()</tt> may be allocated using <tt>malloc()</tt> or <tt>new[]</tt>. But doing so would imply no particular advantage in issuing Infiniband RDMA calls. When send and receive request objects are initialized, as in this constructor, usage of <tt>MPI_Alloc_mem()</tt> to register memory is recommended.
</div>
<div class="Indented">
The destructor of the <tt>Exchg</tt> class is defined as follows: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>Exchg::~Exchg(){
<span class="number-left">2</span>     MPI_Free_mem(sendbuf);
<span class="number-left">3</span>     MPI_Free_mem(recvbuf);
<span class="number-left">4</span>     MPI_Request_free(&amp;req1);
<span class="number-left">5</span>     MPI_Request_free(&amp;req2);
<span class="number-left">6</span>}
</pre>
</div>

</div>
<div class="Indented">
The library function <tt>MPI_Free_mem()</tt> is used to deallocate memory allocated using <tt>MPI_Alloc_mem()</tt>.  Requests allocated when send and receive are initialized by <tt>MPI_Send_init()</tt> and <tt>MPI_Recv_init()</tt> are deallocated. A useful guideline is to confine memory allocation to class constructors and memory deallocation to destructors as far as possible. The definition of this destructor follows that guideline.
</div>
<div class="Indented">
Posting the send and receive requests could not be easier:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Exchg::post(){
     clk.tic();
     MPI_Start(&amp;req1);
     MPI_Start(&amp;req2);
}
</pre>
</div>

</div>
<div class="Indented">
All the message passing information gets tucked away in the request objects when the send and receive are initialized. So the syntax for issuing a request is quite simple. Notice that the argument to <tt>MPI_Start()</tt> is a pointer to <tt>MPI_Request</tt>. The library function <tt>MPI_Start()</tt> is nonblocking. It returns as soon as the request is issued. The clock starts ticking before the requests are initiated.
</div>
<div class="Indented">
The definition of the member function that waits for the requests to complete follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double Exchg::wait(){
     MPI_Wait(&amp;req1, MPI_STATUS_IGNORE);
     MPI_Wait(&amp;req2, MPI_STATUS_IGNORE);
     double cycles = clk.toc();
     return cycles;
}
</pre>
</div>

</div>
<div class="Indented">
The library function <tt>MPI_Wait() </tt> takes a pointer to an <tt>MPI_Request</tt> as its first argument. It blocks until that request completes. The second argument is a pointer to <tt>MPI_Status</tt>. The number of items actually received may be fewer than the maximum number of items the receive buffer can hold. In that case, the status object can be passed to <tt>MPI_Get_count()</tt> to figure out the exact number of items received. Our exchange class uses the same buffer size on processes of ranks <span class="formula">0</span> and <span class="formula">1</span>, and for receiving and sending. There is no need to query the status object. The status object that occurs as the second argument to <tt>MPI_Wait()</tt> also carries error handling information. The error handling information is ignored by the member function <tt>wait()</tt>. The member function <tt>wait()</tt> reads the clock after the requests complete and returns the number of cycles elapsed after the requests were initiated in <tt>post()</tt>.
</div>
<div class="Indented">
In general, between posting a request and waiting for it to complete, a process can do useful computation. MPI provides another function called <tt>MPI_Test()</tt>. This function probes to find out whether the request is complete and returns immediately with the result of the probe. The probe function creates more sophisticated possibilities for overlapping communication across the network with computation within the processors. 
</div>
<div class="Indented">
To conclude our discussion of persistent message passing using the <tt>Exchg</tt> class, we give a code fragment that shows how the <tt>Exchg</tt> class may be used.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int rank;
int nprocs;
int bufsize = 100;
mpi_initialize(rank, nprocs);
Exchg exchg(rank, nprocs, bufsize);
double *sbuf = exchg.getsbuf();
double *rbuf = exchg.getrbuf();
for(int i=0; i &lt; bufsize; i++)
     sbuf[i] = rank*100;
exchg.post();
exchg.wait();
for(int j=0; j &lt; bufsize; j++)
     sbuf[j] = rbuf[j];
MPI_Finalize();
</pre>
</div>

</div>
<div class="Indented">
The function <tt>mpi_initialize()</tt> was defined in section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↑</a>. It is assumed that the MPI run is set up with two processes. The way to set up an MPI run was discussed in section <a class="Reference" href="#sub:mpi-initializing">6.1.1↑</a>. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-exchange-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
Persistent
</td>
<td align="center" valign="top" style="width: 2.8cm;">
Nonblocking (pinned)
</td>
<td align="center" valign="top" style="width: 2.8cm;">
Nonblocking (pipelined)
</td>
<td align="center" valign="top">
Blocking
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>2</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">0.13</span>
</td>
<td align="center" valign="top">
<span class="formula">0.05</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.43</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">0.42</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">0.53</span>
</td>
<td align="center" valign="top">
<span class="formula">0.27</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.43</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.27</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">0.79</span>
</td>
<td align="center" valign="top">
<span class="formula">0.73</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.90</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.88</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.40</span>
</td>
<td align="center" valign="top">
<span class="formula">0.98</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.76</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.72</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.22</span>
</td>
<td align="center" valign="top">
<span class="formula">0.96</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>7</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.74</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.71</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.21</span>
</td>
<td align="center" valign="top">
<span class="formula">0.96</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>8</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.73</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.71</span>
</td>
<td align="center" valign="top" style="width: 2.8cm;">
<span class="formula">1.24</span>
</td>
<td align="center" valign="top">
<span class="formula">0.96</span>
</td>

</tr>

</table>
<div class="caption">
Table 6.2 Bandwidth in bytes/cycle for exchanging data between two hosts over a QDR Infiniband network. All hosts are <span class="formula">3.33</span>  GHz SSE2 machines (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). The data exchanged consists of <span class="formula"><i>n</i></span> double-precision numbers. Each reported bandwidth is the median of several measurements. 
</div>

</div>

</div>

</div>
We emphasize use of the <tt>-bynode</tt> option (or equivalent) when setting up an MPI run. Use of that option is crucial for the validity of the numbers reported in table <a class="Reference" href="#tab:mpi-exchange-1">6.2↑</a>. If that option is absent, the two MPI processes may run on two processor cores of the same computer. None of the message passing will be over the network. When processes run on the same compute node, the faster, easier, and more modular way for them to communicate is using shared memory. 
</div>
<div class="Indented">
The bandwidth for persistent exchange reported in table <a class="Reference" href="#tab:mpi-exchange-1">6.2↑</a> is well in excess of the advertised bandwidths. The maximum reported bandwidth is <span class="formula">1.90 × 3.33 = 6.33</span> GB/s, which is well above the advertised maximum of <span class="formula">40</span> Gbps or <span class="formula">5</span> GB/s for QDR Infiniband. Such high bandwidths are a consequence of implementing MPI message passing using RDMA. The bandwidth reported is the number of bytes transferred into or out of the process with rank <span class="formula">0</span> in a single cycle.
</div>
<div class="Indented">
If the number of double-precision numbers exchanged is <span class="formula"><i>n</i> = 10<sup>2</sup></span> or <span class="formula"><i>n</i> = 10<sup>3</sup></span>, each message is shorter than <span class="formula">12</span> KB. Open MPI, the implementation we use, uses the short message protocol already described in the previous section. The manner in which longer messages are mapped to RDMA transfers remains to be explained.
</div>
<div class="Indented">
To pass messages of size longer than <span class="formula">12</span> KB, the MPI implementation has to choose between RDMA read and RDMA write. Open MPI chooses RDMA read. RDMA read has a bandwidth that is slightly lower than that of RDMA write but is a better fit to MPI semantics. When sender posts a send request, the MPI library will package the key to the registered memory or the send buffer, along with other information about the message such as its tag and communicator. This little package of information can be immediately sent over to the receiving host. The manner in which the remote key is transmitted is not prescribed by the Infiniband standard. The MPI library is free to use channel semantics over Infiniband or even TCP/IP sockets to transfer this little package of information.
</div>
<div class="Indented">
When a receive request is posted at a receiving host, the MPI library can go over all the information received from senders and quickly look for a fit. An RDMA read can be initiated as soon as a match is found. 
</div>
<div class="Indented">
RDMA read is a slightly better fit for MPI message passing for the following reasons. The remote key must be exchanged before an RDMA transaction is initiated. Sending the remote key suits the sender better. MPI semantics allows a receive to consume only part of the data in a sent message. Determining exactly how many items will be consumed from a sent message is easier to calculate at the receiving side.
</div>
<div class="Indented">
Once the RDMA read is complete, the receiver can discover the completion by checking the completion queue. Because RDMA read is completely one sided, the sender has no way to discover the completion directly. The receiver has to send a message to the sender to notify completion of the transaction. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:mpi-exchange-1">6.2↑</a> shows a significant jump in bandwidth from <span class="formula"><i>n</i> = 10<sup>3</sup></span> to <span class="formula"><i>n</i> = 10<sup>4</sup></span>. This jump corresponds to the transition from the short message protocol to RDMA read. 
</div>
<div class="Indented">
For <span class="formula"><i>n</i> ≥ 10<sup>4</sup></span>, the bandwidth is maximum for <span class="formula"><i>n</i> = 10<sup>5</sup></span> and then begins to fall slightly. The network appears to prefer message lengths of around <span class="formula">1</span> MB. It is possible, but not certain, that this preference for message lengths of around <span class="formula">1</span> MB is related to the size of buffers inside the host channel adapter. 
</div>
<h? class="Subsubsection">
<b><u>Nonblocking exchange</u></b>
</h?>
<div class="Unindented">
Persistent send and receive, which we just discussed, is nonblocking. There is a more direct way to initiate nonblocking send and receive operations, which avoids the need to initialize request objects.
</div>
<div class="Indented">
A function for exchanging data between two MPI processes using nonblocking calls follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double exchange_nonblocking(int rank, int nprocs, 
<span class="number-left">2</span>			    double* sendbuf, 
<span class="number-left">3</span>			    double* recvbuf, 
<span class="number-left">4</span>			    int bufsize){
<span class="number-left">5</span>     TimeStamp clk;
<span class="number-left">6</span>     int tag = 0;
<span class="number-left">7</span>     double cycles=0;
<span class="number-left">8</span>     MPI_Request req1, req2;
<span class="number-left">9</span>     int other = (rank==0)?nprocs-1:0;
<span class="number-left">10</span>     clk.tic();
<span class="number-left">11</span>     MPI_Isend(sendbuf, bufsize, MPI_DOUBLE, other, 
<span class="number-left">12</span>	          tag, MPI_COMM_WORLD, &amp;req1);
<span class="number-left">13</span>     MPI_Irecv(recvbuf, bufsize, MPI_DOUBLE, other, 
<span class="number-left">14</span>	          tag, MPI_COMM_WORLD, &amp;req2);
<span class="number-left">15</span>     MPI_Wait(&amp;req1, MPI_STATUS_IGNORE); 
<span class="number-left">16</span>     MPI_Wait(&amp;req2, MPI_STATUS_IGNORE);
<span class="number-left">17</span>     cycles = clk.toc();
<span class="number-left">18</span>     return cycles;
<span class="number-left">19</span>}
</pre>
</div>

</div>
<div class="Indented">
The nonblocking <tt>MPI_Isend() </tt> occurs on lines 11 and 12. Its first six arguments are as before: buffer, buffer size or number of items in the buffer, type of each item, destination, tag, and communicator. The seventh and last argument is a pointer to a request object. The nonblocking <tt>MPI_Irecv()</tt> occurs on lines 13 and 14. Its fourth argument is <tt>other</tt>, which is the source of the message  received. The seventh and last argument is a pointer to a request object as before.
</div>
<div class="Indented">
This function uses <tt>MPI_Wait()</tt> on lines 15 and 16 to wait for the completion of the nonblocking send and receive. A program could do some useful computation after posting a nonblocking send or receive and before waiting for its completion. This function does not. The function returns the number of cycles used during the exchange.
</div>
<div class="Indented">
How this function uses the Infiniband network depends on the way the send and receive buffers (<tt>sendbuf</tt> on line 2 and <tt>recvbuf</tt> on line 3) are allocated. If the buffers are allocated using <tt>MPI_Alloc_mem()</tt>, this function is no different from persistent exchange. It will map to RDMA read in exactly the same way. There is not much to choose between persistent and nonblocking communication in that case.
</div>
<div class="Indented">
We will assume that both <tt>sendbuf</tt> and <tt>recvbuf</tt> are allocated using <tt>new[]</tt> or <tt>malloc()</tt>. Even with this assumption, it is preferable to use RDMA for large messages. But <tt>new[]</tt> and <tt>malloc()</tt> do not register memory, and<tt> </tt>difficulties arise when registering memory.
</div>
<div class="Indented">
If the data in the send buffer of one process is to be transferred to the receive buffer of another process using RDMA, both the buffers must be registered. Registering the memory has the following effect. The host channel adapter will note down the map from virtual to physical memory, as  already mentioned. But another element we have not mentioned so far becomes quite important here. This other element in registering memory is that physical memory page frames must be pinned. Otherwise the page frames may be assigned to different pages by the operating system kernel when the host channel adapter is moving data between DRAM and the network. 
</div>
<div class="Indented">
Although pinned memory<span class="FootOuter"><span class="SupFootMarker"> [106] </span><span class="HoverFoot"><span class="SupFootMarker"> [106] </span>The limit on pinned memory can be quite low. On the author’s desktop it is just 64 KB. However, on many high-performance clusters, this limit is set to infinity to enable RDMA communication. The limit may be found using the GNU/Linux command <tt>ulimit -a</tt>. </span></span> is essential if RDMA is to avoid the use of wasteful buffers, it is problematic for an Infiniband Verb to pin memory allocated outside MPI’s control using <tt>new[]</tt> or <tt>malloc()</tt>. We will discuss three different ways this problem can be tackled.
</div>
<div class="Indented">
The safest method is for MPI to register memory using an Infiniband Verb when a send/receive transaction is initiated and deregister the memory after the transaction is completed. The safe method is never used. It is too expensive and defeats the entire purpose of the Infiniband architecture. Pinning and unpinning memory require calling the operating system kernel. These calls are exposed to much of the complexity of the kernel’s memory management system and are expensive. The Infiniband architecture sets up host channel adapters, protection domains, and queue pairs so that messages can be sent directly from user processes to the host channel adapter while bypassing the kernel. Thus, to register and deregister memory at each transaction would defeat one of the principal goals of Infiniband design, which is to bypass the kernel during send and receive. Another safe method is for the MPI library to use internal buffers that are registered and pinned in advance. This safe method is also expensive and defeats the other principal goal of Infiniband design---avoiding wasteful buffering and buffer copies.
</div>
<div class="Indented">
The second method is fast but not perfectly safe. It is the method used by default by most MPI implementations. In this method, buffers are registered before communication, if they are not already registered. But the buffers are not deregistered once the communication ends. The expectation is that the same buffers will be repeatedly used for communication. Later communication incurs no overhead because of registering and deregistering memory. The deregistration may be performed during <tt>MPI_Finalize()</tt>. 
</div>
<div class="Indented">
This method is unsafe for the following reason. The user process may deallocate memory without knowledge of the MPI library. If it does that, the virtual memory table inside the host channel adapter becomes invalid. The user process may later claim some more memory that is mapped to the same virtual addresses as the deallocated memory but to different page frames in DRAM. Dangerous memory errors will follow. Techniques such as replacing <tt>malloc()</tt> and <tt>free()</tt> with MPI versions at link time can render this method safe. It is unclear how completely these are implemented within MPI libraries and how effective the implementations are in preventing memory errors. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:mpi-exchange-1">6.2↑</a> shows that the combination of memory allocation using C/C++ libraries and nonblocking communication leads only to a small loss in bandwidth if the second method is used. This method corresponds to the column marked &ldquo;pinned&rdquo; in the table. 
</div>
<div class="Indented">
While the second method is the default method, Open MPI implements a third method. This third method is turned on by setting the Open MPI parameter
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">mpi_leave_pinned
</pre>
</div>

</div>
<div class="Indented">
to zero. This third method is called pipelined RDMA.<span class="FootOuter"><span class="SupFootMarker"> [107] </span><span class="HoverFoot"><span class="SupFootMarker"> [107] </span>For pipelined RDMA, see <span class="bibcites">[<a class="bibliocite" name="cite-56" href="#biblio-56"><span class="bib-index">56</span></a>]</span>. </span></span> Table <a class="Reference" href="#tab:mpi-exchange-1">6.2↑</a> shows that the third method leads to a loss of bandwidth but is still quite fast.
</div>
<div class="Indented">
In pipelined RDMA,  the long message is broken up into pieces. Some of the pieces are sent using channel semantics. When these pieces are in transit, the MPI library registers some of the other pieces, and it deregisters them as the transfer completes. Pipelined RDMA overlaps registration and deregistration of memory with communication over the network so that the bandwidth is not severely compromised. It is perfectly safe. 
</div>
<div class="Indented">
Pipelined RDMA uses RDMA write, not RDMA read. The activity of breaking the message into pieces and overlapping memory registration with network activity is best performed at the sender’s end. Although RDMA write is one sided, like RDMA read, it provides a facility to pack immediate data into the last packet. This immediate data causes the host channel adapter to recognize completion of the RDMA write and inform the responder or receiver. The information is passed up to the Verb layer by placing a completion element on the completion queue.
</div>
<div class="Indented">
Of course, the pros and cons of the various methods for implementing nonblocking communication are rendered moot if the user process allocates buffers using <tt>MPI_Alloc_mem()</tt>. With such memory allocation, communication is safe but not necessarily fast.
</div>
<div class="Indented">
It might seem that it is always better to allocate send and receive buffers with <tt>MPI_Alloc_mem()</tt>. The MPI library can be relieved of the hassle of registering and deregistering memory. Yet there are important situations where the programmer is better off using <tt>new[]</tt> or <tt>malloc()</tt>. One such situation occurs when we discuss the matrix transpose later in this chapter. In Open MPI, <tt>MPI_Alloc_mem()</tt> assigns page frames to the allocated virtual memory pages immediately. The page frames will not be allocated in memory that is nearest to the processor core that accesses it for the first time or most frequently. In multithreaded programs, the use of <tt>MPI_Alloc_mem()</tt> may imply additional overhead in copying to send buffers and copying out of receive buffers. Such is the speed of the Infiniband network that this additional overhead in filling and emptying buffers can be quite significant, as we find in a later section on the matrix transpose.
</div>
<h? class="Subsubsection">
<b><u>Blocking exchange</u></b>
</h?>
<div class="Unindented">
The syntax of the blocking calls <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt> were discussed in section <a class="Reference" href="#sub:mpi-unsafe">6.1.2↑</a>, and there is not much to add. The function defined below reverses the order of send and receive on the process of rank <span class="formula">1</span> (or <tt>nprocs-1</tt>) to avoid deadlock.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double exchange_blocking(int rank, int nprocs, 
			 double* sendbuf, double* recvbuf, 
			 int bufsize){
     TimeStamp clk;
     int tag = 0;
     double cycles=0;
     if(rank==0){
          clk.tic();
          MPI_Send(sendbuf, bufsize, MPI_DOUBLE, 
	           nprocs-1, tag, MPI_COMM_WORLD);
          MPI_Recv(recvbuf, bufsize, MPI_DOUBLE, 
	           nprocs-1, tag, MPI_COMM_WORLD, 
                         MPI_STATUS_IGNORE);
          cycles = clk.toc();
     }
     else if(rank==nprocs-1){
          MPI_Recv(recvbuf, bufsize, MPI_DOUBLE, 
	      0, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
          MPI_Send(sendbuf, bufsize, MPI_DOUBLE, 
	           0, tag,MPI_COMM_WORLD);
     }
     return cycles;
}
​
</pre>
</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:mpi-exchange-1">6.2↑</a> shows that blocking communication nearly halves the network bandwidth. The Infiniband network is bidirectional, but blocking communication forces unidirectional communication. Blocking communication should not be used on modern networks such as Infiniband. It is much too inefficient and has a tendency to cause deadlocks.
</div>
<h? class="Subsubsection">
<b><u>Cyclic exchange</u></b>
</h?>
<div class="Unindented">
Using data exchange to learn MPI syntax, as we did, has a few advantages. Although the setting with only two processes communicating is quite limited, the syntax carries over with few changes to the setting where many processes are communicating with each other. Here we will look at cyclic data exchange.
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-exchange-cyclic-2">6.7↓</a> illustrates cyclic exchange of data. The processes are arranged on a cycle, and each process communicates with the neighbor to its left as well as the neighbor to its right. The communication between pairs of neighboring processes is much like the data exchange we have studied in detail. Consequently, no new points of MPI syntax arise. Therefore, we give the interface to the <tt>Cycle</tt> class but omit its definition from the text.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-exchange-cyclic-2"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/cyclic.png" alt="figure FIGS/chapter5/cyclic.png" style="max-width: 296px; max-height: 66px;"/>

</div>
<div class="caption">
Figure 6.7 Cyclic data exchange.
</div>

</div>

</div>

</div>
<div class="Indented">
The main purpose is to examine network bandwidth as the number of communicating agents increases. Network bandwidth depends on the pattern of communication between host computers. Two hosts exchanging data is one of the simplest patterns. The cyclic pattern is also a simple one, but it allows us to examine how well the network bandwidth holds up when there are many communicating agents on the network. We will find that it holds up quite well. 
</div>
<div class="Indented">
The <tt>Cycle</tt> class follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class Cycle{
private:
     double *sendbufl, *sendbufr;
     double *recvbufl, *recvbufr;
     int bufsize;
     MPI_Request sreq1, sreq2;
     MPI_Request rreq1, rreq2;
     TimeStamp clk;
public:
     Cycle(int rank, int nprocs, int bufsize);
     ~Cycle();
     void post();
     double wait();
     double *getsbufl(){return sendbufl;}
     double *getsbufr(){return sendbufr;}
     double *getrbufl(){return recvbufl;}
     double *getrbufr(){return recvbufr;}
};
</pre>
</div>

</div>
<div class="Indented">
This class has an interface similar to the <tt>Exchg</tt> class. The difference is that it has send and receive buffers to communicate with the process to its left as well as the process to its right.
</div>
<div class="Indented">
To time cyclic exchange, the send and receive requests are posted using <tt>post()</tt> on each MPI process. The <tt>post()</tt> is immediately followed by <tt>wait()</tt>, although in general a program may perform some computations before it waits for the posted requests to be complete. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-exchange-cyclic-2"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
<span class="formula"><span class="text">nprocs</span> = 2</span>
</td>
<td align="center" valign="top">
<span class="formula"><span class="text">nprocs</span> = 10</span>
</td>
<td align="center" valign="top">
<span class="formula"><span class="text">nprocs</span> = 20</span>
</td>
<td align="center" valign="top">
<span class="formula"><span class="text">nprocs</span> = 50</span>
</td>
<td align="center" valign="top">
<span class="formula"><span class="text">nprocs</span> = 100</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.66</span>
</td>
<td align="center" valign="top">
<span class="formula">0.627</span>
</td>
<td align="center" valign="top">
<span class="formula">0.64</span>
</td>
<td align="center" valign="top">
<span class="formula">0.43</span>
</td>
<td align="center" valign="top">
<span class="formula">0.50</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.92</span>
</td>
<td align="center" valign="top">
<span class="formula">1.58</span>
</td>
<td align="center" valign="top">
<span class="formula">1.49</span>
</td>
<td align="center" valign="top">
<span class="formula">1.63</span>
</td>
<td align="center" valign="top">
<span class="formula">1.62</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10<sup>8</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.71</span>
</td>
<td align="center" valign="top">
<span class="formula">1.56</span>
</td>
<td align="center" valign="top">
<span class="formula">1.59</span>
</td>
<td align="center" valign="top">
<span class="formula">1.69</span>
</td>
<td align="center" valign="top">
<span class="formula">1.59</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.3 Bandwidths in bytes/cycle for cyclic data exchange on QDR Infiniband. All hosts are <span class="formula">3.33</span>  GHz SSE2 machines (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). The number of double-precision numbers in each message is <span class="formula"><i>n</i></span>.
</div>

</div>

</div>
Table <a class="Reference" href="#tab:mpi-exchange-cyclic-2">6.3↑</a> shows that the network bandwidth holds up quite well as the number of communicating agents increases from <span class="formula">2</span> to <span class="formula">100</span>. The bandwidth is measured by each host as the number of bytes transferred into or out of it in a single cycle. The reported numbers are medians of a large number of trials and correspond to the process of rank <span class="formula">0</span>. For this simple pattern of communication, the bandwidths realized are often in excess of the advertised QDR Infiniband port bandwidth of <span class="formula">5</span> GB/s, which corresponds to <span class="formula">1.5</span> bytes/cycle. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.3.2">6.3.2</a> Jacobi iteration<a class="Label" name="sub:mpi-Jacobi-iteration"> </a>
</h3>
<div class="Unindented">
The Jacobi iteration is a technique for solving linear systems that arise when certain partial differential equations are discretized. Jacobi is an iterative method. The method is started off with an initial guess for the solution. Successive iterations improve the guess with each iteration, reducing the error by a factor greater than <span class="formula">1</span>. The iterations are stopped when the error is deemed to be low enough. 
</div>
<div class="Indented">
Poisson’s equation <span class="formula"><i>u</i><sub><i>xx</i></sub> + <i>u</i><sub><i>yy</i></sub> = <i>f</i></span> is amenable to the Jacobi iteration once the equation is discretized using finite differences or finite elements. The Jacobi iteration by itself is not a particularly effective method for solving such differential equations. However, the Jacobi iteration can be supplemented by other ideas, such as multigrid, and turned into a powerful method.
</div>
<div class="Indented">
The data in the Jacobi iteration is laid out as a matrix <span class="formula"><i>a</i><sub><i>ij</i></sub></span>, <span class="formula">0 ≤ <i>i</i> &lt; <i>m</i></span> and <span class="formula">0 ≤ <i>j</i> &lt; <i>n</i></span>. The matrix is split between MPI processes by column. Each MPI process gets a contiguous set of columns. The iteration is taken to be  <div class="formula">
<i>b</i><sub><i>ij</i></sub> ← <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>a</i><sub><i>i</i> − 1, <i>j</i></sub> + <i>a</i><sub><i>i</i> + 1, <i>j</i></sub> + <i>a</i><sub><i>i</i>, <i>j</i> − 1</sub> + <i>a</i><sub><i>i</i>, <i>j</i> + 1</sub></span><span class="ignored">)/(</span><span class="denominator">4</span><span class="ignored">)</span></span>.
</div>
In this form, the iteration does not directly correspond to any partial differential equation. There is no right-hand side vector corresponding to the source term in the partial differential equation. We have simplified the Jacobi iteration to focus better on certain aspects of its implementation. Nevertheless, we do have to specify boundary conditions on the data <span class="formula"><i>a</i><sub><i>ij</i></sub></span> at the boundary points <span class="formula"><i>i</i> = 0</span> or <span class="formula"><i>i</i> = <i>m</i> − 1</span> or <span class="formula"><i>j</i> = 0</span> or <span class="formula"><i>j</i> = <i>n</i> − 1</span>. Our version of the Jacobi iteration generates indices that fall outside the domain for boundary points <span class="formula"><i>a</i><sub><i>ij</i></sub></span>. For example, <span class="formula"><i>a</i><sub><i>i</i> − 1, <i>j</i></sub></span> has a negative row index if <span class="formula"><i>i</i> = 0</span>. We interpret all row indices modulo <span class="formula"><i>m</i></span> and all column indices module <span class="formula"><i>n</i></span>. Thus, row indices of <span class="formula"> − 1</span> and <span class="formula"><i>m</i></span> are interpreted as <span class="formula"><i>m</i> − 1</span> and <span class="formula">0</span>, respectively, and column indices of <span class="formula"> − 1</span> and <span class="formula"><i>n</i></span> are interpreted as <span class="formula"><i>n</i> − 1</span> and <span class="formula">0</span>, respectively. In differential equations jargon, we are assuming periodic boundary conditions here. 
</div>
<div class="Indented">
This section illustrates how OpenMP constructs may be mixed with MPI message passing. There will be only one MPI process on each node of a <span class="formula">12</span>-core<tt> <span class="formula">3.33</span></tt>  GHz SSE2 computer (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>), but that MPI process will use OpenMP constructs to create multiple threads. The matrix <span class="formula"><i>a</i><sub><i>ij</i></sub></span> is split across the compute nodes, and each compute node will do a further split to assign part of the Jacobi iteration to each of its threads. As far as program optimization is concerned, it will emerge that the MPI part is irrelevant. The program speed is determined by optimizing the Jacobi iteration within a single node.
</div>
<h? class="Subsubsection">
<b><u>The Jacobi class</u></b>
</h?>
<div class="Unindented">
If the matrix <span class="formula"><i>a</i><sub><i>ij</i></sub></span> is <span class="formula"><i>m</i> × <i>n</i></span> globally, each MPI process out of a total of <span class="formula"><i>P</i></span> processes gets approximately <span class="formula"><i>n</i> ⁄ <i>P</i></span> columns. The Jacobi iteration entails accessing entries in neighboring rows and columns. Thus, each process must exchange messages with the process to its left and the process to its right. The pattern of communication is cyclic.
</div>
<div class="Indented">
The organization of data in the <tt>Jacobi2D</tt> class, whose definition follows, is guided by the pattern of communication. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>class Jacobi2D{
<span class="number-left">2</span>private:
<span class="number-left">3</span>	int nthreads;
<span class="number-left">4</span>	int dim1;
<span class="number-left">5</span>	int dim2;
<span class="number-left">6</span>	double *a, *b;
<span class="number-left">7</span>	int lrank;
<span class="number-left">8</span>	int rrank;
<span class="number-left">9</span>	MPI_Request reqlist[4];
<span class="number-left">10</span>	void leftrightinit();
<span class="number-left">11</span>	void initialize(int col1, int col2);
<span class="number-left">12</span>	void update(int col1, int col2);
<span class="number-left">13</span>	void copy(int col1, int col2);
<span class="number-left">14</span>public:
<span class="number-left">15</span>	Jacobi2D(int rank, int nprocs, int d1, int d2, 
<span class="number-left">16</span>								int nth);
<span class="number-left">17</span>	~Jacobi2D();
<span class="number-left">18</span>	void initializepp();
<span class="number-left">19</span>	void postsendrecv();
<span class="number-left">20</span>	void wait();
<span class="number-left">21</span>	void updatepp();
<span class="number-left">22</span>	void copypp();
<span class="number-left">23</span>	double* geta(){return a+dim1;}
<span class="number-left">24</span>};
<span class="number-left">25</span>​
</pre>
</div>

</div>
<div class="Indented">
The dimension of the matrix stored on each processor node is <span class="formula"><i>dim</i>1 × <i>dim</i>2</span> (lines 4 and 5). The matrix is stored in the arrays <tt>a[]</tt> and <tt>b[]</tt> (line 6). In each step of Jacobi, the entries of <tt>b[]</tt> are computed using the entries of <tt>a[]</tt>. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-jacobi"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/jacobi.png" alt="figure FIGS/chapter5/jacobi.png" style="max-width: 346px; max-height: 144px;"/>
<div class="caption">
Figure 6.8 Sketch showing data layout of <tt>Jacobi2D</tt> class across three nodes. Each processor node holds a matrix of dimension <span class="formula"><i>dim</i>1 × <i>dim</i>2</span>. In addition, there is an extra column on the right, which holds a copy of the rightmost column of the node to the right (see the arrows above) and an extra column to the left. The extra column to the left holds a copy of the leftmost column of the node to the left (see the arrows below). 
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-jacobi">6.8↑</a> shows the data layout assumed by the <tt>Jacobi2D</tt> class. Although the matrix in each node is only <span class="formula"><i>dim</i>1 × <i>dim</i>2</span>, the actual matrix stored is <span class="formula"><i>dim</i>1 × (<i>dim</i>2 + 2)</span>. The leftmost column is a copy of a column to the left and the rightmost column a copy of the column to the right, as shown in the figure. Both the arrays <tt>a[]</tt> and <tt>b[]</tt> are of length <span class="formula"><i>dim</i>1 × (<i>dim</i>2 + 2)</span>. Thus, during any single Jacobi iteration, a processor node can update all its <span class="formula"><i>dim</i>2</span> columns using data stored on itself.
</div>
<div class="Indented">
Assuming <tt>jacobi</tt> is an object of class <tt>Jacobi</tt>2D, any single iteration looks as follows: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	jacobi.postsendrecv();
	jacobi.wait();
	jacobi.updatepp();
	jacobi.copypp();
</pre>
</div>

</div>
<div class="Indented">
First, every node sends its leftmost and rightmost columns to its left and right neighbors, respectively, using the member function <tt>postsendrecv()</tt> (line 18). All the nodes wait for the sending and receiving to complete. The member function <tt>updatepp()</tt> (line 20) applies the Jacobi iteration to <tt>a[]</tt> and saves the result in <tt>b[]</tt>. The member function <tt>copypp()</tt> (line 21) copies <tt>b[]</tt> back to <tt>a[]</tt> to prepare for the next iteration. Notice that the first data member in the class <tt>Jacobi2D</tt> (line 3) is <tt>nthreads</tt>. Both copying and the Jacobi iteration are done in parallel using Open MP with a thread count of <tt>nthreads</tt>.
</div>
<div class="Indented">
The data layout of this class is partly determined by the communication pattern that arises when MPI processes execute the Jacobi iteration in parallel. If we had tiled the matrix into rectangles while splitting columns as well as rows, the communication pattern would have changed. The data layout would change correspondingly. Similarly, for other discretized systems, such as finite differences in 3D or finite elements, the data structures must reflect the structure of the discretized systems. When discretized systems are solved in parallel, the additional data structures that arise must reflect the communication pattern, as we see here with the <tt>Jacobi2D</tt> class. The data members <tt>lrank</tt> (rank of node to the left, line 7), <tt>rrank</tt> (rank of node to the right, line 8), and <tt>reqlist[]</tt> (line 9) are used to set up sends and receives. 
</div>
<h? class="Subsubsection">
<b><u>Implementation of the Jacobi class</u></b>
</h?>
<div class="Unindented">
The implementation of the <tt>Jacobi2D</tt> class includes member functions of two types. The class constructor, the destructor, and the member functions for sending and receiving messages fall under the first type. These member functions do not make use of OpenMP constructs at all. The member functions for default initialization and carrying out the Jacobi iteration fall under the second type. These functions use OpenMP constructs. MPI communication must be restricted to the master thread. Thread-level parallelism is used to carry out activities other than message passing. The division of member functions into two types is a natural consequence. 
</div>
<div class="Indented">
We first discuss member functions that do not use OpenMP constructs. This part of <tt>Jacobi2D</tt> is similar to the <tt>Cycle</tt> class, which is not surprising given that the pattern of communication is cyclic. Indeed, an alternative implementation may use a <tt>Cycle</tt> class object inside <tt>Jacobi2D</tt> to send and receive messages. The definition of the constructor of the <tt>Jacobi2D</tt> class follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Jacobi2D::Jacobi2D(int rank, int nprocs, 
		int d1, int d2, int nth){
	nthreads = nth; assrt(nthreads%2 == 0);
	dim1 = d1; assrt(dim1%2 == 0);
	dim2 = d2; assrt(dim2%nthreads == 0);
	assrt(dim1%2 == 0);
	assrt(dim2%2 == 0);
​
	a = new double[dim1*(dim2+2)];
	b = new double[dim1*(dim2+2)];
​
	lrank = (rank==0)?nprocs-1:rank-1;
	rrank = (rank==nprocs-1)?0:rank+1;
​
	int tag = 0;
	double *left = a;
	double *al = a + dim1;
	double *ar = a + dim1*dim2;
	double *right = a + dim1*(dim2+1);
	MPI_Send_init(al, dim1, MPI_DOUBLE, lrank, tag, 
		      MPI_COMM_WORLD, reqlist+0);
	MPI_Send_init(ar, dim1, MPI_DOUBLE, rrank, tag, 
		      MPI_COMM_WORLD, reqlist+1);
	MPI_Recv_init(left, dim1, MPI_DOUBLE, lrank, tag, 
		      MPI_COMM_WORLD, reqlist+2);
	MPI_Recv_init(right, dim1, MPI_DOUBLE, rrank, tag, 
		      MPI_COMM_WORLD, reqlist+3);
}
</pre>
</div>

</div>
<div class="Indented">
This constructor uses the same tag for both left-going and right-going messages, unlike the constructor for the <tt>Cycle</tt> class. The <tt>Jacobi2D</tt> class must be used only when the number of MPI processes is greater than two. The dimensions <span class="formula"><i>dim</i>1</span> and <span class="formula"><i>dim</i>2</span> are assumed to be divisible by two and <tt>nthreads</tt>, respectively, to reduce clutter in the implementation. The number of threads is also assumed to be divisible by two.
</div>
<div class="Indented">
The arrays <tt>left[]</tt> and <tt>right[]</tt> point to the leftmost and rightmost columns (dashed columns in figure <a class="Reference" href="#fig:mpi-jacobi">6.8↑</a>). The arrays <tt>al[]</tt> and <tt>ar[]</tt> point to the leftmost and rightmost columns &ldquo;owned&rdquo; by the current node. The array <tt>al[]</tt> is sent to the node on the left of rank <tt>lrank</tt>, and the array <tt>ar[]</tt> is sent to the node on the right of rank <tt>rrank</tt>. Data from the left is received into <tt>left</tt> and data from the right into <tt>right</tt>. The pattern of communication is as shown in figure <a class="Reference" href="#fig:mpi-jacobi">6.8↑</a>.
</div>
<div class="Indented">
The definition of the destructor is included for completeness.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Jacobi2D::~Jacobi2D(){
	assrt(gl_mpi_onoff == MPION);
	MPI_Request_free(reqlist+0);
	MPI_Request_free(reqlist+1);
	MPI_Request_free(reqlist+2);
	MPI_Request_free(reqlist+3);
	delete[] a;
	delete[] b;
}
</pre>
</div>

</div>
<div class="Indented">
The constructor initializes sends and receives. The actual sending and receiving is left to the following member function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Jacobi2D::postsendrecv(){
	MPI_Startall(4, reqlist);
}
</pre>
</div>

</div>
<div class="Indented">
Once the sends and receives are posted, each node waits for them to complete.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Jacobi2D::wait(){
	MPI_Waitall(4, reqlist, MPI_STATUSES_IGNORE);
}
</pre>
</div>

</div>
<div class="Indented">
We now turn to member functions of the <tt>Jacobi2D</tt> class that use OpenMP constructs: the member functions for default initialization, carrying out the Jacobi iteration, and copying back the updated data.
</div>
<div class="Indented">
The public member function <tt>initializepp()</tt>, whose definition is given below, is invoked to initialize both <tt>a[]</tt> and <tt>b[]</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Jacobi2D::initializepp(){
#pragma omp parallel				\
	num_threads(nthreads)			\
	default(shared)				
	{
		int tid = omp_get_thread_num();
		int col1 = tid*dim2/nthreads;
		int col2 = (tid+1)*dim2/nthreads;
		initialize(col1, col2);
	}
}
</pre>
</div>

</div>
<div class="Indented">
The helper member function <tt>initialize()</tt> ensures that <tt>a[]</tt> and <tt>b[]</tt> are initialized, the former to a checkerboard pattern of <span class="formula"> + 1</span> and <span class="formula"> − 1</span>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Jacobi2D::initialize(int col1, int col2){
	for(int j=col1+1; j &lt; col2+1; j++)
		for(int i=0; i &lt; dim1; i++){
			a[i+j*dim1] = ((i+j-1)%2==0)?1:-1;
			b[i+j*dim1] = 0;
		}
}
</pre>
</div>

</div>
<div class="Indented">
Notice that <tt>col1</tt> and <tt>col2</tt> are shifted right by <span class="formula">1</span> to ensure that the leftmost and rightmost columns are skipped. The assumption made in the class constructor, through <tt>assrt()</tt> statements, that <span class="formula"><i>dim</i>1</span> and <tt>nthreads</tt> are even and <span class="formula"><i>dim</i>2</span> is divisible by <tt>nthreads</tt> simplifies the checkerboard initialization of <tt>a[]</tt>. Thanks to that assumption, the MPI process is freed from checking the parity of its rank before initializing data into a checkerboard-like pattern.
</div>
<div class="Indented">
The initialization of the <tt>Jacobi2D</tt> class serves two purposes. The first purpose is verification of the correctness of the implementation. If the global matrix is initialized into a chessboard-like pattern of <span class="formula">±1</span>, the pattern reverses with each Jacobi iteration. This reversal in the initialization pattern over a large number of iterations is a good test of the correctness of the implementation. It tests the correctness of communication with neighbors as well as the Jacobi update that is local to each MPI process.
</div>
<div class="Indented">
The other purpose of the initialization functions is to enable allocation of page frames in near memory. If the problem size is large, the arrays <tt>a[]</tt> and <tt>b[]</tt> will hold many pages of data. For program speed, these pages must be mapped to page frames  near to the thread that accesses the pages most frequently. The member functions for default initialization split the data between threads in exactly the same manner as the member functions that implement the Jacobi iteration. If the <tt>Jacobi2D</tt> object is initialized using the member functions immediately after it is created, the page frame for each thread is allocated during first touch, which occurs during initialization. Thus, the page frames will end up near  threads that use them most frequently. The initialization of <tt>b[]</tt> to zero on line 5 is significant in this respect, although it is not necessary for program correctness.
</div>
<div class="Indented">
The task of carrying out the Jacobi iteration on entries of <tt>a[]</tt> and storing the result in <tt>b[]</tt> is left to the following two member functions:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void Jacobi2D::update(int col1, int col2){
<span class="number-left">2</span>	for(int j=col1+1; j &lt; col2+1; j++)
<span class="number-left">3</span>		for(int i=0; i &lt; dim1; i++){
<span class="number-left">4</span>			int iup = (i+dim1-1)%dim1;
<span class="number-left">5</span>			int idown = (i+1)%dim1;
<span class="number-left">6</span>			b[i+j*dim1] = 0.25*(a[iup+j*dim1]
<span class="number-left">7</span>					    + a[idown+j*dim1]
<span class="number-left">8</span>					    + a[i+(j-1)*dim1]
<span class="number-left">9</span>					    + a[i+(j+1)*dim1]);
<span class="number-left">10</span>		}
<span class="number-left">11</span>}
<span class="number-left">12</span>​
<span class="number-left">13</span>void Jacobi2D::updatepp(){
<span class="number-left">14</span>#pragma omp parallel				\
<span class="number-left">15</span>	num_threads(nthreads)			\
<span class="number-left">16</span>	default(shared)				
<span class="number-left">17</span>	{
<span class="number-left">18</span>		int tid = omp_get_thread_num();
<span class="number-left">19</span>		int col1 = tid*dim2/nthreads;
<span class="number-left">20</span>		int col2 = (tid+1)*dim2/nthreads;
<span class="number-left">21</span>		update(col1, col2);
<span class="number-left">22</span>	}
<span class="number-left">23</span>}
</pre>
</div>

</div>
<div class="Indented">
Notice that <tt>updatepp()</tt> splits columns between threads in exactly the same way as the initialization routine. Almost all the time expended during a Jacobi step is inside <tt>update()</tt>. The <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of <tt>b[]</tt> is set equal to the average of the North, South, East, and West entries stored in <tt>a[]</tt> on lines 6 through 9. To access the West and East entries of <tt>a[]</tt>, the program simply decrements and increments <span class="formula"><i>j</i></span> on lines 8 and 9. That works because the first and last columns of <tt>b[]</tt> or <tt>a[]</tt> receive special treatment (see figure <a class="Reference" href="#fig:mpi-jacobi">6.8↑</a>). Assignments to conditional expressions are used on lines 4 and 5 to generate indices to go North and South. Such conditional expressions prevent the compiler from generating good code for the loop, severely degrading performance of the <tt>Jacobi2D</tt> class.
</div>
<div class="Indented">
The two member functions copy <tt>b[]</tt> to <tt>a[]</tt> at the end of every iteration:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Jacobi2D::copy(int col1, int col2){
	for(int j=col1+1; j &lt; col2+1; j++)
		for(int i=0; i &lt; dim1; i++)
			a[i+j*dim1] = b[i+j*dim1];
}
​
void Jacobi2D::copypp(){
#pragma omp parallel				\
	num_threads(nthreads)			\
	default(shared)				
	{
		int tid = omp_get_thread_num();
		int col1 = tid*dim2/nthreads;
		int col2 = (tid+1)*dim2/nthreads;
		this-&gt;copy(col1, col2);
	}
}
</pre>
</div>

</div>
<div class="Indented">
The structure of the member functions <tt>copypp()</tt> and <tt>copy()</tt> is similar to that of member functions used for default initialization and carrying out the Jacobi iteration. The division of work between threads is by columns and is done the same way while copying, initializing, or updating. Dividing data between columns the same way ensures that the threads access near memory most of the time, assuming the object is initialized using the member function <tt>initializepp()</tt> soon after it is created. If it is not initialized appropriately, the program will be much slower.
</div>
<h? class="Subsubsection">
<b><u>Iterating with the Jacobi class</u></b>
</h?>
<div class="Unindented">
The function for iterating and timing the <tt>Jacobi2D</tt> class looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double time_jacobi(int rank, int nprocs, 
<span class="number-left">2</span>		      int dim1, int dim2,
<span class="number-left">3</span>		      int nitns){
<span class="number-left">4</span>	int nth = get_nthreads();
<span class="number-left">5</span>	Jacobi2D jacobi(rank, nprocs, dim1, dim2, nth);
<span class="number-left">6</span>	jacobi.initializepp();
<span class="number-left">7</span>	TimeStamp clk;
<span class="number-left">8</span>	clk.tic();
<span class="number-left">9</span>	for(int i=0; i &lt; nitns; i++){
<span class="number-left">10</span>		jacobi.postsendrecv();
<span class="number-left">11</span>		jacobi.wait();
<span class="number-left">12</span>		jacobi.updatepp();
<span class="number-left">13</span>		jacobi.copypp();
<span class="number-left">14</span>	}
<span class="number-left">15</span>	return clk.toc();
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The OpenMP constructs are hidden away inside the implementation of the <tt>Jacobi2D</tt> class. There are no OpenMP constructs within the iteration loop. 
</div>
<div class="Indented">
The <tt>wait()</tt> on line 11 immediately follows <tt>postsendrecv()</tt> on line 12. The implementation waits for the message passing to be complete after initiating sends and receives. No attempt is made to overlap computation with communication because there is no need for it. Message passing takes up an insignificant amount of time in this program, and any effort to hide its cost by overlapping it with computation would be wasted. Indeed, an attempt at such an optimization may make the loop nests in the member functions <tt>update()</tt> and <tt>copy()</tt> more complicated, leading to a slightly slower program.
</div>
<h? class="Subsubsection">
<b><u>Timing the Jacobi class</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:mpi-jacobi-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
nprocs
</td>
<td align="center" valign="top">
dim1
</td>
<td align="center" valign="top">
dim2
</td>
<td align="center" valign="top">
cycles/entry/itn
</td>

</tr>
<tr>
<td align="center" valign="top">
4
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">12 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">8.3</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">12 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">7.2</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">12 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">7.7</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.4 Performance of the <tt>Jacobi2D</tt> class on <tt>nprocs</tt> nodes, each a 12-core <span class="formula">3.33</span>  GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>

</div>
<div class="Indented">
The three trials reported in table <a class="Reference" href="#tab:mpi-jacobi-1">6.4↑</a> take <span class="formula"><i>dim</i>1 = 10<sup>5</sup></span> and <span class="formula"><i>dim</i>2 = 12, 000</span> on each MPI process. The number of MPI processes, with one process per Xeon 5680 compute node, is varied from <span class="formula">4</span> to <span class="formula">100</span>. The size of the matrix local to each MPI process or compute node is thus equal to <span class="formula">10<sup>5</sup> × 12, 002</span>. The performance figure reported is the number of cycles consumed per iteration divided by the number of entries local to each MPI process. 
</div>
<div class="Indented">
During each iteration, four arithmetic operations (three additions and a multiplication) are performed to update each entry. Corresponding to each update, two entries, one within the array <tt>a[]</tt> and one within <tt>b[]</tt>, are accessed. Thus, approximately <span class="formula">16</span> bytes of data are accessed for every set of four arithmetic operations, and the figure is <span class="formula">32</span> bytes if we include the cost of copying from <tt>b[]</tt> to <tt>a[]</tt>. The cost of this computation will be determined by memory accesses. The bandwidth to memory realized on a <span class="formula">3.33</span>  GHz machine is between <span class="formula">12</span> and <span class="formula">14</span> GB/s. 
</div>
<div class="Indented">
The realized bandwidth to memory and the speed of this program can be improved considerably. In fact, the running time of this program can be nearly halved. This program spends much of its time within the nested for-loop on lines 2 through 10 in the member function <tt>update()</tt>. That loop nest, unfortunately, has too many inefficiencies in it. The body of the inner loop contains two conditional expressions, and the loop nest does not use blocking.
</div>
<div class="Indented">
It is amusing that although the communication pattern and OpenMP thread level parallelism have such a big impact on the structure of this program, its speed is determined by a single nested and sequential for-loop. To improve the speed of this program, the conditional assignments must be removed from the body of the program. The for-loops must be rewritten to implement the Jacobi iteration in blocks to improve cache and TLB usage. Yet another optimization is to use <tt>restrict</tt> qualifiers. 
</div>
<div class="Indented">
The tendency of MPI implementations to subsume compilation/linking within commands such as <tt>mpicxx</tt>, <tt>mpiCC</tt>, or <tt>mpiicpc</tt> is one we do not endorse. In many programs such as this one optimization of C/C++ compilation is of far greater consequence than MPI message passing. The user’s access to the compiler should not be obscured by hiding the compiler beneath another layer. MPI is a library, and one should be able to include it and link to it like any library.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.3.3">6.3.3</a> Matrix transpose<a class="Label" name="sub:mpi-transpose"> </a>
</h3>
<div class="Unindented">
So far in this chapter, we have considered data exchange between two MPI processes and cyclic transfer of data between several MPI processes. Both of these are especially simple patterns of communication. In this section, we implement the transposition of large matrices stored on several host computers connected by QDR Infiniband network. The number of host computers ranges from <span class="formula">10</span> to <span class="formula">100</span> and the largest matrix handled has <span class="formula">5 × 10<sup>4</sup></span> rows and <span class="formula">5 × 10<sup>5</sup></span> columns. There is exactly one MPI process per host computer. 
</div>
<div class="Indented">
In the matrix transpose, as we implement it, there is communication between every pair of host computers. Such a setting, where every possible line of communication is active, exercises additional features of the Infiniband network and the MPI library implementation. The Infiniband architecture relies on the host channel adapter to convert data stored in page frames in DRAM memory to Infiniband data packets, each of which is 2 KB or less on the system we use. The MPI processes initiate RDMA read and write transactions by writing directly to queue pairs that reside on the host channel adapter. When many lines of communication are active, there will be a large number of queue pairs on each host channel adapter. The communication pattern tests the ability of the MPI library implementation as well as the host channel adapter to manage a large number of queue pairs.
</div>
<div class="Indented">
In a cyclic pattern of communication, every MPI process communicates only with its neighbors to the left and right. It is likely that neighboring host computers are connected to the same switch at the leaf of the Infiniband fat-tree. If that is the case, most of the network traffic does not reach switches at higher levels of the fat-tree. The all-to-all pattern of communication that occurs in the matrix transpose is a better test of the capability of the Infiniband fat-tree network to sustain high bandwidth at each host channel adapter.
</div>
<div class="Indented">
The matrix transpose brings out another  important aspect of networking---namely, the overhead of copying into and out of buffers. During a matrix transpose, the data format changes in such a way that copying into and out of buffers is inevitable. The receiver’s view of the data differs significantly from the sender’s view of the data. We will find that the performance of the MPI program is determined largely by copying into and out of buffers. 
</div>
<div class="Indented">
The Jacobi iteration of the previous section does not require application-level buffering, whereas the matrix transpose of this section does. The data layout in a 2D Jacobi iteration is so simple that the sender’s view of the data transmitted coincides with the receiver’s view. Both the sender and receiver think of the transmitted data as a column of the global matrix and store it as such. If the program can be designed so that the sender and receiver have the same view of transmitted data, it is usually advantageous to adopt such a design. Data does not need to be copied into send buffers or copied out of receive buffers. Applications such as 2D or 3D finite differences on a regular rectangular grid are amenable to this type of design.
</div>
<div class="Indented">
The more difficult scenario is when the data is irregular, or structured but changes format between the sender and the receiver, as in matrix transpose. In such scenarios, the MPI application has to copy into send buffers and out of receive buffers. The way buffering is handled becomes as important as it is in the TCP/IP protocol stack or in the hardware implementation of the Infiniband network architecture. The matrix transpose program uses buffering to handle the change in data layout between the matrix at the sending MPI process and the transposed matrix at the receiving process.
</div>
<h? class="Subsubsection">
<b><u>Balanced matrix partitions</u></b>
</h?>
<div class="Unindented">
If an <span class="formula"><i>m</i> × <i>n</i></span> matrix is stored in column-major order, with a leading dimension equal to column size in the array <tt>a[]</tt>, the <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of the matrix is <tt>a[i+j*m]</tt>. In row-major order, the <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry is at <tt>a[j+i*n]</tt>. If the matrix is transposed and stored in column-major order, the <span class="formula">(<i>j</i>, <i>i</i>)</span>th entry of the transposed matrix would be at <tt>a[j+i*n]</tt>. On a single computer, transposing a matrix is equivalent to switching from column-major order to row-major order. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-transpose-1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/transpose.png" alt="figure FIGS/chapter5/transpose.png" style="max-width: 160px; max-height: 159px;"/>

</div>
<div class="caption">
Figure 6.9 Columnwise and rowwise matrix partitions of a matrix between four processor nodes. When the columnwise partitioning is switched to rowwise partitioning, submatrix blocks must be transmitted between nodes. For example, the block labeled <span class="formula"><i>A</i></span> must be sent from <span class="formula"><i>P</i>0</span> to <span class="formula"><i>P</i>3</span> and the block labeled <span class="formula"><i>B</i></span> must be sent from <span class="formula"><i>P</i>3</span> to <span class="formula"><i>P</i>1</span>. 
</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-transpose-1">6.9↑</a> shows a matrix is partitioned along columns as well as rows. If the global matrix is <span class="formula"><i>M</i> × <i>N</i></span>, it is partitioned across <span class="formula"><i>P</i></span> MPI processes by allocating approximately <span class="formula"><i>N</i> ⁄ <i>P</i></span> columns to each process. Each process gets a contiguous set of columns. Matrix transposition is equivalent to switching the partitioning scheme from columnwise to rowwise, but with a twist. As shown in the figure, the set of columns of process <span class="formula"><i>P</i><sub><i>i</i></sub></span> intersects the set of rows of process <span class="formula"><i>P</i><sub><i>j</i></sub></span> in a block matrix. During matrix transposition, this block must be transmitted from process <span class="formula"><i>P</i><sub><i>i</i></sub></span> to process <span class="formula"><i>P</i><sub><i>j</i></sub></span>. Bidirectional data transfer occurs between every pair of processes. 
</div>
<div class="Indented">
The twist arises as follows. Every block that is transmitted must be transposed. The block that is transmitted is stored in column-major order with leading dimension equal to <span class="formula"><i>M</i></span>, the column size of the global matrix. When it is transposed and stored, the lead dimension changes to <span class="formula"><i>N</i></span>, the row size of the global matrix.
</div>
<div class="Indented">
The function <tt>BlockDivide()</tt>, defined below, is used for columnwise as well as rowwise partitioning.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void BlockDivide(long n, int P, long *fst){
<span class="number-left">2</span>     long  Q = n/P;
<span class="number-left">3</span>     long  R = n-Q*P;
<span class="number-left">4</span>     fst[0] = 0;
<span class="number-left">5</span>     for(int p=0; p &lt; R; p++)
<span class="number-left">6</span>          fst[p+1] = fst[p] + (Q+1);
<span class="number-left">7</span>     for(int p=R; p &lt; P; p++)
<span class="number-left">8</span>          fst[p+1] = fst[p]+Q;
<span class="number-left">9</span>}
</pre>
</div>

</div>
<div class="Indented">
<tt>BlockDivide()</tt> partitions <span class="formula"><i>n</i></span> items between <tt><span class="formula"><i>P</i></span> </tt>processes. The partitioning is stored in the array <tt>fst[]</tt>. The number of items that each process gets would be exactly equal to the quotient <span class="formula"><i>Q</i> = <i>n</i> ⁄ <i>P</i></span> (line 2) if the remainder <span class="formula"><i>R</i> = <i>n</i> − <i>QP</i></span> (line 3) were zero. If <span class="formula">0, 1, …, <i>n</i> − 1</span> are the <span class="formula"><i>n</i></span> items, the items <span class="formula"><i>i</i></span> assigned to process <span class="formula"><i>p</i></span> lie in the interval <span class="formula"><span class="text">fst[p]</span> ≤ <i>i</i> &lt; <span class="text">fst[p+1]</span></span>. Equivalently, the assignment to process <span class="formula"><i>p</i></span> begins at <tt>fst[p]</tt>. The for-loop on lines 5 and 6 assigns <span class="formula"><i>Q</i> + 1</span> items to each of the first <span class="formula"><i>R</i></span> processes. The remaining <span class="formula"><i>P</i> − <i>R</i></span> processes are assigned <span class="formula"><i>Q</i></span> items by the for-loop on lines 7 and 8. 
</div>
<div class="Indented">
It must be noted that <tt>fst[]</tt> is an array with <span class="formula"><i>P</i> + 1</span> entries, although there are only <span class="formula"><i>P</i></span> processes. The entry <tt>fst[P]</tt> always gets sets to <span class="formula"><i>n</i></span>.
</div>
<div class="Indented">
The division of work between the processes is nearly evenly balanced. Some of the processes get at most one extra item. When the number of items (columns and rows in matrix transposition) is large, as it must be for parallelization to be meaningful, the slight imbalance by a single item will be inconsequential.
</div>
<div class="Indented">
The general principles of algorithm design assume a highly simplified but still useful and pertinent model of the computer. General principles that have a direct bearing on parallel programming are few. Load balancing is one of them. Load balancing requires that work should be nearly evenly partitioned between processes that utilize equally powerful hardware. The speed of a parallel program is determined by the slowest process, which is the one with the highest load. 
</div>
<div class="Indented">
The principal of balancing the load is no more than common sense. Indeed, it barely requires knowledge of arithmetic. Yet it must not be taken lightly because the consequences of ignoring it can be severe. For example, a careless division of <span class="formula"><i>n</i></span> items between <span class="formula"><i>P</i></span> processes might assign <span class="formula"><i>Q</i> + <i>R</i></span> items to process <span class="formula"><i>P</i><sub>0</sub></span> and <span class="formula"><i>Q</i></span> items to all others. If <span class="formula"><i>n</i></span> is much larger than <span class="formula"><i>P</i><sup>2</sup></span>, <span class="formula"><i>Q</i></span> will be much larger than <span class="formula"><i>R</i></span>, and the resulting imbalance will not be too great. However, if <span class="formula"><i>n</i></span> is of the order of <span class="formula"><i>P</i><sup>2</sup></span>, which is not unrealistic, process <span class="formula"><i>P</i><sub>0</sub></span> may end up with nearly twice the work of any other process. 
</div>
<h? class="Subsubsection">
<b><u>The Transpose class</u></b>
</h?>
<div class="Unindented">
The <tt>Transpose</tt> class defined below does not hold any part of the matrix to be transposed nor does it store the transposed matrix. It holds send and receive buffers and a few other items. It provides public member functions for data transfers that arise during matrix transpose and to copy data from matrices to its own buffers.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>class Transpose{
<span class="number-left">2</span>private:
<span class="number-left">3</span>	int p; //process rank
<span class="number-left">4</span>	int P; //number of processes
<span class="number-left">5</span>	long  M;
<span class="number-left">6</span>	long  N;
<span class="number-left">7</span>	long *fstM;
<span class="number-left">8</span>	long *fstN;
<span class="number-left">9</span>	double *sendbuf, *recvbuf;
<span class="number-left">10</span>	MPI_Request *sreqlist, *rreqlist;
<span class="number-left">11</span>	
<span class="number-left">12</span>public:
<span class="number-left">13</span>	Transpose(int rank, int nprocs, long Mi, long Ni);
<span class="number-left">14</span>	~Transpose();
<span class="number-left">15</span>	long ffstM(int rank){return fstM[rank];};
<span class="number-left">16</span>	long ffstN(int rank){return fstN[rank];};
<span class="number-left">17</span>	void copyTOsendbuf(double *localMN);
<span class="number-left">18</span>	void postsend();
<span class="number-left">19</span>	void postrecv();
<span class="number-left">20</span>	void wait();
<span class="number-left">21</span>	void copyFROMrecvbuf(double *localNM);
<span class="number-left">22</span>	void transpose(double *localMN, double *localNM);
<span class="number-left">23</span>};
</pre>
</div>

</div>
<div class="Indented">
This class stores just enough data to implement the matrix transpose. It stores the process rank (<tt>p</tt> on line 3), the total number of MPI processes (<tt>P</tt> on line 4), and dimensions of the global matrix (<tt>M</tt> and <tt>N</tt> on lines 5 and 6). The arrays <tt>fstM[]</tt> (line 7) and <tt>fstN[]</tt> (line 8) are used to partition the <span class="formula"><i>M</i></span> rows and the <span class="formula"><i>N</i></span> columns, respectively. This partitioning is done identically on all the MPI processes, and each process has its own copy of the arrays <tt>fstM[]</tt> and <tt>fstN[]</tt> on its own <tt>Transpose</tt> object. Each MPI process relies on its knowledge of the partitioning of rows and columns across all processes when sending and receiving data. Data is mapped from the locally stored part of the <span class="formula"><i>M</i> × <i>N</i></span> matrix to the send buffer (<tt>sendbuf</tt> on line 9) using <tt>fstM[]</tt>. Data is mapped from the receive buffer (<tt>recvbuf</tt> on line 10) to the locally stored part of the transposed <span class="formula"><i>N</i> × <i>M</i></span> matrix using <tt>fstN[]</tt>. 
</div>
<div class="Indented">
The public interface to the <tt>Transpose</tt> class consists of the constructor, the destructor, and functions to step through the matrix transpose. There is also a single member function, namely, <tt>transpose()</tt> (line 21), for effecting the transposition. The two arguments of this member function are <tt>localMN[]</tt>, which stores the local part of the <span class="formula"><i>M</i> × <i>N</i></span> matrix, and <tt>localNM[]</tt>, which stores the local part of the <span class="formula"><i>N</i> × <i>M</i></span> matrix.
</div>
<div class="Indented">
The class constructor is defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>Transpose::Transpose(int rank, int nprocs, 
<span class="number-left">2</span>					long Mi, long Ni)
<span class="number-left">3</span>	:p(rank), P(nprocs), M(Mi), N(Ni)
<span class="number-left">4</span>{    
<span class="number-left">5</span>	fstM = new long[P+1]; fstN = new long[P+1];
<span class="number-left">6</span>	BlockDivide(M, P, fstM);
<span class="number-left">7</span>	BlockDivide(N, P, fstN);
<span class="number-left">8</span>	long ncols = fstN[p+1]-fstN[p];
<span class="number-left">9</span>	long nrows = fstM[p+1]-fstM[p];
<span class="number-left">10</span>​
<span class="number-left">11</span>	assrt(ncols &gt; 0);
<span class="number-left">12</span>	sendbuf = new double[ncols*M];
<span class="number-left">13</span>​
<span class="number-left">14</span>	assrt(nrows &gt; 0);
<span class="number-left">15</span>	recvbuf = new double[nrows*N];
<span class="number-left">16</span>​
<span class="number-left">17</span>	sreqlist = new MPI_Request[P];
<span class="number-left">18</span>	rreqlist = new MPI_Request[P];
<span class="number-left">19</span>}
<span class="number-left">20</span>​
</pre>
</div>

</div>
<div class="Indented">
The constructor uses <tt>BlockDivide()</tt> to calculate the entries of <tt>fstM[]</tt> (on line 6) and <tt>fstN[]</tt> (on line 7). The matrix is partitioned columnwise, and the columns held by process of rank <span class="formula"><i>p</i></span> have indices<div class="formula">
<span class="text">fstN</span>[<i>p</i>] ≤ <i>j</i> &lt; <span class="text">fstN</span>[<i>p</i> + 1].
</div>
When the matrix as transposed, it is in effect partitioned rowwise (see figure <a class="Reference" href="#fig:mpi-transpose-1">6.9↑</a>), and the rows it will hold have indices<div class="formula">
<span class="text">fstM</span>[<i>p</i>] ≤ <i>i</i> &lt; <span class="text">fstM</span>[<i>p</i> + 1].
</div>
The number of columns <tt>ncols</tt> of the matrix and the number of rows <tt>nrows</tt> of the transposed matrix held on the current process of rank <span class="formula"><i>p</i></span> is calculated on lines 8 and 9, respectively.
</div>
<div class="Indented">
The send buffer is allocated on line 12. Each process sends out all the data it holds (we do not attempt to optimize for the diagonal blocks that remain on the same processor node). Thus, the size of the send buffer, which is <tt>ncols*M</tt> on line 12, accounts for every column and every entry of the matrix held by the process. For similar reasons, the size of the receive buffer allocated on line 15 is <tt>nrows*N</tt>, accounting for every entry of the transposed matrix held by the process.
</div>
<div class="Indented">
After the discussion extolling the benefits of registering memory explicitly in section <a class="Reference" href="#sub:mpi-send-recv">6.3.1↑</a>, it may seem a bit strange that the send and receive buffers are allocated using <tt>new[]</tt>, rather than <tt>MPI_Alloc_mem()</tt>. As mentioned earlier, there is a reason for allocating with <tt>new[]</tt>. <tt>MPI_Alloc_mem()</tt> would register memory explicitly, and pages in virtual memory would also be assigned to page frames immediately after allocation. With <tt>new[]</tt>, in contrast, demand paging remains in place. With demand paging, page frames are assigned in near memory during first access. Allocation of page frames in near memory speeds up the <tt>Transpose</tt> class significantly.
</div>
<div class="Indented">
The class destructor is listed for the sake of completeness.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Transpose::~Transpose(){
     delete[] fstM;
     delete[] fstN;
     delete[] sreqlist;
     delete[] rreqlist;
     delete[] sendbuf;
     delete[] recvbuf;
 }
</pre>
</div>

</div>
<div class="Indented">
A matrix stored across <span class="formula"><i>P</i></span> MPI process is transposed when each process makes a single call to the member function <tt>transpose()</tt>. The definition of that member function gives an overview of the <tt>Transpose</tt> class.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void Transpose::transpose(double *localMN, 
<span class="number-left">2</span>	double *localNM){
<span class="number-left">3</span>	TimeStamp clk;	
<span class="number-left">4</span>	clk.tic();
<span class="number-left">5</span>	copyTOsendbuf(localMN);
<span class="number-left">6</span>	trans_timer.scopy += clk.toc();
<span class="number-left">7</span>​
<span class="number-left">8</span>	clk.tic();
<span class="number-left">9</span>	postsend();
<span class="number-left">10</span>	postrecv();
<span class="number-left">11</span>	wait();
<span class="number-left">12</span>	trans_timer.mpi += clk.toc();
<span class="number-left">13</span>​
<span class="number-left">14</span>	clk.tic();
<span class="number-left">15</span>	copyFROMrecvbuf(localNM);
<span class="number-left">16</span>	trans_timer.rcopy += clk.toc();
<span class="number-left">17</span>}
</pre>
</div>

</div>
<div class="Indented">
The part of the global <span class="formula"><i>M</i> × <i>N</i></span> matrix held by the current MPI process is in <tt>localMN[]</tt> (line 1). The part of the transposed <span class="formula"><i>N</i> × <i>M</i></span> matrix that will be held by the same MPI process goes to <tt>localNM[]</tt> (line 2). 
</div>
<div class="Indented">
To effect a matrix transpose, each process copies <tt>localMN[]</tt> to its send buffer (line 5), posts sends and receives (lines 9 and 10), waits for the sends and receives to complete (line 11), and finally copies its receive buffer to <tt>localNM[]</tt>. The <tt>wait()</tt> on line 11 immediately follows sending and receiving on lines 9 and 10. This class does not attempt to overlap communication across the network with copying to and from buffers. Overlapping network and processor activity is a sophisticated optimization that we will get to near the end of this section.
</div>
<div class="Indented">
The definition of <tt>transpose()</tt> also displays some of the timing infrastructure of this class. The program variable <tt>trans_timer</tt> is an externally defined <tt>struct</tt>. Timing information will be omitted later.
</div>
<h? class="Subsubsection">
<b><u>Sending data</u></b>
</h?>
<div class="Unindented">
During matrix transpose, each process sends a block matrix to every process (see figure <a class="Reference" href="#fig:mpi-transpose-1">6.9↑</a>). The block matrix must be transposed. The block matrix may be transposed when it is either copied to the send buffer or copied out of the receive buffer. We transpose before sending. 
</div>
<div class="Indented">
The definition of a function for transposing follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void transposewlda(double *in, int lda, 
			 	  long nrows, long ncols,
			   	double *out){
	for(long i=0; i &lt; nrows; i++)
		for(long j=0; j &lt; ncols; j++)
			out[j+i*ncols] = in[i+j*lda];
}
</pre>
</div>

</div>
<div class="Indented">
The array <tt>in[]</tt> will typically have a leading dimension of <span class="formula"><i>M</i></span> that is greater than the number of rows. However, the array <tt>out[]</tt>, which holds the transposed matrix and corresponds to a segment of the send buffer, is tightly packed with leading dimension equal to <tt>ncols</tt>. Notice the interchange of <span class="formula"><i>i</i></span> and <span class="formula"><i>j</i></span> in the body of the inner for-loop. This is not the best program for transposing a matrix, and we will make it better later.
</div>
<div class="Indented">
The member function <tt>copyTOsendbuf()</tt> copies <tt>localMN[]</tt> to the send buffer.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void Transpose::copyTOsendbuf(double *localMN){
<span class="number-left">2</span>     long ncols = fstN[p+1] - fstN[p];
<span class="number-left">3</span>     for(long q=0; q &lt; P; q++){
<span class="number-left">4</span>          long nrows = fstM[q+1] - fstM[q];
<span class="number-left">5</span>          long sbufindex = ncols * fstM[q];
<span class="number-left">6</span>          long localindex = fstM[q];
<span class="number-left">7</span>          long lda = M;
<span class="number-left">8</span>          transposewlda(localMN+localindex, lda, 
<span class="number-left">9</span>			     nrows, ncols, 
<span class="number-left">10</span>			     sendbuf+sbufindex);
<span class="number-left">11</span>     }
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 2, <tt>ncols</tt> is initialized to the number of columns of the <span class="formula"><i>M</i> × <i>N</i></span> matrix stored locally. The local MPI process has rank <tt>p</tt>, whereas the for-loop variable <tt>q</tt> (line 3) loops over the ranks of all the <tt>P</tt> processes. The number of rows of the block matrix to be sent by the current process of rank <tt>p</tt> to the process of rank <tt>q</tt> is calculated on line 4 using <tt>fstM[]</tt>. The index into the send buffer is equal to the total number of entries to be sent to processes with rank less than <tt>q</tt> (line 5). The index into <tt>localMN</tt> is simply equal to the index of the first entry in its first column that gets sent to the process of rank <tt>q</tt>. The row number of that entry is equal to <tt>fstM[q]</tt> (line 7). The call to the transposing function (lines 9 through 11) extracts the submatrix to be sent from the current process of rank <tt>p</tt> to the process of rank <tt>q</tt> by adding <tt>localindex</tt> to <tt>localMN</tt> and setting <tt>lda</tt> equal to <tt>M</tt>. 
</div>
<div class="Indented">
The member function for posting send requests uses <tt>fstM[]</tt> and <tt>fstN[]</tt> in a similar manner.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Transpose::postsend(){
     long ncols = fstN[p+1]-fstN[p];
     for(int q=0; q &lt; P; q++){
          long nrows = fstM[q+1]-fstM[q];
          long sbufindex = ncols*fstM[q];
          int count = ncols*nrows; //MPI cant do long
          int tag = 0;
          MPI_Isend(sendbuf+sbufindex,count,MPI_DOUBLE,
	               q, tag, MPI_COMM_WORLD, sreqlist+q);
     }
}
</pre>
</div>

</div>
<div class="Indented">
The number of columns (<tt>ncols</tt>) to be sent is equal to the number of columns owned by the current process of rank <tt>p</tt>. It is calculated using <tt>fstN[]</tt> outside the for-loop. The number of rows to be sent to the process of rank <tt>q</tt> is equal to the number of rows of the transposed matrix assigned to the process of rank <tt>q</tt>. It is calculated inside the loop body using <tt>fstM[]</tt>. The calculation of the index into the send buffer here matches that in the member function <tt>copyTOsendbuf()</tt>, as it must for correctness. 
</div>
<div class="Indented">
The send buffer <tt>sendbuf</tt> was allocated using <tt>new[]</tt>. The send buffer is registered using an Infiniband Verb by the Open MPI implementation during the first invocation of the member function <tt>postsend()</tt>. The memory gets deregistered only at the end of the program. 
</div>
<h? class="Subsubsection">
<b><u>Receiving data</u></b>
</h?>
<div class="Unindented">
The member functions of the <tt>Transpose</tt> class for receiving data closely parallel the member functions for sending. The member function <tt>postrecv()</tt> parallels <tt>postsend()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Transpose::postrecv(){
     long ncols = fstM[p+1]-fstM[p];
     for(long q=0; q &lt; P; q++){
          long nrows = fstN[q+1]-fstN[q];
          long rbufindex = ncols*fstN[q];
          int count = nrows*ncols;//MPI cant do long
          int tag = 0;
          MPI_Irecv(recvbuf+rbufindex,count,MPI_DOUBLE, 
	               q, tag, MPI_COMM_WORLD, rreqlist+q);
     }
}
</pre>
</div>

</div>
<div class="Indented">
Once the receive requests have been posted, the program must wait for the requests to complete.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void Transpose::wait(){
     for(long q=0; q &lt; P; q++)
          MPI_Wait(sreqlist+q, MPI_STATUS_IGNORE);
     for(long q=0; q &lt; P; q++)
          MPI_Wait(rreqlist+q, MPI_STATUS_IGNORE);
}
</pre>
</div>

</div>
<div class="Indented">
In principle, we only need to wait for the receive requests to complete before copying out of the receive buffers. Waiting for the send requests to complete can be put off until the send buffer needs to be reused. Doing so brings no benefit and only makes the structure of the program more complicated.
</div>
<div class="Indented">
Data is copied out of the receive buffers as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void copywlda(double *in, int nrows, int ncols, 
	               double *out, int lda){
     for(long j=0; j &lt; ncols; j++)
          for(long i=0; i &lt; nrows; i++)
               out[i+j*lda] = in[i+j*nrows];
}
​
void Transpose::copyFROMrecvbuf(double *localNM){
     long ncols = fstM[p+1]-fstM[p];
     for(long q=0; q &lt; P; q++){
          long nrows = fstN[q+1]-fstN[q];
          long rbufindex = ncols*fstN[q];
          long localindx = fstN[q];
          long lda = N;
          copywlda(recvbuf+rbufindex, nrows, ncols, 
	           localNM+localindx, lda);
     }
}
</pre>
</div>

</div>
<div class="Indented">
The member function <tt>copyFROMrecvbuf()</tt> extracts the segment of the buffer received from the process with rank <tt>q</tt> as well as the submatrix of <tt>localNM</tt> it must be copied into. The copying is done by <tt>copywlda()</tt>. There is no transposing when copying out of the receive buffers. Notice that in the body of the inner for-loop of <tt>copywlda()</tt>, the indices <span class="formula"><i>i</i></span> and <span class="formula"><i>j</i></span> are not exchanged.
</div>
<h? class="Subsubsection">
<b><u>Faster copying for improved network bandwidth</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:mpi-transpose-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
nprocs (or <span class="formula"><i>P</i></span>)
</td>
<td align="center" valign="top">
<span class="formula"><i>M</i></span>
</td>
<td align="center" valign="top">
<span class="formula"><i>N</i> ⁄ <i>P</i></span>
</td>
<td align="center" valign="top">
b/w
</td>
<td align="center" valign="top">
send copy
</td>
<td align="center" valign="top">
MPI
</td>
<td align="center" valign="top">
recv copy
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.19</span>
</td>
<td align="center" valign="top">
77.5%
</td>
<td align="center" valign="top">
15.5%
</td>
<td align="center" valign="top">
6.4%
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.20</span>
</td>
<td align="center" valign="top">
70.4%
</td>
<td align="center" valign="top">
21.2%
</td>
<td align="center" valign="top">
8.5%
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.20</span>
</td>
<td align="center" valign="top">
70.9%
</td>
<td align="center" valign="top">
20.5%
</td>
<td align="center" valign="top">
7.9%
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.25</span>
</td>
<td align="center" valign="top">
64.6%
</td>
<td align="center" valign="top">
26.2%
</td>
<td align="center" valign="top">
10.2%
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.5 Bandwidth in bytes transferred into or out of a single node per cycle on QDR Infiniband and <span class="formula">12</span>-core <span class="formula">3.33</span>  GHz SSE2 machines (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). The percentage of time spent in copying to the send buffer, MPI message passing, and copying out of the receive buffer are also reported.
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:mpi-transpose-1">6.5↑</a> shows that 75% of the running time of our implementation of the matrix transpose is spent copying into and out of buffers. As a result, the bandwidth realized is unimpressive. The bidirectional bandwidth at a host is about <span class="formula">0.7</span> GB/s, which is a small fraction of the advertised peak of <span class="formula">5</span> GB/s for QDR Infiniband. The data reported in the table are medians of <span class="formula">100</span> measurements.
</div>
<div class="Indented">
A great proportion of the time is being spent copying in and out of buffers. The functions <tt>copywlda()</tt> and <tt>transposewlda()</tt> are inefficient. We know that a single core cannot reach high bandwidth to memory and a direct implementation of transpose results in too many cache and TLB misses. In particular, <tt>transposewlda()</tt> fails to utilize cache lines completely. The cure for these ills is to alter the implementation of transpose to use blocking and to use OpenMP thread-level parallelism during copying.
</div>
<div class="Indented">
The definition of <tt>transposewlda()</tt> is altered as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#define B 50
void transposewlda(double *in, int lda, 
			      long nrows, long ncols,
			      double *out){
#pragma omp parallel for					\
     num_threads(NTHREADS)				\
     default(none)						\
     shared(in, lda, nrows, ncols, out)
     for(int i=0; i &lt; nrows; i+=B)
          for(int j=0; j &lt; ncols; j+=B){
               int iib = (nrows-i &lt; B)?nrows-i:B;
               int jjb = (ncols-j &lt; B)?ncols-j:B;
               for(int ii=0; ii &lt; iib;ii++)
				for(int jj=0; jj &lt; jjb;jj++)
				out[j+jj+(i+ii)*ncols] 
				     = in[i+ii+(j+jj)*lda];
          }
}
</pre>
</div>

</div>
<div class="Indented">
This for-loop nest implements matrix transpose block by block. In a large problem, most blocks are <span class="formula"><i>B</i> × <i>B</i></span>, but the block sizes are adjusted using <tt>iib</tt> and <tt>jjb</tt> at the far edges of the matrix. The outermost loop is parallelized using OpenMP. Matrix transpose with blocking was discussed in chapters 4 and 5. The choice of <span class="formula"><i>B</i> = 50</span> as the block size here is influenced by that earlier discussion. 
</div>
<div class="Indented">
The definition of <tt>copylda()</tt> is altered as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void copywlda(double *in, int nrows, int ncols, 
	               double *out, int lda){
#pragma omp parallel for					\
     num_threads(NTHREADS)				\
     default(none)						\
     shared(in, nrows, ncols, out, lda)
     for(long j=0; j &lt; ncols; j++)
          for(long i=0; i &lt; nrows; i++)
               out[i+j*lda] = in[i+j*nrows];
}
</pre>
</div>

</div>
<div class="Indented">
The modification here is simpler. The outer for-loop has been qualified using the OpenMP parallel-for construct.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-transpose-2"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
nprocs (or <span class="formula"><i>P</i></span>)
</td>
<td align="center" valign="top">
<span class="formula"><i>M</i></span>
</td>
<td align="center" valign="top">
<span class="formula"><i>N</i> ⁄ <i>P</i></span>
</td>
<td align="center" valign="top">
b/w
</td>
<td align="center" valign="top">
send copy
</td>
<td align="center" valign="top">
MPI
</td>
<td align="center" valign="top">
recv copy
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.93</span>
</td>
<td align="center" valign="top">
16.9%
</td>
<td align="center" valign="top">
68.6%
</td>
<td align="center" valign="top">
14.5%
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.81</span>
</td>
<td align="center" valign="top">
15.1%
</td>
<td align="center" valign="top">
71.1%
</td>
<td align="center" valign="top">
13.0%
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.76</span>
</td>
<td align="center" valign="top">
13.7%
</td>
<td align="center" valign="top">
74.3%
</td>
<td align="center" valign="top">
12.0%
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.82</span>
</td>
<td align="center" valign="top">
14.1%
</td>
<td align="center" valign="top">
72.3%
</td>
<td align="center" valign="top">
13.6%
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.6 Improved bandwidth in bytes per cycle, following the use of OpenMP thread-level parallelism for copying into and out of buffers. The network was QDR Infiniband and the host computers were <span class="formula">3.33</span>  GHz SSE2 machines (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>
Table <a class="Reference" href="#tab:mpi-transpose-2">6.6↑</a> shows the considerable improvement in bandwidth from using faster programs to copy into and out of buffers. The bidirectional bandwidth realized at a host is greater than <span class="formula">2.5</span> GB/s in every case. Nearly 55% of the advertised bandwidth of QDR Infiniband is reached with <span class="formula">100</span> host computers on the network.
</div>
<div class="Indented">
Yet there is much room for improvement. Table <a class="Reference" href="#tab:mpi-transpose-2">6.6↑</a> shows that nearly 30% of the cycles are still being used when copying into and out of buffers. It is not possible to make the copying much better but we can try to overlap MPI message passing with copying.
</div>
<h? class="Subsubsection">
<b><u>Overlapping network and processor activity</u></b>
</h?>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-transpose-3"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
nprocs (or <span class="formula"><i>P</i></span>)
</td>
<td align="center" valign="top">
<span class="formula"><i>M</i></span>
</td>
<td align="center" valign="top">
<span class="formula"><i>N</i> ⁄ <i>P</i></span>
</td>
<td align="center" valign="top">
b/w
</td>
<td align="center" valign="top">
send copy
</td>
<td align="center" valign="top">
MPI
</td>
<td align="center" valign="top">
recv copy
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.39</span>
</td>
<td align="center" valign="top">
27%
</td>
<td align="center" valign="top">
-
</td>
<td align="center" valign="top">
24%
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.14</span>
</td>
<td align="center" valign="top">
23%
</td>
<td align="center" valign="top">
-
</td>
<td align="center" valign="top">
20%
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.22</span>
</td>
<td align="center" valign="top">
23%
</td>
<td align="center" valign="top">
-
</td>
<td align="center" valign="top">
21%
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5 × 10<sup>3</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.11</span>
</td>
<td align="center" valign="top">
19%
</td>
<td align="center" valign="top">
-
</td>
<td align="center" valign="top">
20%
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.7 Bandwidth in bytes per cycle with network activity overlapped with processor activity as well as fast copies. Multiply by the clock frequency of <span class="formula">3.33</span>  GHz to get bandwidth in GB/s. The network was QDR Infiniband and all host computers were <span class="formula">12</span>-core <span class="formula">3.33</span>  GHz SSE2 machines (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>
Table <a class="Reference" href="#tab:mpi-transpose-3">6.7↑</a> shows the improvement in bandwidth when network activity is overlapped with processor activity, in addition to using fast copies as discussed. The realized network bandwidth is 75% of the peak of <span class="formula">5</span> GB/s with <span class="formula">100</span> nodes. With <span class="formula">10</span> nodes the realized bandwidth is more than 90% of the peak. 
</div>
<div class="Indented">
The new class <tt>FastTrans</tt> shows how network activity may be overlapped with processor activity. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class FastTrans{
private:
	int p; //process rank
	int P; //number of processes
	long  M;
	long  N;
	long *fstM;
	long *fstN;
	double *sendbuf, *recvbuf;
	MPI_Request *sreqlist, *rreqlist;
	int *sendorder;
	int *rcvd_list;
public:
	FastTrans(int rank, int nprocs, long Mi, long Ni);
	~FastTrans();
	long ffstM(int rank){return fstM[rank];};
	long ffstN(int rank){return fstN[rank];};
	void copyTOsendbuf(int q, double *localMN);
	void postsend(int q);
	void postrecv(int q);
	void wait(); /* wait on all sends */
	void copyFROMrecvbuf(int q, double *localNM);
	void transpose(double *localMN, double *localNM);
};
</pre>
</div>

</div>
<div class="Indented">
The class <tt>FastTrans</tt> is similar to <tt>Transpose</tt> in many ways, although there are some changes. In the private section, two new items appear: <tt>sendorder</tt> and <tt>rcvd_list</tt>. These are used by the member function <tt>transpose()</tt> to order sends and keep track of receives.
</div>
<div class="Indented">
In the public section, the member functions <tt>postsend()</tt> and <tt>postrecv()</tt> take the argument <tt>int q</tt>. The send and receive requests are no longer initiated simultaneously. The member function <tt>transpose()</tt> orders sends and receives to overlap network activity with processor activity.
</div>
<div class="Indented">
The array <tt>sendorder[]</tt> is initialized by the class constructor.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>FastTrans::FastTrans(int rank, int nprocs, long Mi, 
<span class="number-left">2</span>	long Ni):p(rank), P(nprocs), M(Mi), N(Ni)
<span class="number-left">3</span>{    
<span class="number-left">4</span>	fstM = new long[P+1];
<span class="number-left">5</span>	fstN = new long[P+1];
<span class="number-left">6</span>	BlockDivide(M, P, fstM);
<span class="number-left">7</span>	BlockDivide(N, P, fstN);
<span class="number-left">8</span>	long ncols = fstN[p+1]-fstN[p];
<span class="number-left">9</span>	long nrows = fstM[p+1]-fstM[p];
<span class="number-left">10</span>​
<span class="number-left">11</span>	assrt(ncols &gt; 0);
<span class="number-left">12</span>	sendbuf = new double[ncols*M];
<span class="number-left">13</span>​
<span class="number-left">14</span>	assrt(nrows &gt; 0);
<span class="number-left">15</span>	recvbuf = new double[nrows*N];
<span class="number-left">16</span>​
<span class="number-left">17</span>	sreqlist = new MPI_Request[P];
<span class="number-left">18</span>	rreqlist = new MPI_Request[P];
<span class="number-left">19</span>​
<span class="number-left">20</span>	sendorder = new int[P];
<span class="number-left">21</span>	rcvd_list = new int[P];
<span class="number-left">22</span>	
<span class="number-left">23</span>	sendorder[0] = 0;
<span class="number-left">24</span>	for(int i=1; i &lt; P/2 + P%2; i++){
<span class="number-left">25</span>		sendorder[2*(i-1)+1] = i;
<span class="number-left">26</span>		sendorder[2*(i-1)+2] = P-i;
<span class="number-left">27</span>	}
<span class="number-left">28</span>	if(P%2 == 0)
<span class="number-left">29</span>		sendorder[P-1] = P/2;
<span class="number-left">30</span>	
<span class="number-left">31</span>	for(int i=0; i &lt; P; i++)
<span class="number-left">32</span>		sendorder[i] = (sendorder[i]+p)%P;
<span class="number-left">33</span>}
</pre>
</div>

</div>
<div class="Indented">
The initialization of the row and column partition indices <tt>fstM[]</tt> and <tt>fstN[]</tt> are as in the class <tt>Transpose</tt>. The send and receive buffers have exactly the same size as before. However, the sending and receiving will be done in quite a different manner. On lines 23 through 29, the array <tt>sendorder[]</tt> is initialized to be<div class="formula">
0, 1, <i>P</i> − 1, 2, <i>P</i> − 2, 3, <i>P</i> − 3, …
</div>
This send order is valid assuming the local rank is <span class="formula"><i>p</i> = 0</span>. To obtain the send order for the current process of rank <span class="formula"><i>p</i></span>, the for-loop on lines 31 and 32 modifies entries of <tt>sendorder[]</tt> by adding the rank <span class="formula"><i>p</i></span> module <span class="formula"><i>P</i></span>, where <span class="formula"><i>P</i></span> is the total number of MPI processes. 
</div>
<div class="Indented">
The order of sends assumes that a process of rank <span class="formula"><i>p</i></span> is next to those of rank <span class="formula">(<i>p</i> − 1)<span class="mathrm"> mod</span><i>P</i></span> and <span class="formula">(<i>p</i> + 1)<span class="mathrm"> mod</span><i>P</i></span>. In fact, the assignment of ranks to processor nodes can be almost random. No such assumption can be true. Even so, there appears to be some benefit in the reciprocity found between the send orders of processes of two different ranks.
</div>
<div class="Indented">
The definition of the destructor is given for completeness.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">FastTrans::~FastTrans(){
	delete[] fstM;
	delete[] fstN;
	delete[] sreqlist;
	delete[] rreqlist;
	delete[] sendbuf;
	delete[] recvbuf;
	delete[] sendorder;
	delete[] rcvd_list;
}
</pre>
</div>

</div>
<div class="Indented">
The definitions of <tt>copyTOsendbuf()</tt> and <tt>copyFROMrecvbuf()</tt> are the same as before, except they take an argument <tt>int q</tt>. They copy only that part of the matrix that is being sent to or received from the process of rank <span class="formula"><i>q</i></span>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void FastTrans::copyTOsendbuf(int q, double *localMN){
	long ncols = fstN[p+1] - fstN[p];
	long nrows = fstM[q+1] - fstM[q];
	long sbufindex = ncols * fstM[q];
	long localindex = fstM[q];
	long lda = M;
	transposewlda(localMN+localindex, lda, 
		      nrows, ncols, 
		      sendbuf+sbufindex);
}
​
void FastTrans::copyFROMrecvbuf(int q,double *localNM){
	long ncols = fstM[p+1]-fstM[p];
	long nrows = fstN[q+1]-fstN[q];
	long rbufindex = ncols*fstN[q];
	long localindx = fstN[q];
	long lda = N;
	copywlda(recvbuf+rbufindex, nrows, ncols, 
		 localNM+localindx, lda);
}
​
</pre>
</div>

</div>
<div class="Indented">
Unlike the corresponding member functions of the class <tt>Transpose</tt>, these member functions do not use a for-loop to communicate with every other MPI process. As a result, they are slightly more transparent. The functions <tt>transposewlda()</tt> and <tt>copywlda()</tt> are exactly the same as the fast versions given earlier and are omitted.
</div>
<div class="Indented">
The member function <tt>postsend()</tt> and <tt>postrecv()</tt> communicate with a single other process of rank <span class="formula"><i>q</i></span>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void FastTrans::postsend(int q){
	long ncols = fstN[p+1]-fstN[p];
	long nrows = fstM[q+1]-fstM[q];
	long sbufindex = ncols*fstM[q];
	int count = ncols*nrows; //MPI can’t handle long
	int tag = 0;
	MPI_Isend(sendbuf+sbufindex, count, MPI_DOUBLE,
		  q, tag, MPI_COMM_WORLD, sreqlist+q);
}
​
void FastTrans::postrecv(int q){
	long ncols = fstM[p+1]-fstM[p];
	long nrows = fstN[q+1]-fstN[q];
	long rbufindex = ncols*fstN[q];
	int count = nrows*ncols;//MPI can’t handle long
	int tag = 0;
	MPI_Irecv(recvbuf+rbufindex, count, MPI_DOUBLE, 
		  q, tag, MPI_COMM_WORLD, rreqlist+q);
}
</pre>
</div>

</div>
<div class="Indented">
As before, the absence of a for-loop makes these functions slightly more transparent. 
</div>
<div class="Indented">
The member function <tt>wait()</tt> waits for sends to complete but not for receives.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void FastTrans::wait(){
	MPI_Waitall(P, sreqlist, MPI_STATUSES_IGNORE);
}
</pre>
</div>

</div>
<div class="Indented">
The MPI function <tt>MPI_Waitall()</tt> is used to wait for all the <span class="formula"><i>P</i></span> send requests in <tt>sreqlist[]</tt>. We do not wait for receives to complete in the same manner. Waiting for receives is folded into the structure of <tt>transpose()</tt> and is part of the strategy for overlapping network activity with copying to and from buffers.
</div>
<div class="Indented">
The member function <tt>transpose()</tt>, whose definition we now turn to, is the heart of the class <tt>FastTrans</tt>. It is much more complicated than the corresponding member function of the class <tt>Transpose</tt>. Before getting into it, we ask what we want it to do. Of course, at one level, the answer is simple: we want it to overlap network activity with copying to and from buffers. The other member functions have been defined to facilitate this overlap. The member functions copy to and from buffers one block at a time and not all at once. Similarly, the sending and receiving is done one process at a time and not all at once.
</div>
<div class="Indented">
The true answer is a good deal more complicated. To overlap network activity with processor activity, we must understand exactly when RDMA read requests are issued by the Open MPI library. When an RDMA request is issued, the network activity becomes decoupled from processor activity.
</div>
<div class="Indented">
To gain a sense of how RDMA requests are issued, we went back to the original <tt>Transpose</tt> class. We modified the version 1.6 of the Open MPI library by inserting print statements at many locations. The original class effects a matrix transpose by copying to send buffers, initiating a series of nonblocking sends and receives, waiting for them to complete, and finally copying from the receive buffers. The program was run with <span class="formula"><i>P</i> = 20</span> (or <tt>nprocs</tt>=20). The following output was recorded from the process of rank <span class="formula">0</span>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">DV MESG:: MPI_Isend() entry
DV MESG:: MPI_Isend() exit
[Isend message repeats total of 20 times]
...
DV MESG:: MPI_Irecv() entry
DV MESG:: MPI_Irecv() exit
[Irecv message repeats total of 20 times]
...
DV MESG:: MPI_Wait() entry
DV MESG:: MPI_Wait() exit
DV MESG:: MPI_Wait() entry
DV MESG:: issuing RDMA read 
[RDMA read message repeats total of 19 times]
...
DV MESG:: MPI_Wait() exit
DV MESG:: MPI_Wait() entry
[Wait message repeats 38 times after RDMA]
...
</pre>
</div>

</div>
<div class="Indented">
No RDMA request is issued during the first <span class="formula">20</span> sends followed by the first <span class="formula">20</span> receives. No RDMA request is issued during the first <tt>MPI_Wait()</tt> either. The first wait on process of rank <span class="formula">0</span> waits for the send from rank <span class="formula">0</span> to itself to complete. That send involves no network activity. All the RDMA requests are issued during the second wait, which waits on the send to process of rank <span class="formula">1</span>. The waits for the <span class="formula">38</span> other sends and receives follow. None of those results in an RDMA request. All  <span class="formula">19</span> RDMA requests from the process of rank <span class="formula">0</span> during a single matrix transpose are generated after the second wait.
</div>
<div class="Indented">
If we attempt to overlap network activity with copying to and from buffers while thinking that RDMA requests may be issued during sends or receives, we will not make much headway. As shown here, Open MPI is very reluctant to issue RDMA requests during sends or receives. It does so only very occasionally. However, when we were waiting on one particular send request, the Open MPI library issued RDMA requests corresponding to all other processor nodes, although most of them were not a subject of the request we were waiting on.
</div>
<div class="Indented">
The library function <tt>MPI_Wait()</tt> is a blocking call and therefore unsuitable for overlapping network activity with processor activity. Other library functions, such as <tt>MPI_Test()</tt> and <tt>MPI_Testany()</tt>, serve the purpose. <tt>MPI_Test()</tt> returns immediately with information about whether the request supplied to it is complete or not. Analogously, <tt>MPI_Testany()</tt> takes an array of requests and returns immediately. If any one of the requests is completed, it indicates that using a flag. Fortunately, the Open MPI library is willing to generate RDMA requests during <tt>MPI_Testany()</tt>.<span class="FootOuter"><span class="SupFootMarker"> [108] </span><span class="HoverFoot"><span class="SupFootMarker"> [108] </span>In view of the considerable investment in Infiniband and Infiniband-like technologies, it is unfortunate that it is so difficult to find out when MPI send/receive result in RDMA transfers, with the answer being implementation dependent.</span></span>
</div>
<div class="Indented">
The definition of the member function <tt>transpose()</tt>, which is the heart of the class <tt>FastTrans</tt>, follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void FastTrans::transpose(double *localMN, 
<span class="number-left">2</span>					double *localNM){
<span class="number-left">3</span>	for(int q=0; q &lt; P; q++)
<span class="number-left">4</span>		postrecv(q);
<span class="number-left">5</span>	int nrcvd = 0;
<span class="number-left">6</span>	for(int i=0; i &lt; P; i++){
<span class="number-left">7</span>		int q = sendorder[i];
<span class="number-left">8</span>		copyTOsendbuf(q, localMN);
<span class="number-left">9</span>		postsend(q);
<span class="number-left">10</span>		int flag;
<span class="number-left">11</span>		MPI_Testany(P, rreqlist, &amp;q, &amp;flag, 
<span class="number-left">12</span>			    MPI_STATUS_IGNORE);
<span class="number-left">13</span>		if(flag != 0){
<span class="number-left">14</span>			rcvd_list[nrcvd] = q;
<span class="number-left">15</span>			nrcvd++;
<span class="number-left">16</span>		}
<span class="number-left">17</span>	}
<span class="number-left">18</span>	
<span class="number-left">19</span>	int ncpyd = 0;
<span class="number-left">20</span>	while(nrcvd &lt; P){
<span class="number-left">21</span>		if(ncpyd &lt; nrcvd){
<span class="number-left">22</span>			int q = rcvd_list[ncpyd];
<span class="number-left">23</span>			copyFROMrecvbuf(q, localNM);
<span class="number-left">24</span>			ncpyd++;
<span class="number-left">25</span>		}
<span class="number-left">26</span>		int q;
<span class="number-left">27</span>		int flag;
<span class="number-left">28</span>		MPI_Testany(P, rreqlist, &amp;q, &amp;flag, 
<span class="number-left">29</span>				MPI_STATUS_IGNORE);
<span class="number-left">30</span>		if(flag != 0){
<span class="number-left">31</span>			rcvd_list[nrcvd] = q;
<span class="number-left">32</span>			nrcvd++;
<span class="number-left">33</span>		}
<span class="number-left">34</span>	}
<span class="number-left">35</span>​
<span class="number-left">36</span>	assrt(nrcvd == P);
<span class="number-left">37</span>	
<span class="number-left">38</span>	while(ncpyd &lt; P){
<span class="number-left">39</span>		int q = rcvd_list[ncpyd];
<span class="number-left">40</span>		copyFROMrecvbuf(q, localNM);
<span class="number-left">41</span>		ncpyd++;
<span class="number-left">42</span>	}
<span class="number-left">43</span>​
<span class="number-left">44</span>	wait(); /* verify sending is finished */
<span class="number-left">45</span>}
</pre>
</div>

</div>
<div class="Indented">
The member function <tt>transpose()</tt> begins by posting all the receives (lines 3 and 4). An RDMA read request can be issued when a receive is matched with a send from an external process. Because the receives are nonblocking, the program will run past the for-loop on lines 3 and 4 almost instantly.
</div>
<div class="Indented">
Beginning on line 5, the program keeps track of the number of receives that are known to be complete in the variable <tt>nrcvd</tt>. On line 5, it is initialized to be zero.
</div>
<div class="Indented">
The for-loop that runs from lines 6 through 17 overlaps copying to send buffer with network activity. It begins by picking the destination <span class="formula"><i>q</i></span>, which is next in send order (line 7). On line 8, a member function is invoked to copy that part of <tt>localMN[]</tt>, the array that holds the part of the global <span class="formula"><i>M</i> × <i>N</i></span> matrix assigned to the current processor node, which must be sent to <span class="formula"><i>q</i></span>. A nonblocking send is issued by calling a member function on line 9.
</div>
<div class="Indented">
To understand how network activity gets overlapped with processor activity, we must take a careful look at the call 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">		MPI_Testany(P, rreqlist, &amp;q, &amp;flag, 
			    MPI_STATUS_IGNORE);
</pre>
</div>

</div>
<div class="Indented">
that occurs on lines 11 and 12. Here, <tt>rreqlist[]</tt> is the array of receive requests corresponding to the nonblocking receives issued in the for-loop on lines 2 and 3. There are in total <span class="formula"><i>P</i></span> of them. The fourth argument is a pointer to an <tt>int</tt> equal to <tt>&amp;flag</tt> above. When the function returns, <tt>flag</tt> is <span class="formula">0</span> if none of the requests in the list is complete. If any of the requests is complete, <tt>flag</tt> is <span class="formula">1</span>. The third argument is also a pointer to an <tt>int</tt> and equal to <tt>&amp;q</tt> above. If a request is complete, <span class="formula"><i>q</i></span> will equal the corresponding index in the array <tt>rreqlist[]</tt>. The request object <tt>rreqlist[q]</tt> is set to <tt>MPI_REQUEST_NULL</tt>, which means that <tt>rreqlist[q]</tt> becomes inactive and is no longer considered pending.
</div>
<div class="Indented">
Crucially, if the MPI library notices that it can issue an RDMA read corresponding to any pending read, it goes ahead and issues it during <tt>MPI_Testany()</tt>.<span class="FootOuter"><span class="SupFootMarker"> [109] </span><span class="HoverFoot"><span class="SupFootMarker"> [109] </span>This statement, of course, may apply only to the Open MPI implementation.</span></span>
</div>
<div class="Indented">
The if-block on lines 13 through 16 records nonblocking receives that are complete.
</div>
<div class="Indented">
When the for-loop exits on line 17, all the copying to send buffer is done. However, the receives may not all be complete. The while-loop from lines 20 through 34 overlaps copying from receive buffer to the array <tt>localNM[]</tt> with network activity. The invocation of <tt>MPI_Testany()</tt> on lines 28 and 29 has the same role as before. It records any receive that is newly completed and issues RDMA reads that can be issued. Thanks to its ability to issue RDMA reads, the copying from receive buffer in this while-loop is overlapped with network activity, which is managed by the host channel adapter.
</div>
<div class="Indented">
When the while-loop exits on line 34, there may still be completed receives that have not been copied out of the receive buffer. The while-loop on lines 38 through 42 cleans those up. Finally, the member function <tt>transpose()</tt> ensures that all pending sends are complete by invoking the member function <tt>wait()</tt> on line 44 and exits.
</div>
<div class="Indented">
By examining the trace of Open MPI (customized with print statements), we find that RDMA reads are sprinkled across calls to <tt>MPI_Testany()</tt>. Thus, our design is working, which is also confirmed by the improvement in network bandwidth seen in table <a class="Reference" href="#tab:mpi-transpose-3">6.7↑</a>.
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:mpi-transpose-3">6.7↑</a> shows a degradation in bandwidth from 90% of the peak to 75% of the peaks as the number of communicating nodes increases from <span class="formula">10</span> to <span class="formula">100</span>. It is likely that optimizations which mostly prevent that degradation exist. However, implementing such optimizations is not feasible on most high-performance clusters. On most clusters, the user has no control over which nodes in the network are assigned to a program. Even if that information is available, information about network topology and internals of the network are almost never available. So any attempt to sequence sends and receives cleverly by taking network internals into account is frustrated at the beginning. In addition, network activity initiated by other users has an unpredictable effect and makes refined optimizations difficult.
</div>
<div class="Indented">
If we attempt to increase the number of nodes from <span class="formula">10<sup>2</sup></span> to <span class="formula">10<sup>4</sup></span>, it is equally certain that more scaling issues will appear. The largest clusters in 2014 have about <span class="formula">10<sup>4</sup></span> nodes. In our design, <tt>MPI_Testany()</tt> looks at all pending receive requests. When the number of nodes is really large, it may pay to block the pending receive requests that are tested for completion. 
</div>
<h? class="Subsubsection">
<b><u>One MPI process per core is a bad idea</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:mpi-transpose-4"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="right" valign="top">
<tt>nprocs:</tt>
</td>
<td align="center" valign="top">
<span class="formula">120</span>
</td>
<td align="center" valign="top">
<span class="formula">240</span>
</td>
<td align="center" valign="top">
<span class="formula">600</span>
</td>
<td align="center" valign="top">
<span class="formula">1200</span>
</td>

</tr>
<tr>
<td align="right" valign="top">
bw:
</td>
<td align="center" valign="top">
0.70
</td>
<td align="center" valign="top">
0.60
</td>
<td align="center" valign="top">
0.53
</td>
<td align="center" valign="top">
0.27
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.8 Network bandwidth (bw) in bytes/cycle when an MPI process is assigned to every core on QDR Infiniband network with <span class="formula">12</span>-core <span class="formula">3.33</span>  GHz SSE2 machines (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). Therefore, dividing <tt>nprocs</tt> by 12 gives the number of nodes.
</div>

</div>

</div>

</div>
<div class="Indented">
Assigning one MPI process for each core remains quite popular. It saves users the trouble of learning about threads. Table <a class="Reference" href="#tab:mpi-transpose-4">6.8↑</a> gives part of the reason it is a bad idea. To generate the numbers of this table, the same MPI program used for table <a class="Reference" href="#tab:mpi-transpose-3">6.7↑</a> was used. The program was compiled with the same C/C++ compiler with the same flags and linked against the same Open MPI library. The difference was that <tt>NTHREADS</tt> (a <tt>#define</tt> macro variable) was set to <span class="formula">1</span> to turn off threading and an MPI process was assigned to every core.
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:mpi-transpose-4">6.8↑</a> shows that the bandwidth of data transfer across the network at a node drops precipitously. With <span class="formula">10</span> nodes it is less than 50% of the peak, and with <span class="formula">100</span> nodes it is not even 20%. 
</div>
<div class="Indented">
Having a process on every core increases contention for the host channel adapter. Even if the loss in performance were not so severe, it is a bad idea. Assigning a process to every core ignores the modularity of hardware design. The processor node is a hardware module with multiple cores, and there is one network card on every node most of the time. There is modularity in hardware design from the register level up. There is a single memory system to manage all the cores. It is difficult to sustain good design practices if users at one level ignore the modularity of software or hardware they rely on. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.3.4">6.3.4</a> Collective communication<a class="Label" name="sub:mpi-coll"> </a>
</h3>
<div class="Unindented">
The MPI programs we have written so far transfer data using versions of <tt>MPI_Send()</tt> and <tt>MPI_Recv()</tt>. Such function calls are made autonomously by individual processes, but their completion is subject to matching calls made by other processes. The MPI library provides several function calls for collective communication. A collective function call must be made by all processes in a group. The result of collective calls such as <tt>Bcast</tt>, <tt>Scatter</tt>, <tt>Gather</tt>, and <tt>Alltoall</tt> is data transfer involving all the processes. 
</div>
<div class="Indented">
The only collective call we have encountered so far is <tt>MPI_Barrier()</tt> in the function <tt>unsafe()</tt> on page <a class="Reference" href="#page:mpi-unsafe-comm-unsafe()">1↑</a>. The barrier was inserted with the sole purpose of exhibiting MPI syntax. It had no effect on the function <tt>unsafe()</tt>. 
</div>
<div class="Indented">
In this section, we look at <tt>MPI_Bcast()</tt>, <tt>MPI_Scatter()</tt>, <tt>MPI_Alltoall()</tt>, and <tt>MPI_Reduce()</tt>. All these collective functions transfer data between participating processes and can, in principle, be implemented using send and receive operations between processes. For broadcast, the send/receive implementation is too slow. 
</div>
<div class="Indented">
The all-to-all transfer of data effected by <tt>MPI_Alltoall()</tt> can be implemented using send/receive. However, if we restrict ourselves to send/receive syntax introduced so far, the resulting implementation is not as fast as the library function. In deriving an effective send/receive implementation, we encounter an aspect of send/receive syntax of much value. Not surprisingly, this new syntax is of value because it could be related to a facility provided by the Infiniband hardware.
</div>
<div class="Indented">
In typical MPI programs, the problem to be solved is split between several processes. Not infrequently, the solution involves reduction operations such as finding the minimum, maximum, or sum of data scattered across several computers and processes. The <tt>MPI_Reduce()</tt> function may be implemented using send/receive operations, but an effective send/receive implementation requires a tree-like or recursive pattern of communication. Just like matrix transpose, reduction provides opportunities for overlapping network activity with processor activity. In the case of the matrix transpose, the processor activity involves copying to and from buffers. In reduction, the processors must also minimize, maximize, or sum. 
</div>
<div class="Indented">
Thus, with regard to broadcast, all-to-all communication, and reduction, it is instructive to examine send/receive implementations. In each case, comparison with a send/receive implementation brings to light special features of Infiniband hardware. 
</div>
<h? class="Subsubsection">
<b><u>Broadcast</u></b>
</h?>
<div class="Unindented">
In a broadcast operation, the root process sends out data to every other process. The following function was used to time <tt>MPI_Bcast()</tt>: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double bcast(int rank, int nprocs, double *buffer, 
	           int bufsize){
     TimeStamp clk;
     int root = 0;
     clk.tic();
     MPI_Bcast(buffer, bufsize, MPI_DOUBLE, root, 
	          MPI_COMM_WORLD);
     double cycles = clk.toc();
     return cycles;
}
</pre>
</div>

</div>
<div class="Indented">
This <tt>bcast()</tt> function must be called by each MPI process with exactly the same value for <tt>bufsize</tt>. The <tt>bcast()</tt> function calls <tt>MPI_Bcast()</tt>,  which is a collective operation. In general, all processes in the communicator must make the call for a collective operation to take effect. In addition, the calls must match. More specifically, the number of bytes must be exactly equal in all the invocations of <tt>MPI_Bcast()</tt>. An MPI receive can match an MPI send although it consumes only part of the data in the send. The matching rules for collective functions are stricter and do not allow partial consumption of data. The effect of <tt>MPI_Bcast()</tt> is to copy the buffer of the root process to all the other processes. In this example, the root process has rank <span class="formula">0</span>.
</div>
<div class="Indented">
A broadcast operation can be implemented by having the root process send a message to every other process individually. But table <a class="Reference" href="#tab:mpi-collectives-bcast">6.9↓</a> shows that such an implementation would be much worse than using <tt>MPI_Bcast()</tt>. The send/receive implementation realizes only a fraction of the bandwidth and gets worse rapidly as the number of host computers increases.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-collectives-bcast"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">

</td>
<td align="center" valign="top" style="width: 2cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
<span class="formula"><i>n</i> = 1</span><br/>
(cycles)
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula"><i>n</i> = 10<sup>5</sup></span> (bytes/cycle)
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
<span class="formula"><i>n</i> = 10<sup>6</sup></span><br/>
(bytes/cycle)
</div>

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula"><i>P</i> = 10</span>
</td>
<td align="center" valign="top">
bcast
</td>
<td align="center" valign="top" style="width: 2cm;">
7,100
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.39</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.70</span>
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
send/recv
</td>
<td align="center" valign="top" style="width: 2cm;">
10,000
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.11</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula"><i>P</i> = 20</span>
</td>
<td align="center" valign="top">
bcast
</td>
<td align="center" valign="top" style="width: 2cm;">
8,800
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.78</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.46</span>
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
send/recv
</td>
<td align="center" valign="top" style="width: 2cm;">
23,000
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.05</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.05</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula"><i>P</i> = 50</span>
</td>
<td align="center" valign="top">
bcast
</td>
<td align="center" valign="top" style="width: 2cm;">
11,000
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.78</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.78</span>
</td>

</tr>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
send/recv
</td>
<td align="center" valign="top" style="width: 2cm;">
84,000
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.02</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.02</span>
</td>

</tr>

</table>
<div class="caption">
Table 6.9 The broadcast operation on <span class="formula"><i>P</i></span> MPI processes, with one process per host computer and buffers of size equal to <span class="formula"><i>n</i></span> double-precision numbers. The network was QDR Infiniband, and each computer was a <span class="formula">3.33</span> GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
With <span class="formula"><i>n</i> = 1</span> for the buffer size, table <a class="Reference" href="#tab:mpi-collectives-bcast">6.9↑</a> shows that <tt>MPI_Bcast()</tt> returns in less than a few microseconds on a 3.33  GHz SSE2 machine. For <span class="formula"><i>n</i> = 10<sup>5</sup></span> and <span class="formula"><i>n</i> = 10<sup>6</sup></span>, the bandwidth is reported as the number of bytes transferred out of the root per cycle. The maximum bidirectional bandwidth at a single port of the QDR Infiniband switch is advertised to <span class="formula">1.5</span> bytes/cycle. The broadcast operation uses the link in only one direction, and we find that that it reaches more than <span class="formula">0.75</span> bytes/cycle in some instances. The performance does not degrade as the number of host computers increases. On the contrary, it gets better. 
</div>
<div class="Indented">
Infiniband, like many other network architectures, supports broadcast in hardware. The MPI library function <tt>MPI_Bcast()</tt> uses the hardware-level architectural facility for broadcasting. The advantages of supporting broadcast in hardware are obvious. In the fat-tree, a single Infiniband switch may be connected to many other switches, which in turn may be connected to many host computers. The hardware can move the broadcast data to a switch just once and route it to all the host computers that may be reached from that switch. The send/receive implementation is forced to send data through the switch multiple times, once for each host computer that may be reached from that switch. The Internet supports a broadcast protocol for radio broadcasts, sporting events, and the like. 
</div>
<div class="Indented">
The MPI standard states that all collective function calls are blocking. Therefore, a broadcast initiated at the root must wait until every process has made a matching call to <tt>MPI_Bcast()</tt>. The overhead due to blocking will be of the order of several thousand cycles and is insignificant when the data being transferred is a megabyte or more, as shown by table <a class="Reference" href="#tab:mpi-collectives-bcast">6.9↑</a>.
</div>
<h? class="Subsubsection">
<b><u>All-to-all data transfer</u></b>
</h?>
<div class="Indented">
<tt>MPI_Scatter()</tt> has the following prototype 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int MPI_Scatter(void *, int, MPI_Datatype, 
                void *, int, MPI_Datatype,
                int, MPI_Comm)
</pre>
</div>

</div>
<div class="Indented">
A call can be made as in
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">MPI_Scatter(sendbuf, n, MPI_DOUBLE, recvbuf, n, 
			MPI_DOUBLE, root, MPI_COMM_WORLD)
</pre>
</div>

</div>
<div class="Indented">
In this call, the count following <tt>sendbuf</tt> and the count following <tt>recvbuf</tt> are both given as <tt>n</tt>. The meaning of the call is somewhat opaque partly because this is a collective call and partly because the syntax is not suggestive of the role played by <tt>root</tt>. Every process has to supply a <tt>recvbuf</tt>, but only the root needs to supply a valid <tt>sendbuf</tt>. The <tt>sendbuf</tt> supplied by the other processes is ignored. 
</div>
<div class="Indented">
The other complication in <tt>MPI_Scatter()</tt>is related to the send and receive counts (these are given as <span class="formula"><i>n</i></span> in the call above) and their interpretations. The relation of receive count to receive buffer is easier to understand. On each process, including the root, <tt>recvbuf[]</tt> must be large enough to hold <span class="formula"><i>n</i></span> items. The size of <tt>sendbuf[]</tt> on the root must be <span class="formula"><i>n</i> × <span class="text">nprocs</span></span> items (assuming the <tt>WORLD</tt> communicator) and not just <span class="formula"><i>n</i></span> items. The effect of this collective call is to copy items <span class="formula"><i>r</i>∗<i>n</i></span> through <span class="formula">(<i>r</i> + 1)∗<i>n</i> − 1</span> in the <tt>sendbuf[]</tt> of the root to the <tt>recvbuf[]</tt> of the process of rank <span class="formula"><i>r</i></span> for every possible value of <span class="formula"><i>r</i></span>.
</div>
<div class="Indented">
The amount of data sent should be exactly equal to the amount of data received. Why then does the function ask for both a send count and a receive count? In principle, the data types could be different for different receiving processes, and the syntax allows for that. Although it is uncommon to use different data types at the sender and receiver, or to use anything other than <tt>WORLD</tt> as the communicator, MPI syntax burdens every single usage with its rules for data types and communicators. Like any library, MPI has its imperfections. These imperfections exist partly because MPI came into prominence in the early days of parallel computing and has been widely used ever since. As with the x86 architecture and its tortured instruction encoding, success comes with a cost. 
</div>
<div class="Indented">
The following function illustrates usage of <tt>MPI_Scatter()</tt>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double scatter_all2all(int rank, int nprocs, 
		       double *sendbuf, double *recvbuf, 
		       int n){
     TimeStamp clk;
     clk.tic();
     for(int root=0; root &lt; nprocs; root++)
          MPI_Scatter(sendbuf, n, MPI_DOUBLE, 
		recvbuf+n*root,n,MPI_DOUBLE, 
		root,MPI_COMM_WORLD);
     double cycles = clk.toc();
     return cycles;
}
</pre>
</div>

</div>
<div class="Indented">
This function must be called simultaneously by all the processes. Both <tt>sendbuf[]</tt> and <tt>recvbuf[]</tt> must be of length <span class="formula"><i>n</i> × <span class="text">nprocs</span></span>. In this function, each process takes its turn being the root. The data in <tt>sendbuf[]</tt> of all processes may be arranged in a matrix with one column per process: <div class="formula">
<span class="array"><span class="arrayrow"><span class="bracket align-left">⎛</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎜</span></span><span class="arrayrow"><span class="bracket align-left">⎝</span></span></span><span class="array"><span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>00</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>01</sub>
</span>
<span class="arraycell align-c">
⋯
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
<i>a</i><sub>10</sub>
</span>
<span class="arraycell align-c">
<i>a</i><sub>11</sub>
</span>
<span class="arraycell align-c">
⋯
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋮
</span>
<span class="arraycell align-c">
⋱
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>
<span class="arraycell align-c">
 
</span>

</span>
<span class="arrayrow">
<span class="arraycell align-c">

</span>
<span class="arraycell align-c">

</span>
<span class="arraycell align-c">

</span>
<span class="arraycell align-c">
<i>a</i><sub><span class="text">nprocs-1</span>, <span class="text">nprocs-1</span></sub>
</span>

</span>
</span><span class="array"><span class="arrayrow"><span class="bracket align-right">⎞</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎟</span></span><span class="arrayrow"><span class="bracket align-right">⎠</span></span></span>.
</div>
Here, the <tt>sendbuf[]</tt> of the process of rank <span class="formula"><i>r</i></span> is lined up in column <span class="formula"><i>r</i></span>. The entry <span class="formula"><i>a</i><sub><i>ir</i></sub></span> represents 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">sendbuf[i*n],...,sendbuf[(i+1)*n-1]
</pre>
</div>

</div>
<div class="Indented">
in the <tt>sendbuf[]</tt> array of the process of rank <span class="formula"><i>r</i></span>. The effect of a collective invocation of the function is to copy the data in row <span class="formula"><i>i</i></span> of the matrix to the <tt>recvbuf[]</tt> of the process of rank <span class="formula"><i>i</i></span>. In particular, the entry <span class="formula"><i>a</i><sub><i>ir</i></sub></span> is copied to
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">recvbuf[r*n],...,recvbuf[(r+1)*n-1]
</pre>
</div>

</div>
<div class="Indented">
in the <tt>recvbuf[]</tt> array of the process of rank <span class="formula"><i>i</i></span>.
</div>
<div class="Indented">
The MPI library provides a single function call that effects this all-to-all transfer of data. Its usage is as shown below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double mpi_all2all(int rank, int nprocs, 
		   double *sendbuf, double *recvbuf, 
		   int n){
	TimeStamp clk;
	clk.tic();
	MPI_Alltoall(sendbuf, n, MPI_DOUBLE, recvbuf, n,
	       MPI_DOUBLE, MPI_COMM_WORLD);
	double cycles = clk.toc();
	return cycles;
}
</pre>
</div>

</div>
<div class="Indented">
Once again the number of items in the data transfer must be indicated as <span class="formula"><i>n</i></span> for both <tt>sendbuf[]</tt> and <tt>recvbuf[]</tt> because MPI allows send and receive data types to be different in general, a facility we never use. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-coll-scatter-sendrecv-alltoall"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>P</i></span>
</td>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
scatter
</td>
<td align="center" valign="top">
send/recv
</td>
<td align="center" valign="top">
alltoall
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.27</span>
</td>
<td align="center" valign="top">
<span class="formula">1.45</span>
</td>
<td align="center" valign="top">
<span class="formula">1.53</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.27</span>
</td>
<td align="center" valign="top">
<span class="formula">1.54</span>
</td>
<td align="center" valign="top">
<span class="formula">1.56</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.13</span>
</td>
<td align="center" valign="top">
<span class="formula">1.07</span>
</td>
<td align="center" valign="top">
<span class="formula">1.13</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.13</span>
</td>
<td align="center" valign="top">
<span class="formula">1.12</span>
</td>
<td align="center" valign="top">
<span class="formula">1.14</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.05</span>
</td>
<td align="center" valign="top">
<span class="formula">1.23</span>
</td>
<td align="center" valign="top">
<span class="formula">1.35</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.05</span>
</td>
<td align="center" valign="top">
<span class="formula">1.34</span>
</td>
<td align="center" valign="top">
<span class="formula">1.41</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.02</span>
</td>
<td align="center" valign="top">
<span class="formula">1.03</span>
</td>
<td align="center" valign="top">
<span class="formula">1.16</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">10<sup>5</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">0.02</span>
</td>
<td align="center" valign="top">
<span class="formula">1.09</span>
</td>
<td align="center" valign="top">
<span class="formula">1.19</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.10 Bandwidth in bytes/cycle for three implementations of all-to-all transfer on <span class="formula"><i>P</i></span> MPI processes, one per host computer. Multiply by <span class="formula">3.33</span> GHZ to get bandwidth in GB/s. The network was QDR Infiniband, and each host computer was a <span class="formula">3.33</span>  GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). Data consisting of <span class="formula"><i>n</i></span> double-precision numbers is exchanged between every pair. The reported numbers are medians. 
</div>

</div>

</div>
Table <a class="Reference" href="#tab:mpi-coll-scatter-sendrecv-alltoall">6.10↑</a> shows that all-to-all transfer between MPI processes is much faster using <tt>MPI_Alltoall()</tt>  than with repeated calls to <tt>MPI_Scatter()</tt>. Both collective functions are blocking calls, but the transfer of data in each scatter operation is from the root to other processes. The scatter operations do not utilize the bidirectional nature of the Infiniband network. 
</div>
<div class="Indented">
The network bandwidth is, as always, measured as the number of bytes transferred into or out of a single host computer (which is for convenience the MPI process of rank <span class="formula">0</span>) in a single cycle. The all-to-all operations in MPI reach (and even exceed) the peak advertised bandwidth of the QDR Infiniband network. In contrast, the bandwidth realized by the scatter implementation falls rapidly as the number of processes increases (see table <a class="Reference" href="#tab:mpi-coll-scatter-sendrecv-alltoall">6.10↑</a>). In the scatter implementation, each process takes its turn to be the root, which serializes network traffic and fails to utilize much of the parallelism in the all-to-all operation.
</div>
<div class="Indented">
As shown by the table, a send/receive implementation can perform nearly as well as MPI’s all-to-all function. To do that well, the send/receive implementation must use <tt>MPI_Startall() </tt> to start all the sends and receives simultaneously. Doing so enables the MPI library to sequence sends and receives in a good order and perhaps to issue RDMA read requests expeditiously. Certain Infiniband vendors support technology that allows programs to initiate multiple sends and receives simultaneously on the host channel adapter.<span class="FootOuter"><span class="SupFootMarker"> [110] </span><span class="HoverFoot"><span class="SupFootMarker"> [110] </span>An example of such a technology is CORE-Direct available on ConnectX-2 hardware from Mellanox corporation. </span></span>
</div>
<div class="Indented">
Because the plan is to initiate several send and receive operations simultaneously, <tt>MPI_Isend()</tt> and <tt>MPI_IRecv()</tt> are not appropriate. Instead, the send/receive implementation of all-to-all data transfer begins by initializing send and receive operations. The initiation of the operations is deferred to another function.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">MPI_Request *all2all_init(int rank, int nprocs, 
			  double *sendbuf, double *recvbuf, 
			  int n){
     MPI_Request *reqlist = new MPI_Request[2*nprocs];
     for(int i=0; i &lt; nprocs; i++) {
          int dest = i;
          int tag = rank;
          MPI_Send_init(sendbuf+dest*n, n, MPI_DOUBLE, 
		dest, tag, MPI_COMM_WORLD, reqlist+i);
     }
     for(int i=0; i &lt; nprocs; i++){
          int source = i;
          int tag = source;
          MPI_Recv_init(recvbuf+source*n,n,MPI_DOUBLE,
             source, tag, MPI_COMM_WORLD, 
             reqlist+nprocs+i);
     }
     return reqlist;
}
</pre>
</div>

</div>
<div class="Indented">
This function returns an array of <tt>2*nprocs</tt> objects of type <tt>MPI_Request</tt>. A request object is created for sending to and receiving from each process. In this implementation, a process sends data to itself using MPI messages. The request objects are destroyed, when no longer needed, using the following function: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void all2all_finalize(MPI_Request *reqlist){
	delete[] reqlist;
}
</pre>
</div>

</div>
<div class="Indented">
The send/receive implementation of all-to-all transfer is completed by the following function definition: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double all2all(int rank, int nprocs, MPI_Request* 
		reqlist){
	TimeStamp clk;
	clk.tic();
	MPI_Startall(2*nprocs, reqlist);
	MPI_Waitall(2*nprocs, reqlist, MPI_STATUS_IGNORE);
	double cycles = clk.toc();
	return cycles;
}
</pre>
</div>

</div>
<div class="Indented">
The MPI library function <tt>MPI_Startall()</tt> sends all the requests to the MPI library in one batch. At that point, it is up to the Open MPI library (or the network hardware) to sequence the requests intelligently and issue RDMA requests promptly.
</div>
<h? class="Subsubsection">
<b><u>Reduce</u></b>
</h?>
<div class="Indented">
If <span class="formula"><i>f</i>(<i>x</i><sub>0</sub>, …, <i>x</i><sub><i>P</i> − 1</sub>)</span> takes the minimum, maximum, or sum of its arguments, the reduce operation based on <span class="formula"><i>f</i></span> gathers data <span class="formula"><i>x</i><sub><i>r</i></sub></span> from each process of rank <span class="formula"><i>r</i></span>, <span class="formula">0 ≤ <i>r</i> &lt; <i>P</i></span>, and deposits the result with either one or all the processes. The following function illustrates MPI’s reduction capability:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double reducemin(int rank, int nprocs, 
		 double *sendbuf, double *recvbuf, 
		 int bufsize){
	TimeStamp clk;
	clk.tic();
	MPI_Allreduce(sendbuf, recvbuf, bufsize, 
	  MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);
	double cycles = clk.toc();
	return cycles;
}
</pre>
</div>

</div>
<div class="Indented">
The array <tt>sendbuf[]</tt> stores the data to be reduced. The effect of calling the MPI library function <tt>MPI_Allreduce() </tt> is to gather the <tt>sendbuf[]</tt> array from all the processes, apply a reduction operation, and store the result in the array <tt>recvbuf[]</tt>. All processes must call <tt>reducemin()</tt> to initiate the reduction. Here, the reduction operation is given as <tt>MPI_MIN</tt>.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-coll-allreduce"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>P</i></span>
</td>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
allreduce
</td>
<td align="center" valign="top">
send/recv
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">1</span>
</td>
<td align="center" valign="top">
<span class="formula">3.4 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">4.2 × 10<sup>4</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">3.7 × 10<sup>7</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">8.7 × 10<sup>7</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">1</span>
</td>
<td align="center" valign="top">
<span class="formula">4.6 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5.6 × 10<sup>4</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">3.8 × 10<sup>7</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.0 × 10<sup>8</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">1</span>
</td>
<td align="center" valign="top">
<span class="formula">7.1 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">7.7 × 10<sup>4</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">4.1 × 10<sup>7</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.2 × 10<sup>8</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">1</span>
</td>
<td align="center" valign="top">
<span class="formula">8.9 × 10<sup>4</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">9.6 × 10<sup>4</sup></span>
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
<span class="formula">10<sup>6</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">5.1 × 10<sup>7</sup></span>
</td>
<td align="center" valign="top">
<span class="formula">1.5 × 10<sup>8</sup></span>
</td>

</tr>

</table>
<div class="caption">
Table 6.11 The number of cycles consumed by reduction on <span class="formula"><i>P</i></span> MPI processes, one per host, on QDR Infiniband with each host a <span class="formula">3.33</span>  GHz SSE2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). The data on each process is a <tt>double </tt>array of size <span class="formula"><i>n</i></span>. The MPI function is compared with a send/receive implementation.
</div>

</div>

</div>

</div>
 Table <a class="Reference" href="#tab:mpi-coll-allreduce">6.11↑</a> shows that using <tt>MPI_Allreduce()</tt> is nearly three times faster than a send/receive implementation that does not overlap processor activity with network activity. Details of that tree-based implementation are omitted. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.3.5">6.3.5</a> Parallel I/O in MPI<a class="Label" name="sub:mpi-diskio"> </a>
</h3>
<div class="Unindented">
In parallel I/O, a number of MPI processes simultaneously write to or read from the same file. Typical file systems lock a file when it is being accessed by one process, which precludes other processes from accessing it. Parallel I/O is possible only if the file system allows it.
</div>
<div class="Indented">
For parallel I/O to be meaningful, the file system must be capable of storing or <i>striping</i> a single file across multiple hard disks. If the entire file is stored on a single hard disk, accesses from multiple processes will have to be serialized, and parallel I/O from multiple processes will not bring any great advantage.
</div>
<div class="Indented">
MPI syntax for parallel I/O is quite complicated. We step through the syntax once for writing and once for reading a file in parallel. For both writing and reading, we exhibit and exercise MPI functionality for parallel I/O in its simplest form. The function <tt>write_mpi()</tt> defined below is called simultaneously by MPI processes to write to a file in parallel. The data to be written is the array of bytes <tt>data[]</tt> of length <tt>len</tt>, <tt>data[]</tt> and <tt>len</tt> being the first two arguments to <tt>write_mpi()</tt>. The other two arguments to <tt>write_mpi()</tt> are <tt>fname</tt>, which is the name of the file, and <tt>disp</tt>. Because many processes may write to the same file, each process must specify its <i>view</i> of the file. A process’s view of the file is the part of the file that is visible to it. The final argument <tt>disp</tt> specifies the beginning of the view as an absolute displacement (in bytes) from the beginning of the file. It must be noted that both <tt>len</tt> and <tt>disp</tt> are of type <tt>long</tt> and not <tt>int</tt>. An <tt>int</tt> can count up to <span class="formula">2<sup>31</sup> − 1</span>, which is not enough for files that can go up to terabytes or petabytes. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void write_mpi(void *data, long len, 
<span class="number-left">2</span>	       char *fname, long disp){
<span class="number-left">3</span>	MPI_File ofile;
<span class="number-left">4</span>	MPI_File_open(MPI_COMM_WORLD, 
<span class="number-left">5</span>			fname, 
<span class="number-left">6</span>			MPI_MODE_CREATE|MPI_MODE_WRONLY,
<span class="number-left">7</span>			MPI_INFO_NULL,
<span class="number-left">8</span>			&amp;ofile);
<span class="number-left">9</span>	char datarep[200];
<span class="number-left">10</span>	sprintf(datarep, "native");
<span class="number-left">11</span>	MPI_File_set_view(ofile, 
<span class="number-left">12</span>			    disp, 
<span class="number-left">13</span>			    MPI_BYTE,
<span class="number-left">14</span>			    MPI_BYTE,
<span class="number-left">15</span>			    datarep,
<span class="number-left">16</span>			    MPI_INFO_NULL);
<span class="number-left">17</span>	long offset = 0;
<span class="number-left">18</span>	int maxcount = 2000*1000*1000;
<span class="number-left">19</span>	while(len &gt; 0){
<span class="number-left">20</span>		int count = (len&lt;maxcount)?len:maxcount;
<span class="number-left">21</span>		MPI_File_write_at(ofile,
<span class="number-left">22</span>			      offset,
<span class="number-left">23</span>			      data,
<span class="number-left">24</span>			      count,
<span class="number-left">25</span>			      MPI_BYTE,
<span class="number-left">26</span>			      MPI_STATUS_IGNORE);
<span class="number-left">27</span>		offset += count;
<span class="number-left">28</span>		data = ((char *)data+count);
<span class="number-left">29</span>		len -= count;
<span class="number-left">30</span>	}
<span class="number-left">31</span>	MPI_File_close(&amp;ofile);
<span class="number-left">32</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 3, <tt>ofile</tt> is defined to be of type <tt>MPI_File</tt>. <tt>MPI_File</tt> is defined as a pointer to a <tt>struc</tt>t in <tt>mpi.h</tt>. The <tt>struct</tt> it points to will hold information about file attributes, file size, and file location. 
</div>
<div class="Indented">
The <tt>MPI_File_open()</tt> call, which spans lines 4 through 8, is a collective call. Accordingly, its first argument (on line 4) is the <tt>WORLD</tt> communicator. The second argument on line 5 is the file name. The third argument on line 6 says that the file should be created if it does not already exist and the file is opened for writing. In C/C++, a file that is opened for writing loses its previous data but not so in MPI. Line 6 is a null argument, and line 7 supplies the file pointer as the final argument. 
</div>
<div class="Indented">
The <tt>MPI_File_set_view()</tt> call,  which spans lines 11 through 16, is also a collective call. Each process specifies the part of the file that is visible to itself. The first argument (line 11) is the file pointer, and the second argument (line 12) is <tt>disp</tt>, which marks the beginning of the file view of the calling process, as already explained. The third argument is the <i>elementary type,</i> and the fourth argument is the <i>file type</i> for accessing the file. The elementary and file types can be used to specify file views of mind-bending complexity. Lines 13 and 14 give both arguments as <tt>MPI_BYTE</tt>, thus specifying the process’s view of the file as a sequence of bytes. Viewing the file as a sequence of bytes keeps things simple for the programmer, the MPI implementation, the file system, and the hardware. The fifth argument (of type <tt>char *</tt> and not <tt>const char * )</tt>is given as <tt>datarep</tt> (line 15). The variable <tt>datarep</tt> holds the character string &ldquo;native&rdquo; (lines 9 and 10). Presumably, a byte is a byte in the native representation. The final info argument is given as null on line 16.
</div>
<div class="Indented">
The <tt>MPI_File_write_at()</tt> call  spanning lines 21 through 26 gets down to the business of writing to the file. But there is a problem. The <tt>count</tt> (line 24) of the number of bytes to be written is an <tt>int</tt> and not a <tt>long</tt>. The process will not be able to write all the data in 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">data[0],...,data[len-1]
</pre>
</div>

</div>
<div class="Indented">
if <tt>len</tt>, which is of type <tt>long</tt>, is greater than <span class="formula">2<sup>31</sup> − 1</span>. However, the offset relative to the beginning of the file view can be of type <tt>long</tt> (line 22). The variable <tt>maxcount</tt>, which is the maximum number of bytes the process will try to write with a single function call, is initialized to <span class="formula">2 × 10<sup>9</sup></span> (which is less than <span class="formula">2<sup>31</sup> − 1</span>) on line 18. The while-loop spanning lines 19 through 30 writes to the file in stages. The variable <tt>len</tt> is the number of bytes yet to be written at the top of the loop, and line 20 calculates the <tt>count</tt> of bytes to be written in the current iteration. Once the data is written, the variable <tt>offset</tt>, which is the offset relative to the beginning of the file, and <tt>data</tt>, which points to the first entry in the array that is yet to be written, are both incremented by <tt>count</tt> (lines 27 and 28). The variable <tt>len</tt> is decremented by <tt>count</tt> (line 29) because the number of bytes yet to be written is fewer by <tt>count</tt>.
</div>
<div class="Indented">
The library function <tt>MPI_File_write_at()</tt> is not collective, but MPI provides a collective version (obtained by appending <tt>_all</tt> to the function name). The collective version would deadlock if the while-loop spanning lines 11 through 30 did not have exactly the same iteration count on every MPI process. Therefore, we have used the noncollective version. The collective version can be slightly faster, however.
</div>
<div class="Indented">
The <tt>MPI_File_close()</tt> call  (line 31) is collective, like the calls for opening a file and setting its view. Calls to a collective MPI function made by different processes must match. As long as we restrict ourselves to the <tt>MPI_BYTE</tt> data type, the matching requirements of functions for opening and closing files, or setting the process’s view of an open file, are satisfied. 
</div>
<div class="Indented">
The definition of <tt>read_mpi()</tt> closely parallels that of <tt>write_mpi()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void read_mpi(void *data, long len, 
<span class="number-left">2</span>	      char *fname, long disp){
<span class="number-left">3</span>	MPI_File ifile;
<span class="number-left">4</span>	MPI_File_open(MPI_COMM_WORLD, 
<span class="number-left">5</span>			fname, 
<span class="number-left">6</span>			MPI_MODE_RDONLY,
<span class="number-left">7</span>			MPI_INFO_NULL,
<span class="number-left">8</span>			&amp;ifile);
<span class="number-left">9</span>	char datarep[200];
<span class="number-left">10</span>	sprintf(datarep, "native");
<span class="number-left">11</span>	MPI_File_set_view(ifile, 
<span class="number-left">12</span>			    disp, 
<span class="number-left">13</span>			    MPI_BYTE,
<span class="number-left">14</span>			    MPI_BYTE,
<span class="number-left">15</span>		    	datarep,
<span class="number-left">16</span>		    	MPI_INFO_NULL);
<span class="number-left">17</span>	long offset = 0;
<span class="number-left">18</span>	int maxcount = 2000*1000*1000;
<span class="number-left">19</span>	while(len &gt; 0){
<span class="number-left">20</span>		int count = (len&lt;maxcount)?len:maxcount;
<span class="number-left">21</span>		MPI_File_read_at(ifile,
<span class="number-left">22</span>				     offset,
<span class="number-left">23</span>				     data,
<span class="number-left">24</span>				     count,
<span class="number-left">25</span>				     MPI_BYTE,
<span class="number-left">26</span>				     MPI_STATUS_IGNORE);
<span class="number-left">27</span>		offset += count;
<span class="number-left">28</span>		data = ((char *)data+count);
<span class="number-left">29</span>		len -= count;
<span class="number-left">30</span>	}
<span class="number-left">31</span>	MPI_File_close(&amp;ifile);
<span class="number-left">32</span>}
</pre>
</div>

</div>
<div class="Indented">
The function <tt>read_mpi()</tt> differs from <tt>write_mpi()</tt> in opening the file for reading rather than writing (see line 6) and using the function to read on line 21 instead of the function to write.
</div>
<div class="Indented">
The two functions <tt>write_mpi()</tt> and <tt>read_mpi()</tt> match line by line except at two points, and some programmers may argue that the function definitions should reflect this parallelism. The two functions may be collapsed into one, saving lines of code, by passing a flag to indicate whether the function should write or read. The usage of the flag at two points far away from each other breaks the flow of the code and makes it hard to read. The C++ language provides more structured facilities such as inheritance and templates to avoid code bloat in a fairly smooth way. There are surely many instances where such facilities are appropriate, although they fall outside the domain of this author. The parallelism between <tt>write_mpi()</tt> and <tt>read_mpi()</tt> is syntactic and not semantic. Writing to a file and reading from it are different operations. Keeping the function definitions apart has the advantage of prioritizing meaning over form. 
</div>
<h? class="Subsubsection">
<b><u>Lustre file system</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:mpi-diskio-lustre"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/lustre.png" alt="figure FIGS/chapter5/lustre.png" style="max-width: 335px; max-height: 203px;"/>

</div>
<div class="caption">
Figure 6.11 Lustre file system. The processor nodes (clients) as well as the metadata server and the object storage servers are connected to a single big Infiniband switch in this depiction. More generally, they may be connected to different switches, which are in turn connected to each other. 
</div>

</div>

</div>

</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-diskio-lustre">6.11↑</a> is a depiction of a Lustre file system<span class="FootOuter"><span class="SupFootMarker"> [111] </span><span class="HoverFoot"><span class="SupFootMarker"> [111] </span>For the Lustre file system, see the operations manual at <a class="FlexURL" href="http://wiki.lustre.org/index.php/Learn">http://wiki.lustre.org/index.php/Learn</a>.</span></span> realized over Infiniband. It is similar to figure <a class="Reference" href="#fig:mpi-infini-net-arch-1">6.3↑</a>, with the difference that the file system components are shown in more detail. As in that figure, HCA and TCA are acronyms for Host/Target Channel Adapter. Unlike HCA, TCA do not support the Verb layer. The Metadata Server connected to the network makes information about file names and directories available to clients (or processor nodes). The metadata about file names and directories is stored in the Metadata Target. In general, several Metadata Servers can be connected to the same Metadata Target to guard against server failure. The Metadata Target is a storage module with redundancies built in to guard against disk failure.
</div>
<div class="Indented">
Figure <a class="Reference" href="#fig:mpi-diskio-lustre">6.11↑</a> shows two among possibly many Object Storage Servers connected to the network. Each Object Storage Server is connected to two Object Storage Targets. Object Storage Targets are the components that hold data. Like the Metadata Target, they too will have redundancies built in. As many as eight Object Storage Targets can be connected to the same server. The Object Storage Server is their interface to the network.
</div>
<div class="Indented">
A typical configuration may have hundreds of processor nodes, tens of Object Storage Targets, and one Metadata Server. The Lonestar system at the Texas Advanced Computing Center has more than <span class="formula">1, 000</span> processor nodes and <span class="formula">90</span> Object Storage Targets. The total capacity of its Lustre file system is <span class="formula">1</span> petabyte. There are Lustre file systems of <span class="formula">10</span> times that capacity.
</div>
<div class="Indented">
The number of stripes of a Lustre file may be retrieved using the <tt>lfs getstripe</tt> command. For a 2 TB file, <tt>lfs getstripe</tt> produced the following output:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">lmm_stripe_count:   50
lmm_stripe_size:    1048576
lmm_stripe_offset:  15
obdidx		 objid		objid		 group
    15	      57347983	    0x36b0f8f	             0
     9	      56348442	    0x35bcf1a	             0
    65	      45034528	    0x2af2c20	             0
     4	      56025829	    0x356e2e5	             0
        ...
</pre>
</div>

</div>
<div class="Indented">
This file has <span class="formula">50</span> stripes, which means that it is spread across <span class="formula">50</span> object storage targets. The stripe size is <span class="formula">2<sup>20</sup></span> bytes or <span class="formula">1</span> MB, which is Lustre’s default value for stripe size. The list of <span class="formula">50</span> object storage targets, beginning with <tt>obdidx</tt> 15, is only partially shown.
</div>
<div class="Indented">
If the stripe size is <span class="formula">1</span> MB, the first <span class="formula">1</span> MB is stored on the first object storage target (here the one with <tt>obdidx</tt> of 15), the next <span class="formula">1</span> MB on the next target (here the one with <tt>obdidx</tt> of 9), and so on in a cyclic fashion. The <tt>lfs setstripe</tt> command may be used to set the stripe count and stripe size of a directory or newly created file. 
</div>
<div class="Indented">
The C++ class <tt>LustreFile</tt> given below is an easy interface for striping and using Lustre files from MPI.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class LustreFile{
private:
	char dir[200];
	char fname[200];
	int rank;
	int nprocs;
	long totalsize;
	long localsize;
public:
	LustreFile(int ranki, int nprocsi,
		const char *diri, const char *fnamei, 
		long sizei);
	long getlocalsize() {return localsize;}
	void setstripe(int count, int stripesize=1);
	void printinfo();
	void write(double *v);
	void read(double *v);
};
</pre>
</div>

</div>
<div class="Indented">
This class is useful for writing and reading a sequence of <tt>double</tt>s. 
</div>
<div class="Indented">
The class constructor has the following definition:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">LustreFile::LustreFile(int ranki, int nprocsi,
		       const char *diri, const char *fnamei, 
		       long sizei){
	rank = ranki;
	nprocs = nprocsi;
	strcpy(dir, diri);
	strcpy(fname, fnamei);
	totalsize = sizei;
	localsize = totalsize/nprocs;
	if(rank==(nprocs-1))
		localsize = totalsize - (nprocs-1)*localsize;
}
</pre>
</div>

</div>
<div class="Indented">
All the MPI processes must participate in the creation of an object of class <tt>LustreFile</tt>. If all the processes create a class object as in 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Lustrefile lustre(rank, nprocs, dir, fname, totalsize)
</pre>
</div>

</div>
<div class="Indented">
there will of course be a separate <tt>LustreFile</tt> object on each process. To exercise the functionality of this class, the objects must match across all the processes. In particular, the directory, file name, and total size must be identical in all the MPI processes. 
</div>
<div class="Indented">
The total size is the number of <tt>double</tt>s stored in the entire file. The class constructor calculates the number of <tt>double</tt>s in the local view of each process. The local size is the same in all MPI processes except possibly the last (of rank <tt>nprocs-1</tt>). An alternative, and possibly more useful, interface to the class might be for each process to call the constructor with the local size of the file. The constructor can then compute the total size using a reduction operation.
</div>
<div class="Indented">
This constructor does not create the file. The file is created when the member function <tt>setstripe()</tt> is invoked.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void LustreFile::setstripe(int count, int stripesize){
<span class="number-left">2</span>	if(rank==0){
<span class="number-left">3</span>		char cmd[200];
<span class="number-left">4</span>		sprintf(cmd, "rm %s/%s", dir, fname);
<span class="number-left">5</span>		system(cmd);
<span class="number-left">6</span>		sprintf(cmd, 
<span class="number-left">7</span>		"lfs setstripe --size %dM --count %d %s/%s",
<span class="number-left">8</span>		stripesize, count, dir, fname);
<span class="number-left">9</span>		system(cmd);
<span class="number-left">10</span>	}
<span class="number-left">11</span>	MPI_Barrier(MPI_COMM_WORLD);
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
The task of creating the file is left to the process of rank <span class="formula">0</span>. The rank <span class="formula">0</span> process removes the file if it is already present. On line 4, the character string <tt>cmd[]</tt> is constructed. On line 5, the library function <tt>system()</tt> (it is declared in <tt>stdlib.h</tt>) invokes the shell and executes the command. The command for striping is composed as a string on lines 6, 7, and 8. If we want <span class="formula">50</span> stripes and a stripe size of <span class="formula">4, 000</span> MB, the command is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">lfs setstripe --size 4000 M --count 50 &lt;filename&gt;
</pre>
</div>

</div>
<div class="Indented">
Such a command is issued through the shell on line 9. It creates a new file.
</div>
<div class="Indented">
Although the process of rank <span class="formula">0</span> is the only one to handle striping, all the processes must call the member function <tt>setstripe()</tt>. The barrier at the end (line 10) ensures that none of the processes returns before file creation is complete. The barrier is essential for ensuring that none of the processes attempts to access the file before it is created. 
</div>
<div class="Indented">
The member functions <tt>write()</tt> and <tt>read()</tt> invoke <tt>write_mpi()</tt> and <tt>read_mpi()</tt> as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void LustreFile::write(double *v){
	long disp = 8*rank*(totalsize/nprocs);
	char fnamex[200];
	sprintf(fnamex, "%s/%s", dir, fname);
	write_mpi((void *)v, 8*localsize, fnamex, disp);
}
​
void LustreFile::read(double *v){
	long disp = 8l*rank*(totalsize/nprocs);
	char fnamex[200];
	sprintf(fnamex, "%s/%s", dir, fname);
	read_mpi((void *)v, 8l*localsize, fnamex, disp);
}
</pre>
</div>

</div>
<div class="Indented">
The class definition is completed by the member function <tt>printinfo()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void LustreFile::printinfo(){
	if(rank!=0)
		return;
	char cmd[200];
	sprintf(cmd, "ls -l %s", dir);
	system(cmd);
	sprintf(cmd , "lfs getstripe %s/%s", dir, fname);
	system(cmd);
}
</pre>
</div>

</div>
<div class="Indented">
Like <tt>setstripe()</tt>, <tt>printinfo()</tt> uses <tt>system()</tt> to issue commands through the shell.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:mpi-diskio-lustre-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
nprocs
</td>
<td align="center" valign="top">
stripes
</td>
<td align="center" valign="top" style="width: 2.3cm;">
write (lsize=8MB)
</td>
<td align="center" valign="top" style="width: 2.3cm;">
read (lsize=8MB)
</td>
<td align="center" valign="top" style="width: 2.3cm;">
write (lsize=20GB)
</td>
<td align="center" valign="top" style="width: 2.3cm;">
read (lsize=20GB)
</td>

</tr>
<tr>
<td align="center" valign="top">
1
</td>
<td align="center" valign="top">
1
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">0.5</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">0.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">0.2</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">0.5</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top">
10
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">0.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">1.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">1.6</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">2.3</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top">
20
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">0.6</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">2.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">3.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">4.1</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">1.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">3.2</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">6.9</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">6.2</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
100
</td>
<td align="center" valign="top">
50
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">2.6</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">6.3</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">6.6</span>
</td>
<td align="center" valign="top" style="width: 2.3cm;">
<span class="formula">7.6</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.12 Bandwidth in GB/s on a 1 petabyte Lustre file system connected via QDR Infiniband. The reported numbers are maximums over five trials. The total size of the file divided by the number of processes (nprocs) is given as <i>lsize</i>.
</div>

</div>

</div>
Table <a class="Reference" href="#tab:mpi-diskio-lustre-1">6.12↑</a> reports bandwidth measurements on file sizes that vary from <span class="formula">8</span> MB to <span class="formula">2</span> TB. Some of the measurements in the table, especially the ones with <i>lsize</i> of 8 MB, are probably influenced by caching. On the Lonestar cluster, the Lustre file system reaches a bandwidth of nearly <span class="formula">7</span> GB/s with <span class="formula">50</span> stripes. The cluster is used by a large number of scientists, and it is quite possible that the measurements are influenced by other jobs being run on the cluster. We do not think that is likely, however. Similar numbers were obtained during measurements made on different days at different times.
</div>
<div class="Indented">
Like almost all file systems, the Lustre file system also goes through a page cache on each client (for page caches see, section <a class="Reference" href="#sec:memory-diskio">4.3↑</a>). The maximum size of this page cache is recorded in the file 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">/proc/sys/lustre/max_dirty_mb.txt
</pre>
</div>

</div>
<div class="Indented">
as <span class="formula">12.6</span> GB. That is half the total memory on each <span class="formula">3.33</span>  GHz SSE2 computer in this cluster. Even when each MPI process writes and then reads <span class="formula">20</span> GB of data to the same file, the page cache may still influence the measured bandwidth. It is not worth the trouble to completely eliminate the page cache effects. Eliminating cache effects will lead to contortions that are applicable to few real programs. Programs, which do not saturate available memory, will benefit from the page cache. The runs corresponding to the last two columns of table <a class="Reference" href="#tab:mpi-diskio-lustre-1">6.12↑</a> hold a <span class="formula">20</span> GB array on each MPI process. More than 80% of the <span class="formula">24</span> GB of memory available on each Xeon 5680 host computer is used up and unavailable for page caching. Early versions of Lustre did not use caches on the Object Storage Servers, but newer versions can cache on the Object Storage Servers as well.<span class="FootOuter"><span class="SupFootMarker"> [112] </span><span class="HoverFoot"><span class="SupFootMarker"> [112] </span>Thanks to Oleg Drokin, Andrew Lundgren, and Cliff White for information about caching in Lustre, posted on an Internet forum in response to a question posed by Jordan Mendler.</span></span>
</div>
<div class="Indented">
If the number of stripes is fixed at <span class="formula">50</span>, table <a class="Reference" href="#tab:mpi-diskio-lustre-1">6.12↑</a> shows that nearly the same bandwidth is realized with either <span class="formula">50</span> or <span class="formula">100</span> processes. With <span class="formula">10</span> stripes, the read bandwidth is approximately <span class="formula">2.6</span> GB/s, and the write bandwidth is approximately <span class="formula">1.70</span> GB/s with <span class="formula">10</span>, <span class="formula">20</span>, or <span class="formula">50</span> clients running on as many Xeon 5680 host computers (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for information about the machine). If the number of stripes is <span class="formula">10</span> and the file is accessed by a single client, the bandwidths are somewhat less than <span class="formula">1</span> GB/s. Lustre remains robust when the number of client processes is increased while fixing the number of stripes.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Suppose <span class="formula">2<sup><i>k</i></sup></span> MPI processes are assumed to correspond to the <span class="formula">2<sup><i>k</i></sup></span> vertices of a <span class="formula"><i>k</i></span>-dimensional hypercube. Implement a class that allows each process to exchange data with its <span class="formula"><i>k</i></span> neighbors. Measure the bandwidth of data transfer with persistent, nonblocking, and blocking communication.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why <tt>MPI_Isend()</tt> and <tt>MPI_IRecv()</tt> must be normally preferred over the blocking and persistent versions for sending and receiving messages.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Rerun the Jacobi example while initializing all the data from a single core. By how much does the program slow down? Why does it slow down?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Much of the time in the Jacobi example is spent in <tt>update()</tt>. The loop nest in <tt>update()</tt> can be improved in two ways. First, it can be blocked. Second, it can be rewritten so that the modular arithmetic for computing <tt>iup</tt> and <tt>idown</tt> is replaced by something as simple as <tt>i=i-1</tt> or <tt>i=i+1</tt>, which holds for all except terminal values of <tt>i</tt>. The terminal values of <tt>i</tt> will need to be treated separately. Make these two changes and discuss the resulting speedup. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why blocking data is a good idea for <tt>transposewlda()</tt> but not for <tt>copywlda()</tt>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  3D data <span class="formula"><i>d</i><sub><i>ijk</i></sub></span> with <span class="formula">0 ≤ <i>i</i> &lt; <i>L</i></span>, <span class="formula">0 ≤ <i>j</i> &lt; <i>M</i></span>, and <span class="formula">0 ≤ <i>k</i> &lt; <i>N</i></span> can be stored in array of length <span class="formula"><i>LMN</i></span> in six different ways. For example, if <span class="formula"><i>i</i></span> is innermost and <span class="formula"><i>k</i></span> is outermost,<tt> <span class="formula"><i>d</i><sub><i>ijk</i></sub></span> </tt>will be in the location with global index <span class="formula"><i>i</i> + <i>j</i> × <i>L</i> + <i>k</i> × <i>LM</i></span>. The other five storage formats correspond to the five other orderings of <span class="formula"><i>i</i>, <i>j</i>, <i>k</i></span>. The array must be partitioned between <span class="formula"><i>P</i></span> MPI processes, such that only the outermost index is split. Write an MPI program to convert from one storage format to another.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Recursive doubling is a method of effecting a reduction where each process begins by representing its rank as a binary number <span class="formula"><i>b</i><sub><i>k</i></sub>…<i>b</i><sub>1</sub><i>b</i><sub>0</sub></span>. At level <span class="formula"><i>i</i></span>, it flips its <span class="formula"><i>i</i></span>th bit from <span class="formula"><i>b</i><sub><i>i</i></sub></span> to <span class="formula">1 − <i>b</i><sub><i>i</i></sub></span> to find the rank of its <span class="formula"><i>i</i></span>th level peer. All the pairs of peers at level <span class="formula"><i>i</i></span> exchange their data and take the minimum (or do whatever other reduction operation). The reduction is complete when all processes step through levels <span class="formula"><i>i</i> = 0</span> to <span class="formula"><i>i</i> = <i>k</i></span>. Implement this reduction algorithm.<span class="FootOuter"><span class="SupFootMarker"> [113] </span><span class="HoverFoot"><span class="SupFootMarker"> [113] </span>For a discussion of recursive doubling and other strategies for reduction, see <span class="bibcites">[<a class="bibliocite" name="cite-53" href="#biblio-53"><span class="bib-index">53</span></a>]</span>.</span></span> 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-6.4">6.4</a> The Internet<a class="Label" name="sec:ntwksmpi-Internet"> </a>
</h2>
<div class="Unindented">
The main function of the Internet, as it exists today, is to connect people. There is no doubt that the Internet is one of the most successful technologies. The Internet is not the result of a single act of invention. Even now its design is evolving to accommodate new uses and technologies. 
</div>
<div class="Indented">
Ever since the invention of the transistor, computing technology has been driven simultaneously by market forces and the possibility of technological innovation. The market forces driving the Internet today are among the most powerful. At the same time, a number of entities are well organized for innovation. The scope for new technologies in this area appears considerable. One may struggle to get bandwidth of even 1 MB/s between Michigan and Texas today, but that will surely change in the not too distant future.
</div>
<div class="Indented">
It would be folly for scientific programmers to ignore the Internet and think that MPI is the end of the world in networking as far as scientific applications are concerned. The volume of investment in the Internet is far, far greater, and so is the potential for technological innovation. The highly decentralized nature of the Internet gives it powerful advantages over centralized technologies such as MPI and Infiniband.
</div>
<div class="Indented">
The Internet will intrude into scientific computing in more and more ways. The maintenance of databases is one area where that has already happened. In many scientific applications, moving data from one place to another is too expensive, leaving no choice but to coordinate data analysis over the Internet.
</div>
<div class="Indented">
In this section, we give an overview of the Internet at the level of the TCP/IP protocol.  The TCP/IP protocol is packetized. Indeed, it was one of the first packetized network protocols. In section <a class="Reference" href="#sub:internet-IP-addresses">6.4.1↓</a>, we outline the manner in which TCP/IP packets are formed. The TCP protocol adds application information, specifically the port numbers at the source and destination. The IP protocol adds address information so that the packet may be routed from source to destination.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:internet-Send-and-receive">6.4.2↓</a> describes the way <tt>send()</tt> and <tt>recv()</tt> work.  These functions, part of the TCP/IP/sockets API, are defined by the operating system kernel---in our case, the Linux kernel. A new wrinkle here is the manner in which a connection is terminated. Normally, <tt>recv()</tt> returns the number of bytes received. It returns zero if the sender has closed the connection. The sender may close the connection by closing a socket. Servers as well as clients are implemented using <tt>send()</tt> and <tt>recv()</tt>.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:internet-server">6.4.3↓</a> describes how a server is set up. To begin with, a socket is bound to a port. Well-known protocols such as <tt>http</tt> and <tt>https</tt> have port numbers known to all computers (80 and 443, respectively). For the client-server example in this section, we make up our own port number. After the socket is bound to a port, it is marked for listening using the function <tt>listen()</tt>.
</div>
<div class="Indented">
The socket may then be used by the server to accept connections using the function <tt>accept()</tt>. Once a connection is accepted, the client and server can begin to communicate. Typical servers are multithreaded and can engage multiple clients simultaneously. The server in our simple example is single-threaded. So if it is communicating with a client, all the other clients will have to wait until the communication is fully over.
</div>
<div class="Indented">
The client initiates a connection using <tt>connect()</tt>, which is the counterpart of <tt>accept()</tt>. If a matching server is running, a connection is made. A client program is described in section <a class="Reference" href="#sub:internet-Client">6.4.4↓</a>. 
</div>
<div class="Indented">
In all probability, the reader’s computer can run either servers or clients. To run the example in this section in real time, it is enough to have an ssh connection to some remote site so that the client and server may be run at different locations.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:internet-latency">6.4.5↓</a> is a discussion of Internet latency, which we define as the round-trip time. The path that packets take from a destination back to the source can be different from the path they take from the source to the destination. A significant fraction of Internet latency can be due to the finite speed of light. During their journey between source and destination, TCP/IP packets spend time in buffers maintained by enormous routers. The buffers are typically linked lists and are therefore exposed to latency to DRAM memory. Buffering is another source of latency.
</div>
<div class="Indented">
On a good day, the bandwidth between Michigan and Texas can be <span class="formula">2</span> MB/s. That figure is lower than the bandwidth to hard disk by a factor of <span class="formula">100</span>.  The bandwidth to hard disk can be lower than the bandwidth to DRAM by a factor of nearly a <span class="formula">1000</span>. The gap between Internet bandwidth and the bandwidth to hard disk may narrow in the coming years. In section <a class="Reference" href="#sub:internet-bandwidth">6.4.6↓</a>, we relate Internet bandwidth to congestion windows.
</div>
<div class="Indented">
On the whole, the aim of this section is to give a view of the Internet from inside a computer program. The discussion is illustrated using programs throughout. Algorithms for flow control and congestion control internal to the Internet are not discussed.
</div>
<div class="Indented">
Our discussion is based on functions in the sockets API, such as <tt>getaddrinfo(), bind()</tt>, <tt>listen()</tt>, <tt>accept()</tt>, <tt>connect()</tt>, <tt>recv()</tt>, and <tt>send()</tt>.  The prototypes of these functions are in various header files.<span class="FootOuter"><span class="SupFootMarker"> [114] </span><span class="HoverFoot"><span class="SupFootMarker"> [114] </span>A readable introduction to the sockets API is given by <span class="bibcites">[<a class="bibliocite" name="cite-50" href="#biblio-50"><span class="bib-index">50</span></a>]</span>. See <a class="FlexURL" href="http://beej.us/guide/bgnet/">http://beej.us/guide/bgnet/</a>. <span class="bibcites">[<a class="bibliocite" name="cite-54" href="#biblio-54"><span class="bib-index">54</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-49" href="#biblio-49"><span class="bib-index">49</span></a>]</span> are two well-known textbooks on computer networks.</span></span> Here we list all the header files to be included in our programs.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/socket.h&gt;
#include &lt;netinet/in.h&gt;
#include &lt;netinet/tcp.h&gt;
#include &lt;netdb.h&gt;
#include &lt;arpa/inet.h&gt;
</pre>
</div>

</div>
<div class="Indented">
There are possibly a few redundancies here.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.4.1">6.4.1</a> IP addresses<a class="Label" name="sub:internet-IP-addresses"> </a>
</h3>
<div class="Unindented">
To the average person, the Internet is a place for news, commerce, and entertainment. To the programmer, the Internet is a software interface to produce and consume TCP/IP packets. The variety of information exchanged over the Internet eventually turns into TCP/IP packets. Thus, looking at TCP/IP packets is a good place to start.
</div>
<h? class="Subsubsection">
<b><u>TCP/IP packets</u></b>
</h?>
<div class="Unindented">
Suppose a server on the Internet wants to send data to a host computer connected to it. We may think of the data to be sent as a bit stream:<div class="formula">
<i>b</i><sub>0</sub>, <i>b</i><sub>1</sub>, <i>b</i><sub>2</sub>, …
</div>
The first step is to break down the bit stream into Transmission Control Protocol (TCP) packets of the following type:<div class="formula">
<i>b</i><sub>0</sub>, <i>b</i><sub>1</sub>, …, <i>b</i><sub><i>k</i></sub> + <span class="text">source port</span> + <span class="text">destination port</span> + <span class="text">sequence number</span> + <span class="text">more TCP info</span>.
</div>
The number of bytes in a TCP packet can go up to <span class="formula">64</span> KB, but such large packets are unusual. Most packets are only <span class="formula">1.5</span> KB for reasons that will become evident presently. The source port identifies the application on the server that is sending the data, and the destination identifies the application on the client. When a TCP packet arrives, the operating system kernel looks at the destination port to route the packet to the correct application. The packets, which hold segments of the bit stream the sender is transmitting, may arrive out of order at the destination. The packets may take different routes from source to destination. The <span class="formula">32</span>-bit sequence number is used by the operating system kernel to arrange the TCP segments in order. The application at the destination sees the same bit stream transmitted by the source. 
</div>
<div class="Indented">
The packet hops from router to router as it makes its way from source to destination. The routing is determined by Internet Protocol (IP) information. The IP packets look as follows:<div class="formula">
<span class="text">TCP packet</span> + <span class="text">source address</span> + <span class="text">destination address</span> + <span class="text">more IP info</span>.
</div>
The routers do not look at the data inside the TCP portion of TCP/IP packets. Their job is to look at the destination address and send the packet to either the destination or some other router. If the server is <tt>www.xsede.org</tt>, for example, the text string <tt>www.xsede.org</tt> corresponds to an IP address. The client computer also has an IP address. Both these IP addresses show up in the IP header. 
</div>
<div class="Indented">
For the most part, the Internet may be identified with the TCP/IP protocol. The TCP/IP protocol is a software standard. It makes no assumption about the hardware transmitting packets between computers. When an IP packet is ready to be transmitted, the operating system kernel hands it over to a device driver. The hardware-dependent information is handled by the device drivers and falls out of the purview of the TCP/IP protocol. 
</div>
<div class="Indented">
This author’s desktop is connected to the Internet via Ethernet. Ethernet is a multiple access network based on coaxial copper cables. In a multiple access network, many computers use the same cable to talk to each other (for security purposes, each computer’s connection to Ethernet is via a switch). The device driver (the device driver in use is reported by the Linux command <tt>lspci -v</tt>) turns the TCP/IP packet into an Ethernet packet.<div class="formula">
<span class="text">TCP/IP packet</span> + <span class="text">MAC address</span> + <span class="text">more Ethernet info</span>.
</div>
The Media Access Control (MAC) address identifies the local computer or router to which the packets are forwarded via Ethernet. The TCP/IP packet includes the address of the destination, which could be many thousands of miles away. The MAC address identifies a router that is in the basement of the same building as the author’s desktop. The Address Resolution Protocol (ARP)  is used to figure out the MAC address from the IP address of the destination. The MAC address is easily predicted most of the time and is stored in an ARP cache. 
</div>
<div class="Indented">
Ethernet packets are limited to <span class="formula">1, 500</span> bytes, which is the Maximum Transmission Unit (MTU) configured on the author’s desktop (the MTU is reported by the Linux command <tt>ifconfig</tt>). The kernel ensures that TCP/IP packets fit into a single MTU.
</div>
<div class="Indented">
Between two routers connected via optical fiber, or for a host computer connected to the Internet via a technology other than Ethernet, the MTU may be different from <span class="formula">1, 500</span> bytes. Even an Ethernet connection may be configured to use an MTU other than <span class="formula">1, 500</span> bytes. The dependence of MTU on network hardware and the hardware independence of TCP/IP mean that fragmentation of IP packets is inevitable. If an IP packet that arrives at a router is too large for its next hop, the router must fragment the IP packet, and the packet has to be reassembled by the receiver. The frequency of IP packet fragmentation in Internet traffic is unclear. 
</div>
<h? class="Subsubsection">
<b><u>IP addresses</u></b>
</h?>
<div class="Unindented">
The IP address is crucial to the architecture of the Internet. Data leaves the source computer in the form of TCP/IP packets. The TCP/IP packets hop from router to router until they end up at the destination. During every hop, the destination address is inspected to determine the hardware link to be used to route the packet. The IP address is visible to both the application and the hardware link layer. 
</div>
<div class="Indented">
The function defined below takes a character string such as <tt>www.xsede.org</tt> as its only argument (<tt>inaddr</tt>) and returns the IP address in a form that is intelligible to various levels of the network stack.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>struct addrinfo *saddrlist(const char *inaddr){
<span class="number-left">2</span>	struct addrinfo hint;
<span class="number-left">3</span>	memset(&amp;hint, 0, sizeof(hint));
<span class="number-left">4</span>	hint.ai_family = AF_UNSPEC;
<span class="number-left">5</span>	hint.ai_socktype = SOCK_STREAM;
<span class="number-left">6</span>	struct addrinfo *llist;
<span class="number-left">7</span>	getaddrinfo(inaddr, NULL, &amp;hint, &amp;llist);
<span class="number-left">8</span>	return llist;
<span class="number-left">9</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 2, this function defines <tt>hint</tt> to be of type <tt>struct addrinfo</tt>. The function returns a pointer to <tt>struct addrinfo</tt>. The sockets API is not the best example of clean design in the UNIX world. However, <tt>struct addrinfo</tt> makes the API a little more pleasant to use. It has the following definition:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">struct addrinfo {
	int              ai_flags;
	int              ai_family;
	int              ai_socktype;
	int              ai_protocol;
	socklen_t        ai_addrlen;
	struct sockaddr *ai_addr;
	char            *ai_canonname;
	struct addrinfo *ai_next;
};
</pre>
</div>

</div>
<div class="Indented">
The last field <tt>ai_next</tt> is a pointer that links to another <tt>struct addrinfo</tt> object to form a linked list. We will explain other fields as they arise. On line 3 of <tt>saddrlist()</tt>, <tt>memset()</tt> sets all the fields of the object <tt>hint</tt> to be zero. Lines 4 and 5 give <tt>ai_family</tt> and <tt>ai_socktype</tt> as <tt>AF_UNSPEC</tt> and <tt>SOCK_STREAM</tt>, respectively. The address family can be either IPv4 or IPv6. By saying <tt>AF_UNSPEC</tt>, we are allowing it to be either. The socket type is <tt>SOCK_STREAM</tt> for TCP sockets. There are other types for datagrams and control messages. To an overwhelming extent, the Internet is powered by TCP sockets. The field <tt>ai_protocol</tt> is set to <span class="formula">0</span>, meaning that all protocols are allowed. We can set it to specific values to restrict the <tt>hint</tt> to TCP, UDP, or multicast.
</div>
<div class="Indented">
The actual call to <tt>getaddrinfo()</tt> is made on line 7 of <tt>saddrlist()</tt>. The function <tt>getaddrinfo()</tt> returns <span class="formula">0</span> on success and various error codes otherwise. Because our objective is to illustrate TCP/IP and no more, we omit all error handling. The first argument <tt>inaddr</tt> could be <tt>www.xsede.org</tt> or some other Internet address. The second argument is given as NULL, but specific values can be used to specify services such as http or ssh. The third argument is the pointer to <tt>hint</tt>. The final argument is the pointer to <tt>llist</tt>. On line 6 of <tt>saddrlist()</tt>, the variable <tt>llist</tt> is defined to be of type <tt>struct addrinfo</tt> *. On return, it is a pointer to a linked list of <tt>struct addrinfo</tt> objects, each of which holds information about a socket corresponding to the argument <tt>inaddr</tt> and confirming with the hint. 
</div>
<div class="Indented">
The function <tt>printlist()</tt> defined below walks through <tt>llist</tt> and prints some of the information contained in each node of the linked list.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void printlist(struct addrinfo *llist){
	while(llist != NULL){
		printinfo(llist);
		llist = llist-&gt;ai_next;
	}
}
</pre>
</div>

</div>
<div class="Indented">
The actual printing is done by the function <tt>printinfo()</tt>.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void printinfo(struct addrinfo *p){
<span class="number-left">2</span>	char ipver[200];
<span class="number-left">3</span>	void *addr;	
<span class="number-left">4</span>	if(p-&gt;ai_family==AF_INET){
<span class="number-left">5</span>		struct sockaddr_in *ipv4 = 
<span class="number-left">6</span>			(struct sockaddr_in *)p-&gt;ai_addr;
<span class="number-left">7</span>	addr = &amp;(ipv4-&gt;sin_addr);
<span class="number-left">8</span>	strcpy(ipver, "IPv4");
<span class="number-left">9</span>	}
<span class="number-left">10</span>	else if(p-&gt;ai_family==AF_INET6){
<span class="number-left">11</span>		struct sockaddr_in6 *ipv6 = 
<span class="number-left">12</span>			(struct sockaddr_in6 *)p-&gt;ai_addr;
<span class="number-left">13</span>		addr = &amp;(ipv6-&gt;sin6_addr);
<span class="number-left">14</span>		strcpy(ipver, "IPv6");
<span class="number-left">15</span>	}
<span class="number-left">16</span>	else
<span class="number-left">17</span>		return;
<span class="number-left">18</span>	char ipstr[INET6_ADDRSTRLEN];
<span class="number-left">19</span>	inet_ntop(p-&gt;ai_family, addr, ipstr, 
<span class="number-left">20</span>	INET6_ADDRSTRLEN);
<span class="number-left">21</span>	std::cout&lt;&lt;ipver&lt;&lt;" address : "&lt;&lt;ipstr&lt;&lt;endl;
<span class="number-left">22</span>}
</pre>
</div>

</div>
<div class="Indented">
This function is the only example of object-oriented programming  in this book, and it is in plain C (except for line 21). The <tt>struct addrinfo</tt> object <tt>*p</tt> could be an IPv4 or IPv6 address. IPv4 addresses are handled in the if-block from lines 4 through 9 and IPv6 addresses in the else-if block from lines 10 through 15. The principal difference lies in the interpretation of <tt>p-&gt;ai_addr</tt> (lines 5/6 and 11/12). In either case, a pointer <tt>addr</tt>, which points to the IP address, is extracted (lines 7 and 13). The IP address is <span class="formula">32</span> bits for IPv4 and <span class="formula">64</span> bits for IPv6. The function call <tt>inet_ntop()</tt> on line 17 converts the IP address to a more presentable format (<tt>ntop</tt> stands for network to presentation). 
</div>
<div class="Indented">
For a complete understanding of <tt>printinfo()</tt>, we will look into <tt>struct</tt> <tt>sockaddr</tt>, <tt>struct sockaddr_in</tt>, and <tt>struct sockaddr_in6</tt>. We shall only mention that the first two are exactly <span class="formula">16</span> bytes, whereas <tt>struct sockaddr_in6</tt> has two extra bytes to accommodate longer IPv6 addresses. The first <span class="formula">2</span> bytes in each of the three <tt>struct</tt>s is a <tt>short int</tt> storing the address family. This field is called <tt>sa_family</tt>, <tt>sin_family</tt>, and <tt>sin6_family</tt> in the three <tt>struct</tt>s, respectively, but has the same interpretation. 
</div>
<div class="Indented">
We define one more function to simplify printing IP addresses:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void print_ipaddr(const char* addr){
	std::cout&lt;&lt;"        IP addresses of "&lt;&lt;addr&lt;&lt;endl;
	struct addrinfo *llist;
	llist = saddrlist(addr);
	printlist(llist);
	cout&lt;&lt;endl;
	freeaddrinfo(llist);
}
</pre>
</div>

</div>
<div class="Indented">
The linked list that <tt>llist</tt> points to is allocated within <tt>getaddrinfo()</tt>, which is called inside <tt>saddrlist()</tt>. The linked list is freed using <tt>freeaddrinfo()</tt>. 
</div>
<div class="Indented">
The function call <tt>print_ipaddr(&ldquo;www.sagemath.org&rdquo;)</tt> reports the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">           IP addresses of www.sagemath.org 
IPv4 address : 23.235.44.223
</pre>
</div>

</div>
<div class="Indented">
For <tt>www.ft.com</tt>, we get 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">           IP addresses of www.ft.com 
IPv4 address : 23.67.60.67 
IPv4 address : 23.67.60.96 
</pre>
</div>

</div>
<div class="Indented">
in Michigan and 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">           IP addresses of www.ft.com 
IPv4 address : 192.124.233.18 
IPv4 address : 192.124.233.9 
</pre>
</div>

</div>
<div class="Indented">
in Texas. Many commercial sites, such as the Financial Times, use content mirroring services and locate servers at multiple points. A text string such as <tt>www.ft.com</tt> resolves into different IP addresses at different geographic locations. 
</div>
<div class="Indented">
The use of IPv6 addresses is yet uncommon in the United States, but some Internet search engines have IPv6 addresses. Here are a few examples:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">IPv6 address : 2001:559:0:4f::6011:4dd0 
IPv6 address : 2001:559:0:4f::6011:4d20 
IPv6 address : 2607:f8b0:400f:801::1013 
</pre>
</div>

</div>
<div class="Indented">
IPv4 addresses are only <span class="formula">32</span> bits and can take on <span class="formula">4</span> billion different values (although not all of these are valid addresses). There are so many points of connection to the Internet, via desktops, laptops, and mobile devices, that the address space has been expanded to <span class="formula">64</span> bits in IPv6.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.4.2">6.4.2</a> Send and receive<a class="Label" name="sub:internet-Send-and-receive"> </a>
</h3>
<div class="Unindented">
Every network interface comes down to sending and receiving data. We use the following function for sending data:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int block_send(int sockfd, void *buf, int len){
<span class="number-left">2</span>	int total_sent = 0;
<span class="number-left">3</span>	int num_sends=0;
<span class="number-left">4</span>	while(total_sent &lt; len){
<span class="number-left">5</span>		int ns = send(sockfd, buf, len-total_sent, 0);
<span class="number-left">6</span>		buf = (char *)buf+ns;
<span class="number-left">7</span>		total_sent += ns;
<span class="number-left">8</span>		num_sends += 1;
<span class="number-left">9</span>	}
<span class="number-left">10</span>	return num_sends;
<span class="number-left">11</span>}
</pre>
</div>

</div>
<div class="Indented">
This function sends <tt>len</tt> bytes out of the buffer <tt>buf[]</tt>. Where the data gets sent to is determined by the socket file descriptor <tt>sockfd</tt> (line 1). To send data in the form of TCP/IP packets, the Linux kernel needs the IP address of the destination as well as the port numbers at the source and  the destination. The sending and receiving applications cannot be identified without the port number. Although the socket file descriptor <tt>sockfd</tt> is a mere <tt>int</tt>, the Linux kernel can use it to look up the IP address of the destination as well as the port numbers at the source and the destination. The information is hidden from the application program, although there are ways to get hold of it. The manner in which the socket file descriptors are set up is described later.
</div>
<div class="Indented">
The function <tt>block_send()</tt> uses the Linux system call <tt>send()</tt> (line 5). The first argument is the socket file descriptor, the second argument is the buffer, and the third  argument is the number of bytes to be sent. The final argument is a flag, which is given as <span class="formula">0</span> on line 5. Other flag values such as <tt>MSG_MORE</tt> may be used to alter socket behavior. The <tt>send()</tt> may not transmit all the data. It returns the number of bytes transmitted and <span class="formula"> − 1</span> on error. The while-loop (lines 4 through 9) repeatedly calls <tt>send()</tt> until all the data has been sent out.
</div>
<div class="Indented">
The function for receiving is quite similar.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int block_recv(int sockfd, void *buf, int len){
<span class="number-left">2</span>	int total_recv = 0;
<span class="number-left">3</span>	int num_recv = 0;
<span class="number-left">4</span>	while(total_recv &lt; len){
<span class="number-left">5</span>		int nr = recv(sockfd, buf, len-total_recv, 0);
<span class="number-left">6</span>		if(nr==0){
<span class="number-left">7</span>			num_recv=0;
<span class="number-left">8</span>			break;
<span class="number-left">9</span>		}
<span class="number-left">10</span>		buf = (char *)buf+nr;
<span class="number-left">11</span>		total_recv += nr;
<span class="number-left">12</span>		num_recv += 1;
<span class="number-left">13</span>	}
<span class="number-left">14</span>	return num_recv;
<span class="number-left">15</span>}
</pre>
</div>

</div>
<div class="Indented">
There is only one thing to add. If <tt>recv()</tt> (line 5) returns <span class="formula">0</span>, it is a signal that the opposite party has terminated the connection. Normally, this function executes <tt>recv()</tt> in a while-loop and returns the total number of receives (line 14). However, if the connection is terminated, it sets <tt>num_recv</tt> to zero (line 7), breaks out of the loop (line 8), and returns <span class="formula">0</span>, so that the caller knows the connection has been terminated.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.4.3">6.4.3</a> Server<a class="Label" name="sub:internet-server"> </a>
</h3>
<div class="Unindented">
The server described here receives a sequence of double-precision numbers from its client and sends back their partial sums. Before we get to partial sums, we must explain how the server listens for connections on a designated port. Special port numbers are designated for major applications. For example, http uses port 80 and https uses port 443. On Linux, the list of port numbers in use can be found in the file <tt>/etc/services</tt>. The port number for our series summing server is taken to be <span class="formula">28537</span>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">const char* PORTNUM="28537";
</pre>
</div>

</div>
<div class="Indented">
To begin with, the server opens a socket to this port and marks the socket for listening.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int bind2port(const char* portnum){
<span class="number-left">2</span>	struct addrinfo hint;
<span class="number-left">3</span>	hint.ai_family = AF_UNSPEC;
<span class="number-left">4</span>	hint.ai_socktype = SOCK_STREAM;
<span class="number-left">5</span>	hint.ai_flags = AI_PASSIVE;
<span class="number-left">6</span>	struct addrinfo *llist;
<span class="number-left">7</span>	getaddrinfo(NULL, portnum, &amp;hint, &amp;llist);
<span class="number-left">8</span>	int sock2port = socket(llist-&gt;ai_family,
<span class="number-left">9</span>	                       llist-&gt;ai_socktype,
<span class="number-left">10</span>                           llist-&gt;ai_protocol);
<span class="number-left">11</span>	bind(sock2port,llist-&gt;ai_addr,llist-&gt;ai_addrlen);
<span class="number-left">12</span>	int backlog=10;
<span class="number-left">13</span>	listen(sock2port, backlog);
<span class="number-left">14</span>	int yes = 1;
<span class="number-left">15</span>	setsockopt(sock2port, SOL_SOCKET, SO_REUSEADDR, 
<span class="number-left">16</span>	           &amp;yes, sizeof(int));
<span class="number-left">17</span>	return sock2port;
<span class="number-left">18</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 5, the <tt>ai_flags</tt> field of <tt>hint</tt> is given as <tt>AI_PASSIVE</tt> to indicate that the socket will accept connections from any Internet address. The <tt>getaddrinfo()</tt> call on line 7 uses the hint to prepare another <tt>struct addrinfo</tt> object <tt>llist</tt>. The second argument to <tt>getaddrinfo()</tt> is the service. It can be a character string such as &ldquo;http&rdquo; or &ldquo;ssh&rdquo; or it can be a character string representing a port number as in &ldquo;<span class="formula">28537</span>.&rdquo; 
</div>
<div class="Indented">
Various fields of <tt>llist</tt> are accessed when opening the socket (lines 8, 9, 10). Because of the way the hint was set up, we will get a TCP socket.
</div>
<div class="Indented">
The socket is bound to a port on line 11. The type of the second argument to <tt>bind()</tt> is <tt>const struct sockaddr *</tt>. The port to which the socket is bound is obtained from the <tt>struct</tt> the second argument points to. The fact that this socket accepts connections from any Internet address is indicated within the same <tt>struct</tt>. It is possible to set up that <tt>struct</tt> explicitly and do away with the hint and <tt>getaddrinfo()</tt>, but the <tt>getaddrinfo()</tt> interface we have used is much cleaner. The <tt>getaddrinfo()</tt> interface works for both IPv4 and IPv6 addresses. Its other advantage is to hide the conversion of the port number from host to network format (most likely little endian to big endian format). 
</div>
<div class="Indented">
The system call <tt>listen()</tt> on line 13 marks the socket for listening with a backlog of <span class="formula">10</span>. The socket <tt>sock2port</tt> will keep up to <span class="formula">10</span> clients waiting for a connection but no more. The system call <tt>listen()</tt> is nonblocking. It does not actually listen. 
</div>
<div class="Indented">
Lines 15 and 16 use <tt>setsockopt()</tt>  to ensure that the port address can be safely reused immediately after the server is shut down. This function returns the socket file descriptor <tt>sock2port</tt> (line 17). 
</div>
<div class="Indented">
Many servers have firewalls that block incoming connections on all ports except a chosen few. For this program to work, the firewall must be modified to accept connections on port <span class="formula">28537</span>.
</div>
<div class="Indented">
Once the socket has been bound to a port and marked for listening, it is ready to accept connections.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">int connect2client(int sock2port){
	int sock2client;
	sock2client = accept(sock2port, NULL, NULL);
	return sock2client;
}
</pre>
</div>

</div>
<div class="Indented">
The first argument to the system call <tt>accept()</tt> is a socket file descriptor. The second argument could be a pointer to the address of another socket. Here <tt>sock2port</tt> has already been bound to a local port and marked for listening. Therefore, the second argument must be given as <tt>NULL</tt>. The third argument, which is the size of the <tt>struct</tt> the second argument points to, must also be <tt>NULL</tt>. By default, the system call <tt>accept()</tt> blocks until a connection is made. It returns a socket called <tt>sock2client</tt> here. The operating system kernel can use this socket to find out the IP address of the client as well as the port numbers at the client and server for this connection. The port number <span class="formula">28537</span> was used by the server to listen for connections. Once a connection is made, the connection gets a different port number on the server as well as the client. Port numbers are generated dynamically by the Linux kernel. If the server is multithreaded, it may accept new connections even while it is servicing clients. 
</div>
<div class="Indented">
After connecting to a client, the server runs the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void partialsum_server(int sock2client, int blocksize){
<span class="number-left">2</span>	double *recvbuf = new double[blocksize];
<span class="number-left">3</span>	double *sendbuf = new double[blocksize];
<span class="number-left">4</span>	double S=0;
<span class="number-left">5</span>	while(1){
<span class="number-left">6</span>		int nrecv=block_recv(sock2client, recvbuf, 
<span class="number-left">7</span>		                     8*blocksize);
<span class="number-left">8</span>		if(nrecv==0)
<span class="number-left">9</span>			break;
<span class="number-left">10</span>		for(int i=0; i &lt; blocksize; i++){
<span class="number-left">11</span>			S += recvbuf[i];
<span class="number-left">12</span>			sendbuf[i] = S;
<span class="number-left">13</span>		}
<span class="number-left">14</span>		block_send(sock2client, sendbuf, 8*blocksize);
<span class="number-left">15</span>	}
<span class="number-left">16</span>	close(sock2client);
<span class="number-left">17</span>	delete[] recvbuf;
<span class="number-left">18</span>	delete[] sendbuf;
<span class="number-left">19</span>}
</pre>
</div>

</div>
<div class="Indented">
This function uses the variable <tt>S</tt> defined on line 4 to keep track of partial sums. On line 6, it receives <tt>blocksize</tt> double-precision numbers in the buffer <tt>recvbuf[]</tt> (allocated on line 2). If the returned value <tt>nrecv</tt> is zero, the client has terminated the connection and the server breaks out of the while-loop on line 9. If it receives a block of numbers, it updates the partial sum and stores it in <tt>sendbuf[]</tt> (lines 11 and 12). The array of partial sums is sent back to the client (line 13). 
</div>
<div class="Indented">
The server is invoked using the following function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void server(int blocksize){
	int sock2port= bind2port(PORTNUM);
	while(1){
		int sock2client = connect2client(sock2port);
		partialsum_server(sock2client, blocksize); 
	}
}
</pre>
</div>

</div>
<div class="Indented">
This function binds to the port <tt>28537</tt> (which is the value of <tt>PORTNUM</tt>) and listens for connections. As soon as it makes a connection to a client, it calls <tt>partialsum_server()</tt>.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.4.4">6.4.4</a> Client<a class="Label" name="sub:internet-Client"> </a>
</h3>
<div class="Unindented">
The client mirrors the server in some ways but differs in others. The client begins by establishing a connection with the server.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>int connect2server(const char *server, 
<span class="number-left">2</span>		   const char *portnum){
<span class="number-left">3</span>	struct addrinfo hint;
<span class="number-left">4</span>	memset(&amp;hint, 0, sizeof(hint));
<span class="number-left">5</span>	hint.ai_family = AF_UNSPEC;
<span class="number-left">6</span>	hint.ai_socktype = SOCK_STREAM;
<span class="number-left">7</span>	struct addrinfo *llist;
<span class="number-left">8</span>	getaddrinfo(server, portnum, &amp;hint, &amp;llist);
<span class="number-left">9</span>	int sock2server = socket(llist-&gt;ai_family, 
<span class="number-left">10</span>	                         llist-&gt;ai_socktype,
<span class="number-left">11</span>	                         llist-&gt;ai_protocol);
<span class="number-left">12</span>	connect(sock2server, llist-&gt;ai_addr, 
<span class="number-left">13</span>	                     llist-&gt;ai_addrlen);
<span class="number-left">14</span>	freeaddrinfo(llist);
<span class="number-left">15</span>	return sock2server;
<span class="number-left">16</span>}
</pre>
</div>

</div>
<div class="Indented">
The argument <tt>server</tt> (line 1) is the name of machine on which the server is running. It could be <tt>login.univ.edu</tt>, for example. The hint specifies a TCP connection (line 6) but not the address family (line 5). The call to <tt>getaddrinfo()</tt> on line 7  gives the name of the server (as in the program for printing IP addresses) but also specifies a port number. The specified port number is the second argument to this function. It should be the character string &ldquo;28537&rdquo; to agree with our earlier convention. It opens a TCP socket on lines 9, 10, and 11. The socket <tt>sock2server</tt> is connected to the server on lines 12 and 13 using the system call <tt>connect()</tt>. There is much subtlety in the way connections are established and terminated over the Internet that is not discussed here. 
</div>
<div class="Indented">
The system call <tt>connect()</tt> is the client-side counterpart of <tt>accept()</tt>. The second argument to <tt>connect()</tt> is of type <tt>const struct sockaddr *</tt>. The <tt>struct</tt> it points to (<tt>*llist</tt>) has information about the IP address of the server as well as the port number on which it is expected to be listening. That <tt>struct</tt> of course is constructed by <tt>getaddrinfo()</tt> (line 8). In general, <tt>getaddrinfo()</tt> returns a linked list of <tt>struct</tt>s. In principle, for robustness, we must loop through entries of that linked list until a connection attempt succeeds with one of them. For reasons given earlier, no attempt is made to detect and handle error conditions.
</div>
<div class="Indented">
To find the partial sums of a series, the client uses the following program:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void partialsum_client(int sock2server, 
		       double *series, int n,
		       int blocksize,
		       double *psum,
		       StatVector&amp; stat_ts,
		       StatVector&amp; stat_tr){
	assert(n%blocksize==0);
	int count = n/blocksize;
	double cycles;
	TimeStamp clk;
	for(int i=0; i &lt; count; i++){
		clk.tic();
		int nsend=block_send(sock2server, series, 
		                     8*blocksize);
		cycles = clk.toc();
		stat_ts.insert(cycles);
		clk.tic();
		int nrecv=block_recv(sock2server, psum, 
								8*blocksize);
		cycles = clk.toc();
		stat_tr.insert(cycles);
		series += blocksize;
		psum += blocksize;
	}
	close(sock2server);
}
</pre>
</div>

</div>
<div class="Indented">
There is not much new to say here. The client alternately calls <tt>block_send()</tt> to send a block of <tt>series[]</tt> and <tt>block_recv()</tt> to store the partial sums in <tt>psum[]</tt>. The calls to <tt>block_send()</tt> and <tt>block_recv()</tt> are timed. Statistics for the number of cycles consumed during send and receive are gathered using <tt>StatVector</tt> objects.
</div>
<div class="Indented">
The listing of the client program is completed by the following function definition:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void client(const char* server, int blocksize){
	int n;
	if(blocksize &lt; 1000)
		n = 1000*blocksize*10;
	else
		n=1000*1000*20;
	n = (n/blocksize)*blocksize;
	int sock2server = connect2server(server, PORTNUM);
	double *series= new double[n];
	for(int i=0; i &lt; n; i++)
		series[i]=(i%2==0)?4.0/(2*i+1):-4.0/(2*i+1);
	double *psum=new double[n];
	int count = n/blocksize;
	StatVector stat_ts(count), stat_tr(count);
	TimeStamp clk;
	clk.tic();
	partialsum_client(sock2server, series, n, 
                blocksize, psum, stat_ts, stat_tr);
	double cycles = clk.toc();
	/* output code omitted */
	std::cout&lt;&lt;"bandwidth = "
	         &lt;&lt;16.0*n/cycles*1e9*CPUGHZ/1e3
	         &lt;&lt;" kbps"&lt;&lt;endl;
	delete[] series;
	delete[] psum;
}
</pre>
</div>

</div>
<div class="Indented">
The first argument is the name of the server.<span class="FootOuter"><span class="SupFootMarker"> [115] </span><span class="HoverFoot"><span class="SupFootMarker"> [115] </span>To run this program, one needs to know the name of the computer that is running the server either in the form <tt>www.somewhere.org</tt> or as an IP address. There are many web services you can use to find out the IP address of your computer. </span></span> The second argument to <tt>client()</tt> is <tt>blocksize</tt>. This argument must have the same value it does on <tt>server()</tt> for this program to run correctly. Notice that the number of terms in the series is always an integer multiple of the block size. The array <tt>series[]</tt> is initialized with the Leibniz series. 
</div>
<div class="Indented">
Most of the code for producing output is omitted. But we do show the bandwidth calculation, which includes bytes sent as well as bytes received. The constant <tt>CPUGHZ</tt> must be defined equal to the CPU clock speed in GHz. 
</div>
<div class="Indented">
The bandwidth calculated here is not the bidirectional bandwidth. The client first sends and then receives and resumes sending only after the receive is complete. To compute the bidirectional bandwidth, both the server and client should use separate threads to send and receive. The data transfers should be streamed and should not block. One advantage of the program in its current form is that it may be used to find the network latency, which is our next topic.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.4.5">6.4.5</a> Internet latency<a class="Label" name="sub:internet-latency"> </a>
</h3>
<div class="Unindented">
To find the network latency, we take the block size to be <span class="formula">100</span> double-precision numbers or <span class="formula">800</span> bytes. Such a block fits comfortably within a single TCP/IP packet smaller than <span class="formula">1.5</span> KB (Ethernet MTU). Thus, the client in effect sends a single TCP/IP packet and waits for the partial sums, which arrive in a single TCP/IP packet.
</div>
<div class="Indented">
All measurements were performed by running the server at the Texas Advanced Computing Center (TACC) and the client at the University of Michigan (UM). The median time for <span class="formula"><span class="text">send</span> + <span class="text">recv</span></span> was close to <span class="formula">55</span> milliseconds in several trials. Therefore, we may take the round trip latency between UM and TACC to be <span class="formula">55</span> milliseconds.
</div>
<div class="Indented">
Understanding why the latency is <span class="formula">55</span> milliseconds is a fascinating problem. The distance from UM to TACC is more than 1,000 miles. Therefore, the velocity of light by itself seems to account for more than <span class="formula">10</span> milliseconds of the latency.
</div>
<div class="Indented">
In fact, things are much more complicated. The Linux command <tt>traceroute</tt>  shows that the packets take a different route from UM to TACC and the way back.<span class="FootOuter"><span class="SupFootMarker"> [116] </span><span class="HoverFoot"><span class="SupFootMarker"> [116] </span>Thanks to Barmar (email expert) for a July 2005 post on a Comcast forum explaining a few of the intricacies of <tt>traceroute</tt>.</span></span> A packet sent from UM to TACC travels from the author’s desktop to a router located <span class="formula">2.3</span> miles almost immediately south (after several hops). The router <span class="formula">2.3</span> miles south of the author’s desktop is managed by Merit Networks. From that router, it jumps to a router in Lubbock, Texas, covering most of the distance from UM to TACC in a single hop. The router in Lubbock, Texas, is managed by LEARN. The packet travels from Lubbock, Texas, to a TACC router in several quick hops. Much of the latency in the journey from UM to TACC seems to be incurred at routers located <span class="formula">2.3</span> miles south of the author’s UM desktop and at the router in Lubbock, Texas. 
</div>
<div class="Indented">
On the way back, from TACC to UM, the packets take an entirely different route. The TACC servers send the packet to a router in suburban Los Angeles. The routers in Los Angeles are managed by National LambdaRail. After several quick hops in Los Angeles, the packet makes a long trip to a router located <span class="formula">2.3</span> miles south of the author’s desktop at UM. Although the postal address is the same, the IP address of this router is not the same as the one that sent packets to Lubbock, Texas. After several quick hops, the packet lands on the author’s desktop. Much of the latency during the return trip (excluding latency due to finite speed of light in optical fibers) seems to be incurred at a router in Los Angeles and another router <span class="formula">2.3</span> miles south of the author’s desktop. It is difficult to be certain without access to the routers.
</div>
<div class="Indented">
For a lot of Internet traffic, the return route is not the same as the forward route. If a company that owns a router recognizes that a packet is destined for routers owned by some other company, it gets rid of the packet as quickly as possible. 
</div>
<div class="Indented">
The long round trip from UM to TACC to Los Angeles back to UM is almost <span class="formula">4, 000</span> miles. Velocity of light might account for almost <span class="formula">20</span> milliseconds of the observed latency. That still leaves around <span class="formula">35</span> milliseconds of latency for us to explain. 
</div>
<div class="Indented">
The routers buffer a lot of IP packets using linked lists. One may imagine a router that picks up a packet and almost immediately routes it to the appropriate network interface. If routing were to behave in that manner, very little latency would accumulate at the routers. However, it is impossible for a router that executes flow control and congestion control algorithms---which are vital for the survival of the Internet---to route packets immediately. The router has to buffer packets.
</div>
<div class="Indented">
It is helpful to consider a simpler situation than Internet routing to get some intuition for why buffering is essential. Suppose there is a pipeline, let us say a processor pipeline, with two stages labeled P1 and P2. If P1 and P2 take exactly <span class="formula">10</span> cycles for every operation, there is no need for a buffer between the two pipeline stages. The stages P1 and P2 can be synced so that P1 passes its result to P2 as soon as it is done. Suppose, however, that the average time for an operation in either stage is <span class="formula">10</span> cycles, but there is a variance of <span class="formula">5</span> cycles. In such a situation, buffering is essential to keep the pipeline flowing. If P1 performs an operation that takes only <span class="formula">2</span> cycles while P2 is working on an operation that takes <span class="formula">15</span> cycles, the result from P1 must be buffered. As the number of stages and  variance in the number of cycles consumed by a pipeline stage increase, there is a corresponding need to increase the size of the buffers.
</div>
<div class="Indented">
Internet routers may be thought of as stages in pipelines between more than a billion Internet hosts that act as sources or destinations. The hosts operate at hugely varying rates. The loads on the routers can be utterly unpredictable. The Internet would collapse if the routers did not buffer. Flow control and congestion control are impossible without buffering.
</div>
<div class="Indented">
Our surmise is that <span class="formula">35</span> milliseconds of latency that accrues on routers located <span class="formula">2.3</span> miles south of the author’s desktop and on routers in Lubbock, Texas, and Los Angeles are mainly due to buffering. The IP packets at a router are maintained in a linked list. Traversing a single link in a linked list incurs overhead that is determined by the latency to DRAM memory. Although the routers may use sophisticated hashing and other data structures to minimize the number of links traversed, exposure to latency to DRAM cannot be avoided in the dynamic environment in which the routers exist. In addition, the routers likely run multiple threads on multiple processor cores. If so, spinlocks, which must be used when shared data structures are accessed from multiple threads, must be another significant source of overhead. Our surmise is that the big routing stations between UM, TACC, and back buffer at least <span class="formula">10<sup>5</sup></span> IP packets. It would not be surprising if the buffers were <span class="formula">100</span> times that estimate. 
</div>
<div class="Indented">
Internet latencies are worse than Infiniband latencies by more than a factor of <span class="formula">10<sup>4</sup></span>. Of course, a big reason is geographical separation, and nothing much can be done about it. However, it may be possible to improve the latencies that accrue at the routers. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-6.4.6">6.4.6</a> Internet bandwidth<a class="Label" name="sub:internet-bandwidth"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:mpi-internet-bw-1"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="right" valign="top">
Block Size
</td>
<td align="center" valign="top">
1 May (Noon)
</td>
<td align="center" valign="top">
3 May (2 pm)
</td>
<td align="center" valign="top">
3 May (3 pm)
</td>

</tr>
<tr>
<td align="right" valign="top">
<span class="formula">80</span> KB
</td>
<td align="center" valign="top">
<span class="formula">0.8</span> MB/s
</td>
<td align="center" valign="top">
<span class="formula">1.6</span> MB/s
</td>
<td align="center" valign="top">
<span class="formula">0.8</span> MB/s
</td>

</tr>
<tr>
<td align="right" valign="top">
<span class="formula">800</span> KB
</td>
<td align="center" valign="top">
<span class="formula">0.9</span> MB/s
</td>
<td align="center" valign="top">
<span class="formula">0.9</span> MB/s
</td>
<td align="center" valign="top">
<span class="formula">0.9</span> MB/s
</td>

</tr>
<tr>
<td align="right" valign="top">
<span class="formula">8000</span> KB
</td>
<td align="center" valign="top">
*
</td>
<td align="center" valign="top">
<span class="formula">1.9</span> MB/s
</td>
<td align="center" valign="top">
<span class="formula">2.5</span> MB/s
</td>

</tr>

</table>

</div>
<div class="caption">
Table 6.13 Internet bandwidth between UM and TACC in 2014. 
</div>

</div>

</div>
Table <a class="Reference" href="#tab:mpi-internet-bw-1">6.13↑</a> shows the bandwidth recorded between UM and TACC on three different occasions. In each of the trials reported, <span class="formula">160</span> MB of data was sent from the client to the server. The server sent back <span class="formula">160</span> MB of data. The client and server traded data in varying block sizes reported in the table. The bandwidths vary much more from trial to trial than latencies. 
</div>
<div class="Indented">
Host computers can inject packets into the network at great speed. If an application issues a <tt>send()</tt> system call, the data to be sent must fall through the TCP layer, the IP layer, the ARP layer, and the device driver before it reaches the network. The Linux TCP/IP stack is heavily optimized. One major optimization in Linux as well as many other implementations is to avoid repeated copies. In fact, the data is copied just once from the application’s user space to kernel buffers. The kernel buffers for TCP/IP packets are of the type <tt>struct sk_buff</tt>. This <tt>struct</tt> is organized in such a way that the TCP header, IP header, and Ethernet header can be tacked on without copying the data. 
</div>
<div class="Indented">
The realized bandwidth is determined to a great extent by TCP’s congestion control algorithm. TCP implements both flow control and congestion control. In flow control, the sender keeps track of the available room in the receiver’s buffer. The sender slows down if there is too little room in the receiver’s buffer. The sender continually adjusts its speed to avoid overwhelming (or starving) the receiver with packets.
</div>
<div class="Indented">
The basic flow of packets in TCP is from the sender to the receiver. The receiver may have to reorder the packets, and some packets may be dropped by the routers. Just as in Infiniband, the receiver sends an ACK when it receives a packet. The sender has to retransmit packets if they have not been acknowledged for a long while. The ACK packets the sender receives have a field in which the receiver advertises the window available in its buffers. The ACK packets may carry data if the receiver has executed a <tt>send()</tt> system call after receiving. The receiver’s advertised window is the basis for flow control.
</div>
<div class="Indented">
For data transfer between UM and TACC, congestion control has a much greater bearing on realized bandwidth than flow control. In this setting, both endpoints can inject and remove packets quickly from the network, but the Internet would strain and buckle if all the hosts inject packets at a rapid rate. In TCP, senders keep track of dropped packets and maintain a congestion window. The size of the congestion window is abruptly reduced when packets are lost and gradually increased when packets are not getting lost. The sender tries to ensure that the number of packets in flight, which is the number sent but not yet acknowledged, is within the congestion window.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-internet-congestion-1"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/cgwin_z220_tacc.png" alt="figure FIGS/chapter5/cgwin_z220_tacc.png" style="width: 365px; max-width: 609px; height: 275px; max-height: 459px;"/>
<div class="caption">
Figure 6.12 Congestion window as a function of time in three separate connections between University of Michigan and Texas Advanced Computing Center. 
</div>

</div>

</div>

</div>
Figure <a class="Reference" href="#fig:mpi-internet-congestion-1">6.12↑</a> shows the variation of the congestion window with time in three trials. In each of the three trials, the client at UM sent <span class="formula">10</span> MB of data to the server at TACC and received <span class="formula">10</span> MB in return. Data was exchanged in <span class="formula">80</span> KB blocks. The congestion windows displayed in the figure were measured at UM. 
</div>
<div class="Indented">
Evidently, there is excellent correlation between the average congestion window and realized bandwidth. In the three trials, average congestion window sizes of <span class="formula">17</span>, <span class="formula">40</span>, and <span class="formula">65</span> are correlated with bandwidths of <span class="formula">317</span> KB/s, <span class="formula">500</span> KB/s, and <span class="formula">1050</span> KB/s, respectively. 
</div>
<div class="Indented">
The congestion window is extracted as follows:<span class="FootOuter"><span class="SupFootMarker"> [117] </span><span class="HoverFoot"><span class="SupFootMarker"> [117] </span>The extraction of the congestion window is described by René Pfeiffer (Measuring TCP congestion windows, <i>Linux Gazette</i>, March 2007 (#136)). See <a class="FlexURL" href="http://linuxgazette.net/136/pfeiffer.html">http://linuxgazette.net/136/pfeiffer.html</a>.) </span></span>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">struct tcp_info info;
int tisize = sizeof(struct tcp_info);
getsockopt(sockfd, SOL_TCP, TCP_INFO, 
	       &amp;info, (socklen_t *)&amp;tisize);
int cgwin = info.tcpi_snd_cwnd;
</pre>
</div>

</div>
<div class="Indented">
Here <tt>info</tt> is a <tt>struct</tt> of size <tt>tisize</tt> bytes. The crucial syntax here is the call to <tt>getsockopt()</tt>. Its first argument is a socket. It could be the socket used by the server to talk to the client or the socket used by the client to talk to the server. The second and third arguments <tt>SOL_TCP</tt> and <tt>TCP_INFO</tt> ask the function to fill <tt>info</tt> with the right kind of information. When <tt>getsockopt()</tt> returns, <tt>info</tt> has a variety of information about packets lost, packets acknowledged, packets retransmitted, and so on. In the code fragment, the congestion window is extracted and stored in <tt>cgwin</tt>.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:mpi-internet-congestion-2"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter5/cgwin_z220_abacus.png" alt="figure FIGS/chapter5/cgwin_z220_abacus.png" style="width: 365px; max-width: 609px; height: 275px; max-height: 459px;"/>
<div class="caption">
Figure 6.13 Congestion window measured between the author’s third floor office at the University of Michigan and a computer in the basement of the same building. The recorded bandwidth reached more than 100 MB/s. The round-trip latency appeared to be <span class="formula">0.23</span> milliseconds. 
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
The limit on the congestion window implies a connection between Internet latency and bandwidth. Figure <a class="Reference" href="#fig:mpi-internet-congestion-2">6.13↑</a> shows that the congestion window in a connection between two computers nearby. The congestion window ramps up quickly and then stays constant around <span class="formula">85</span>. The upper limit on the congestion window appears to be about <span class="formula">85</span>. Because the congestion window is the number of packets in flight, we have <div class="formula">
<span class="text">bandwidth</span> ≤ <span class="text">Max congestion window</span> × <span class="text">MTU</span> ⁄ (<span class="text">half of round-trip time</span>).
</div>
For the connection between nearby computers in figure <a class="Reference" href="#fig:mpi-internet-congestion-2">6.13↑</a>, the MTU is approximately <span class="formula">1.5</span> KB, and the round-trip time is <span class="formula">0.23</span> milliseconds. Therefore, <span class="formula">110</span> MB/s is an upper bound on the bandwidth. The network is operating at close to its advertised peak of <span class="formula">1</span> Gigabit per second. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Implement a multithreaded version of the partial sum server using Pthreads. Verify that your server is capable of servicing multiple clients simultaneously.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a TCP/IP program to synchronize with a central database. The central database is assumed to have a number of data items. For simplicity, you may forbid deletion of data items. Each client program will upload new data items from the client to update the central database and will then download the entire database, including items pushed by other clients.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a decentralized TCP/IP program that has similar functionality to the program in the previous exercise. To decentralize, each client will have to run a server that accepts requests from other clients that query its database.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-6.5">6.5</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-49"><span class="bib-index">49</span></a>] </span> <span class="bib-authors">A.S. Tanenbaum, D.J. Wetherall</span>: <i><span class="bib-title">Computer Networks</span></i>. <span class="bib-publisher">Prentice Hall</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-50"><span class="bib-index">50</span></a>] </span> <span class="bib-authors">B.J. Hall</span>: <i><span class="bib-title">Beej's Guide to Network Programming Using Internet Sockets</span></i>. <span class="bib-publisher">Lulu Marketplace</span>, <span class="bib-year">2009</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-51"><span class="bib-index">51</span></a>] </span> <span class="bib-authors">C.E. Leiserson</span>: “<span class="bib-title">Fat-trees: Universal networks for hardware-efficient supercomputing</span>”, <i><span class="bib-journal">IEEE Trans. on Computers</span></i>, pp. <span class="bib-pages">892-901</span>, <span class="bib-year">1985</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-52"><span class="bib-index">52</span></a>] </span> <span class="bib-authors">J. Liu, J. Wu, S.P. Kini, P. Wyckoff, D.K. Panda</span>: “<span class="bib-title">High performance RDMA-based MPI implementation over InfiniBand</span>”, <i><span class="bib-journal">International Journal of Parallel Programming</span></i>, pp. <span class="bib-pages">167-198</span>, <span class="bib-year">2004</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-53"><span class="bib-index">53</span></a>] </span> <span class="bib-authors">K. Kandalla, U. Yang, J. Keasler, T. Kolev, A. Moody, H. Subramoni, K. Tomko, J. Vienne, B.R. de Supinski, D.K. Panda</span>: “<span class="bib-title">Designing non-blocking Allreduce with collective offload on Infiniband clusters: a case study with conjugate gradient solvers</span>”, <i><span class="bib-journal">IEEE 26th International Parallel and Distributed Processing Symposium</span></i>, pp. <span class="bib-pages">1156-1167</span>, <span class="bib-year">2012</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-54"><span class="bib-index">54</span></a>] </span> <span class="bib-authors">L.L. Peterson, B. Davie</span>: <i><span class="bib-title">Computer Networks: A Systems Approach</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-55"><span class="bib-index">55</span></a>] </span> <span class="bib-authors">M. Snir, S. Otto, S. Huss-Lederman, D. Walker, J. Dongarra</span>: <i><span class="bib-title">MPI—-The Complete Reference: The MPI Core</span></i>. <span class="bib-publisher">MIT Press</span>, <span class="bib-year">1998</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-56"><span class="bib-index">56</span></a>] </span> <span class="bib-authors">T.S. Woodall, G.M. Shipman, G. Bosilca, A.B. Maccabe</span>: <i><span class="bib-title">Recent Advances in Parallel Virtual Machine and Message Passing Interface</span></i> in <i><span class="bib-booktitle">Lecture Notes in Computer Science</span></i>. <span class="bib-publisher">Springer</span>, <span class="bib-year">2006</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-57"><span class="bib-index">57</span></a>] </span> <span class="bib-authors">W. Gropp, S. Huns-Lederman, A. Lumsdaine, E. Lusk, B. Nitzberg, W. Saphir, M. Snir</span>: <i><span class="bib-title">MPI—-The Complete Reference The MPI Extensions</span></i>. <span class="bib-publisher">MIT Press</span>, <span class="bib-year">1998</span>.
</p>

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-7">7</a> Special Topic: The Xeon Phi Coprocessor<a class="Label" name="chap:The-Xeon-Phi"> </a>
</h1>
<div class="Unindented">
The Top 500 organization<span class="FootOuter"><span class="SupFootMarker"> [118] </span><span class="HoverFoot"><span class="SupFootMarker"> [118] </span>See <a class="FlexURL" href="www.top500.org">www.top500.org</a>.</span></span> has published a list of the top supercomputers every year since 1993. This list has been a huge contribution to scientific computing. The  considerable profits in computer technology lie in entertainment, everyday convenience, creating new enthusiasms, reinforcing preexisting ones, and commerce. The profits from scientific computing are almost negligible. The wide publicity that Top 500 generates has ensured that scientific computing is not ignored by major corporations.
</div>
<div class="Indented">
The Top 500 list ranks computers on the basis of flop (floating point operation) rate realized in solving large linear systems using pivoted LU factorization (the LINPACK benchmark). The computation must be in double-precision. The flop rate has increased from <span class="formula">60</span> GFlops/s (<span class="formula">6 × 10<sup>10</sup></span> flops/s) in 1993 to <span class="formula">34</span> PFlops/s (<span class="formula">3.4 × 10<sup>16</sup></span>) in 2015. A total of <span class="formula">500</span> computers are listed in the rankings.
</div>
<div class="Indented">
The relationship between scientific computing and mainstream computing is symbiotic, although the latter is dominant. Many technologies pertaining to networks, visualization, and other topics have traveled from mainstream to scientific computing. Technologies such as wide register sets were first exploited in scientific computing, helping large computing clusters climb up the Top 500 list. Later these technologies have found applications in more mainstream areas such as image processing and data analysis.
</div>
<div class="Indented">
Co-processors with peak ratings in excess of a tera-flop were first introduced to market by NVIDIA. Such coprocessors are particularly efficacious for dense matrix computations and, in particular, for the LINPACK benchmark used to rate the TOP 500 supercomputers. In 2015, as in some years before, the biggest supercomputers have no choice but to use these coprocessors to hold their place in the TOP 500 list. Because the TOP 500 list is widely publicized, so widely that the topmost positions are a source of national pride, the coprocessors have impressive marketing potential. Intel could not afford to ignore this marketing angle.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:xphi-intro-sketch"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/xeonphi.png" alt="figure FIGS/chapter6/xeonphi.png" style="max-width: 246px; max-height: 116px;"/>
<div class="caption">
Figure 7.1 In this sketch, the Xeon Phi coprocessor is labeled MIC (Many Integrated Cores). The MIC and the Infiniband HCA (host channel adapter) are connected to the chipset on PCIe bus.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
The Intel Many Integrated Cores (MIC) or Xeon Phi coprocessor supplements the processor nodes and increases floating point throughput. Figure <a class="Reference" href="#fig:xphi-intro-sketch">7.1↑</a> shows a typical setup.<span class="FootOuter"><span class="SupFootMarker"> [119] </span><span class="HoverFoot"><span class="SupFootMarker"> [119] </span>Knight’s Landing, the successor of Knight’s Corner microarchitecture, moves the Xeon Phi away from the coprocessor model. Section <a class="Reference" href="#sec:phi-offload">7.2↓</a>, the middle third of this chapter, will cease to be relevant. The rest of the chapter will still apply with minor changes.</span></span> The peak floating point rating of the coprocessor may be three times that of the processor node, going beyond <span class="formula">1</span> TFlop/s.<span class="FootOuter"><span class="SupFootMarker"> [120] </span><span class="HoverFoot"><span class="SupFootMarker"> [120] </span>In 2015.</span></span>
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:phi-architecture">7.1↓</a> is an outline of the architecture of the Xeon Phi core. The Xeon Phi used in this chapter has <span class="formula">61</span> cores. The number of cores will increase in the future. Each core of the Xeon Phi implements the AVX-512 instruction set (see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a>), which is yet to make its way into mainline x86 computers. The AVX-512 instruction set is an enhancement of the AVX2 instruction set used in mainline x86 computers. The AVX-512 instruction set features <span class="formula">512</span>-bit ZMM registers, twice the size of YMM registers in AVX/AVX2. 
</div>
<div class="Indented">
In addition, each core of the Xeon Phi has a hardware thread picker. During each cycle, the thread picker picks one of four or fewer threads mapped to any particular core. The thread picker prefers to schedule in round-robin fashion and never picks the same thread in two successive cycles. Therefore, the natural number of threads to use (on a <span class="formula">61</span>-core Phi) in OpenMP programs is <span class="formula">4 × 61 = 244</span> and not <span class="formula">61</span>, as we explain in section <a class="Reference" href="#sec:phi-architecture">7.1↓</a>.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:phi-architecture">7.1↓</a> also explains how to run programs on the Phi in native mode. In native mode, one can log onto the Phi device like any computer and run programs on it like we do on any computer. In the beginning days, the native mode is not very stable, but when it works it can be a great convenience. Many OpenMP programs can be made to run on the Phi with no changes beyond compiling with the <tt>-mmic</tt> flag.
</div>
<div class="Indented">
Another way to run programs on the device is in the offload mode. This mode, which has been perfectly stable from the beginning, requires modifications to the C/C++ source. New syntax is used to send parts of the computations off to the Phi devices. The offload mode is the topic of section <a class="Reference" href="#sec:phi-offload">7.2↓</a>.
</div>
<div class="Indented">
Section <a class="Reference" href="#sec:phi-examples">7.3↓</a> discusses two main examples. The first example is the Fast Fourier Transfrom (FFT). For <span class="formula"><i>n</i> = 64</span>, the Phi can be twice as fast as its AVX host. However, already for <span class="formula"><i>n</i> = 8, 192</span>, the Phi is no faster than its <span class="formula">2.7</span>  GHz AVX host (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full name of this machine). For <span class="formula"><i>n</i> = 2<sup>26</sup></span>, the AVX host is eight times faster, suggesting that the MKL library is not yet fully optimized for the Phi.
</div>
<div class="Indented">
As discussed in chapter <a class="Reference" href="#chap:The-processor">3↑</a>, the most important optimizations for scientific algorithms such as the FFT are instruction pipeline optimizations. The second example in section <a class="Reference" href="#sec:phi-examples">7.3↓</a> is a discussion of instruction pipeline optimizations for matrix multiplication. We show how to write a program that gets to nearly <span class="formula">50</span>% of the peak floating point capability of a Phi device. The discussion brings out some features of the AVX-512 instruction set that are not found in the SSE2 instruction set used in chapter <a class="Reference" href="#chap:The-processor">3↑</a>. However, not all the techniques found in that chapter are repeated. The discussion only goes far enough to indicate that the application of those techniques would result in a matrix multiplication routine that reaches <span class="formula">85</span>% of peak bandwidth like Intel’s MKL library.
</div>
<div class="Indented">
The peak floating point capability or bandwidth is relevant to only a small fraction of scientific programs. Even a computationally intensive algorithm such as the FFT reaches only <span class="formula">15</span>% of the peak floating point bandwidth. The bandwidth to memory of the Phi device is less than twice that of its AVX host. Scientific programs that benefit from the coprocessor as much as matrix multiplication or the Top 500 headline numbers are few.
</div>
<div class="Indented">
New technologies such as NVIDIA’s GPUs and Intel’s Xeon Phi have gained traction partly because of the Top 500 list. Such new technologies must be welcomed. Even if they do not endure, they point to new directions and encourage innovation in hardware design. Nevertheless, it must be pointed out that the marketing hype is not perfectly concordant with practical realities.
</div>
<div class="Indented">
In earlier chapters, we have seen that the gap between capabilities of processors and the code generated by compilers is growing. That is especially true where instruction pipeline optimizations are involved. The gap can be a factor of <span class="formula">10</span> for AVX2 computers, as we saw in chapter <a class="Reference" href="#chap:The-processor">3↑</a>. The gap is even wider for the coprocessor devices.
</div>
<div class="Indented">
As this gap widens, an even greater gap has developed between the skill of a typical scientific programmer and what it takes to program modern computers efficiently. The mainline x86 computers featuring SSE2, AVX, AVX2, or AVX-512, the last of which is yet to be released to market, have a modular architecture at the hardware level. Processor cores and processor packages share memory.<span class="FootOuter"><span class="SupFootMarker"> [121] </span><span class="HoverFoot"><span class="SupFootMarker"> [121] </span>The allocation of pages in near memory through demand paging and the first-touch policy may be viewed as a violation of modularity with regard to the way memory is shared between processes.</span></span> Programs that respect that modularity may be written using OpenMP or Pthreads. The heterogeneous coprocessor model breaks this modularity.<span class="FootOuter"><span class="SupFootMarker"> [122] </span><span class="HoverFoot"><span class="SupFootMarker"> [122] </span>Knight’s Landing, the successor to Xeon Phi from Intel, has been announced to move away from the coprocessor model.</span></span> On the same computer, we have disparate and dissimilar entities competing to do the same task in a rather ad-hoc manner. Programming such heterogeneous setups is much less tractable.
</div>
<div class="Indented">
The modifications to C/C++ introduced for the Phi appear relatively straightforward at first sight. However, the lack of modularity at the hardware level makes it much harder to organize and maintain programs effectively. NVIDIA’s CUDA model requires fine-grained parallelism within programs, which is quite a bit harder to handle, as we will see in the next chapter.
</div>
<div class="Indented">
The majority, perhaps the vast majority, of programs that feature in scientific research and seminars do not go out of cache. It is unlikely that these programs are well optimized in a meaningful way. The potential offered by coprocessors, although enticing, must be viewed in light of this sobering reality. Many scientific programs are written in Matlab or Python and for good reasons. But these interpreted languages are very slow. A good C program can be more than 1,000 times faster than Matlab or Python on a single core. On multicore processor packages, the speedup from C will be even greater. Yet there is a market for GPU computing in these interpreted languages. While the marketing potential of color coating an anodyne is undeniable, the logic of this exercise is a little difficult to discern.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-7.1">7.1</a> Xeon Phi architecture <a class="Label" name="sec:phi-architecture"> </a>
</h2>
<div class="Unindented">
In this section, we give an overview of the Xeon Phi’s architecture in three steps. The first step is to calculate the peak floating point bandwidth of the Phi in section <a class="Reference" href="#sub:phi-Peak-floating-point">7.1.1↓</a> and compare it to that of its AVX host. The Phi turns out to be better by a factor of <span class="formula">3</span>. This is perhaps the most favorable comparison one can make for the coprocessor. Applications in which that factor is approached are few.
</div>
<div class="Indented">
The second step in section <a class="Reference" href="#sub:phi-hello-world">7.1.2↓</a> is to introduce the thread picker. The thread picker is a hardware module on each Phi core. During every cycle, it picks one of four threads. Section <a class="Reference" href="#sub:phi-hello-world">7.1.2↓</a> also introduces the ZMM registers in the context of the thread picker. Section <a class="Reference" href="#sub:phi-hello-world">7.1.2↓</a> explains how to compile and run a Phi program in native mode.
</div>
<div class="Indented">
Compiling and running a Phi program in native mode differs very little from what is usual on mainline computers. In fact, almost every OpenMP program of chapter <a class="Reference" href="#chap:Threads-and-shared">5↑</a> can be run on the Phi with little effort and almost no modifications. Thus, in section <a class="Reference" href="#sub:phi-memory-system">7.1.3↓</a>, we reuse the same programs to measure the Phi’s latency and bandwidth to memory.
</div>
<div class="Indented">
The Phi’s bandwidth to memory is not even twice that of its AVX host. The copy bandwidth is better by a factor of <span class="formula">1.72</span>. This improvement in bandwidth comes at the expense of a latency that is nearly four times as high, although on the positive side the Phi’s memory system is uniform with no distinction between near and far memory. Co-processors such as the Phi and NVIDIA’s GPUs are worthless for programs that rely on dynamic data structures such as linked lists because such programs are exposed to latency to memory. However, they can yield a speedup in programs whose memory accesses are regular and predictable. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.1.1">7.1.1</a> Peak floating point bandwidth<a class="Label" name="sub:phi-Peak-floating-point"> </a>
</h3>
<div class="Unindented">
Figure <a class="Reference" href="#fig:xphi-intro-sketch">7.1↑</a> shows a typical setup for a Xeon Phi coprocessor. The host computer is a <span class="formula">2.7</span>  GHz AVX machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for its full name). It has <span class="formula">16</span> cores, each equipped with <span class="formula">256</span>-bit YMM registers, wide enough for four double-precision numbers. Each core can issue a <tt>vaddpd</tt> and a <tt>vmulpd</tt> instruction simultaneously in each cycle. Therefore, the peak floating point bandwidth is: <div class="formula">
<span class="text">16 cores</span> × <span class="text">8 flops/cycle/core</span> × <span class="mbox">2.7 billion cycles/sec</span> = 345.6  <span class="text"> Gflops/sec.</span>
</div>
The host is quite powerful by itself.
</div>
<div class="Indented">
The Xeon Phi coprocessor is attached to the chipset on the PCIe bus, like the Infiniband network card of the previous chapter.<span class="FootOuter"><span class="SupFootMarker"> [123] </span><span class="HoverFoot"><span class="SupFootMarker"> [123] </span>In fact, the Xeon Phi card can talk to the Infiniband HCA directly via the chipset without processor intervention. It is unclear whether the facility is really so useful.</span></span> The coprocessor has its own memory (<span class="formula">8</span> GB on our system) and cannot access the processor node’s memory. Any data sent from the processor node to the coprocessor has to flow through the chipset. 
</div>
<div class="Indented">
The Phi used here packages <span class="formula">61</span> cores. Each core implements the fused multiply add instruction (to be discussed later) on the ZMM registers, which are wide enough for eight double-precision numbers. Thus, each core can execute <span class="formula">16</span> double-precision operations in a single cycle. The coprocessor clock is <span class="formula">1.09</span>  GHz. Its peak floating point throughput of <div class="formula">
<span class="text">61 cores</span> × <span class="text">16 flops/cycle/core</span> × <span class="text">1.09 billion cycles/sec</span> = <span class="ensuremath">1.064</span>  <span class="text"> Tflops/sec</span>
</div>
is slightly more than three times that of its AVX host. 
</div>
<div class="Indented">
This peak bandwidth is approached by dense matrix multiplication and LU factorization. However, the peak floating point bandwidth is an irrelevant theoretical number for almost every other application, and even the FFT realizes only <span class="formula">15</span>% of the peak, as we will see later. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.1.2">7.1.2</a> A simple Phi program<a class="Label" name="sub:phi-hello-world"> </a>
</h3>
<div class="Unindented">
From here onward, we use Phi device and MIC device interchangeably. MIC is the acronym of Many Integrated Cores and includes the microarchitecture of Phi-like devices. We begin by explaining why the right number of threads on a MIC/Phi device is typically four times the number of cores.
</div>
<h? class="Subsubsection">
<b><u>Thread pickers and ZMM registers</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:xphi-arch-prelim"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/phiarch.png" alt="figure FIGS/chapter6/phiarch.png" style="max-width: 261px; max-height: 104px;"/>
<div class="caption">
Figure 7.2 A preliminary view of the architecture of a Xeon Phi core.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Processor cores of mainline computers run one thread at a time.<span class="FootOuter"><span class="SupFootMarker"> [124] </span><span class="HoverFoot"><span class="SupFootMarker"> [124] </span>We are assuming that hyperthreading is turned off.</span></span> The operating system must be invoked to switch from one thread to another at a cost of around <span class="formula">10<sup>4</sup></span> cycles. 
</div>
<div class="Indented">
In contrast, four threads can be scheduled to a single Phi core.<span class="FootOuter"><span class="SupFootMarker"> [125] </span><span class="HoverFoot"><span class="SupFootMarker"> [125] </span>Much of the information here is from <i>Intel Xeon Phi Co-processor System Software Developer’s Guide<span class="default"></span></i>, June 2013.</span></span> All four cores can remain active, and the thread picker, which is a hardware module, switches between them in a single cycle (see figure <a class="Reference" href="#fig:xphi-arch-prelim">7.2↑</a>). During every cycle, one of the four threads is picked to run. The thread picker prefers round-robin scheduling, where it cycles between the four threads, and it never picks the same thread in two successive cycles. Therefore, one must create at least two threads for every core to prevent alternate cycles from being wasted. Much of the time it is best to create four threads for every core. On our Phi card, with <span class="formula">61</span> cores, the ideal number of threads is <span class="formula">244</span>.
</div>
<div class="Indented">
Each Phi core has <span class="formula">32</span> ZMM registers (see figure <a class="Reference" href="#fig:xphi-arch-prelim">7.2↑</a>). In fact, the number of registers in hardware is really <span class="formula">32 × 4</span>. Each of the four threads scheduled to a Phi core has its own register file so that the hardware can switch between threads in a single cycle. This replication of registers is another reason to set the number of threads to be four times the number of cores.
</div>
<div class="Indented">
In a thread or program, the registers may be referred to as ZMM0 through ZMM31. The Phi does not support XMM or YMM registers. The ZMM registers must be used for floating point operations. Each ZMM register may be thought of as a vector of eight double-precision numbers.
</div>
<h? class="Subsubsection">
<b><u>Hello world</u></b>
</h?>
<div class="Unindented">
Our first Phi program is no different from any OpenMP program.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;stdio.h&gt;
#include &lt;omp.h&gt;
​
int main(){
#pragma omp parallel
	printf("hello from thread: %d of %d\n", 
	    omp_get_thread_num(), omp_get_num_threads());
}
</pre>
</div>

</div>
<div class="Indented">
The parallel region here omits the <tt>num_threads</tt> clause. Instead, the environment variable 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">OMP_NUM_THREADS
</pre>
</div>

</div>
<div class="Indented">
is set to determine the number of threads in the parallel region. On a MIC device, it may be set as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">export OMP_NUM_THREADS=244
</pre>
</div>

</div>
<div class="Indented">
if the MIC device has <span class="formula">61</span> cores and as
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">export OMP_NUM_THREADS=16
</pre>
</div>

</div>
<div class="Indented">
on its host if the host has <span class="formula">16</span> cores. 
</div>
<div class="Indented">
The program is compiled and linked as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">icpc -mmic  -openmp -c hello.cpp
icpc -mmic  -openmp -o hello.exe hello.o    
</pre>
</div>

</div>
<div class="Indented">
To keep the compilation syntax simple, we have omitted all but the most essential flags. Flags such as <tt>-restrict</tt> and <tt>-O3</tt> for optimization must certainly be used almost all the time. The <tt>-mkl</tt> flag must be used if the MKL library is being invoked. The new flag here is <tt>-mmic</tt>, which makes <tt>icpc</tt> generate an executable that runs on the MIC/Phi device.
</div>
<div class="Indented">
The compilation is done on the host. Although the MIC device is powerful for certain scientific applications, it is not powerful enough to run a compiler. To run the program on a MIC device, one must first connect to it as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">ssh mic0
</pre>
</div>

</div>
<div class="Indented">
If the host file system is not mounted on the MIC device, the executable must be copied to it from the host as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">scp hello.exe mic0:hello.exe
</pre>
</div>

</div>
<div class="Indented">
After sshing to the MIC device, the environment variable <tt>OMP_NUM_THREADS</tt> must be set correctly. Another crucial environment variable is 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">LD_LIBRARY_PATH
</pre>
</div>

</div>
<div class="Indented">
It must be set to include the path to all the runtime libraries that may be needed.
</div>
<div class="Indented">
To receive <span class="formula">244</span> greetings, we may now say <tt>hello.exe</tt>. The same program may be compiled and run on the host. The only change would be to drop the <tt>-mmic</tt> flag.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.1.3">7.1.3</a> Xeon Phi memory system<a class="Label" name="sub:phi-memory-system"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:xphi-ring"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/xphi_ring.png" alt="figure FIGS/chapter6/xphi_ring.png" style="max-width: 265px; max-height: 97px;"/>

</div>
<div class="caption">
Figure 7.3 Phi cores and memory controllers connected using a bidirectional ring. The memory controllers are labeled GBOX.
</div>

</div>

</div>

</div>
<div class="Indented">
The Phi has its own memory, which is accessed through a ring (see figure <a class="Reference" href="#fig:xphi-ring">7.3↑</a>). Eight memory controllers are connected to the same ring. Each memory controller drives two <span class="formula">32</span>-bit (or <span class="formula">4</span>-byte) channels of GDDR5 memory. There are <span class="formula">16</span> channels in total. The Phi supports <span class="formula">4</span> KB as well as <span class="formula">2</span> MB pages. The timing in this chapter was done with 2 MB paging enabled. Latency and bandwidth to memory are measured by making slight changes to earlier programs. 
</div>
<h? class="Subsubsection">
<b><u>Latency to memory</u></b>
</h?>
<div class="Unindented">
Our strategy for measuring latency was as follows. Suppose a measurement is carried out with <span class="formula"><i>n</i></span> pages, each of <span class="formula">4</span> KB. Because a cache line is <span class="formula">64</span> bytes or <span class="formula">512</span> bits, the number of cache lines in <span class="formula"><i>n</i></span> pages is <span class="formula">64<i>n</i></span>. These <span class="formula">64<i>n</i></span> cache lines are first flushed from cache and then accessed in random order. Latency is taken to be equal to the average time of access.
</div>
<div class="Indented">
To measure latency to near and far memory, we used a program with two threads. The <span class="formula"><i>n</i></span> pages used to measure latency were allocated close to one thread and far away from the other.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:xphi-latency"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
AVX (near)
</td>
<td align="center" valign="top">
AVX (far)
</td>
<td align="center" valign="top">
MIC (near)
</td>
<td align="center" valign="top">
MIC (far)
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">4</span>
</td>
<td align="center" valign="top">
<span class="formula">28</span> ns
</td>
<td align="center" valign="top">
<span class="formula">43</span> ns
</td>
<td align="center" valign="top">
<span class="formula">241</span> ns
</td>
<td align="center" valign="top">
<span class="formula">244</span> ns
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">20</span>
</td>
<td align="center" valign="top">
<span class="formula">30</span> ns
</td>
<td align="center" valign="top">
<span class="formula">47</span> ns
</td>
<td align="center" valign="top">
<span class="formula">260</span> ns
</td>
<td align="center" valign="top">
<span class="formula">260</span> ns
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">50</span>
</td>
<td align="center" valign="top">
<span class="formula">72</span> ns
</td>
<td align="center" valign="top">
<span class="formula">135</span> ns
</td>
<td align="center" valign="top">
<span class="formula">291</span> ns
</td>
<td align="center" valign="top">
<span class="formula">291</span> ns
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">100</span>
</td>
<td align="center" valign="top">
<span class="formula">73</span> ns
</td>
<td align="center" valign="top">
<span class="formula">137</span> ns
</td>
<td align="center" valign="top">
<span class="formula">307</span> ns
</td>
<td align="center" valign="top">
<span class="formula">308</span> ns
</td>

</tr>

</table>
<div class="caption">
Table 7.1 Latency to memory in nanoseconds for a <span class="formula">2.7</span>  GHz AVX host (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for its full name) and the Phi coprocessor. The measurements were carried out using <span class="formula"><i>n</i></span> pages.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
The measured latencies for the Phi as well as its <span class="formula">2.7</span>  GHz AVX host are found in table <a class="Reference" href="#tab:xphi-latency">7.1↑</a>. The memory access on the AVX host is definitely nonuniform. The Phi cores have uniform memory access. There is no distinction between near and far memory when coprocessor cores and memory controllers are connected to the same bidirectional ring. All memory accesses go through the same ring logic. The ring is also used for enforcing cache coherence.
</div>
<h? class="Subsubsection">
<b><u>Bandwidth to memory</u></b>
</h?>
<div class="Unindented">
The programs to measure read, write, and copy bandwidths to memory are the same as in chapter <a class="Reference" href="#chap:Threads-and-shared">5↑</a> for the most part. One modification was to adjust the length of the array as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#ifndef __MIC__
	const long len = 2l*1000*1000*1000; 
#else
	const long len = 1l*1000*1000*200; 
#endif
</pre>
</div>

</div>
<div class="Indented">
The preprocessor variable <tt>__MIC__</tt> is defined when the code is targeted at a MIC device but not otherwise. Here it is used to ensure that the length of the array used to determine bandwidth to memory does not overflow on the Phi.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:xphi-bw-to-mem"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
AVX
</td>
<td align="center" valign="top">
MIC
</td>

</tr>
<tr>
<td align="center" valign="top">
Read
</td>
<td align="center" valign="top">
<span class="formula">90</span> GB/s
</td>
<td align="center" valign="top">
<span class="formula">161</span> GB/s
</td>

</tr>
<tr>
<td align="center" valign="top">
Write
</td>
<td align="center" valign="top">
<span class="formula">85</span> GB/s
</td>
<td align="center" valign="top">
<span class="formula">130</span> GB/s
</td>

</tr>
<tr>
<td align="center" valign="top">
Copy
</td>
<td align="center" valign="top">
<span class="formula">50</span> GB/s
</td>
<td align="center" valign="top">
<span class="formula">86</span> GB/s
</td>

</tr>

</table>
<div class="caption">
Table 7.2 Bandwidth to memory from <span class="formula">16</span> threads of the <span class="formula">2.7</span>  GHz AVX host (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for its full name) and <span class="formula">244</span> threads on the Phi (MIC).
</div>

</div>

</div>

</div>
Table <a class="Reference" href="#tab:xphi-bw-to-mem">7.2↑</a> reports bandwidths to memory for the AVX host and the Phi. The Phi realizes higher bandwidth by utilizing a wide interface to memory consisting of <span class="formula">16</span> channels. For estimating copying bandwidth, one byte copied is counted as two, a byte read and a byte written. The copy bandwidth appears to suffer on both the AVX host and the Xeon Phi possibly because of write-back caching. The read bandwidth on the Phi is far short of the theoretical peak.<span class="FootOuter"><span class="SupFootMarker"> [126] </span><span class="HoverFoot"><span class="SupFootMarker"> [126] </span>The theoretical peak is <span class="formula">352</span> GB/s. It may be approached by using prefetch instructions as described by E. Saule, K. Kaya, and U.V. Çatalyürek (Performance evaluation of sparse matrix multiplication kernels on Intel Xeon Phi, <i>arxiv:1302.1078v1</i>, 2013). These authors find that the Phi outperforms cutting-edge GPUs in sparse matrix multiplication.</span></span>
</div>
<h? class="Subsubsection">
<b><u>Bandwidth during matrix transpose</u></b>
</h?>
<div class="Unindented">
In many scientific programs, data is organized in regular 2D or 3D grids. An example is the problem of transposing a matrix. When data is arranged in 2D or 3D grids, it is advantageous to block data accesses. Blocking increases cache hits and reduces TLB misses. The program for transposing a matrix using <span class="formula"><i>B</i> × <i>B</i></span> blocks on a single core is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void blocktransx(double *restrict a, double *restrict b, 
		 int ldb, int m, int n){
  assrt(m%B == 0 &amp;&amp; n%B == 0);
  for(int i=0; i &lt; m; i+=B)
    for(int j=0; j &lt; n; j+=B)
      for(int ii=0; ii &lt; B; ii++)
        for(int jj=0; jj &lt; B; jj++)
          b[j+jj+(i+ii)*ldb] = a[i+ii+(j+jj)*m];
}
</pre>
</div>

</div>
<div class="Indented">
The leading dimension of the <span class="formula"><i>m</i> × <i>n</i></span> matrix stored in <tt>a[]</tt> is assumed to be <span class="formula"><i>m</i></span>, while the leading dimension of the transposed <span class="formula"><i>n</i> × <i>m</i></span> matrix in <tt>b[]</tt> is <tt>ldb</tt>, which may be <span class="formula"><i>n</i></span> or greater. 
</div>
<div class="Indented">
When all <span class="formula">16</span> cores of the <span class="formula">2.7</span>  GHz AVX host work together, bandwidth to memory realized during matrix transpose is <span class="formula">34</span> GB/s. For <span class="formula">244</span> threads on <span class="formula">61</span> cores of the Phi, the bandwidth is <span class="formula">59</span> GB/s. In either case, the bandwidth is nearly the same for block sizes <span class="formula"><i>B</i> = 25, 50, 100</span>.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain how using a hardware thread picker that picks between four threads on every core of the Phi can help with bandwidth to memory.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Estimate the Xeon Phi’s bandwidth to memory when transposing a square matrix in place.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-7.2">7.2</a> Offload<a class="Label" name="sec:phi-offload"> </a>
</h2>
<div class="Unindented">
The offload mode<span class="FootOuter"><span class="SupFootMarker"> [127] </span><span class="HoverFoot"><span class="SupFootMarker"> [127] </span>If the Xeon Phi architecture moves away from the coprocessor model, this section will cease to be relevant.</span></span> has been a stable way to program the Phi device since the beginning. The &ldquo;hello&rdquo; program has shown us that the syntax for writing Phi programs is virtually identical to the syntax for writing ordinary C/C++ programs. In offload mode, the master thread of the main program is assumed to be on the host. However, the program holds several segments that are meant to be outsourced to the Phi devices.
</div>
<div class="Indented">
In section <a class="Reference" href="#sub:phi-offload-init">7.2.1↓</a>, we write a simple function called <tt>mic_init()</tt>. Much of the time, the only thing that needs to be changed when an OpenMP program is run on a Phi/MIC device is the number of threads. Typically, the Phi has more cores, and the number of threads on the Phi is four times the number of cores. The <tt>mic_init()</tt> function sets the number of threads correctly for the host as well as the Phi/MIC device once and for all.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:phi-offload-target">7.2.2↓</a> introduces the <tt>target(mic)</tt> compiler directive. This directive, which is currently implemented only by the Intel compilers, may be applied to function definitions as well as to globally defined variables. Any function that is prefixed with <tt>target(mic)</tt> will be compiled to run on Phi/MIC devices as well as the host. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:phi-offload-leibniz">7.2.3↓</a> uses the familiar Leibniz example to show how the Phi/MIC device can communicate with the host. The <tt>offload</tt> compiler directive is supported by clauses such as <tt>in</tt>, <tt>out</tt>, <tt>nocopy</tt>, and <tt>inout</tt> to enable communication between the coprocessor and the host. Variables are automatically copied into the Phi when an <tt>offload</tt> region is entered and are copied back when the region exits. Arrays are sent back and forth using clauses appended to the <tt>offload</tt> directive.
</div>
<div class="Indented">
The <tt>offload</tt> extension supports addressing an array <tt>v[]</tt> on the host and its copy on the Phi/MIC device using identical syntax. If <tt>v[i]</tt> refers to an entry on the host, <tt>v[i]</tt> refers to its copy or mirror image on the Phi device. The <tt>offload</tt> extension supports partial views of an array. For example, we may offload <tt>v[offset:len]</tt> to copy the segment 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">v[offset], v[offset+1], ...,v[offset+len-1]
</pre>
</div>

</div>
<div class="Indented">
to the Phi device. The entry <tt>v[i]</tt> on the Phi still corresponds to the entry <tt>v[i]</tt> on the host, but on the Phi, the legal range for the index is limited to <span class="formula"><span class="text">offset</span> ≤ <i>i</i> &lt; <span class="text">offset</span> + <span class="text">len</span></span>. These elegant conventions simplify offloading to multiple Phi devices.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:phi-offload-bw">7.2.4↓</a> shows that the bandwidth from host to Phi/MIC device is similar to the bandwidth at a single node of an Infiniband network. The bandwidths are good enough to enable nontrivial usage of coprocessor devices but are far from heart stopping. The bandwidths are less than a tenth of bandwidth to memory from the AVX host.
</div>
<div class="Indented">
The discussion in this section may make offloading to Phi devices seem easy. It is indeed easy, even elegant, in simple programs. However, the basic setup where dissimilar coprocessor devices and the host compete to do the same work breaks modularity at the hardware level. In more complex programs, offloading will look far less felicitous than it does here.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.2.1">7.2.1</a> Initializing to use the MIC device<a class="Label" name="sub:phi-offload-init"> </a>
</h3>
<div class="Unindented">
The host computer can use the function <tt>mic_init()</tt> defined below to find out the number of MIC devices and set the default number of OpenMP threads. In all the offloading programs, it is assumed that <tt>mic_init()</tt> is called at the beginning.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#include &lt;offload.h&gt;
<span class="number-left">2</span>#include &lt;omp.h&gt;
<span class="number-left">3</span>​
<span class="number-left">4</span>int gl_host_nthreads = -1;
<span class="number-left">5</span>int gl_mic_nthreads = -1;
<span class="number-left">6</span>​
<span class="number-left">7</span>void mic_init(int &amp;nmic){
<span class="number-left">8</span>	char *s = getenv("OMP_NUM_THREADS");
<span class="number-left">9</span>	assrt(s != NULL);
<span class="number-left">10</span>	gl_host_nthreads = atoi(s);
<span class="number-left">11</span>	omp_set_num_threads(gl_host_nthreads);
<span class="number-left">12</span>​
<span class="number-left">13</span>	nmic = _Offload_number_of_devices();
<span class="number-left">14</span>	s = getenv("MIC_OMP_NUM_THREADS");
<span class="number-left">15</span>	assrt(s != NULL);
<span class="number-left">16</span>	gl_mic_nthreads = atoi(s);
<span class="number-left">17</span>	for(int i=0; i &lt; nmic; i++)
<span class="number-left">18</span>		omp_set_num_threads_target(TARGET_MIC, i, 
<span class="number-left">19</span>		                           gl_mic_nthreads);
<span class="number-left">20</span>}
</pre>
</div>

</div>
<div class="Indented">
On line 8, this program reads <tt>OMP_NUM_THREADS</tt>. On line 11, the default number of OpenMP threads on the host is set to the value of this environment variable. The default number of threads on the host is globally visible thanks to the definition on line 4. 
</div>
<div class="Indented">
The function used on line 13 to find out the number of MIC devices is declared in <tt>offload.h</tt> (which is included on line 1). The environment variable prefixed with <tt>MIC</tt> is read on line 14, and the for-loop on lines 17 to 19 sets the default number of OpenMP threads on every MIC device to the value of that environment variable. 
</div>
<div class="Indented">
Simply defining <tt>OMP_NUM_THREADS</tt> and the same environment variable with the prefix <tt>MIC</tt> is enough to set the default number of threads correctly. The function <tt>mic_init()</tt> makes that setting explicit within the structure of the program.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.2.2">7.2.2</a> The <tt>target(mic)</tt><span class="default"> declaration specification<a class="Label" name="sub:phi-offload-target"> </a></span>
</h3>
<div class="Unindented">
The <tt>icpc</tt> compiler has the ability to compile a function written in C/C++ to produce assembly for both the host computer and MIC device. 
</div>
<div class="Indented">
The function definition below is prefixed with <tt>__declspec(target(mic))</tt>. It is defined in the compilation unit <tt>leibniz_init.cpp</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">__declspec(target(mic))
double hostmic_sum(double *v, long len){
	printf("sum:host/mic pointer v = %p \n", v);
	double sum = 0;
	
#pragma omp parallel for			\
	reduction(+:sum)
	for(long i=0; i &lt; len; i++)
		sum += v[i];
​
	return sum;
}
</pre>
</div>

</div>
<div class="Indented">
This function simply sums the array <tt>v[]</tt>. When compiled (without the <tt>-mmic</tt> flag), two object files are produced from the same source. The object file <tt>leibniz_init.o</tt> has function definitions using the instruction set of the host computer. The object file <tt>leibniz_initMIC.o</tt> has function definitions using the instruction set of the MIC device. 
</div>
<div class="Indented">
Compiling with the <tt>-S</tt> flag and inspecting the assembly shows that the code for the MIC device uses ZMM registers and <tt>vaddpd</tt> instructions. On the AVX host, the assembly code uses <tt>XMM</tt> registers but not the wider <tt>YMM</tt> registers. That is either a sign that compiler technology is yet to catch up fully with AVX or an indication that the compiler has figured out that the use of <tt>YMM</tt> registers brings no benefit in this program, which is limited by bandwidth to memory.
</div>
<div class="Indented">
The rest of <tt>leibniz_init.cpp</tt> has the following definition:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">/*
 * gl_scl = 4.0^(1.0/3.0)
 */
__declspec(target(mic)) const double gl_scl 
                                = 1.5874010519681994;
​
__declspec(target(mic))
void hostmic_scale(double *v, long len){
	printf("scale:host/mic pointer v = %p \n", v);
​
#pragma omp parallel for
	for(long i=0; i &lt; len; i++)
		v[i] *= gl_scl;
	
#pragma omp parallel
#pragma omp master
	printf("num of threads = %d\n",
	       omp_get_num_threads());
}
​
void leibniz_init(double *v, long len){
#pragma omp parallel for
	for(long i=0; i &lt; len; i+=2){
		v[i] = 1.0/(2*i+1);
		v[i+1] = - 1.0/(2*i+3);
	}
}
</pre>
</div>

</div>
<div class="Indented">
The global variable <tt>gl_scl</tt> is initialized to <span class="formula">4<sup>1 ⁄ 3</sup></span>. Thanks to the <tt>target(mic)</tt> qualifier, it is defined on both the MIC device and the host. The function <tt>hostmic_scale()</tt> multiplies every entry of the array <tt>v[]</tt> by <span class="formula">4<sup>1 ⁄ 3</sup></span>. It, too, is compiled for both the host and MIC device. The last function <tt>leibniz_init()</tt> initializes the array <tt>v[]</tt> to terms of the Leibniz series <span class="formula">1 − 1 ⁄ 3 + 1 ⁄ 5 − ⋯</span>. This function is not qualified using <tt>target(mic)</tt>, and it is compiled only for the host.
</div>
<div class="Indented">
Although compilation of <tt>leibniz_init.cpp</tt> using <tt>icpc</tt> generates an object file <tt>leibniz_init.o</tt> compiled for the host and the object file <tt>leibniz_initMIC.o</tt> compiled for the MIC device, only the first of these needs to be linked. The <tt>icpc</tt> linker appears to automatically look for and find the other object file.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.2.3">7.2.3</a> Summing the Leibniz series<a class="Label" name="sub:phi-offload-leibniz"> </a>
</h3>
<div class="Unindented">
We will discuss four different programs to sum the Leibniz series to compute <span class="formula"><i>π</i></span>. All four programs initialize the array <tt>v[]</tt> to terms of the series <span class="formula">1 − 1 ⁄ 3 + 1 ⁄ 5 − ⋯</span> and then scale each term by <span class="formula">4<sup>1 ⁄ 3</sup></span> three times before summing the series. Together the programs expose nearly all the offload syntax one needs to know. Work is offloaded using the <tt>offload</tt> directive. Communication between the host and the Phi device is controlled using <tt>in</tt>, <tt>out</tt>, <tt>inout</tt>, or <tt>copy</tt> clauses.
</div>
<div class="Indented">
Each program works as follows. To begin with, the array <tt>v[]</tt> is initialized with entries of the series <span class="formula">1 − 1 ⁄ 3 + 1 ⁄ 5 − ⋯</span> on the host using <tt>leibniz_init()</tt> defined above. Subsequently, three calls are made to <tt>hostmic_scale()</tt>, also defined above, and a single call to <tt>hostmic_sum()</tt>, which too was defined above. 
</div>
<div class="Indented">
The three calls to <tt>hostmic_scale() </tt>and the one call to <tt>hostmic_sum()</tt> may be on either the host or Phi device. Each call to <tt>hostmic_scale()</tt> multiplies the terms of the series by <span class="formula">4<sup>1 ⁄ 3</sup></span>, and the effect of three calls is to multiply by <span class="formula">4</span>. The call to <tt>hostmic_sum()</tt> will therefore return an approximation to <span class="formula"><i>π</i></span>.
</div>
<div class="Indented">
In <tt>leibniz1()</tt>, the three calls to <tt>hostmic_scale()</tt> and the call to <tt>hostmic_sum()</tt> are all offloaded to the Phi device. The syntax in this case is the simplest.
</div>
<div class="Indented">
The function <tt>leibniz2()</tt> is similar, except for one big difference. When the master thread encounters a typical offload region, such as the offload region in <tt>leibniz1()</tt>, it blocks until the Phi device exits the offload region. In <tt>leibniz2()</tt>, we show syntax that allows the master thread to offload without blocking and then wait for the offload region to complete. That type of syntax is essential to make the Phi coprocessor and the host work in parallel.
</div>
<div class="Indented">
The function <tt>leibniz3()</tt> exhibits facilities for communication between the host and the Phi coprocessor in considerable detail. In this function, the first two scalings are done on the Phi device and the third scaling on the host. The sum is then computed on the Phi device.
</div>
<div class="Indented">
Finally, the function <tt>leibniz4()</tt> shows how multiple Phi devices may be used in parallel. In <tt>offload</tt> directives, syntax such as <tt>v:length(n)</tt> is used to offload <tt>v[0,...,len-1]</tt> to the coprocessor. Parallelism between Phi devices is facilitated by the syntax <tt>v[offset:len]</tt>, which offloads 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">v[offset, offset+1,...,offset+len-1]
</pre>
</div>

</div>
<div class="Indented">
to the coprocessor.
</div>
<h? class="Subsubsection">
<b><u>Offloading from the processor to the Phi</u></b>
</h?>
<div class="Unindented">
The first of the four functions <tt>leibniz1()</tt> is the simplest.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void leibniz1(){
<span class="number-left">2</span>	int nmic;
<span class="number-left">3</span>	mic_init(nmic);
<span class="number-left">4</span>	long n = 1l*1000*1000*800;
<span class="number-left">5</span>	long nbytes = n*8;
<span class="number-left">6</span>	double* v = (double *)_mm_malloc(nbytes, 64);
<span class="number-left">7</span>	leibniz_init(v, n);
<span class="number-left">8</span>	
<span class="number-left">9</span>	double sum; 
<span class="number-left">10</span>#pragma offload target(mic:0)					\
<span class="number-left">11</span>	mandatory								\
<span class="number-left">12</span>	in(v:length(n) align(64))	
<span class="number-left">13</span>	{
<span class="number-left">14</span>		hostmic_scale(v, n);
<span class="number-left">15</span>		hostmic_scale(v, n);
<span class="number-left">16</span>		hostmic_scale(v, n);
<span class="number-left">17</span>		sum = hostmic_sum(v, n);
<span class="number-left">18</span>	}
<span class="number-left">19</span>	printf("    leibniz1:  sum = %f\n", sum);
<span class="number-left">20</span>​
<span class="number-left">21</span>	_mm_free(v);
<span class="number-left">22</span>	mic_exit();
<span class="number-left">23</span>}
</pre>
</div>

</div>
<div class="Indented">
The number of MIC devices <tt>nmic</tt> is initialized on line 3. The initialization function <tt>mic_init()</tt> also sets the default number of threads for MIC as well as the host, as described. The <span class="formula">6.4</span> GB array <tt>v[]</tt> is initialized to entries of the Leibniz series <span class="formula">1 − 1 ⁄ 3 + 1 ⁄ 5 − ⋯</span> on line 7. 
</div>
<div class="Indented">
The <tt>#pragma offload</tt> construct begins on line 10. The first clause, which is on line 10, is <tt>target(mic:0)</tt>. This clause says that the computation in the ensuing block (lines 13 through 18) must be offloaded to the MIC device numbered 0. If there are <tt>nmic</tt> MIC devices, the devices are numbered from 0 to <tt>nmic-1</tt>. 
</div>
<div class="Indented">
If there is in fact no MIC device on the system, the program by default ignores the offload directive and executes the ensuing block on the host itself. The <tt>mandatory</tt> clause on line 11 forces the program to abort if no MIC device is found. 
</div>
<div class="Indented">
The offload statement block (lines 13 through 18) refers to the array <tt>v[]</tt> and the variables <tt>sum</tt> and <tt>n</tt>. Before an offload region is entered, all the variables input to the offload region are automatically gathered and sent to the MIC device. During exit, the MIC device gathers all the variables to be output and sends them back to the host. Therefore, we do not need to do anything special to propagate the variables <tt>sum</tt> and <tt>n</tt> to the MIC device and back. 
</div>
<div class="Indented">
However, the <span class="formula">6.4</span> GB array <tt>v[]</tt> needs special handling. The <tt>in</tt> clause on line 12 tells the compiler that <tt>v[]</tt> is an input to the offload region. The modifier <tt>length(n)</tt> tells the compiler that this is an array of length <tt>n</tt>. By default, the compiler will allocate a <span class="formula">6.4</span> GB array on the MIC device also named <tt>v[]</tt>. An <tt>in</tt> clause always has the effect of copying the array <tt>v[]</tt> on the host to the corresponding array <tt>v[]</tt> on the MIC device. By default, the array <tt>v[]</tt> that resides on the MIC device is deallocated at exit from the offload region.
</div>
<div class="Indented">
The <tt>printf()</tt> statement on line 19 runs on the host, but the sum it prints is computed on the MIC device.
</div>
<h? class="Subsubsection">
<b><u>The signal/wait facility</u></b>
</h?>
<div class="Unindented">
Next we use the function <tt>leibniz2()</tt> to illustrate the signal/wait facility of the offload construct.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void leibniz2(){
	...
​
#pragma offload target(mic:0)					\
	in(v:length(n) align(64))				\
	signal(0)
	{
		hostmic_scale(v, n);
		hostmic_scale(v, n);
		hostmic_scale(v, n);
		sum = hostmic_sum(v, n);
	}
#pragma offload_wait target(mic:0)  wait(0)
	printf("    leibniz2:  sum = %f\n", sum);
​
	...
}
</pre>
</div>

</div>
<div class="Indented">
This function is exactly the same as <tt>leibniz1()</tt>, except that the <tt>offload</tt> construct is modified and the following <tt>printf</tt> on line 19 is inside an <tt>offload</tt> region. The <tt>offload</tt> construct here omits <tt>mandatory</tt> and adds the <tt>signal</tt> clause. If there is no <tt>signal</tt> clause, the host processor will block until the offload completes execution. With a <tt>signal</tt> clause, it will issue the <tt>offload</tt> and then charge ahead without blocking. In general, using signals enables overlapping host processor activity with MIC activity.
</div>
<div class="Indented">
If a <tt>signal</tt> clause is used, the target MIC device must be numbered. More specifically, the <tt>target</tt> clause can read <tt>target(mic:0)</tt> or <tt>target(mic:1)</tt>, but it cannot read <tt>target(mic)</tt>. The signal can be given any <tt>int</tt> as a number. Here <tt>signal(0)</tt> specifies the signal’s number as <span class="formula">0</span>.
</div>
<div class="Indented">
The matching <tt>offload_wait</tt> construct waits for a signal from MIC device <span class="formula">0</span> with signal number <span class="formula">0</span> before issuing the <tt>printf</tt> statement. Note that the <tt>printf</tt> in <tt>leibniz2()</tt> is issued on the MIC device and not on the host as in <tt>leibniz1()</tt>.
</div>
<h? class="Subsubsection">
<b><u>More complex offload patterns</u></b>
</h?>
<div class="Unindented">
The function <tt>leibniz3()</tt> makes the offload syntax more explicit. Its opening fragment is similar to what we have already seen.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>void leibniz3(){
<span class="number-left">2</span>	int nmic;
<span class="number-left">3</span>	mic_init(nmic);
<span class="number-left">4</span>	assrt(nmic &gt; 0);
<span class="number-left">5</span>	long n = 1l*1000*1000*800;
<span class="number-left">6</span>	long nbytes = n*8;
<span class="number-left">7</span>	printf("            nbytes = %ld\n",nbytes);
<span class="number-left">8</span>	double* v = (double *)_mm_malloc(nbytes, 64);
<span class="number-left">9</span>	leibniz_init(v, n);
<span class="number-left">10</span>	printf("    host pointer v = %p \n", v);
</pre>
</div>

</div>
<div class="Indented">
The following offload construct allocates the <span class="formula">6.4</span> GB array <tt>v[]</tt> on the MIC device and copies to it: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#pragma offload target(mic:0)					\
<span class="number-left">2</span>	in(v:length(n) align(64) alloc_if(1) free_if(0))
<span class="number-left">3</span>	{}
</pre>
</div>

</div>
<div class="Indented">
The default conventions for allocating when entering an offload region and deallocating during exit can be a little confusing. Here the allocation and deallocation have been made explicit. The modifier <tt>alloc_if(1)</tt> says that the array <tt>v[]</tt> should be allocated on the MIC device at entry. The modifier <tt>free_if(0)</tt> says that the array <tt>v[]</tt> should not be freed at exit. It can be reused by later offloads. Because this is an <tt>in</tt> clause, the array <tt>v[]</tt> will always be copied from host to MIC at entry. Such copying is the principal effect of this clause because the body of the construct (line 13) is empty.
</div>
<div class="Indented">
The next offload construct has a nonempty body.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#pragma offload target(mic:0)				\
<span class="number-left">2</span>	nocopy(v:length(n) alloc_if(0) free_if(0))	
<span class="number-left">3</span>	hostmic_scale(v, n);
</pre>
</div>

</div>
<div class="Indented">
The <tt>nocopy</tt> clause (line 15) means that the array <tt>v[]</tt> is copied at neither entry nor  exit. The <tt>alloc_if(0)</tt> and <tt>free_if(0)</tt> modifiers imply that memory is not allocated on the MIC device at entry nor is it deallocated at exit. The sole effect of this offload construct is to offload the function call <tt>hostmic_scale()</tt> on line 16 to the MIC device. Once that call is complete, the array <tt>v[]</tt> that resides on the MIC device will be scaled by <span class="formula">4<sup>1 ⁄ 3</sup></span>, but the array <tt>v[]</tt> on the host is unchanged.
</div>
<div class="Indented">
Further scaling is done as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>#pragma offload target(mic:0)			    \
<span class="number-left">2</span>	out(v:length(n) align(64) alloc_if(0) free_if(0))
<span class="number-left">3</span>	hostmic_scale(v, n);
<span class="number-left">4</span>	
<span class="number-left">5</span>	hostmic_scale(v, n);
</pre>
</div>

</div>
<div class="Indented">
The offload load construct here has an <tt>out</tt> clause. The array <tt>v[]</tt> is not copied from host to MIC at entry, but it is copied from MIC to host at exit. The array is neither allocated nor freed on the MIC device. The offload region (line 19) is again a single call to <tt>host_mic()</tt>. At exit from the offload construct, the array <tt>v[]</tt> is scaled by <span class="formula">4<sup>2 ⁄ 3</sup></span> on both the host and MIC device.
</div>
<div class="Indented">
The function call <tt>hostmic_scale()</tt> on line 21 occurs on the host. At the end of this call, the array <tt>v[]</tt> is scaled by <span class="formula">4</span> on the host and by <span class="formula">4<sup>2 ⁄ 3</sup></span> on the MIC device.
</div>
<div class="Indented">
The definition of <tt>leibniz3()</tt> is completed as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	double sum;
<span class="number-left">2</span>#pragma offload target(mic:0)					\
<span class="number-left">3</span>	in(v:length(n) align(64) alloc_if(0) free_if(1))
<span class="number-left">4</span>	sum = hostmic_sum(v, n);
<span class="number-left">5</span>​
<span class="number-left">6</span>	printf("               sum = %f\n", sum);
<span class="number-left">7</span>	_mm_free(v);
<span class="number-left">8</span>	mic_exit();
<span class="number-left">9</span>}
</pre>
</div>

</div>
<div class="Indented">
Notice that the <tt>in</tt> clause (line 24) copies <tt>v[]</tt> from host to MIC. The array <tt>v[]</tt> is scaled by <span class="formula">4</span> on the host but by only <span class="formula">4<sup>2 ⁄ 3</sup></span> on the MIC device. Therefore, a copy is essential if we are to get the correct value of <span class="formula"><i>π</i></span>. Here <tt>v[]</tt> is freed on the MIC device at exit. The sum is computed on the MIC device on line 25.
</div>
<h? class="Subsubsection">
<b><u>Host and MIC pointers</u></b>
</h?>
<div class="Unindented">
The function <tt>leibniz3()</tt> shows the close connection between the array <tt>v[]</tt> stored on the host and its counterpart on the MIC device. The function prints the pointer <tt>v</tt> on the host (line 10). There are three calls to <tt>hostmic_scale()</tt> (lines 16, 19, and 21), two of them from the MIC device and one from the host. Each of these calls prints the value of the pointer <tt>v</tt>. The following lines are printed:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">          host pointer v = 0x2af1d0000040 
scale:host/mic pointer v = 0x7fe5cd0f7040 
scale:host/mic pointer v = 0x7fe5cd0f7040 
scale:host/mic pointer v = 0x2af1d0000040 
</pre>
</div>

</div>
<div class="Indented">
Evidently, the pointer has a different value on the host and the MIC, although it has the same name <tt>v</tt> in the program.
</div>
<div class="Indented">
During entry to an offload region, the host needs to know the value of the pointer on the MIC device to set up function calls correctly and to implement clauses such as <tt>in</tt>. During exit from the offload region, the MIC device needs to know the value of the pointer on the host to complete <tt>out</tt> clauses and copy data back to the host. 
</div>
<div class="Indented">
The runtime environment on the host and the MIC device conceal all these details and provide a simple interface to the user.<span class="FootOuter"><span class="SupFootMarker"> [128] </span><span class="HoverFoot"><span class="SupFootMarker"> [128] </span>Every <tt>in</tt>, <tt>out</tt>, <tt>inout</tt>, or <tt>nocopy</tt> clause will either set up or access a table mapping between host and MIC pointers. On the host this correspondence is presumably stored by the MIC’s device driver and accessed by the host program using the runtime libraries. On the MIC device, the correspondence between pointers may be stored in the Manycore Program Software Stack (MPSS) and accessed by the offloaded program using runtime libraries.</span></span> If an array <tt>v[]</tt> is offloaded, we may use <tt>v[i]</tt> to refer to the <span class="formula"><i>i</i></span>th entry of the offloaded array on the Phi device and to the <span class="formula"><i>i</i></span>th entry of the original array on the host. The two arrays are like images of each other. 
</div>
<h? class="Subsubsection">
<b><u>Offloading to multiple MIC devices</u></b>
</h?>
<div class="Unindented">
On systems with multiple MIC devices, the host may offload parts of the computation to each device and keep a portion to itself. The most natural way to do that is to split an array <tt>v[]</tt> and assign part of the array to each MIC device.
</div>
<div class="Indented">
The offload syntax for splitting an array across several devices is uncomplicated. However, the way the correspondence is set up between host and MIC pointers is a little more subtle. The opening fragment of the function <tt>leibniz4()</tt> is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void leibniz4(){
	int nmic;
	mic_init(nmic);
	long n = 1l*1000*1000*800;
	long nbytes = n*8;
	double* v = (double *)_mm_malloc(nbytes, 64);
	leibniz_init(v, n);
​
	double sum[nmic];
</pre>
</div>

</div>
<div class="Indented">
Here <tt>sum[]</tt> is an array with as many entries as the number of MIC devices. Each MIC device will leave its part of the sum in one of the entries of the array.
</div>
<div class="Indented">
The new syntax for splitting an array across MIC devices appears below. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	/*
	 * alloc mem on mic devices
	 */
	for(int mc=0; mc &lt; nmic; mc++){
		long shft = mc*n/nmic;
		long len = (mc+1)*n/nmic - mc*n/nmic;
#pragma offload_transfer target(mic:mc)			\
   nocopy(v[shft:len]:align(64) alloc_if(1) free_if(0))
		{}
	}
</pre>
</div>

</div>
<div class="Indented">
In the body of the for-loop, the part of the array <tt>v[]</tt> allocated to the MIC device numbered <tt>mc</tt> begins with the entry <tt>v[shft]</tt> and ends with the entry <tt>v[shft+len-1]</tt>. The offload region here allocates corresponding memory on the MIC device. It has an empty body, and nothing is copied.
</div>
<div class="Indented">
The crucial bit of syntax here is <tt>v[shft:len]</tt>. An array of <tt>len</tt> items is allocated on the MIC device. As before, the pointer <tt>v</tt> on the host corresponds to some other value of <tt>v</tt> on the MIC device inside the body of the offload region. Even in this setting, <tt>v[i]</tt> refers to the <span class="formula"><i>i</i></span>th entry of <tt>v[]</tt> on the Phi as well as on the host, but the legal values of the index <span class="formula"><i>i</i></span> must be in the range <span class="formula"><span class="text">shft</span> ≤ <i>i</i> &lt; <span class="text">shft</span> + <span class="text">len</span></span>. The copy of <tt>v[]</tt> on the Phi is again an image of the array <tt>v[]</tt> on the host, but only a segment of the original array is represented in the image.
</div>
<div class="Indented">
The rest of the definition of <tt>leibniz4()</tt> shows more instances of this offload syntax.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	/*
	 * offload scaling and summing to mic devices
	 */
	for(int mc=0; mc &lt; nmic; mc++){ 
		long shft = mc*n/nmic;
		long len = (mc+1)*n/nmic - mc*n/nmic;
#pragma offload target(mic:mc)				\
	in(v[shft:len]:alloc_if(0) free_if(0))		\
	out(sum[mc:1])							\
	signal(1)
		{
			hostmic_scale(v+shft, len);
			hostmic_scale(v+shft, len);
			hostmic_scale(v+shft, len);
			sum[mc] = hostmic_sum(v+shft, len);
		}
	}
​
	printf("              nmic = %d\n", nmic);
​
	/*
	 * wait for mics to get back
	 */
	for(int mc=0; mc &lt; nmic; mc++){
#pragma offload_wait target(mic:mc)  wait(1)
	}
​
	/*
	 * free mem on mic devices
	 */
	for(int mc=0; mc &lt; nmic; mc++){
		long shft = mc*n/nmic;
		long len = (mc+1)*n/nmic - mc*n/nmic;
#pragma offload_transfer target(mic:mc)			\
 nocopy(v[shft:len]:align(64) alloc_if(0) free_if(1))
	}
​
	double ans = 0;
	for(int mc=0; mc &lt; nmic; mc++)
		ans += sum[mc];
	printf("     leibniz4: sum = %e\n", ans);
​
	_mm_free(v);
	mic_exit();
}
​
</pre>
</div>

</div>
<div class="Indented">
Notice that the first argument to <tt>hostmic_scale()</tt> and to <tt>hostmic_sum()</tt> is <tt>v+shft</tt> and not just <tt>v</tt>. The offload region that scales and sums is preceded by the <tt>signal</tt> clause. It makes no sense to use blocking offloads if multiple MIC devices are in play. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.2.4">7.2.4</a> Offload bandwidth<a class="Label" name="sub:phi-offload-bw"> </a>
</h3>
<div class="Unindented">
If the Phi/MIC devices are used in offload mode, there is a significant cost to the data transfer between the host and MIC devices. The PCIe bus, which connects the host with the MIC devices (via the chipset; see figure <a class="Reference" href="#fig:xphi-intro-sketch">7.1↑</a>), has low bandwidth. Its bandwidth is more than an order of magnitude less than the bandwidth to memory of the MIC devices or even the host. Data transfer between the host and the Xeon Phi coprocessors imposes serious limits on the advantages as well as utility of the coprocessor model.
</div>
<div class="Indented">
To time the transfers between the host and the MIC devices, we use functions such as the following:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">void xfer_in(double *v, long n, int nmic){
	long fst[nmic+1];
	for(int i=0; i &lt;= nmic; i++)
		fst[i] = i*n/nmic;
​
	for(int mc=0; mc &lt; nmic; mc++){
		long shft = fst[mc];
		long len = fst[mc+1] - fst[mc];
#pragma offload target(mic:mc)			        \
	in(v[shft:len]:align(64) alloc_if(0) free_if(0))\
	signal(1)
		dummy(v+shft, len);
		
	}
​
	for(int mc=0; mc &lt; nmic; mc++){
#pragma offload_wait target(mic:mc) wait(1)
	}
}
</pre>
</div>

</div>
<div class="Indented">
This function transfers data into <tt>nmic</tt> devices. Analogous functions transfer data out of the MIC devices or use the <tt>inout</tt> clause to transfer data both into the MIC devices during entry to the offload region and out at exit. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:xphi-offload-bw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
Cycles/Byte<br/>
(One MIC)
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
Bandwidth<br/>
(One MIC)
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
Cycles/Byte<br/>
(Two MICs)
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
Bandwidth<br/>
(Two MICs)
</div>

</td>

</tr>
<tr>
<td align="center" valign="top">
IN
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.42</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">6.48</span> GB/s
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.40</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">6.75</span> GB/s
</td>

</tr>
<tr>
<td align="center" valign="top">
OUT
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.42</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">6.48</span> GB/s
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.40</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">6.75</span> GB/s
</td>

</tr>
<tr>
<td align="center" valign="top">
INOUT
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.81</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">3.24</span> GB/s
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.77</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">3.51</span> GB/s
</td>

</tr>

</table>

</div>
<div class="caption">
Table 7.3 Offload bandwidths in transferring data to one MIC device or simultaneously to two MIC devices. In cycles/byte, the cycles are of the 2.7  GHz AVX host.
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:xphi-offload-bw">7.3↑</a> shows that data can be moved into or out of a single MIC device at the rate of nearly <span class="formula">7</span> GB/s. That rate is quite close to the peak rating of the PCIe express bus. The bandwidth is nearly halved if data is input to the MIC device during entry to the offload region and output from the MIC device at exit. With the <tt>inout</tt> clause, data flowing in and data flowing out cannot be overlapped. 
</div>
<div class="Indented">
With simultaneous transfer to two MIC devices, the bandwidths are almost unchanged. The bandwidth cannot be doubled because both devices are on the same bus. Latencies for data transfer are of the order of a millisecond. 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program to find the sum of a long array that outsources parts of the work to each Phi device on the system. Do you expect the program to result in any speedup?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a program that during a single iteration replaces each entry of a 2D array (with periodic boundaries) by the average of its north, south, east, and west entries. The program must outsource parts of the computation to each Phi device on the system. Do you expect a speedup if only a few iterations are to be performed? What if the number of iterations is large?
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Assume that a matrix, whose dimensions you may assume to be suitably large, is split between the host and Phi coprocessors. Write a program that transposes this matrix. Discuss the difficulties that arise because of the heterogeneity between the processor and the coprocessors.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-7.3">7.3</a> Two examples: FFT and matrix multiplication<a class="Label" name="sec:phi-examples"> </a>
</h2>
<div class="Unindented">
Section <a class="Reference" href="#sub:phi-fft">7.3.1↓</a> looks at the Fast Fourier Transform (FFT) implemented inside the MKL library. For small <span class="formula"><i>n</i></span>, the FFT on the Phi/MIC device is nearly twice as fast as it is on the AVX host. For large <span class="formula"><i>n</i></span> such as <span class="formula"><i>n</i> = 2<sup>26</sup></span>, the FFT does not appear to be well optimized yet and is slower on the Phi/MIC device by more than a factor of <span class="formula">8</span>. The fact that even the FFT reaches only <span class="formula">15</span>% of the peak floating point bandwidth shows how misleading that metric can be.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:phi-mmult">7.3.2↓</a> is a look at the Phi instruction pipeline in the context of matrix multiplication. We write a microkernel in assembly that reaches <span class="formula">50</span>% of the peak floating point bandwidth. The discussion is carried far enough to show that the principles of instruction pipeline optimization, explained in depth in chapter <a class="Reference" href="#chap:The-processor">3↑</a>, apply to Phi/MIC devices. Not all the techniques pertinent to matrix multiplication and developed in earlier chapters are reviewed. To approach the peak bandwidth more closely, as the MKL library does, would require an application of the full range of techniques.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.3.1">7.3.1</a> FFT<a class="Label" name="sub:phi-fft"> </a>
</h3>
<div class="Unindented">
The FFT is a good basis for comparing the Phi against its AVX host. It is one of the most frequently employed scientific algorithms. Unlike in the LINPACK benchmark employed to rate supercomputers, the cost of memory accesses cannot be completely hidden.
</div>
<div class="Indented">
The same class <tt>FFT</tt> was employed to time FFTs on the MIC device as well as the host.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">__declspec(target(mic))
class FFT{
private:
	DFTI_DESCRIPTOR_HANDLE handle;
public:
	FFT(int n, long count);
	~FFT();
	void fwd(double *f){
		DftiComputeForward(handle, f);
	}
	void bwd(double *f){
		DftiComputeBackward(handle, f);
	}
};
</pre>
</div>

</div>
<div class="Indented">
This class, like similar classes discussed in chapter <a class="Reference" href="#chap:C/C++:-Libraries-and">2↑</a>, uses the MKL library. As far as the definition of this class is concerned, no new points arise with respect to either MKL or Phi/MIC syntax. Therefore, it is omitted. The <tt>FFT</tt> class may be used on the host computer, offloaded to the MIC device, or used natively on a MIC device if compiled with the <tt>-mmic</tt> flag. Both forward and backward transforms are in place. The forward transform is suitably normalized.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:xphi-fft-cycles"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
AVX<br/>
<span class="formula"><span class="text">Cycles</span> ⁄ <i>n</i>log<sub>2</sub><i>n</i></span>
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
AVX<br/>
<span class="formula"><span class="text">Cycles</span> ⁄ <span class="text">Byte</span></span>
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
MIC<br/>
<span class="formula"><span class="text">Cycles</span> ⁄ <i>n</i>log<sub>2</sub><i>n</i></span>
</div>

</td>
<td align="center" valign="top" style="width: 2.5cm;">
<div class="PlainVisible">

</div>
<div class="PlainVisible">
MIC<br/>
<span class="formula"><span class="text">Cycles</span> ⁄ <span class="text">Byte</span></span>
</div>

</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">64</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.19</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.07</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.04</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">1, 024</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.17</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.09</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.06</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">8, 192</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.16</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.13</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.15</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">0.12</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">8, 192 × 8, 192</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">2.16</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">4.26</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">17.55</span>
</td>
<td align="center" valign="top" style="width: 2.5cm;">
<span class="formula">28.52</span>
</td>

</tr>

</table>

</div>
<div class="caption">
Table 7.4 Number of cycles consumed by complex to complex FFT of size <span class="formula"><i>n</i></span>. For both the <span class="formula">2.7</span>  GHz AVX host (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for the full name) and the Phi/MIC coprocessor, the reported numbers are host cycles.
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:xphi-fft-cycles">7.4↑</a> shows that the Phi is faster than its host for <span class="formula"><i>n</i> = 64</span> and <span class="formula"><i>n</i> = 1, 024</span>. The multiple levels of cache found on the host are useful, even for a regular and arithmetic-intensive computation such as the FFT. For <span class="formula"><i>n</i> = 8, 192</span>, the Phi and its host are almost of the same speed. For <span class="formula"><i>n</i> = 8, 192<sup>2</sup> = 2<sup>26</sup></span>, MKL’s FFT is not optimized on either platform, but the host is much better.
</div>
<div class="Indented">
If a large number of FFTs are to be computed, we may ask, does it make sense to offload from the AVX host to the MIC coprocessor? The answer is emphatically no. For <span class="formula"><i>n</i> = 8, 192</span>, the AVX host takes just <span class="formula">0.13</span> cycles per byte of data (see table <a class="Reference" href="#tab:xphi-fft-cycles">7.4↑</a>). Table <a class="Reference" href="#tab:xphi-offload-bw">7.3↑</a> shows that sending a byte to the coprocessor and getting it back will take <span class="formula">0.81</span> cycles, swamping the cost of the FFT. 
</div>
<div class="Indented">
Table <a class="Reference" href="#tab:xphi-fft-cycles">7.4↑</a> shows that a complex-to-complex FFT of size <span class="formula"><i>n</i> = 1, 024</span> takes <span class="formula">0.09<i>n</i>log<sub>2</sub><i>n</i></span> host cycles (with the AVX host clocked at <span class="formula">2.7</span> GHz) on the Phi. As shown in chapter <a class="Reference" href="#chap:C/C++:-Libraries-and">2↑</a>, the number of arithmetic operations in a single complex FFT of dimension <span class="formula"><i>n</i></span>, with <span class="formula"><i>n</i></span> a power of <span class="formula">2</span>, may be assumed to be <span class="formula">5<i>n</i>log<sub>2</sub><i>n</i></span>. Therefore, the floating point throughput is <span class="formula">5 ⁄ .09 × 2.7 = 150 <span class="text"> GFlops/sec</span></span>. This floating point throughput is only <span class="formula">15</span>% of the theoretical peak of the Phi.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-7.3.2">7.3.2</a> Matrix multiplication<a class="Label" name="sub:phi-mmult"> </a>
</h3>
<div class="Unindented">
Algorithms that are capable of approaching the theoretical peak are not many. These algorithms must involve a great number of arithmetic operations relative to the data they access. In addition, their structure must permit hiding the cost of multiple accesses of the same data item. Dense numerical linear algebra is the main source of such algorithms. 
</div>
<div class="Indented">
The number of arithmetic operations in <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span>, where <span class="formula"><i>A</i>, <i>B</i>, <i>C</i></span> are <span class="formula"><i>n</i> × <i>n</i></span> matrices, is <span class="formula">2<i>n</i><sup>3</sup></span>. The number of double-precision numbers accessed is only <span class="formula">3<i>n</i><sup>2</sup></span>. For large <span class="formula"><i>n</i></span>, a great number of arithmetic operations are carried out for every byte that is accessed. It is true that every entry of <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, or <span class="formula"><i>C</i></span> is involved in <span class="formula"><i>n</i></span> operations, but we may hide the cost of memory accesses, as shown in chapter <a class="Reference" href="#chap:Memory">4↑</a>.
</div>
<div class="Indented">
Even if the cost of memory accesses is ignored, writing a microkernel that achieves peak floating performance, with all the data items in L1 cache, can be a formidable challenge. It requires knowledge of the register set as well as the instruction pipeline. In this section, we look at a small part of the Xeon Phi’s instruction set<span class="FootOuter"><span class="SupFootMarker"> [129] </span><span class="HoverFoot"><span class="SupFootMarker"> [129] </span>The Xeon Phi’s instruction set is documented in <i>Intel Xeon Phi Co-processor Instruction Set Architecture Reference Manual</i>, September 2012.</span></span> and then write a microkernel that achieves <span class="formula">45</span>% of the peak floating point throughput. Although this microkernel falls short of the best possible by a factor of <span class="formula">2</span>, it still gives a good understanding of the Xeon Phi’s architecture, why it can be so fast, and why it can be so hard to write programs that approach its top speed. Difficulty in approaching peak floating point throughput is characteristic of all modern architectures. Later we turn to Intel’s MKL library, which achieves <span class="formula">85</span>% of the peak floating point throughput.
</div>
<h? class="Subsubsection">
<b><u>Xeon Phi instructions</u></b>
</h?>
<div class="Unindented">
Before taking a closer look at the Phi architecture, we go back to figure <a class="Reference" href="#fig:xphi-arch-prelim">7.2↑</a> and summarize a few features of the Phi architecture. Up to four threads can be scheduled to run on every Phi/MIC core. The hardware switches between threads in a single cycle. It prefers round-robin scheduling, and it never executes the same thread in consecutive cycles. Thus, to get to peak bandwidth, we need at least <span class="formula">2</span> threads per core, although we prefer to have <span class="formula">4</span>.
</div>
<div class="Indented">
There are <span class="formula">32</span> ZMM registers, each of them <span class="formula">64</span> bytes or <span class="formula">512</span> bits wide. The Phi supports all the general-purpose registers such as <tt>%rdi</tt>, <tt>%rdx</tt>, and <tt>%rax</tt>. The entire register file is replicated for each of the four threads on any coprocessor core.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:xphi-pipeline"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/xphi_pipeline.png" alt="figure FIGS/chapter6/xphi_pipeline.png" style="max-width: 352px; max-height: 80px;"/>

</div>
<div class="caption">
Figure 7.4 Xeon Phi pipeline.
</div>

</div>

</div>
Figure <a class="Reference" href="#fig:xphi-pipeline">7.4↑</a> is a more detailed depiction of the Xeon Phi pipeline.<span class="FootOuter"><span class="SupFootMarker"> [130] </span><span class="HoverFoot"><span class="SupFootMarker"> [130] </span>The Xeon Phi pipeline is described in the document <i>Intel Xeon Phi Coprocessor Vector Microarchitecture</i>, September 2012.</span></span> The thread picker picks one of four threads every cycle. The next stage in the pipeline decodes instructions. Instructions that use general-purpose registers are sent to the usual x-86 pipeline. Vector instructions are sent to either the U- or V-pipe. Both these pipes are part of the Vector Processing Unit (VPU) extension of the coprocessor core. The U-pipe can execute all vector instructions. The V-pipe can execute only a few. Notice that the Add/Mult unit of the U-pipe has four cycle latency, which is the same as the number of threads. So if each thread issues an Add/Mult instruction during every cycle in round-robin scheduling, that unit can be kept completely busy. Because the V-pipe is separate from the U-pipe, the coprocessor core can issue two vector instructions in the same cycle. 
</div>
<div class="Indented">
We look at three vector instructions. The first of them is <tt>VMOVAPD</tt>. In usage such as 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">vmovapd (%rdi), %zmm0
</pre>
</div>

</div>
<div class="Indented">
the content of <tt>%rdi</tt> must be a <span class="formula">64</span>-byte-aligned pointer. This instruction moves <span class="formula">64</span> bytes from the location <tt>%rdi</tt> points to the destination register <tt>%zmm0</tt>. In general, <tt>VMOVAPD</tt> can be used to move <span class="formula">64</span> bytes of data between ZMM registers or between memory and a ZMM register. Of the three instructions we look at, this is the only one that can be scheduled on the V-pipe.
</div>
<div class="Indented">
The next instruction we look at is <tt>VFMADD231PD</tt>. FMADD refers to fast multiply and add. As before, V and PD refer to &ldquo;vector&rdquo; and &ldquo;packed double.&rdquo; The effect of 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">vfmadd231pd %zmm3, %zmm2, %zmm1
</pre>
</div>

</div>
<div class="Indented">
is equivalent to <span class="formula"><span class="text">ZMM1</span> = <span class="text">ZMM1</span> + <span class="text">ZMM2</span> × <span class="text">ZMM3</span></span>. Each ZMM register holds eight double-precision numbers. Each set of eight is subjected to the multiply-add operation. This single instruction is responsible for <span class="formula">16</span> flops.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:xphi-swizzle"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/xphi_swizzle.png" alt="figure FIGS/chapter6/xphi_swizzle.png" style="max-width: 274px; max-height: 66px;"/>
<div class="caption">
Figure 7.5 Two swizzle operations applied to a ZMM register. Each entry shown is a double-precision number.
</div>

</div>

</div>

</div>
The first source of the <tt>VFMADD231PD</tt> can be swizzled in two different ways.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">vfmadd231pd %zmm3{cdab}, %zmm2, %zmm1
vfmadd231pd %zmm3{badc}, %zmm2, %zmm1
</pre>
</div>

</div>
<div class="Indented">
The effect of the swizzles is to permute the eight double-precision numbers stored in ZMM3 before subjecting it to the multiply-add operation. The permutations that the two swizzles <tt>{cdab}</tt> and <tt>{badc}</tt> correspond to are shown in figure <a class="Reference" href="#fig:xphi-swizzle">7.5↑</a>. The <span class="formula">64</span>-byte register is divided into halves, with each half holding four double-precision numbers, and both halves are subject to the same permutation.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:xphi-vpermf"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/xphi_vpermf.png" alt="figure FIGS/chapter6/xphi_vpermf.png" style="max-width: 182px; max-height: 66px;"/>
<div class="caption">
Figure 7.6 Effect of applying the <tt>vpermf32x4</tt> instruction to a ZMM register. Each entry shown is a double-precision number.
</div>

</div>

</div>

</div>
The final instruction we consider is <tt>vpermf32x4</tt> shown in figure <a class="Reference" href="#fig:xphi-vpermf">7.6↑</a>. The instruction
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">vpermf32x4 $78, %zmm0, %zmm0
</pre>
</div>

</div>
<div class="Indented">
treats the <tt>ZMM0</tt> register as divided into four blocks of <span class="formula">128</span> bits. In figure <a class="Reference" href="#fig:xphi-vpermf">7.6↑</a>, these blocks are d0-d1, c0-c1, b0-b1, and a0-a1. The number <span class="formula">78</span> determines the permutation that is effected. In binary, <span class="formula">78</span> is <span class="formula">01|00|11|10</span> or <span class="formula">4<sup>3</sup> × 1 + 4<sup>2</sup> × 0 + 4 × 3 + 4<sup>0</sup> × 2</span>. Therefore, in the permutation that takes effect, the block in second position moves to the zeroth position, the block in third position to the first position, and vice versa. Thus, we get the permutation shown in figure <a class="Reference" href="#fig:xphi-vpermf">7.6↑</a>, which swaps the lower half of the ZMM register with the upper half. This instruction cannot be executed on the V-pipe.
</div>
<h? class="Subsubsection">
<b><u>A microkernel for matrix multiplication</u></b>
</h?>
<div class="Unindented">
The assembly code shown in figure <a class="Reference" href="#fig:xphi-microk">7.7↓</a> implements a function with the following declaration:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">asm8x1x8(double *A, double *B, double *C);
</pre>
</div>

</div>
<div class="Indented">
All three pointers must be <span class="formula">64</span>-byte aligned. An <span class="formula">8 × 1</span> column is stored in <tt>A[]</tt>, and a <span class="formula">1 × 8</span> row is stored in <tt>B[]</tt>. The function computes their outer product <span class="formula"><i>AB</i></span> and adds it to the <span class="formula">8 × 8</span> matrix stored in <tt>C[]</tt>. To simplify implementation, the storage format of <span class="formula"><i>C</i></span> is a little odd.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:xphi-microk"> </a><div class="figure">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>.align    16,0x90
<span class="number-left">2</span>.globl    _Z8asm8x1x8PdS_S_
<span class="number-left">3</span>_Z8asm8x1x8PdS_S_:
<span class="number-left">4</span># parameter 1: %rdi (a)
<span class="number-left">5</span># parameter 2: %rsi (b)
<span class="number-left">6</span># parameter 3: %rdx (c)
<span class="number-left">7</span>	vmovapd (%rdi), %zmm0	
<span class="number-left">8</span>	vmovapd (%rsi), %zmm1
<span class="number-left">9</span>	vmovapd (%rdx), %zmm2
<span class="number-left">10</span>​
<span class="number-left">11</span>	vfmadd231pd %zmm1, %zmm0, %zmm2
<span class="number-left">12</span>	vmovapd 64(%rdx), %zmm3
<span class="number-left">13</span>	vfmadd231pd %zmm1{cdab}, %zmm0, %zmm3
<span class="number-left">14</span>​
<span class="number-left">15</span>	vmovapd 128(%rdx), %zmm4
<span class="number-left">16</span>	vfmadd231pd %zmm1{badc}, %zmm0, %zmm4
<span class="number-left">17</span>​
<span class="number-left">18</span>	vmovapd 192(%rdx), %zmm5
<span class="number-left">19</span>	vmovapd %zmm1{badc}, %zmm1
<span class="number-left">20</span>	vfmadd231pd %zmm1{cdab}, %zmm0, %zmm5
<span class="number-left">21</span>​
<span class="number-left">22</span>	vpermf32x4 $78, %zmm1, %zmm1
<span class="number-left">23</span>​
<span class="number-left">24</span>	vmovapd 256(%rdx), %zmm6
<span class="number-left">25</span>	vfmadd231pd %zmm1, %zmm0, %zmm6
<span class="number-left">26</span>​
<span class="number-left">27</span>	vmovapd 320(%rdx), %zmm7
<span class="number-left">28</span>	vfmadd231pd %zmm1{cdab}, %zmm0, %zmm7
<span class="number-left">29</span>​
<span class="number-left">30</span>	vmovapd 384(%rdx), %zmm8
<span class="number-left">31</span>	vfmadd231pd %zmm1{badc}, %zmm0, %zmm8
<span class="number-left">32</span>​
<span class="number-left">33</span>	vmovapd 448(%rdx), %zmm9
<span class="number-left">34</span>	vmovapd %zmm1{badc}, %zmm1
<span class="number-left">35</span>	vfmadd231pd %zmm1{cdab}, %zmm0, %zmm9
<span class="number-left">36</span>​
<span class="number-left">37</span>	vmovapd %zmm2, (%rdx)
<span class="number-left">38</span>	vmovapd %zmm3, 64(%rdx)
<span class="number-left">39</span>	vmovapd %zmm4, 128(%rdx)
<span class="number-left">40</span>	vmovapd %zmm5, 192(%rdx)
<span class="number-left">41</span>	vmovapd %zmm6, 256(%rdx)
<span class="number-left">42</span>	vmovapd %zmm7, 320(%rdx)
<span class="number-left">43</span>	vmovapd %zmm8, 384(%rdx)
<span class="number-left">44</span>	vmovapd %zmm9, 448(%rdx)	
<span class="number-left">45</span>​
<span class="number-left">46</span>	ret
</pre>
</div>
<div class="caption">
Figure 7.7 Assembly code for <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span>, where <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, <span class="formula"><i>C</i></span> are <span class="formula">8 × 1</span>, <span class="formula">1 × 8</span>, and <span class="formula">8 × 8</span> matrices, respectively.
</div>

</div>

</div>

</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:xphi-diags"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter6/xphi_diags.png" alt="figure FIGS/chapter6/xphi_diags.png" style="max-width: 274px; max-height: 116px;"/>
<div class="caption">
Figure 7.8 Eight diagonals of an <span class="formula">8 × 8</span> matrix.
</div>

</div>

</div>

</div>
Figure <a class="Reference" href="#fig:xphi-diags">7.8↑</a> shows the eight diagonals of an <span class="formula">8 × 8</span> matrix. It is assumed that <span class="formula"><i>C</i></span> is stored diagonal by diagonal, with the eight entries of diagonal 1 preceding those of diagonal 2 and so on. This is not an overly restrictive assumption. If each <span class="formula">8 × 8</span> submatrix of a large <span class="formula">8<i>n</i> × 8<i>n</i></span> matrix is stored in this skewed manner, passing back and forth between the skewed and unskewed formats needs to be done only once and is much cheaper than the cost of multiplying such large matrices.
</div>
<div class="Indented">
In figure <a class="Reference" href="#fig:xphi-microk">7.7↑</a>, the function name <tt>asm8x1x8()</tt> is mangled according to C++/Linux conventions (lines 2 and 3). A C++ program (but not a C program) can call the function as simply <tt>asm8x1x8()</tt>. The first three arguments, which are pointers to the matrices <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span>, are passed in the three registers <tt>%rdi</tt>, <tt>%rsi</tt>, and <tt>%rdx</tt> (lines 4, 5, and 6), conforming with the GNU/Linux convention. 
</div>
<div class="Indented">
On lines 7 and 8, the matrices <span class="formula"><i>A</i></span> and <span class="formula"><i>B</i></span> are loaded into the registers <tt>ZMM0</tt> and <tt>ZMM1</tt>, respectively. On line 9, the first diagonal of <span class="formula"><i>C</i></span> is loaded into <tt>ZMM2</tt>. On line 11, the first diagonal of <span class="formula"><i>C</i></span> is updated using the <tt>vfmadd231pd</tt> instruction.
</div>
<div class="Indented">
On line 12, the second diagonal is loaded into the register <tt>ZMM3</tt>. When it is updated on line 13, notice that the <tt>ZMM1</tt> register storing the matrix <span class="formula"><i>B</i></span> is swizzled as <tt>{cdab}</tt>. The <tt>{badc}</tt> swizzle is used on line 16 to update the third diagonal. Both the swizzles are used (lines 19 and 20) to update the fourth diagonal.
</div>
<div class="Indented">
On line 22, the upper and lower halves of the <tt>ZMM1</tt> register, which stores the matrix <span class="formula"><i>B</i></span>, are swapped. The other four diagonals are updated and then stored back in <span class="formula"><i>C</i></span> (lines 37 through 44). 
</div>
<div class="Indented">
This program may be modified to compute the product of an <span class="formula">8 × <i>n</i></span> matrix <span class="formula"><i>A</i></span>, stored in column-major order, with an <span class="formula"><i>n</i> × 8</span> matrix <span class="formula"><i>B</i></span>, stored in row-major order. The product of those two matrices may be computed as the sum of <span class="formula"><i>n</i></span> outer products of <span class="formula">8 × 1</span> matrices (any of <span class="formula"><i>n</i></span> columns of <span class="formula"><i>A</i></span>) with <span class="formula">1 × 8</span> matrices (the corresponding row of <span class="formula"><i>B</i></span>). Each product is added to the matrix <span class="formula"><i>C</i></span>. To modify the program, one simply has to step through the columns of <span class="formula"><i>A</i></span> and the rows of <span class="formula"><i>B</i></span> while reproducing the logic of figure <a class="Reference" href="#fig:xphi-microk">7.7↑</a>. The loading and storing of <span class="formula"><i>C</i></span> needs to be done just once. The choice of <span class="formula"><i>n</i></span> should be large enough to result in many arithmetic operations for every load/store instruction but not so large as to overflow the instruction cache.
</div>
<div class="Indented">
With <span class="formula"><i>n</i> = 48</span>, this microkernel, running on <span class="formula">244</span> threads, reaches <span class="formula">473</span> GFlops/sec on a single Phi coprocessor. The theoretical peak is slightly more than twice as high. Even getting to <span class="formula">50</span>% of peak bandwidth is not bad. We have seen that MKL’s optimized FFT reaches only <span class="formula">15</span>% of the peak. For matrix multiplication, the microkernel can get close to <span class="formula">100</span>% of the peak bandwidth, but writing such a microkernel will require much greater attention to the instruction pipeline. We have paid little attention to the U- and V-pipes. In a nearly optimal microkernel, one has to arrange for almost all non fast-multiply-add instructions to go to the V-pipe. There are many bypasses between pipeline stages, which makes it less expensive to reuse recently computed destinations as sources. So one must take advantage of bypasses as well. These topics as well as instruction alignment are discussed in chapter 3.
</div>
<h? class="Subsubsection">
<b><u>Matrix multiplication using MKL</u></b>
</h?>
<div class="Unindented">
The function <tt>mmult()</tt>, whose definition follows, assumes all three matrices <span class="formula"><i>A</i>, <i>B</i>, <i>C</i></span> to be square of dimension <tt>dim</tt> and to be stored in column-major order with leading dimension equal to <tt>dim</tt>. It makes a single call to the MKL library. The matrices are stored in the arrays <tt>a[]</tt>, <tt>b[]</tt>, and <tt>c[]</tt>, respectively.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">__declspec(target(mic))
void mmult(double *a, double *b, double *c, int dim){
	char transa[3] = "N";
	char transb[3] = "N";
	
	double alpha = 1;
	dgemm(transa, transb, &amp;dim, &amp;dim, &amp;dim, &amp;alpha,
	      a, &amp;dim, b, &amp;dim,
	      &amp;alpha,
	      c, &amp;dim);
}
</pre>
</div>

</div>
<div class="Indented">
This function <tt>mmult()</tt> may be called from the host, natively from a MIC device, or offloaded to a MIC device from the host. It may also be called from the host with automatic offload. 
</div>
<div class="Indented">
With automatic offload, the details of how the operation is offloaded to the MIC device are left to the MKL library. The syntax for automatic offload is shown below.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	mkl_mic_enable();
	mmult(a, b, c, dim);
	mkl_mic_disable();
</pre>
</div>

</div>
<div class="Indented">
The MKL functions to enable and disable automatic offload must be called from the host. 
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:xphi-mmult-bw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>n</i></span>
</td>
<td align="center" valign="top">
AVX (host)
</td>
<td align="center" valign="top">
MIC (native)
</td>
<td align="center" valign="top">
MIC (offload)
</td>
<td align="center" valign="top">
MKL Auto
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">8, 000</span>
</td>
<td align="center" valign="top">
<span class="formula">352</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">837</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">299</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">731</span> GF/s
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10, 000</span>
</td>
<td align="center" valign="top">
<span class="formula">333</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">845</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">365</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">926</span> GF/s
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">12, 000</span>
</td>
<td align="center" valign="top">
<span class="formula">339</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">859</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">412</span> GF/s
</td>
<td align="center" valign="top">
<span class="formula">1214</span> GF/s
</td>

</tr>

</table>

</div>
<div class="caption">
Table 7.5 Floating point bandwidths in GFlops/sec.
</div>

</div>

</div>
On the <span class="formula">2.7</span>  GHz AVX host (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a> for its full name), the floating point throughput is <span class="formula">100</span>% of the theoretical peak (see table <a class="Reference" href="#tab:xphi-mmult-bw">7.5↑</a>). In fact, for <span class="formula"><i>n</i> = 8, 000</span>, the floating point throughput is slightly greater than the theoretical limit of <span class="formula">345.6</span> GFlops/s. The discrepancy is probably because the in-core &ldquo;turbo&rdquo; frequency of <span class="formula">3.5</span> GHz is greater than the input clock frequency of <span class="formula">2.7</span> GHz.
</div>
<div class="Indented">
The MIC device reaches nearly <span class="formula">85</span>% of its peak floating point throughput when run natively. Offloading the function <tt>mmult()</tt> to MKL does not do so well for unknown reasons. MKL appears to be set up for automatic offload. If part of the computation is to be offloaded to a MIC device, it is better to leave the offloading to MKL. With automatic offloading and <span class="formula"><i>n</i> = 12, 000</span>, MKL manages to nearly sum the floating point throughputs of the MIC device and the AVX host to reach <span class="formula">1.2</span> TFlops/sec.<span class="FootOuter"><span class="SupFootMarker"> [131] </span><span class="HoverFoot"><span class="SupFootMarker"> [131] </span>The 1 TFlops/sec barrier was crossed by a supercomputer for the first time in 1997.</span></span>
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  The name of the function <tt>asm8x1x8()</tt> is mangled according to C++ conventions. Unmangle the name and supply a header file to make the function callable from both C and C++.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write an LU factorization routine for the AVX-512 instruction set of the Xeon Phi. Compare with the speed of MKL library’s implementation of the same function. By what factor do you expect your routine to fall short if it does not use any instruction-level optimizations?
</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-8">8</a> Special Topic: Graphics Coprocessor Programming Using CUDA<a class="Label" name="chap:Graphics-co-processor-programmin"> </a>
</h1>
<div class="Unindented">
Graphics libraries such as DirectX and OpenGL are widely used in the computer gaming industry. The graphics libraries provide a number of functions for rendering, shading, texture manipulation, and similar tasks. Graphics devices accelerate the execution of such library functions. If a suitable graphics card and driver are installed, these functions are sent from the processor to the graphics card for faster execution.
</div>
<div class="Indented">
The graphics cards are powerful processors in their own right. A high-end graphics card may have more than a billion transistors, same as a processor package. The instruction set and design of the graphics processor reflects its principal function. The graphics processor prefers to think in terms of pixels, and the instruction set architecture supports threading at a very fine level. If there is 1 GB of data to be handled, an OpenMP program may divide the data between a dozen threads. For a graphics processor program, it is natural to split the data between <span class="formula">10<sup>4</sup></span> or <span class="formula">10<sup>5</sup></span> or even more threads. 
</div>
<div class="Indented">
Despite the focus on pixels and images, the capabilities of graphics processors are fairly general. Such is the diversity and changeability of functions in DirectX and other libraries that it makes little sense for a graphics processor to attempt to hardwire the functions directly. Instead the instruction set architecture is built up using primitives, many of which are fairly general purpose. The graphics device is not flexible enough to run a web server or word processor effectively, but may be used as a coprocessor in many scientific applications.
</div>
<div class="Indented">
As long as graphics devices and drivers are set up mainly to run graphics library functions, it is very hard to use them for scientific computing. In 2007, NVIDIA introduced the Compute Unified Device Architecture (CUDA) framework. CUDA added software layers to the device drivers and the GNU C/C++ compiler to greatly simplify the task of programming graphics coprocessors for scientific use. 
</div>
<div class="Indented">
The use of graphics coprocessors in scientific computation became an immediate sensation. Although there was some hype, it is undeniable that graphics coprocessors were significantly faster for many important applications. The innovative design of NVIDIA’s Tesla, Fermi, and now Kepler and Maxwell devices gives a hint of what can be done with a billion transistors to go beyond the constraints of the x86 architecture and its reliance on backward binary compatibility. Systems with NVIDIA coprocessors began to appear near the top of the Top 500 list in 2010.
</div>
<div class="Indented">
Intel, the foremost champion of the x86 architecture, has closed the gap considerably to the extent that the topmost supercomputer in 2015 uses Intel processors as well as coprocessors. One advantage enjoyed by the CUDA framework is its compilation model, which is the topic of section <a class="Reference" href="#sec:gpu-cuda-intro">8.2↓</a>. Graphics devices attain backward compatibility using an intermediate assembly language that can be mapped to a variety of instruction sets. Removing the constraint of backward binary compatibility creates greater room for rapid innovation in the design of the graphics hardware. It may lower the cost of design as well.
</div>
<div class="Indented">
A disadvantage is that programming graphics coprocessors is harder, much harder, than programming processors and likely to remain that way. In OpenMP, we may split the problem across threads or processes and write much of the program as if it were a sequential program. It is true that subtleties of parallel programming can never be eliminated entirely. Yet the difficulties of parallel programming may be localized to a considerable extent, which is true with Pthreads as well. Such a thing is not possible with CUDA’s highly refined parallelism. A thread must always be conscious that it is part of a warp of <span class="formula">32</span> threads and that multiple warps compose a thread block. The arrangement of threads into blocks and grids affects the structure of even the innermost loops. Writing a program to sum a sequence of numbers becomes quite a task, as we will see in section <a class="Reference" href="#sec:gpu-cuda-intro">8.2↓</a>.
</div>
<div class="Indented">
In the previous chapter, we pointed out the widening gap between programming skill and what it takes to program modern computers optimally. This is a particular concern with respect to CUDA, where even simple tasks can require complex programming. Gamers who pay for graphics devices surely make good use of them, and if Apple Computers is profitable, the people who pay for its products make heavy use of what they buy. In comparison, this author is not convinced that coprocessor deployment in the scientific world matches their utilization.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:gpu-chipset"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter7/gpu.png" alt="figure FIGS/chapter7/gpu.png" style="max-width: 174px; max-height: 116px;"/>

</div>
<div class="caption">
Figure 8.1 Graphics Processing Unit (GPU) is connected to the chipset via the PCIe bus. Both system memory and GPU memory are DRAM.
</div>

</div>

</div>

</div>
<div class="Indented">
The organization of this chapter is similar to that of the previous chapter on the Xeon Phi. Section <a class="Reference" href="#sec:gpu-cuda-architecture">8.1↓</a> introduces the architecture of NVIDIA’s coprocessors, and section <a class="Reference" href="#sec:gpu-cuda-intro">8.2↓</a> is an introduction to programming those devices using CUDA. The examples in section <a class="Reference" href="#sec:gpu-cuda-examples">8.3↓</a> further illustrate how to program graphics coprocessors in relation to architectural considerations. The location of the Graphics Processing Unit (GPU) relative to the processor node and the chip set is similar to that of the Phi as shown in figure <a class="Reference" href="#fig:gpu-chipset">8.1↑</a>.
</div>
<div class="Indented">
In the introduction to the previous chapter, we stated an opinion that coprocessors break modularity at the hardware level. This opinion is applicable to an even greater extent to GPU computing. As long as graphics devices are used to accelerate libraries such as DirectX, their usage is perfectly modular. In effect, the graphics device is a hardware library and fits  nicely into the modular framework. Indeed, the traditional use of graphics devices is an excellent example of modularity at the hardware level. When graphics devices are used for general-purpose computing within the CUDA framework, the situation is different. The CUDA model with its highly refined parallelism differs radically from ordinary C/C++. The heterogeneity that results is quite severe.
</div>
<div class="Indented">
The basis of modular programming is to break up a task into subtasks and the subtasks into yet more subtasks in a hierarchical manner. Uniformity or near uniformity in subdivision promotes modularity. In GPU computing, GPU devices with pixel-level parallelism compete with processor packages. The programming models are quite different. It is difficult to sustain modularity when faced with discordance at such a fundamental level, and the resulting programs are likely to be difficult to maintain.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-8.1">8.1</a> Graphics coprocessor architecture <a class="Label" name="sec:gpu-cuda-architecture"> </a>
</h2>
<div class="Unindented">
The graphics device sits next to the processor node, as in figure <a class="Reference" href="#fig:gpu-chipset">8.1↑</a>. The K20 graphics device we use has <span class="formula">13</span> streaming multiprocessors. Each of these is roughly comparable to a processor core of the <span class="formula">16</span>-core <span class="formula">2.7</span>  GHz AVX host (for the full name of this computer, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>). 
</div>
<div class="Indented">
Internally, a streaming multiprocessor is different from a processor core. In section <a class="Reference" href="#sub:gpu-capability">8.1.1↓</a>, we begin by examining the capability of the graphics device using a simple program. This program is run on the host and involves none of the complexity of CUDA programming. This program may be used to detect a number of available parameters, such as the major and minor revision number, clock rate, and available graphics memory. 
</div>
<div class="Indented">
Sections <a class="Reference" href="#sub:gpu-hostdevice-memory">8.1.2↓</a> and <a class="Reference" href="#sub:gpu-timing">8.1.3↓</a> introduce utilities that simplify programming. These utilities, too, run on the host. Section <a class="Reference" href="#sub:gpu-hostdevice-memory">8.1.2↓</a> presents a simple class for moving data to and back from the graphics device. The timing class in section <a class="Reference" href="#sub:gpu-timing">8.1.3↓</a> uses CUDA events but has a <tt>tic()/toc()</tt> interface similar to the <tt>TimeStamp</tt> class.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:gpu-warps-threads">8.1.4↓</a> is an overview of the K20’s architecture. Basic knowledge of registers, warps, and thread blocks is essential for CUDA programming.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.1.1">8.1.1</a> Graphics processor capability<a class="Label" name="sub:gpu-capability"> </a>
</h3>
<div class="Unindented">
NVIDIA graphics processors come in a great variety. The number of devices that are CUDA enabled and are of compute capability anywhere from 1.0 to 3.5 is more than 100. The first task is to find out exactly which graphics processor is available and what its capability is.
</div>
<div class="Indented">
On a single computer system, several CUDA-enabled graphics processors may be available via the PCIe bus. With the following two lines, we can find out the number of CUDA-enabled graphics processors available: 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	int count;
	cudaGetDeviceCount(&amp;count);
</pre>
</div>

</div>
<div class="Indented">
CUDA programs look much like C programs. However, certain parts of the program run on the host, which is typically an x86 processor running either Linux or Windows, and certain other parts run on the device, which is the graphics processor. The <tt>cudaGetDeviceCount()</tt> function runs on the host. By invoking the operating system or some other means, it finds out the number of devices of capability 1.0 or higher. A program using this function call can be compiled using <tt>nvcc</tt>, which is NVIDIA’s compiler driver. The compiler driver <tt>nvcc</tt> is a sophisticated wrapper around <tt>gcc</tt>, and in this case all that <tt>nvcc</tt> has to do is link the library in which <tt>cudaGetDeviceCount()</tt> is defined. No special option to <tt>nvcc</tt> is necessary. This and other similar functions are declared in <tt>cuda_runtime.h</tt>.<span class="FootOuter"><span class="SupFootMarker"> [132] </span><span class="HoverFoot"><span class="SupFootMarker"> [132] </span>The three main sources of CUDA documentation are (<i>CUDA C Programming Guide</i>, August, 2014), (<i>CUDA Compiler Driver NVCC</i>, August, 2014), and (<i>CUDA Runtime API</i>, August, 2014). Functions like <tt>cudaGetDeviceCount()</tt> are documented in the last of these sources.</span></span>
</div>
<div class="Indented">
Once we know the device count, we can find out about each device.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">    cudaDeviceProp prop;
    cudaGetDeviceProperties(&amp;prop, i);
</pre>
</div>

</div>
<div class="Indented">
Here <tt>cudaDeviceProp</tt> is a structure type with fields corresponding to various properties of the device. The structure is defined in the <tt>cuda_runtime.h</tt> header file. We do not have to bother to include the header file explicitly in our C++ program because <tt>nvcc</tt> will include the right header automatically. The <tt>cudaGetDeviceProperties()</tt> function may talk to the operating system to find out the properties of device number <tt>i</tt>, where <tt>i</tt> is the second argument in the function call. Alternatively, the information about device properties may be left in some location during the installation of the <tt>nvcc</tt> driver. In either case, no code is generated that must run on the graphics device.
</div>
<div class="Indented">
The <tt>prop</tt> variable, which is set up in this way, has information about the graphics device. The line
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">    cout&lt;&lt;"Device Name: "&lt;&lt;prop.name&lt;&lt;endl;
</pre>
</div>

</div>
<div class="Indented">
can be used to print the name of the device. Other fields of the structure <tt>prop</tt> are <tt></tt>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">clockRate
major
minor
totalGlobalMem
l2CacheSize
sharedMemPerBlock
regsPerBlock
maxThreadsPerMultiProcessor
warpSize
maxThreadsPerBlock
maxThreadsDim
maxGridSize
deviceOverlap
</pre>
</div>

</div>
<div class="Indented">
Part of the information returned on our machine was as follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">                       Device Count: 1
                        Device Name: Tesla K20m
                  Clock rate in GHz: 0.7055
              Major revision number: 3
              Minor revision number: 5
                Global memory in GB: 5.03271
</pre>
</div>

</div>
<div class="Indented">
The rest of the output will be given later. The machine we are using has only one graphics device, and it is Tesla K20m. The name of the device is slightly confusing. Tesla, Fermi, Kepler, and Maxwell are the names of successive generations of NVIDIA microarchitecture. The K20m’s microarchitecture is Kepler. The Tesla in its name does not indicate its microarchitecture but appears to signify that it is targeted at the scientific computing market.
</div>
<div class="Indented">
Its clock speed is only <span class="formula">705.5</span> MHz. Having a slow clock enables the K20m to be bigger and carry more transistors. 
</div>
<div class="Indented">
The major and minor revision numbers give the compute capability as 3.5. Graphics device functionality depends on the compute capability. For instance, only devices of capability 1.3 or higher support double-precision arithmetic. 
</div>
<div class="Indented">
The K20 device has <span class="formula">5.03</span> GB of memory. This DRAM memory is directly connected to the graphics device (see figure <a class="Reference" href="#fig:gpu-chipset">8.1↑</a>). For a large-scale computation, <span class="formula">5</span> GB of memory may prove to be too little. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.1.2">8.1.2</a> Host and device memory<a class="Label" name="sub:gpu-hostdevice-memory"> </a>
</h3>
<div class="Unindented">
To set up any computation on the graphics device, data must be transferred from host to device memory. To retrieve the result of a computation on the graphics device, data must be transferred in the other direction from device to host memory. We use the following C++ class to transfer between host and device memory:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>template&lt;class ttype &gt; class dhstmem{
<span class="number-left">2</span>private:
<span class="number-left">3</span>	ttype  *devicemem;
<span class="number-left">4</span>	ttype  *hostmem;
<span class="number-left">5</span>	int n;
<span class="number-left">6</span>	cudaError_t errcode;
<span class="number-left">7</span>public:
<span class="number-left">8</span>	dhstmem(long int nin){
<span class="number-left">9</span>		...
<span class="number-left">10</span>	}
<span class="number-left">11</span>	~dhstmem(){
<span class="number-left">12</span>		...
<span class="number-left">13</span>	}
<span class="number-left">14</span>	ttype  *device(){return devicemem;}
<span class="number-left">15</span>	ttype  *host(){return hostmem;}
<span class="number-left">16</span>	void device2host(){
<span class="number-left">17</span>		...
<span class="number-left">18</span>	}
<span class="number-left">19</span>	void host2device(){
<span class="number-left">20</span>		...
<span class="number-left">21</span>	}
<span class="number-left">22</span>};
</pre>
</div>

</div>
<div class="Indented">
This class is templated. It can be used for data of type <tt>double</tt>, <tt>int</tt>, or some other type. The private section of the class is given in full on lines 3 to 6, but the definitions of some of the public member functions are not shown. In the source, the definitions are included within the class itself but have been removed for better readability. They will be given later. 
</div>
<div class="Indented">
If we want memory equal to a million doubles, we can get that as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">dhstmem&lt;double&gt; dhmem(1000*1000);
</pre>
</div>

</div>
<div class="Indented">
The object <tt>dhmem</tt> will hold pointers to a million doubles in device memory and a million doubles in host memory. The private field <tt>dhmem.n</tt> will be set to a million. If we say 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double *hmem = dhmem.host();
</pre>
</div>

</div>
<div class="Indented">
then <tt>hmem</tt> becomes a pointer to the million doubles allocated in host memory. This is evident from line 15 of the listing. We can initialize this memory in any way we want. If we say
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double *dmem = dhmem.device();
</pre>
</div>

</div>
<div class="Indented">
then <tt>dmem</tt> becomes a pointer to device memory. Although <tt>dmem</tt> is declared and defined on the host, we are not allowed to use it in host code to touch device memory. Typically, <tt>dmem</tt> is passed by the host code as an argument to a function that will run on the graphics device. The function that runs on the graphics device is allowed to use <tt>dmem</tt> to access device memory. Despite its definition to be of type <tt>double *</tt>, <tt>dmem</tt> is not an ordinary pointer. In some way, it should encode information about memory set aside on the device. When a device function is called with this pointer as an argument, that information travels from the host to the device through a device driver. 
</div>
<div class="Indented">
Suppose we initialize host memory and want to copy it to device memory. The following call does that:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">dhmem.host2device();
</pre>
</div>

</div>
<div class="Indented">
After this function call, the data in the host memory that <tt>hmem</tt> points to will be copied to device memory that <tt>dmem</tt> points to. A call such as this has to be routed through the device driver. Because the graphics processor is a peripheral device connected using the PCIe bus, the only way the host can access graphics memory is by talking to the graphics coprocessor using its device driver. Conversely,
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">dhmem.device2host();
</pre>
</div>

</div>
<div class="Indented">
copies from the device to host memory. 
</div>
<div class="Indented">
The class constructor and destructor are defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	dhstmem(long int nin){
<span class="number-left">2</span>		n = nin;
<span class="number-left">3</span>		errcode = cudaMalloc((void **)&amp;devicemem, 
<span class="number-left">4</span>				     n*sizeof(ttype));
<span class="number-left">5</span>		assrt(errorcode==cudaSuccess);
<span class="number-left">6</span>		hostmem = new ttype[n];
<span class="number-left">7</span>	}
<span class="number-left">8</span>	~dhstmem(){
<span class="number-left">9</span>		delete[] hostmem;
<span class="number-left">10</span>		cudaFree(devicemem);
<span class="number-left">11</span>	}
</pre>
</div>

</div>
<div class="Indented">
The call to <tt>cudaMalloc()</tt> on line 3 looks somewhat like a call to <tt>malloc()</tt> to allocate memory. A notable difference is that whereas <tt>malloc()</tt> returns the pointer to allocated memory, <tt>cudaMalloc()</tt> returns an error code. The first argument of <tt>cudaMalloc()</tt> is an address, and the location that address points to is altered to correspond to the allocated memory. In the case of <tt>malloc()</tt> or <tt>new[]</tt> (line 6), the memory allocated is obtained from the operating system. In the case of <tt>cudaMalloc()</tt>, the memory allocated must be obtained from the graphics device. These definitions are inside the class, which is why the names of the constructor and the destructor are not qualified using <tt>template&lt;class ttype&gt; dhstmem::</tt>.
</div>
<div class="Indented">
The public member functions used for transferring between host and device memory are defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	void device2host(){
		errcode = cudaMemcpy(hostmem, devicemem, 
		  n*sizeof(ttype),cudaMemcpyDeviceToHost);
	}
	void host2device(){
		errcode = cudaMemcpy(devicemem, hostmem, 
		 n*sizeof(ttype),cudaMemcpyHostToDevice);
	}
​
</pre>
</div>

</div>
<div class="Indented">
Both member functions use <tt>cudaMemcpy()</tt> to effect the transfer (lines 2 and 9). The last argument is either of the flags 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">cudaMemcpyDeviceToHost OR cudaMemcpyHostToDevice
</pre>
</div>

</div>
<div class="Indented">
to indicate the direction of the transfer. The function <tt>cudaMemcpy()</tt> and the flags are declared and defined in header files and libraries that are automatically included and linked by the <tt>nvcc</tt> driver.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.1.3">8.1.3</a> Timing CUDA kernels<a class="Label" name="sub:gpu-timing"> </a>
</h3>
<div class="Unindented">
The simplest programs have much to teach us as long as we time them carefully. Therefore, we begin by looking at mechanisms to time kernels that run on the graphics coprocessor. 
</div>
<div class="Indented">
We use the following class to time the graphics processor. The private data members of the class are shown in full. For the public member functions, the interface is shown, and the definitions will be given later. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class hstTimer{
private:
	cudaEvent_t start, stop;
public:
	hstTimer(){...};
	~hstTimer(){...};
	void tic(){...};
	float toc(){...};
};
</pre>
</div>

</div>
<div class="Indented">
The private data members are two events named <tt>start</tt> and <tt>stop</tt>. The events will be &ldquo;sent&rdquo; to the graphics processor to record the time on the graphics processor and will be relayed back to the host.
</div>
<div class="Indented">
To time a piece of code, we can use the class as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">hstTimer hclk;
hclk.tic();
[code]
float tms = hclk.toc();
</pre>
</div>

</div>
<div class="Indented">
The variable <tt>tms</tt> gets set to the time elapsed between <tt>tic()</tt> and <tt>toc()</tt> in milliseconds.
</div>
<div class="Indented">
All member functions of <tt>hstTimer</tt> are defined within the scope of the class. Therefore, the names of the member functions are not qualified with <tt>hstTimer::</tt> in their definitions shown below. The constructor and the destructor are unremarkable.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">	hstTimer(){
		cudaEventCreate(&amp;start);
		cudaEventCreate(&amp;stop);
	}
	~hstTimer(){
		cudaEventDestroy(start);
		cudaEventDestroy(stop);
	}
</pre>
</div>

</div>
<div class="Indented">
The events <tt>start</tt> and <tt>stop</tt> are probably opaque pointers.<span class="FootOuter"><span class="SupFootMarker"> [133] </span><span class="HoverFoot"><span class="SupFootMarker"> [133] </span>Opaque pointers point to structures whose internal details are either hidden from us or we don’t care about.</span></span> Therefore, it is natural that their addresses are passed during event creation and the pointers themselves are passed during event destruction. It is good practice to check the error codes returned during event creation and destruction, although we have not done so here.
</div>
<div class="Indented">
The member functions <tt>tic()</tt> and <tt>toc()</tt> are defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	void tic(){
<span class="number-left">2</span>		cudaEventRecord(start, 0);
<span class="number-left">3</span>	}
<span class="number-left">4</span>	float toc(){
<span class="number-left">5</span>		float time;
<span class="number-left">6</span>		cudaEventRecord(stop, 0);
<span class="number-left">7</span>		cudaEventSynchronize(start);
<span class="number-left">8</span>		cudaEventSynchronize(stop);
<span class="number-left">9</span>		cudaEventElapsedTime(&amp;time, start, stop);
<span class="number-left">10</span>		return time;//milliseconds
<span class="number-left">11</span>	}
</pre>
</div>

</div>
<div class="Indented">
The call to <tt>cudaEventRecord()</tt> on line 2 has the effect of sending the event from the host to the graphics device in a streaming fashion. The second argument, which is zero on line 2, merits discussion. The host can talk to the graphics device over several streams, and the second argument can be used to indicate the stream on which the event is recorded. But if the second argument is 0, the event is recorded after all preceding operations in the CUDA context have been completed. 
</div>
<div class="Indented">
The <tt>stop</tt> event is recorded by the member function <tt>tic()</tt> on line 6. When CUDA events are recorded using function calls, such as the ones on lines 2 and 6, the function call returns immediately after sending the event to the graphics device. It does not wait until the event is actually recorded. The function calls are nonblocking. The events <tt>stop</tt> and <tt>start</tt> are not available to be read on the host immediately after <tt>cudaEventRecord()</tt>. The calls to <tt>cudaEventSynchronize()</tt> (lines 7 and 8) block until the events are ready to be read. On line 9, the two events are read, and the time elapsed is calculated.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.1.4">8.1.4</a> Warps and thread blocks<a class="Label" name="sub:gpu-warps-threads"> </a>
</h3>
<div class="Unindented">
<div class="float">
<a class="Label" name="fig:gpu-kepler"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter7/kepler_arch.png" alt="figure FIGS/chapter7/kepler_arch.png" style="max-width: 188px; max-height: 375px;"/>

</div>
<div class="caption">
Figure 8.2 The Kepler microarchitecture consists of <span class="formula">13</span> streaming multiprocessors and <span class="formula">6</span> memory controllers.
</div>

</div>

</div>

</div>
<div class="Indented">
The K20’s design is so different from that of conventional processors that to program it we need to understand a few things about its architecture. A sketch of its microarchitecture is shown in figure <a class="Reference" href="#fig:gpu-kepler">8.2↑</a>. 
</div>
<div class="Indented">
To get a better sense of the microarchitecture, we return to the <tt>cudaDeviceProp</tt> program of section 7.1 and show some more of its output.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">          Total L2 cache (in bytes): 1310720
    Shared memory per MP (in bytes): 49152
         Number of registers per MP: 65536
       Max number of threads per MP: 2048
</pre>
</div>

</div>
<div class="Indented">
The heart of the microarchitecture is the streaming multiprocessor, and there are <span class="formula">13</span> of these in Kepler. The K20 has <span class="formula">1.3</span> MB of L2 cache on chip, a small figure compared to the <span class="formula">20</span> MB cache of its <span class="formula">2.7</span>  GHz AVX host. The L2 cache is shared by the <span class="formula">13</span> multiprocessors.
</div>
<div class="Indented">
Each multiprocessor has <span class="formula">64</span> KB of on-chip memory for itself. This memory is split between L1 cache and shared memory. The use of shared memory is under program control. On the K20, <span class="formula">48</span> KB is assigned to shared memory by default, and only <span class="formula">16</span> KB is left for the L1 cache. The split between L1 cache and shared memory can be changed by calling a function in the CUDA runtime library.
</div>
<div class="Indented">
The K20 coprocessor’s 13 streaming multiprocessors are comparable with the AVX host’s 16 processors. The K20 begins to look very different from its Sandy Bridge host if we note that each multiprocessor has as many as <span class="formula">64<span class="text"> K</span> = 65, 536</span> registers, each <span class="formula">32</span> bits wide. The AVX processor’s register file of sixteen <span class="formula">256</span>-bit YMM registers looks puny in comparison.
</div>
<div class="Indented">
A large register file is central to how the K20 and other graphics coprocessors work. A large number of threads can be resident on the multiprocessor, and each thread gets its own subset of registers. Because the register file is split between threads, the streaming multiprocessor can switch between threads with zero overhead. 
</div>
<div class="Indented">
Each multiprocessor in the K20 can hold at most <span class="formula">2, 048</span> threads. Therefore, each thread gets at least <span class="formula">64</span> registers.
</div>
<div class="Indented">
When Intel brought out the Xeon Phi to reclaim ground in supercomputing, the large number of registers of the K20 and other graphics coprocessors was a key parameter the Xeon Phi matched. The Xeon Phi has <span class="formula">61</span> processor cores with four threads resident on each processor. The register file consisting of 32 <span class="formula">512</span>-bit ZMM registers is replicated for each thread. So the total number of registers on the Phi is equivalent to <span class="formula">125</span>K <span class="formula">32</span>-bit registers, which is approximately a sixth of the number of registers on the K20.
</div>
<div class="Indented">
A thread on a graphics device such as the K20 is different from a thread on an x86 machine or the Phi. To get a sense of how threads work in the K20, we give the rest of the output of the <tt>cudaDeviceProp</tt> program from section 7.1.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">         Number of threads per warp: 32
Maximum number of threads per block: 1024
 Maximum of each dimension of block: 1024 x 1024 x 64
  Maximum of each dimension of grid: ---&gt;
  	              ---&gt; 2147483647 x 65535 x 65535
</pre>
</div>

</div>
<div class="Indented">
Warps, thread blocks, and grids of thread blocks are the basis of GPU programming.
</div>
<div class="Indented">
On the graphics device, a thread never really exists by itself. Threads are grouped into warps. On the K20 and most NVIDIA devices, a warp is <span class="formula">32</span> threads. The warp is fundamental to GPU programming. Instructions are executed by the entire warp and not by individual threads. 
</div>
<div class="Indented">
The threads in a warp must execute exactly the same instruction. If some threads in a warp diverge as a result of if-statements, for example, then the threads that do not enter the if-block idle and wait while other threads are still in the if-block. Thread divergence within a warp can incur heavy penalties. Because a K20 multiprocessor has a limit of <span class="formula">2, 048</span> threads, at most <span class="formula">64</span> warps can be resident on a single multiprocessor.
</div>
<div class="Indented">
In the Kepler microarchitecture, a multiprocessor selects up to four warps to execute during each cycle. Double-precision instructions use two <span class="formula">32</span>-bit registers. Therefore, only two of the four warps can be executing double-precision instructions. Kepler has the ability to dispatch two instructions per warp during every cycle. Because the <span class="formula">755</span> MHz K20’s cycles are so long, the ability to dispatch two instructions per cycle is probably essential.
</div>
<div class="Indented">
If threads are grouped into warps, warps are in turn grouped into thread blocks, as shown in figure <a class="Reference" href="#fig:gpu-warp-block-grid">8.3↓</a>. On the K20, a thread block can have at most <span class="formula">1, 024</span> threads or <span class="formula">32</span> warps. Threads are scheduled onto multiprocessors one thread block at a time. Switching between warps that are already resident on a multiprocessor has zero overhead. However, scheduling a new thread block will incur overhead.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:gpu-warp-block-grid"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter7/warp_block_grid.png" alt="figure FIGS/chapter7/warp_block_grid.png" style="max-width: 376px; max-height: 249px;"/>
<div class="caption">
Figure 8.3 Warps, thread blocks, and grids.
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
In figure <a class="Reference" href="#fig:gpu-warp-block-grid">8.3↑</a>, the thread block shown is one dimensional. In general, the threads in a thread block can be two or even three dimensional and arranged along <span class="formula"><i>x</i></span>, <span class="formula"><i>y</i></span>, and <span class="formula"><i>z</i></span> axes. For purposes of grouping threads in a multidimensional block into warps, the <span class="formula"><i>x</i></span> axis takes precedence over <span class="formula"><i>y</i></span> and <span class="formula"><i>y</i></span> over <span class="formula"><i>z</i></span>.
</div>
<div class="Indented">
Because a multiprocessor can hold <span class="formula">64</span> warps whereas a thread block can hold only <span class="formula">32</span> on the K20, in general, one should have at least two thread blocks on each of the <span class="formula">13</span> multiprocessors. The total number of thread blocks can be many more than <span class="formula">26</span>, however. The thread blocks themselves are arranged in a grid. The grid is one dimensional in figure <a class="Reference" href="#fig:gpu-warp-block-grid">8.3↑</a>, but in general it may be two or three dimensional. The number of thread blocks in a grid can even go into the millions. There will be an overhead to schedule new thread blocks on multiprocessors, but the overhead can be hidden if there is sufficient parallelism in the program. 
</div>
<div class="Indented">
The K20 has <span class="formula">13</span> multiprocessors, which can dispatch two warps executing a fused-add-multiply double-precision instruction every cycle. Its peak bandwidth is<div class="formula">
13 <span class="text"> MPs</span> × 2 <span class="text"> warps/cycle/MP</span> × 32 <span class="text"> instructions/warp</span> × <span class="mbox">2</span> <span class="text">flops/instruction</span> × <span class="mbox">0.706</span> <span class="text">GHz</span>
</div>
or <span class="formula">1.17</span> TFlops/sec, exceeding the Xeon Phi’s theoretical peak of <span class="formula">1.064</span> TFlops/sec.
</div>
<div class="Indented">
Kepler devices have six memory controllers (see figure <a class="Reference" href="#fig:gpu-kepler">8.2↑</a>). The interface to memory is <span class="formula">40</span> bytes or <span class="formula">320</span> bits wide. Thanks to prefetching and doubling of data rate, the effective memory clock is <span class="formula">5.2</span> GHz, although the actual clock will have much longer cycles. The theoretical peak bandwidth to memory is <span class="formula">208</span> GB/s. 
</div>
<div class="Indented">
In Kepler devices, the latency to memory is between <span class="formula">200</span> and <span class="formula">400</span> cycles. The latency to memory can be more than half a microsecond and is not far short of Infiniband network latencies. It takes a great deal of parallelism in the instruction stream to hide such a long latency. If there is sufficient parallelism, memory load/store instructions can be issued every cycle by all the executing warps and overlapped so that lots of memory instructions complete every cycle and effectively hide the long latency to memory.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Print the <tt>multiProcessorCount</tt> field of the <tt>prop</tt> structure and find out the number of multiprocessors on your graphics device.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the <tt>dhstmem</tt> class to determine the latency of transfers between host and device. Note that if a copy from host to device memory is less than <span class="formula">64</span> KB, the <tt>cudaMemcpy()</tt> function is nonblocking.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Use the <tt>dhstmem</tt> class to determine the bandwidth of transfers between host and device memory.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  CUDA provides a facility to associate memory transfers with streams. Memory transfers associated with different streams may be overlapped. Modify the <tt>dhstmem</tt> class to use streams and replace <tt>cudaMemcpy()</tt> by <tt>cudaMemcpyAsync()</tt>, its nonblocking version. Use the new class to estimate bidirectional bandwidth, with transfers into device overlapped with transfers from device.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Compare the <tt>hstTimer</tt> and <tt>TimeStamp</tt> classes and determine how closely they agree.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why a slower clock allows the K20 to pack more transistors.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Each multiprocessor of the K20 has <span class="formula">192</span> single-precision &ldquo;CUDA cores.&rdquo; Use this fact to explain why the peak single-precision floating point throughput is thrice, and not just twice, the double-precision throughput.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-8.2">8.2</a> Introduction to CUDA<a class="Label" name="sec:gpu-cuda-intro"> </a>
</h2>
<div class="Unindented">
Section <a class="Reference" href="#sub:gpu-leibniz">8.2.1↓</a> uses the Leibniz example to introduce rudiments of CUDA programming. The first program to sum the Leibniz series relies on help from the x86 processor. The program introduces syntax for dealing with thread blocks and grids. Even for this simple program, one needs to ensure that sufficiently many thread blocks are created and that each thread block has the right dimensions.
</div>
<div class="Indented">
The other program in section <a class="Reference" href="#sub:gpu-leibniz">8.2.1↓</a> also sums the Leibniz series but without taking help from the x86 processors. In this program, each thread computes part of the sum, and all the threads add their portion to the global result. For correctness, accesses of the global result by individual threads must be mutually exclusive. Mutual exclusion is enforced using an atomic exchange instruction supported in CUDA. The program shows how to handle warp divergence that arises during mutual exclusion. 
</div>
<div class="Indented">
The program for summing the Leibniz series is faster on the K20 than on the Phi by a factor of <span class="formula">2.5</span>. The Phi is faster for some programs and the K20 for others. The factor of <span class="formula">2.5</span> must not be treated as a universal value. In the case of the Leibniz series, the K20’s greater speed seems to be due to greater facility in handling divisions than x86 cores.
</div>
<div class="Indented">
The first program for summing the Leibniz series is perhaps not so much harder to code than the corresponding OpenMP program, which runs on the Phi or any x86 machine. However, the second program, in which the entire sum is found on the K20, is easily 100 times harder to code. In addition, the enforcement of mutual exclusion implies that there is a cost associated with creating too many thread blocks. The ability to create many thread blocks makes CUDA programming more flexible. However, for summing the Leibniz series, it can slow the program down considerably.
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:gpu-CUDA-compilation">8.2.2↓</a> introduces the CUDA compilation model. Every CUDA program may be compiled into PTX, which is an intermediate language, or into machine instructions, or both. If the same program is run on a machine with a newer instruction set, the CUDA driver extracts the PTX from the executable and automatically generates code for the newer instruction set before running the executable.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.2.1">8.2.1</a> Summing the Leibniz series<a class="Label" name="sub:gpu-leibniz"> </a>
</h3>
<div class="Unindented">
Threads are grouped into warps and warps into thread blocks. Thread blocks are arranged in a grid. The hierarchical grouping of threads can never be forgotten in writing CUDA programs. Thus, it is useful to have a utility header file <tt>const.hh</tt> that defines constants that other programs can reference.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">const int NWARP = 32;
const int THinBLK = 1024;
const int BLKinMP = 2;
const int NMP = 13;
​
const int SQRTT = 32;
const int MAXTHMP = 2048;
const int GPUCLKMHZ = 706;
</pre>
</div>

</div>
<div class="Indented">
<tt>NWARP</tt> is the number of threads in a warp.<span class="FootOuter"><span class="SupFootMarker"> [134] </span><span class="HoverFoot"><span class="SupFootMarker"> [134] </span>Alternatively, the CUDA defined variable <tt>warpSize</tt> may be used to determine the number of threads in a warp.</span></span> <tt>THinBLK</tt> is the maximum number of threads in a block. In our programs, the number of threads in a block is always set to this value. <tt>BLKinMP</tt> is the maximum number of blocks that can reside on a multiprocessor, assuming each thread block to have <span class="formula">1, 024</span> threads. The number of multiprocessors is <tt>NMP</tt>. <tt>SQRTT</tt> is the square root of the number of threads in a block. It is useful when working with two-dimensional thread blocks. The other two <tt>const</tt>s are self-explanatory.
</div>
<div class="Indented">
The only parameter we vary when launching threads is the total number of blocks, which is denoted <tt>NBLK</tt> later. This is typically at least equal to <tt>BLKinMP*NMP</tt>, or <span class="formula">26</span>, but can be much larger.
</div>
<h? class="Subsubsection">
<b><u>Summing with help from the CPU</u></b>
</h?>
<div class="Unindented">
Our first program for summing the Leibniz series 
</div>
<div class="Indented">
<div class="formula">
<i>π</i> = <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">1</span><span class="ignored">)</span></span> − <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">3</span><span class="ignored">)</span></span> + <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">5</span><span class="ignored">)</span></span> − <span class="fraction"><span class="ignored">(</span><span class="numerator">4</span><span class="ignored">)/(</span><span class="denominator">7</span><span class="ignored">)</span></span> + ⋯
</div>
on the K20 does not in fact sum the series entirely. It leaves part of the work to its AVX host. To begin with, we give the skeleton of the function that runs on the graphics device while omitting its body.<span class="FootOuter"><span class="SupFootMarker"> [135] </span><span class="HoverFoot"><span class="SupFootMarker"> [135] </span>In addition to documentation from NVIDIA, CUDA programming is explained in <span class="bibcites">[<a class="bibliocite" name="cite-59" href="#biblio-59"><span class="bib-index">59</span></a>]</span> and <span class="bibcites">[<a class="bibliocite" name="cite-58" href="#biblio-58"><span class="bib-index">58</span></a>]</span>.</span></span>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>__global__ void 
<span class="number-left">2</span>__launch_bounds__(THinBLK, BLKinMP)
<span class="number-left">3</span>leibniz(long int n, double *result){
<span class="number-left">4</span>   ...
<span class="number-left">5</span>}
</pre>
</div>

</div>
<div class="Indented">
Evidently, the definition of the <tt>leibniz</tt> function begins in a manner that is quite different from the C++ function definitions we are used to. It looks different because this function runs on the graphics device and not the x86 host.
</div>
<div class="Indented">
The <tt>__global__</tt> keyword on line 1 is part of the C language extensions in the CUDA framework. The <tt>__global__</tt> qualifier indicates that the ensuing function definition is meant to run on the graphics device and something more. Not all functions that run on the graphics device are equal. Some of them can be called only from functions that run on the graphics device, and some can be called only from functions that run on the host processor. The <tt>__global__</tt> qualifier introduces a device function that can be called from the host. Such functions are called kernels. Device functions that can be called only by kernels or other device functions are introduced using <tt>__device__</tt>. We encounter such functions later.
</div>
<div class="Indented">
Line 2 gives the launch parameters for this kernel. The launch parameters give the number of threads in a thread block and the number of thread blocks expected to be resident on a single multiprocessor.<span class="FootOuter"><span class="SupFootMarker"> [136] </span><span class="HoverFoot"><span class="SupFootMarker"> [136] </span>Strictly speaking, the launch parameters are an upper bound on the number of threads in a block and a lower bound on the number of blocks resident on a multiprocessor. The bounds are merely advisory.</span></span> The launch parameters are advisory in nature. They help the <tt>nvcc</tt> compiler determine how many registers to use in compiling the kernel.
</div>
<div class="Indented">
The complete definition of the <tt>leibniz()</tt> kernel follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>__global__ void 
<span class="number-left">2</span>__launch_bounds__(THinBLK, BLKinMP)
<span class="number-left">3</span>leibniz(long int n, double *result){
<span class="number-left">4</span>	int tid = threadIdx.x+blockIdx.x*blockDim.x;
<span class="number-left">5</span>	double ans=0;
<span class="number-left">6</span>	int step = blockDim.x*gridDim.x;
<span class="number-left">7</span>	for(long int i=tid; i &lt; n; i+=step)
<span class="number-left">8</span>		ans = ans + 4.0/(2.0*i+1.0);
<span class="number-left">9</span>	if(tid%2==1)
<span class="number-left">10</span>		ans = -ans;
<span class="number-left">11</span>	result[tid] = ans;
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
This program introduces four important CUDA defined variables on lines 4 and 6:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">threadIdx.x, blockIdx.x, blockDim.x, gridDim.x
</pre>
</div>

</div>
<div class="Indented">
These parameters determine the layout of threads (see figure <a class="Reference" href="#fig:gpu-warp-block-grid">8.3↑</a>) and the position of a particular thread within it.
</div>
<div class="Indented">
CUDA kernels are never launched one thread at a time. Every time the kernel is launched, we must specify the  number of threads in a block and the number of blocks in the grid. Because the Leibniz series is laid out in a line and for simplicity, we assume the thread block and grid to be one dimensional.
</div>
<div class="Indented">
If we imagine a thread block as a single vertical column (similar to figure <a class="Reference" href="#fig:gpu-warp-block-grid">8.3↑</a> but with the threads in a warp laid out vertically), <tt>blockDim.x</tt>, which is the number of threads in a block, is equal to the number of rows. The CUDA-defined variable <tt>threadIdx.x</tt> allows each thread to access its location along a thread block. It is then the same as the row index.
</div>
<div class="Indented">
The number of thread blocks in the grid is equal to <tt>gridDim.x</tt>. In figure <a class="Reference" href="#fig:gpu-warp-block-grid">8.3↑</a>, it is the number of columns. The index of the thread block a particular thread block belongs to is <tt>blockIdx.x</tt>. In the figure, that would be the column index. We mention again that we are assuming the kernel to be launched with one-dimensional thread blocks and grid.
</div>
<div class="Indented">
On line <span class="formula">4</span>, each thread computes <tt>tid</tt>, its position in the grid as a whole. The index calculation is identical to the one that arises in locating the <span class="formula">(<i>i</i>, <i>j</i>)</span>th entry of an <span class="formula"><i>m</i> × <i>n</i></span> matrix laid out in an array in column-major format. Its index in the array would be <span class="formula"><i>i</i> + <i>j</i> × <i>m</i></span>. Here, <tt>tid</tt> is set to 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">threadIdx.x+blockIdx.x*blockDim.x
</pre>
</div>

</div>
<div class="Indented">
in the same fashion.
</div>
<div class="Indented">
On line 6, <tt>step</tt> is set equal to the total number of threads. In the loop on lines 7 and 8, each thread picks the term in the Leibniz series that corresponds to its position and skips terms in steps of <tt>step</tt>. 
</div>
<div class="Indented">
The threads do not attempt to combine their sums into a global sum. On line 11, each thread stores its sum in the location <tt>result[tid]</tt> in global memory. Here global memory refers to the DRAM memory of the graphics processor. The task of adding all the numbers in <tt>result[]</tt> to produce the <span class="formula"><i>n</i></span>th partial sum of the Leibniz series is left to the host processor.
</div>
<div class="Indented">
Variables <tt>tid</tt>, <tt>ans</tt>, <tt>step</tt>, and <tt>i</tt> defined on lines 4, 5, 6, and 7, respectively, are local to each thread. They are typically stored in registers. 
</div>
<div class="Indented">
The host code that invokes the kernel looks as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	/*fac assumed to be defined earlier*/
<span class="number-left">2</span>	int NBLK = BLKinMP*NMP*fac; 
<span class="number-left">3</span>	int nthreads = THinBLK*NBLK;
<span class="number-left">4</span>	
<span class="number-left">5</span>	double *dresult, *result;
<span class="number-left">6</span>	dhstmem&lt;double&gt; dhmem(nthreads);
<span class="number-left">7</span>	dresult = dhmem.device();
<span class="number-left">8</span>	result = dhmem.host();
<span class="number-left">9</span>​
<span class="number-left">10</span>	leibniz&lt;&lt;&lt;NBLK, THinBLK&gt;&gt;&gt;(n, dresult);
<span class="number-left">11</span>		
<span class="number-left">12</span>	dhmem.device2host();
<span class="number-left">13</span>	double ans = 0;
<span class="number-left">14</span>	for(int i=0; i &lt; nthreads; i++)
<span class="number-left">15</span>		ans += result[i];
</pre>
</div>

</div>
<div class="Indented">
The number of thread blocks <tt>NBLK</tt> is calculated on line 2. On the K20, it is a multiple of <span class="formula">26</span> with the multiplier being <tt>fac</tt>. The total number of threads will be <tt>NBLK</tt>, the number of blocks, times <tt>THinBLK</tt>, the number of threads per block (<span class="formula">1, 024</span> in our program). Because each thread returns its sum in a separate entry of <tt>result[]</tt>, the amount of memory to be allocated on the device is set equal to the number of threads (lines 3 and 6).
</div>
<div class="Indented">
Lines 5 through 8 use the templated <tt>dhstmem</tt> class to allocate memory on the device, which is matched with the same amount of memory allocated on the host. The variable <tt>dresult</tt> (line 7) is a pointer to device memory and <tt>result</tt> (line 8) is a pointer to host memory. 
</div>
<div class="Indented">
The <tt>leibniz</tt> kernel is invoked on line 10. The parameters inside the angle brackets give the total number of thread blocks and the number of threads in each thread block. For the invocation on line 10, the thread block and grid are both one dimensional. Two- and three-dimensional thread blocks and grids use different syntax. Of course, the kernel has been written with the understanding that the thread blocks and grid used when it is invoked will be one dimensional. 
</div>
<div class="Indented">
Using triple angular brackets to specify the execution configuration of a kernel invocation, as on line 10, is a sensible choice. By themselves, <tt>&lt;</tt> and &gt; are relational operators and <tt>&lt;&lt;</tt> and <tt>&gt;&gt;</tt> are overloaded for output and input in C++. In the kernel invocation
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">leibniz&lt;&lt;&lt;NBLK, THinBLK&gt;&gt;&gt;(n, dresult);
</pre>
</div>

</div>
<div class="Indented">
the execution configuration specifies the number of thread blocks and the number of threads per thread block. 
</div>
<div class="Indented">
The arguments of the <tt>leibniz()</tt> kernel call are <tt>n</tt> and <tt>dresult</tt>. The usual pass by value semantics of C and C++ holds. The way values are passed during a kernel call is quite complicated and must involve the device driver. These complications are expertly handled by the CUDA framework and need not concern the programmer. It must be noted that the second argument <tt>dresult</tt> is a pointer to device memory, not host memory.
</div>
<div class="Indented">
If there are too many threads per thread block and the number of registers on an SM is insufficient to accommodate a thread block, the kernel will not launch. The following line may be used to catch errors:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">printf("CUDA: %s\n", 
           cudaGetErrorString(cudaGetLastError()));
</pre>
</div>

</div>
<div class="Indented">
This error does not occur in our program. As usual, error checking is omitted to improve readability of the code.
</div>
<div class="Indented">
Each thread returns its part of the partial sum of the Leibniz series in a location in device memory. On line 12, the device locations are copied to the host, and the host goes on to generate the partial sum of the Leibniz series in the variable <tt>ans</tt>. 
</div>
<div class="Indented">
A later section will discuss compilation in greater depth. Here we note that if the entire program is in the source file <tt>leibniz_all.cu</tt>, it may be compiled as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">nvcc -arch=sm_35 -o leibniz_all.exe leibniz_all.cu
</pre>
</div>

</div>
<div class="Indented">
The <span class="formula">35</span> in <tt>sm_35</tt> is a reference to the compute capability of the K20 being <span class="formula">3.5</span>.
</div>
<h? class="Subsubsection">
<b><u>Warp divergence</u></b>
</h?>
<div class="Unindented">
On the Phi or AVX host, the OpenMP program for summing the Leibniz series is almost identical to a sequential program. The program splits the work between threads, and after that point each thread runs a sequential program. At the end, the threads must perform a reduction operation to compute the total sum. The task of splitting the work is made automatic by the <tt>parallel for</tt> construct. The reduction can also be done in a number of simple ways.
</div>
<div class="Indented">
There are many, many more threads in a CUDA program, and the arrangement of threads into blocks and grids can affect the structure of even the innermost loops. We see this already in the <tt>leibniz()</tt> kernel from the way in which <tt>tid</tt> and <tt>offset</tt> are calculated. This aspect of CUDA programs will become clearer when we try to make the graphics device compute the entire sum without help from the host.
</div>
<div class="Indented">
In writing the <tt>leibniz()</tt> kernel, we paid no attention to the fact that threads are dispatched to the execution units in warps. Nor did we heed the fact that all threads in a warp execute the same instruction during the same cycle. This aspect of NVIDIA devices becomes important when making the graphics coprocessor compute the entire Leibniz sum on its own.
</div>
<div class="Indented">
Let us consider the manner in which the following code fragment is executed by threads grouped into warps:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>	int x = threadIdx.x;
<span class="number-left">2</span>	if(x%2==0)
<span class="number-left">3</span>		x = x+1;
<span class="number-left">4</span>	ans += x;
</pre>
</div>

</div>
<div class="Indented">
Because a warp is a group of 32 threads that execute the same instruction, the discussion will be more more precise if phrased using K20 instructions. However, the C statements in this code fragment map almost directly to instructions and will suffice just as well. 
</div>
<div class="Indented">
Let us suppose all <span class="formula">32</span> threads in some warp execute line 1. Once this warp instruction completes, all threads in the warp will check the condition on line 2. The condition evaluates to <span class="formula">1</span> (or true) for half the threads in the warp, and it evaluates to <span class="formula">0</span> for the other threads. The statement on line 3 will correspond to an entire instruction, and when a warp instruction is issued, only half the threads in the warp will be active. The inactive threads do nothing, but they stay in sync with the rest of the warp. Line 4 is executed by all the threads in the warp.
</div>
<div class="Indented">
Branch instructions can split the threads in a warp so that only some threads are active in some regions of the code. The hardware has to use data structures to keep track of the execution sequence of the different threads so that it knows which threads are active and which ones are not. Warp divergence means that when a warp instruction is issued, the cores corresponding to the inactive threads must idle. In addition to wasting execution cycles, the manner in which warps diverge and converge has a bearing on program correctness, as we will now see.
</div>
<h? class="Subsubsection">
<b><u>Summing it all using atomic instructions</u></b>
</h?>
<div class="Unindented">
In the following kernel, <tt>result</tt> and <tt>lock</tt> are pointers to single <tt>double</tt> and <tt>int</tt> locations, respectively. The partial sum of the Leibniz series will be put in <tt>*result</tt>. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>//result and lock must be initialized to zero.
<span class="number-left">2</span>__global__ void 
<span class="number-left">3</span>__launch_bounds__(THinBLK, BLKinMP)
<span class="number-left">4</span>leibniztotal(long int n, double* result, int* lock){
<span class="number-left">5</span>	int tid = threadIdx.x+blockIdx.x*blockDim.x;
<span class="number-left">6</span>	double ans=0;
<span class="number-left">7</span>	int step = blockDim.x*gridDim.x;
<span class="number-left">8</span>	for(long int i=tid; i &lt; n; i+=step)
<span class="number-left">9</span>		ans = ans + 4.0/(2.0*i+1.0);
<span class="number-left">10</span>	if(tid%2==1)
<span class="number-left">11</span>		ans = -ans;
<span class="number-left">12</span>	atomicAddDouble(ans, result, lock);
<span class="number-left">13</span>}
</pre>
</div>

</div>
<div class="Indented">
Two possibilities exist for warp divergence in this listing. In the for-loop on lines 8 and 9, <tt>n</tt> may be a multiple of neither <tt>step</tt> nor the warp size, which will cause exactly one warp to diverge near the end of the loop. All the warps will diverge when executing the conditional on lines 10 and 11.
</div>
<div class="Indented">
These two instances of warp divergence have little impact on program efficiency and hide no subtlety related to program correctness. The warp divergence that occurs inside the device function <tt>atomicAddDouble()</tt> called on line 12 has a more subtle effect on program execution. 
</div>
<div class="Indented">
Each thread calls <tt>atomicAddDouble()</tt> to add its <tt>ans</tt> to the global <tt>*result</tt>. Of course, it is not correct to simply use the statement <tt>*result += ans</tt>. If the threads attempt to simultaneously use the global location <tt>*result</tt>, they will get in each other’s way and <tt>*result</tt> will have an unpredictable and inconsistent value. The purpose of <tt>atomicAddDouble()</tt> is to ensure mutual exclusion between the threads and put <tt>*result += ans</tt> in a critical region that is executed by only one thread at a time. Crucially, the global locations <tt>*result</tt> and <tt>*lock</tt> must both be initialized to <span class="formula">0</span>. The threads will use the location <tt>*lock</tt> to coordinate and decide which thread has the permission to enter its critical region.
</div>
<div class="Indented">
Our first attempt at implementing <tt>atomicAddDouble()</tt> follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>__device__ void atomicAddDouble(double ans, 
<span class="number-left">2</span>         volatile double *result, volatile int *lock){
<span class="number-left">3</span>	int lockcopy=1;
<span class="number-left">4</span>	while(lockcopy==1)
<span class="number-left">5</span>		lockcopy = atomicExch((int *)lock, 1);
<span class="number-left">6</span>	*result += ans;
<span class="number-left">7</span>	atomicExch((int *)lock, 0);
<span class="number-left">8</span>}
</pre>
</div>

</div>
<div class="Indented">
This code has a fatal flaw. It will always deadlock. 
</div>
<div class="Indented">
Let us begin by understanding how this code is supposed to work. On line 1, the qualifier <tt>__device__</tt> is appended to the function definition. This qualifier is another C language extension of the CUDA framework. Functions defined using the <tt>__device__</tt> qualifier may be called by other device functions or kernels but not by host code. 
</div>
<div class="Indented">
On line 2, the pointers <tt>result</tt> and <tt>lock</tt> are declared with the <tt>volatile</tt> qualifier. The <tt>volatile</tt> qualifier is a hint to the compiler that several threads use these pointers. Therefore, the compiler knows that <tt>*result</tt> and <tt>*lock</tt> may change due to the action of some other thread. Thus, every time <tt>*result</tt> and <tt>*lock</tt> are used, the compiler will generate a reference to global memory and not get by with values cached in registers. 
</div>
<div class="Indented">
If we go back to the body of the function <tt>atomicAddDouble()</tt>, on line 5 we find the primitive <tt>atomicExch()</tt>. This primitive is provided by CUDA as an extension to the C language. When a thread executes <tt>atomicExch(lock, 1)</tt>, with <tt>lock</tt> a pointer to an <tt>int</tt>, <tt>atomicExch(lock, 1)</tt> will return the content of the location <tt>*lock</tt> and will move <tt>1</tt> to the location <tt>*lock</tt>. The two actions of reading <tt>*lock</tt> and then writing to <tt>*lock</tt> are guaranteed to be an atomic unit. Between the actions of reading <tt>*lock</tt> and then writing to that location, the hardware guarantees that no other thread is allowed to read or write that location. 
</div>
<div class="Indented">
Our first attempt at implementing <tt>atomicAddDouble()</tt> attempts to enclose line 6, which is <tt>*result+=ans</tt>, within a critical region. If a thread reads the content of <tt>*lock</tt> to be <span class="formula">0</span>, it assumes that the lock is open and  can enter the critical region. Before entering the critical region, it writes <span class="formula">1</span> to <tt>*lock</tt>, as a result of <tt>atomicExch()</tt>, so that no other thread can be in the critical region simultaneously. On line 5, the value of <tt>*lock</tt> is read into <tt>lockcopy</tt>, which is a local variable, and 1 is written to <tt>*lock</tt> using an atomic operation. If <tt>*lock</tt> is already <span class="formula">1</span>, then its value won’t change, and the lock remains shut. 
</div>
<div class="Indented">
On line 7, the thread exits from the critical region by writing <span class="formula">0</span> to <tt>*lock</tt> and opening the lock. 
</div>
<div class="Indented">
CUDA syntax for writing device and kernel code encourages us to think of each thread as an independent entity. However, the hardware executes instructions warp by warp and not thread by thread. If we think of how a warp of 32 threads may step through our first attempt at <tt>addAtomicDouble()</tt>, it immediately becomes clear that the code will deadlock.<span class="FootOuter"><span class="SupFootMarker"> [137] </span><span class="HoverFoot"><span class="SupFootMarker"> [137] </span>Thanks to a member of an NVIDIA forum for explaining this point to me.</span></span><tt> </tt>
</div>
<div class="Indented">
Suppose all 32 threads of a warp execute line 3 and set their local variables <tt>lockcopy</tt> to 1. All threads of the warp will check the while-loop condition on line 4 using the same warp instruction. The condition will be valid for the entire warp and all the threads will next execute the <tt>atomicExch()</tt> instruction on line 5. Several warps may be competing to read <tt>*lock</tt>. Let us suppose this warp is lucky and one of its threads reads <tt>*lock</tt> to be <span class="formula">0</span>. Then all the other 31 threads in the warp must read <tt>*lock</tt> to be <span class="formula">1</span>. At this point, we have warp divergence. The thread that read <span class="formula">0</span> exits from the loop and waits for the other threads in the warp to converge to it. The other 31 threads will go back to line 4 and check the while-loop condition and then do the atomic exchange on line 5 repeatedly. While the other 31 threads are in this spin-loop, the thread that read <span class="formula">0</span> from <tt>*lock</tt> and diverged from the warp is inactive. The thread that read <span class="formula">0</span> cannot enter the critical region and execute the statement on line 6 until the other 31 threads in the warp converge with its execution sequence. The other 31 threads cannot get out of the spin-loop until the thread that read <span class="formula">0</span> gets to line 7 and releases the lock, which means the program is in a deadlock. 
</div>
<div class="Indented">
The following implementation of <tt>atomicAddDouble()</tt> does not deadlock. Crucially, it is assumed that <tt>*result</tt> and <tt>*lock</tt> are global locations that are initialized to zero before the kernel is called.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>__device__ void atomicAddDouble(double value, 
<span class="number-left">2</span>	volatile double *result, volatile int *lock){
<span class="number-left">3</span>	for(int i=0; i &lt; NWARP; i++){
<span class="number-left">4</span>		if(threadIdx.x%NWARP==i){
<span class="number-left">5</span>			int lockcopy=1;
<span class="number-left">6</span>			while(lockcopy==1)
<span class="number-left">7</span>			   lockcopy=atomicExch((int *)lock, 1);
<span class="number-left">8</span>			*result += value;
<span class="number-left">9</span>			atomicExch((int *)lock, 0);
<span class="number-left">10</span>		}
<span class="number-left">11</span>	}
<span class="number-left">12</span>}
</pre>
</div>

</div>
<div class="Indented">
This implementation has an if-statement (lines 4 though 10) nested inside a for-loop (lines 3 through 11). The threads in a warp take turns, and an attempt to enter the critical region is made only inside the if-block. Therefore, there is no deadlock.
</div>
<h? class="Subsubsection">
<b><u>Timing the kernels</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:gpu-leibniz"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>NBLKS</i></span>
</td>
<td align="center" valign="top">
Partial
</td>
<td align="center" valign="top">
Total
</td>
<td align="center" valign="top">
<span class="formula"><i>NBLKS</i></span>
</td>
<td align="center" valign="top">
Partial
</td>
<td align="center" valign="top">
Total
</td>

</tr>
<tr>
<td align="center" valign="top">
3
</td>
<td align="center" valign="top">
<span class="formula">0.65</span>
</td>
<td align="center" valign="top">
<span class="formula">0.65</span>
</td>
<td align="center" valign="top">
26
</td>
<td align="center" valign="top">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top">
<span class="formula">0.12</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
7
</td>
<td align="center" valign="top">
<span class="formula">0.28</span>
</td>
<td align="center" valign="top">
<span class="formula">0.28</span>
</td>
<td align="center" valign="top">
260
</td>
<td align="center" valign="top">
<span class="formula">0.11</span>
</td>
<td align="center" valign="top">
<span class="formula">0.18</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
13
</td>
<td align="center" valign="top">
<span class="formula">0.15</span>
</td>
<td align="center" valign="top">
<span class="formula">0.15</span>
</td>
<td align="center" valign="top">
2600
</td>
<td align="center" valign="top">
<span class="formula">0.12</span>
</td>
<td align="center" valign="top">
<span class="formula">0.88</span>
</td>

</tr>

</table>
<div class="caption">
Table 8.1 Number of cycles per term to sum the Leibniz series on the K20 coprocessor. The cycles are cycles of the <span class="formula">2.7</span>  GHz AVX host. Data is given for both the kernel in which each thread returns its own sum (partial) and the kernel in which the threads combine to do a reduction (total).
</div>

</div>

</div>

</div>

</div>
<div class="Indented">
Because the K20 has <span class="formula">13</span> multiprocessors and two thread blocks of <span class="formula">1, 024</span> threads can reside on each of them, anything less than <span class="formula">26</span> blocks leaves the hardware under-utilized. Table <a class="Reference" href="#tab:gpu-leibniz">8.1↑</a> shows that the Leibniz summing program does not reach its best speed with fewer than <span class="formula">26</span> thread blocks.
</div>
<div class="Indented">
Lots of thread blocks do not slow down the <tt>leibniz()</tt> kernel, although the <tt>leibniztotal()</tt> kernel, which uses atomic instructions to return a single sum, is slowed down considerably (see table <a class="Reference" href="#tab:gpu-leibniz">8.1↑</a>). This outcome is partly because the timing measurements were taken using <span class="formula"><i>n</i> = 10<sup>10</sup></span> terms of the Leibniz series. Much of the overhead of syncing would disappear if <span class="formula"><i>n</i></span> were larger. Even so, the numbers in the table have a message. Using lots of blocks is not a problem if the thread blocks are mostly independent of each other. If the blocks need to sync frequently, the overhead can be substantial.
</div>
<div class="Indented">
Summing the Leibniz series on the K20 is nearly <span class="formula">2.5</span> times faster than on the Xeon Phi. The K20 is faster for some problems and the Xeon Phi for others. The main distinction is the programming model. Yet the speedup of <span class="formula">2.5</span> is a little surprising. The speedup is probably due to divisions being handled better on the K20 than on the Xeon Phi.
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.2.2">8.2.2</a> CUDA compilation<a class="Label" name="sub:gpu-CUDA-compilation"> </a>
</h3>
<div class="Unindented">
So far we have made only brief remarks about compiling CUDA programs. The graphics device compilation model is a little different from compiling for the host, and we will look at it in greater depth here.
</div>
<div class="Indented">
To begin with, we shall suppose that the entire Leibniz program discussed in the previous section is in one file called <tt>leibniz_all.cu</tt>. The compilation command in full, with \ being the line continuation character, is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>nvcc -O3 \
<span class="number-left">2</span>-prec-div=true -ftz=false -Drestrict="__restrict__"\ 
<span class="number-left">3</span>-arch=sm_35 \
<span class="number-left">4</span>-Xptxas=-v -dc leibniz_all.cu 
</pre>
</div>

</div>
<div class="Indented">
Here <tt>nvcc</tt> (line 1) is the name of NVIDIA’s compiler driver, which is a wrapper around GNU’s gcc/g++. The <tt>-O3</tt> option (line 1) sets the optimization level. 
</div>
<div class="Indented">
The options on line 2 are more generic compiling options. Precise division is required, and the flush to zero optimization is turned off. The keyword <tt>restrict</tt> is defined as a macro that expands to <tt>__restrict__</tt>. Some of our utility programs that print tables and so on use the <tt>restrict</tt> keyword, which is part of the C99 standard and supported in C++ programs by Intel’s <tt>icpc</tt> compiler. In GNU, a C++ program must use <tt>__restrict__</tt> in place of <tt>restrict</tt>. 
</div>
<div class="Indented">
The <tt>-arch</tt> option (line 3) specifies the compute capability as 3.5. To see what effect it has, we run the command 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">cuobjdump leibniz_all.o
</pre>
</div>

</div>
<div class="Indented">
on the object file. The output of this command, with some lines omitted, is
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">Fatbin elf code:
================
arch = sm_35
code version = [1,7]
...
​
Fatbin ptx code:
================
arch = sm_35
code version = [3,2]
...
</pre>
</div>

</div>
<div class="Indented">
The object file is a fatbin (in CUDA terminology), including both binary elf code and ptx code. The binary code will only run on devices whose architecture exactly matches <tt>sm_35</tt>, but the ptx code will run on any device of compute capability 3.5 or higher.
</div>
<div class="Indented">
The PTX is an assembly-like intermediate language. It is in text not in binary format. The inner loop of the <tt>leibniz()</tt> kernel looks as follows in PTX:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">BB2_3:
	.loc 1 13 1
	cvt.rn.f64.s64	%fd6, %rd10;
	fma.rn.f64 	%fd7, %fd6, 0d4000000000000000,\
	           	            0d3FF0000000000000;
	mov.f64 	%fd8, 0d4010000000000000;
	.loc 3 3614 3
	div.rn.f64 	%fd9, %fd8, %fd7;
	.loc 1 13 94
	add.f64 	%fd12, %fd12, %fd9;
	.loc 1 12 17
	add.s64 	%rd10, %rd10, %rd3;
	.loc 1 12 1
	setp.lt.s64	%p2, %rd10, %rd6;
	@%p2 bra 	BB2_3;
</pre>
</div>

</div>
<div class="Indented">
The PTX can be compiled into elf binary format of any device of compute capability 3.5 or higher.
</div>
<div class="Indented">
If the <tt>leibniz_all.exe</tt> program is run on an <tt>sm_35</tt> device, the binary elf will run. If the device has a compute capability that is higher than 3.5, then the driver’s just-in-time compiler will generate the binary from the PTX that is embedded in the <tt>.exe</tt> program. So the <tt>.exe</tt> will run on the K20 as well as later generation Maxwell devices. 
</div>
<div class="Indented">
The <tt>nvcc</tt> compiler driver offers options for generating <tt>.ptx</tt> files with only the PTX or <tt>.cubin</tt> files with only the elf binary or even <tt>.o</tt> files with the elf binary for multiple compute capabilities as well as PTX.
</div>
<div class="Indented">
Going back to the <tt>nvcc</tt> compilation command, line 4 has the setting <tt>-Xptxas=-v</tt>. So the <tt>-v</tt> verbose option is passed to the PTX assembly phase. Its effect is to print the register usage of every device function and kernel. If register usage is excessive, a kernel may fail to launch. There are further options to control register usage in the PTX phase.
</div>
<div class="Indented">
Another  important option on line 4 is <tt>-dc</tt>. Usually <tt>-c</tt> generates the object file without linking and may be read as &ldquo;compile only.&rdquo; Similarly, <tt>-dc</tt> generates an object file that includes both host and device code but does not link. Because we are using the <tt>-dc</tt> option, we may call the source <tt>leibniz_all.cpp</tt> instead of <tt>leibniz_all.cu</tt> and nothing changes.
</div>
<div class="Indented">
The command for generating the executable is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">nvcc -arch=sm_35 -o leibniz_all.exe leibniz_all.o
</pre>
</div>

</div>
<div class="Indented">
No libraries are being linked here, but for other programs we can link libraries on the command line. The <tt>-lcublas</tt> option links the cuBLAS library. The executable may be run on any computer equipped with an NVIDIA graphics coprocessor of capability 3.5 or higher.
</div>
<div class="Indented">
To make a point about backward compatibility, we may go back and compile with <tt>-arch=sm_20</tt> instead of <tt>-arch=sm_35</tt>. The resulting object file and executable have binary only for <tt>sm_20</tt> devices. The executable, however, still runs on the K20, which is not an <tt>sm_20</tt> device. The driver’s just-in-time compiler generates the binary for the K20 from the PTX that is embedded inside the executable.
</div>
<div class="Indented">
The improvements made by NVIDIA from Tesla to Fermi to Kepler instruction set architectures appear quite substantial. The freedom to make such substantial changes while still being able to run older executables on newer devices is a consequence of just-in-time compilation of PTX embedded in the executables.
</div>
<div class="Indented">
As always, the organization of source files must reflect the structure of the program, and it is not a good idea to put everything in one source file (in fact, it is a terribly bad idea). In the case of the Leibniz program, the source files are 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">leibniz.cu, atomicAdd.cu, time_leibniz.cu
</pre>
</div>

</div>
<div class="Indented">
In addition, some utilities for timing and making tables discussed in earlier chapters are also linked. With regard to linking multiple object files as well as libraries to generate an executable, there is nothing new to add to what we learned in chapter 2.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a CUDA program to evaluate the sum <div class="formula">
<span class="limits"><sup class="limit">∞</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>m</i> = 1</sub></span><span class="limits"><sup class="limit">∞</sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>n</i> = 1</sub></span><span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator"><i>m</i><sup>4</sup> + <i>n</i><sup>4</sup></span><span class="ignored">)</span></span>.
</div>

</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a CUDA kernel to find the maximum of an array of numbers.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Explain why warp divergence is impossible to avoid when sorting an array of numbers or when merging two presorted arrays.
</div>
<div class="--Separator--">

</div>
<h2 class="Section">
<a class="toc" name="toc-Section-8.3">8.3</a> Two examples<a class="Label" name="sec:gpu-cuda-examples"> </a>
</h2>
<div class="Unindented">
In this section, we look at two more examples to better understand the CUDA programming model and the speed of the K20 graphics device. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:gpu-Bandwidth-to-memory">8.3.1↓</a> compares the K20’s bandwidth to memory to that of the Phi and the AVX host. The K20’s read bandwidth is lower than that of the Phi, but its copy bandwidth is higher. The AVX host has read/copy bandwidths that are between a third and a half of the K20. If the disparity in memory bandwidths is significant, so is the much more extensive caching found on the AVX host. On programs where the memory accesses are not regular and structured, caching can be a great advantage. 
</div>
<div class="Indented">
Section <a class="Reference" href="#sub:gpu-mmult">8.3.2↓</a> shows two implementations of matrix multiplication. The first implementation is relatively easy. However, it reaches only around a <span class="formula">20</span>th of the peak capability of the K20 device. The other implementation using shared memory, which functions as a user-managed cache, reaches an eighth of the peak capability. These programs are comparable to the <tt>multIJK()</tt> program of chapter <a class="Reference" href="#chap:The-processor">3↑</a> with regard to how efficiently they utilize the machine. For comparison, the <tt>multIJK()</tt> program too falls short of the peak capability on AVX2 by a similar factor, which is a tenth. However, the shared memory program in CUDA is perhaps 100 times harder to write than <tt>multIJK()</tt> or its OpenMP version. 
</div>
<div class="Indented">
To approach peak bandwidth, there is no choice but to optimize for the instruction pipeline. We do not consider such optimization here,<span class="FootOuter"><span class="SupFootMarker"> [138] </span><span class="HoverFoot"><span class="SupFootMarker"> [138] </span>For instruction pipeline optimizations pertinent to GPUs, see <span class="bibcites">[<a class="bibliocite" name="cite-60" href="#biblio-60"><span class="bib-index">60</span></a>]</span>.</span></span> although the basic principles of optimizing for the instruction pipeline do not change drastically from what is described in chapter <a class="Reference" href="#chap:The-processor">3↑</a>. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.3.1">8.3.1</a> Bandwidth to memory<a class="Label" name="sub:gpu-Bandwidth-to-memory"> </a>
</h3>
<div class="Unindented">
The following functions are used to measure the K20’s bandwidth to memory:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">__global__ 
__launch_bounds__(THinBLK, BLKinMP)
void add(double *list, int n, double *result){
	int tid = threadIdx.x + blockIdx.x*blockDim.x;
	int stride = blockDim.x*gridDim.x;
	double ans = 0;
	for(int i=tid; i &lt; n; i = i + stride)
		ans += list[i]; 
	result[tid] = ans;
}
​
__global__
__launch_bounds__(THinBLK, BLKinMP)
void copy(double *list, int n, double *copy){
	int tid = threadIdx.x + blockIdx.x*blockDim.x;
	int stride = blockDim.x*gridDim.x;
	for(int i=tid; i &lt; n; i = i + stride)
		copy[i] = list[i]; 
}
</pre>
</div>

</div>
<div class="Indented">
Notice that data accesses from the threads are interleaved as in <div class="formula">
0, 1, 2, 3, …, 0, 1, 2, 3, …, 0, 1, 2, 3, …
</div>
and not blocked as in <div class="formula">
0, 0, 0, …, 1, 1, 1, …, 2, 2, 2, …, 3, 3, 3, …
</div>
Blocking memory accesses is the right thing to do on x86 processors. On the K20 and other graphics devices, blocking will lose more than 90% of the bandwidth. Partly because instructions are executed warp by warp, memory accesses must be interleaved.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="tab:gpu-membw"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">

</td>
<td align="center" valign="top">
AVX
</td>
<td align="center" valign="top">
 Phi
</td>
<td align="center" valign="top">
K20
</td>

</tr>
<tr>
<td align="center" valign="top">
Read b/w
</td>
<td align="center" valign="top">
<span class="formula">91</span> GB/s
</td>
<td align="center" valign="top">
<span class="formula">161</span> GB/s
</td>
<td align="center" valign="top">
<span class="formula">137</span> GB/s
</td>

</tr>
<tr>
<td align="center" valign="top">
Copy b/w
</td>
<td align="center" valign="top">
<span class="formula">50</span> Gb/s
</td>
<td align="center" valign="top">
<span class="formula">86</span> GB/s
</td>
<td align="center" valign="top">
<span class="formula">133</span> GB/s
</td>

</tr>

</table>

</div>
<div class="caption">
Table 8.2 Read and copy bandwidths in GB/s. The K20 is compared to the Phi and its <span class="formula">2.7</span>  GHz AVX host (for the full name of the machine, see table <a class="Reference" href="#tab:appendix-machines-used">9.1↓</a>).
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:gpu-membw">8.2↑</a> compares the bandwidth to memory of the AVX host, the Phi, and the K20.<span class="FootOuter"><span class="SupFootMarker"> [139] </span><span class="HoverFoot"><span class="SupFootMarker"> [139] </span>If <span class="formula">1</span> GB of data is copied, it counts as <span class="formula">2</span> GB for the purpose of calculating bandwidth.</span></span> The Xeon Phi wins the race in reading, and the K20 is the winner, by far, for copying. 
</div>
<h3 class="Subsection">
<a class="toc" name="toc-Subsection-8.3.2">8.3.2</a> Matrix multiplication<a class="Label" name="sub:gpu-mmult"> </a>
</h3>
<div class="Unindented">
If <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span> are <span class="formula"><i>N</i> × <i>N</i></span> matrices, then <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span> requires <span class="formula">2<i>N</i><sup>3</sup></span> floating point operations (flops). Half the operations are additions and half are multiplications. The amount of memory involved in matrix multiplication is <span class="formula"><span class="scriptfont">O</span><span class="symbol">(</span><i>N</i><sup>2</sup><span class="symbol">)</span></span>, but the number of flops is <span class="formula"><span class="scriptfont">O</span><span class="symbol">(</span><i>N</i><sup>3</sup><span class="symbol">)</span></span>. If <span class="formula"><i>N</i></span> is large, there is a possibility that the cost of memory accesses can be hidden almost completely, and the program’s speed is determined by the peak floating point bandwidth. On the K20, the peak floating point bandwidth is <span class="formula">1.17</span> TFlops/s. Although the programs we describe do not approach that speed, we compare them against a cuBLAS program that does.
</div>
<h? class="Subsubsection">
<b><u>Global memory</u></b>
</h?>
<div class="Unindented">
The first program for multiplying square matrices is as follows:<span class="FootOuter"><span class="SupFootMarker"> [140] </span><span class="HoverFoot"><span class="SupFootMarker"> [140] </span>This program is adapted from (<i>CUDA C Programming Guide</i>, August, 2014).</span></span>
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">__global__ 
__launch_bounds__(THinBLK, BLKinMP)
void mmult_gmem(double *A, double *B, double * C, 
		int N){
	int tidx = threadIdx.x;
	int tidy = threadIdx.y;
	int bidx = blockIdx.x;
	int bidy = blockIdx.y;
	int i = bidx*blockDim.x + tidx;
	int j = bidy*blockDim.y + tidy;
	for(int k = 0; k &lt; N; k++)
		C[i+j*N] += A[i+k*N]*B[k+j*N];
}
</pre>
</div>

</div>
<div class="Indented">
This kernel is assumed to be launched with two-dimensional thread blocks in a two-dimensional grid. The size of the thread block is assumed to <tt>SQRTT x SQRTT</tt>. On the K20, we take <tt>SQRTT</tt> to be <span class="formula">32 = <span class="sqrt"><span class="radical">√</span><span class="ignored">(</span><span class="root">1, 024</span><span class="ignored">)</span></span></span> because <span class="formula">1, 024</span> is the maximum number of threads in a block. 
</div>
<div class="Indented">
The matrix dimension <tt>N</tt> is assumed to be divisible by <tt>SQRTT</tt>. The grid is assumed to be <tt>N/SQRTT x N/SQRTT</tt>. The total number of threads is thus equal to <span class="formula"><i>N</i><sup>2</sup></span>. There is a natural map from threads to each entry of the <span class="formula"><i>N</i> × <i>N</i></span> matrix <span class="formula"><i>C</i></span>. The <tt>mmult_gmem()</tt> kernel calculates the indices of <span class="formula">(<i>i</i>, <i>j</i>)</span> of the entry the thread maps to and then updates the corresponding entry of <tt>C[]</tt>. Thus, each entry of the matrix <tt>C[]</tt> is given to a different thread for updating.
</div>
<div class="Indented">
The syntax for launching this kernel using two-dimensional thread blocks and grids is shown in the listing below. Multiple dimensions are specified using <tt>dim3</tt> objects.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">double mmult(double *A, double *B, double *C, int N){ 
	assrt(SQRTT*SQRTT==THinBLK);
	assrt(N%SQRTT==0);
	
	dim3 grid(N/SQRTT, N/SQRTT);
	dim3 tblk(SQRTT, SQRTT);
	
	dhstmem&lt;double&gt; dhA(N*N);
	dhstmem&lt;double&gt; dhB(N*N);
	dhstmem&lt;double&gt; dhC(N*N);
	for(int i=0; i &lt; N*N; i++){
		dhA.host()[i] = A[i];
		dhB.host()[i] = B[i];
		dhC.host()[i] = C[i];
	}
	dhA.host2device();
	dhB.host2device();
	dhC.host2device();
​
	mmult_gmem&lt;&lt;&lt;grid, tblk&gt;&gt;&gt;(dhA.device(),
               dhB.device(), dhC.device(), N);
​
	dhC.device2host();
	for(int i=0; i &lt; N*N; i++)
		C[i] = dhC.host()[i];
}
</pre>
</div>

</div>
<div class="Indented">
Copying data into and out of the device using the <tt>dhstmem</tt> class implies extra copying. That is an overhead we accept in the interest of a more modular program.
</div>
<h? class="Subsubsection">
<b><u>Shared memory</u></b>
</h?>
<div class="Unindented">
In the Kepler microarchitecture, each multiprocessor owns some on-chip memory that is split between L1 cache and shared memory (see figure <a class="Reference" href="#fig:gpu-kepler">8.2↑</a>). On the K20, by default, there is <span class="formula">48</span> KB of shared memory for each of its <span class="formula">13</span> multiprocessors. Accesses of shared memory are considerably faster than global memory accesses. 
</div>
<div class="Indented">
The following program assumes <tt>SQRTT x SQRTT</tt> thread blocks and <tt>N/SQRTT x N/SQRTT</tt> grid just as before. However, it uses shared memory and blocking. 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>__global__ 
<span class="number-left">2</span>__launch_bounds__(THinBLK, BLKinMP)
<span class="number-left">3</span>void mmult_smem(double *A,double *B,double * C,int N){
<span class="number-left">4</span>	int tidx = threadIdx.x;
<span class="number-left">5</span>	int tidy = threadIdx.y;
<span class="number-left">6</span>	int bidx = blockIdx.x;
<span class="number-left">7</span>	int bidy = blockIdx.y;
<span class="number-left">8</span>​
<span class="number-left">9</span>	__shared__ double smem[3*THinBLK];
<span class="number-left">10</span>	double *smemA = smem;
<span class="number-left">11</span>	double *smemB = smem+THinBLK;
<span class="number-left">12</span>	double *smemC = smem+2*THinBLK;
<span class="number-left">13</span>	int i = tidx + bidx*blockDim.x;
<span class="number-left">14</span>	int j = tidy + bidy*blockDim.y;
<span class="number-left">15</span>​
<span class="number-left">16</span>	smemC[tidx + SQRTT*tidy] = C[i + N*j];
<span class="number-left">17</span>	for(int k = 0; k &lt; N; k += SQRTT){
<span class="number-left">18</span>		smemA[tidx + SQRTT*tidy] = A[i + N*(k+tidy)]; 
<span class="number-left">19</span>		smemB[tidx + SQRTT*tidy] = B[(k+tidx) + N*j];
<span class="number-left">20</span>		__syncthreads();
<span class="number-left">21</span>		for(int kk=0; kk &lt; SQRTT; kk++){
<span class="number-left">22</span>			smemC[tidx+SQRTT*tidy] += 
<span class="number-left">23</span>		   smemA[tidx+SQRTT*kk]*smemB[kk+SQRTT*tidy];
<span class="number-left">24</span>		}
<span class="number-left">25</span>		__syncthreads();
<span class="number-left">26</span>	}
<span class="number-left">27</span>	C[i + N*j] = smemC[tidx + SQRTT*tidy];
<span class="number-left">28</span>}
</pre>
</div>

</div>
<div class="Indented">
This is not the longest C/C++ listing in this book, but it may be the hardest to decipher.
</div>
<div class="Indented">
<div class="float">
<a class="Label" name="fig:gpu-mmult-smem"> </a><div class="figure">
<div class="center">
<img class="embedded" src="FIGS/chapter7/mmult_smem.png" alt="figure FIGS/chapter7/mmult_smem.png" style="max-width: 352px; max-height: 101px;"/>

</div>
<div class="caption">
Figure 8.4 Depiction of <span class="formula"><i>C</i> = <i>C</i> + <i>AB</i></span>, with <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span> being square matrices. Each submatrix shown is <tt>SQRTT x SQRTT</tt>, matching the dimensions of the thread block.
</div>

</div>

</div>
To help decipher this program, we turn to figure <a class="Reference" href="#fig:gpu-mmult-smem">8.4↑</a>. Each submatrix <tt>CC</tt> as well as the submatrices <tt>A1</tt>, <tt>B1</tt>, and so on are assumed to be <tt>SQRTT x SQRTT</tt> matching the dimensions of the thread block. Because the grid is of dimension <tt>N/SQRTT x N/SQRTT,</tt> we may assume that each submatrix <tt>CC</tt> belongs to a certain thread block. The entries of <tt>CC</tt> are further split between the threads in the thread block.
</div>
<div class="Indented">
The kernel <tt>mmult_smem()</tt> thinks of the updating of <tt>CC</tt> in block terms:<div class="formula">
<span class="text">CC</span> +  = <span class="text">A1</span> × <span class="text">B1</span> + <span class="text">A2</span> × <span class="text">B2</span> + ⋯ + <span class="text">An</span> × <span class="text">Bn</span>
</div>
where <span class="formula"><span class="text">n</span> = <span class="text">N</span> ⁄ <span class="text">SQRTT</span></span>. 
</div>
<div class="Indented">
On line 9, the program claims shared memory using the <tt>__shared</tt>__ qualifier. The memory is split between a subblock of <span class="formula"><i>A</i></span> (line 10), a subblock of <span class="formula"><i>B</i></span> (line 11), and the subblock <span class="formula"><i>CC</i></span> (line 12). Each thread also computes the indices <span class="formula">(<i>i</i>, <i>j</i>)</span> of the entry of <span class="formula"><i>C</i></span> it will update (lines 13 and 14).
</div>
<div class="Indented">
The outer for-loop (line 16) steps through the blocks <tt>Aj, Bj</tt>. The index <span class="formula"><i>k</i></span> is incremented in steps of <tt>SQRTT</tt>. So to get the <span class="formula"><i>j</i></span>th block, we must take k=j*SQRTT. Each subblock <tt>Aj</tt> is split between the <tt>SQRTT x SQRTT</tt> threads in the thread block. On line 18, the subblock <tt>Aj</tt> is loaded into shared memory, with each thread loading exactly one entry. Similarly, on line 19, the subblock <tt>Bj</tt> is loaded into shared memory.
</div>
<div class="Indented">
In the inner for-loop (line 21), each thread updates the entry of <tt>CC</tt> that corresponds to its position in the thread block. The inner for-loop accesses entries of <span class="formula"><i>A</i></span>, <span class="formula"><i>B</i></span>, and <span class="formula"><i>C</i></span> that are already in shared memory. 
</div>
<div class="Indented">
The <tt>__syncthreads()</tt> function called on line 20, just before the inner for-loop, is one of the CUDA intrinsic functions. It is a barrier across all the threads in the same thread block. Another call to <tt>__syncthreads()</tt> is made after the inner for-loop. These barriers are essential to separate the copying to shared memory outside the inner for-loop from the accesses to the copied locations inside the body of the inner for-loop.
</div>
<div class="Indented">
The <tt>mmult_smem()</tt> is an example of a program where the thread hierarchy affects the structure of the entire loop nest, making the code difficult to comprehend.
</div>
<h? class="Subsubsection">
<b><u>cuBLAS</u></b>
</h?>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:gpu-mmult-gemm"> </a><div class="table">
<div class="center">
<table>
<tr>
<td align="center" valign="top">
<span class="formula"><i>N</i></span>
</td>
<td align="center" valign="top">
gmem
</td>
<td align="center" valign="top">
smem
</td>
<td align="center" valign="top">
cuBLAS
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">8, 000</span>
</td>
<td align="center" valign="top">
<span class="formula">47</span> GFlops/s
</td>
<td align="center" valign="top">
<span class="formula">126</span> GFlops/s
</td>
<td align="center" valign="top">
<span class="formula">1045</span> GFlops/s
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">10, 000</span>
</td>
<td align="center" valign="top">
<span class="formula">47</span> GFlops/s
</td>
<td align="center" valign="top">
<span class="formula">127</span> GFlops/s
</td>
<td align="center" valign="top">
<span class="formula">1038</span> GFlops/s
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="formula">12, 000</span>
</td>
<td align="center" valign="top">
<span class="formula">48</span> GFlops/s
</td>
<td align="center" valign="top">
<span class="formula">127</span> GFlops/s
</td>
<td align="center" valign="top">
<span class="formula">1047</span> GFlops/s
</td>

</tr>

</table>

</div>
<div class="caption">
Table 8.3 Comparison of floating throughput of the <tt>mmult_gmem()</tt> (gmem) and <tt>mmult_smem()</tt> (smem) kernels with cuBLAS.
</div>

</div>

</div>

</div>
<div class="Indented">
Table <a class="Reference" href="#tab:gpu-mmult-gemm">8.3↑</a> compares the global and shared memory implementations of matrix multiplication with cuBLAS. The use of shared memory speeds up the program by more than a factor of <span class="formula">2</span>, but the sped-up program reaches only slightly more than 10% of peak floating point throughput. In contrast, cuBLAS gets close to the theoretical floating point peak of <span class="formula">1.17</span> TFlops/s.
</div>
<div class="Indented">
On older Intel machines, a fairly straightforward matrix multiply goes beyond 25% of the peak floating point throughput. On the K20, even the quite complex <tt>mmult_smem()</tt> kernel gets just around 10%. That disparity is partly because the Intel compilers are more mature for older Intel machines. However, it is also true that fine-grained parallelism with threads arranged in a hierarchy makes it harder to write programs. 
</div>
<div class="Indented">
The implementation of cuBLAS is proprietary. Matrix blocking and good use of registers are key ideas.<span class="FootOuter"><span class="SupFootMarker"> [141] </span><span class="HoverFoot"><span class="SupFootMarker"> [141] </span>For an explanation of how to implement dense linear algebra routines, including matrix multiplication, efficiently in CUDA, see <span class="bibcites">[<a class="bibliocite" name="cite-60" href="#biblio-60"><span class="bib-index">60</span></a>]</span>.</span></span> The <tt>CuMult</tt> class is our interface to the matrix multiplication routine in cuBLAS.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#include &lt;cublas_v2.h&gt;
​
class CuMult{
private:
	int N;
	dhstmem&lt;double&gt; memA, memB, memC;
	cublasHandle_t h;
public:
	CuMult(int Ni);
	~CuMult();
	/*
	 * C = C + A*B, A[], B[], C[] are NxN matrices
	 * returns time in millisecs
	 */
	double mult(double *A, double *B, double *C);
};
</pre>
</div>

</div>
<div class="Indented">
The handle <tt>h</tt> is one of the private data members. It is used to access the cuBLAS facilities.
</div>
<div class="Indented">
The class constructor is defined as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">CuMult::CuMult(int Ni)
	:N(Ni), memA(N*N), memB(N*N), memC(N*N){
	cublasStatus_t code;
​
	code = cublasCreate(&amp;h);
	assrt(code == CUBLAS_STATUS_SUCCESS);
}
</pre>
</div>

</div>
<div class="Indented">
The main task of the constructor is to initialize the handle by calling <tt>cublasCreate()</tt>. It also initializes <tt>vhstmem</tt> objects <tt>memA/B/C</tt> used for sending and retrieving data from the device. The destructor releases the handle.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">CuMult::~CuMult(){
	cublasStatus_t code;
​
	code = cublasDestroy(h);
	assrt(code == CUBLAS_STATUS_SUCCESS);
}
</pre>
</div>

</div>
<div class="Indented">
The definition of the member function <tt>mult()</tt> follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing"><span class="number-left">1</span>double CuMult::mult(double *A, double *B, double *C){
<span class="number-left">2</span>	cublasStatus_t code;
<span class="number-left">3</span>​
<span class="number-left">4</span>	for(int i=0; i &lt; N*N; i++){
<span class="number-left">5</span>		memA.host()[i] = A[i];
<span class="number-left">6</span>		memB.host()[i] = B[i];
<span class="number-left">7</span>		memC.host()[i] = C[i];
<span class="number-left">8</span>	}
<span class="number-left">9</span>	memA.host2device();
<span class="number-left">10</span>	memB.host2device();
<span class="number-left">11</span>	memC.host2device();
<span class="number-left">12</span>​
<span class="number-left">13</span>	double alpha = 1.0;
<span class="number-left">14</span>	double beta = 1.0;
<span class="number-left">15</span>	
<span class="number-left">16</span>​
<span class="number-left">17</span>	hstTimer hclk;
<span class="number-left">18</span>	hclk.tic();
<span class="number-left">19</span>	code = cublasDgemm(h, 
<span class="number-left">20</span>			     CUBLAS_OP_N, CUBLAS_OP_N,
<span class="number-left">21</span>			     N, N, N,
<span class="number-left">22</span>			     &amp;alpha,
<span class="number-left">23</span>			     memA.device(), N, 
<span class="number-left">24</span>			     memB.device(), N,
<span class="number-left">25</span>			     &amp;beta,
<span class="number-left">26</span>			     memC.device(), N);
<span class="number-left">27</span>	double tms = hclk.toc();
<span class="number-left">28</span>	assrt(code == CUBLAS_STATUS_SUCCESS);
<span class="number-left">29</span>	
<span class="number-left">30</span>	memC.device2host();
<span class="number-left">31</span>	for(int i=0; i &lt; N*N; i++)
<span class="number-left">32</span>		C[i] = memC.host()[i];
<span class="number-left">33</span>	
<span class="number-left">34</span>	return tms;
<span class="number-left">35</span>}
</pre>
</div>

</div>
<div class="Indented">
The matrices <tt>A[]</tt>, <tt>B[]</tt>, and <tt>C[]</tt> are copied into <tt>dhstmem</tt> objects (lines 4 through 8) and then copied to device memory (lines 9, 10, and 11). The cuBLAS function implements <span class="formula"><i>C</i> = <i>α</i><i>AB</i> + <i>β</i><i>C</i></span>. Both the parameters <tt><span class="formula"><i>α</i></span></tt> and <span class="formula"><i>β</i></span> are set to <span class="formula">1</span> (lines 13 and 14). 
</div>
<div class="Indented">
In the call to the library function <tt>cublasDgemm()</tt>, the first argument is the handle <tt>h</tt> initialized by the constructor (line 19). The arguments on line 20 specify that neither <span class="formula"><i>A</i></span> nor <span class="formula"><i>B</i></span> needs to be transposed. Line <span class="formula">21</span> specifies that all the matrices are <span class="formula"><i>N</i> × <i>N</i></span>. The leading dimensions of all three matrices are given as <span class="formula"><i>N</i></span> (lines 23, 24, and 26). 
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Write a CUDA kernel for transposing a matrix. Measure the bandwidth to memory of your kernel.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  Build a C++ class interfacing to the cuBLAS functions that solve linear systems. Determine the peak floating point throughput of cuBLAS linear solvers.
</div>
<div class="--Separator--">

</div>
<div class="Description">
<span class="Description-entry">Exercise:</span>  On platforms with multiple GPU devices, the cuBLAS functions can be sent to a chosen device or tiled between the GPU devices. Modify the <tt>CuMult</tt> class and endow it with such functionality.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-8.4">8.4</a> References
</h2>
<div class="Unindented">
<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-58"><span class="bib-index">58</span></a>] </span> <span class="bib-authors">D.B. Kirk, W.W. Hwu</span>: <i><span class="bib-title">Programming Massively Parallel Processors</span></i>. <span class="bib-publisher">Morgan Kaufmann</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-59"><span class="bib-index">59</span></a>] </span> <span class="bib-authors">J. Sanders, E. Kandrot</span>: <i><span class="bib-title">CUDA by Example</span></i>. <span class="bib-publisher">Addison-Wesley</span>, <span class="bib-year">2010</span>.
</p>
<p class="biblio">
<span class="entry">[<a class="biblioentry" name="biblio-60"><span class="bib-index">60</span></a>] </span> <span class="bib-authors">V. Volkov, J.W. Demmel</span>: “<span class="bib-title">Benchmarking GPUs to tune dense linear algebra</span>”, <i><span class="bib-journal">Proceedings of the 2008 ACM/IEEE Conference on Supercomputing</span></i>, pp. <span class="bib-pages">1-11</span>, <span class="bib-year">2008</span>.
</p>

</div>
<div class="Indented">

</div>
<h1 class="Chapter">
<a class="toc" name="toc-Chapter-9">9</a> Machines Used, Plotting, Python, GIT, Cscope, and gcc
</h1>
<div class="Unindented">
In this appendix, we give a list of machines used in this book as well as several pointers for downloading and using the program code. 
</div>
<div class="Indented">
The command line is not used as much anymore. However, the command line, supplemented by a good <tt>.bashrc</tt> file, encourages a logical view of the computer. A good <tt>.bashrc</tt> file must alter the prompt to display the current directory at the command prompt. Although this appears to be a minor point, it is essential for maintaining a logical view of the file system. The prompt can also be altered to display the GIT branch within a GIT repository.<span class="FootOuter"><span class="SupFootMarker"> [142] </span><span class="HoverFoot"><span class="SupFootMarker"> [142] </span>The author’s <tt>.bashrc</tt> file is found under <tt>sys/</tt> in the GIT repo of this book.</span></span> 
</div>
<div class="Indented">
The two principal editors among Linux programmers are <tt>vi</tt> and <tt>emacs</tt>. Purists tend to prefer <tt>vi/vim</tt> for its greater simplicity and much cleaner design. <tt>Emacs</tt> can be more powerful, although it can seem a little arbitrary, especially without a good <tt>.emacs</tt> file.<span class="FootOuter"><span class="SupFootMarker"> [143] </span><span class="HoverFoot"><span class="SupFootMarker"> [143] </span>The author’s <tt>.emacs</tt> file is found under <tt>sys/</tt> in the GIT repo of this book.</span></span> Both <tt>vi/vim</tt> and <tt>emacs</tt> can be launched from the command line.<span class="FootOuter"><span class="SupFootMarker"> [144] </span><span class="HoverFoot"><span class="SupFootMarker"> [144] </span>On MacOSX, the default <tt>emacs</tt><span class="default"> does not work so well. A better <tt>emacs</tt> can be downloaded from the Internet.</span></span></span> 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-9.1">9.1</a> Machines used
</h2>
<div class="Unindented">
<div class="float">
<a class="Label" name="tab:appendix-machines-used"> </a><div class="table">
<div class="center">
<span class="scriptsize"><table>
<tr>
<td align="center" valign="top">
<span class="scriptsize">Computer</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Instn</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Registers</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Microarchitecture</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Year</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Moniker</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">Xeon 5650 (12 core)</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">SSE2</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">XMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Nehalem/Westmere</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2001/2010</span>
</td>
<td align="center" valign="top">
<span class="scriptsize"><span class="formula">2.6</span>  GHz SSE2</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">Xeon 5680 (12 core)</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">SSE2</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">XMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Nehalem/Westmere</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2001/2010</span>
</td>
<td align="center" valign="top">
<span class="scriptsize"><span class="formula">3.33</span>  GHz SSE2</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">E5-2660 (16 core)</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">AVX</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">YMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Sandy Bridge</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2011/2012</span>
</td>
<td align="center" valign="top">
<span class="scriptsize"><span class="formula">2.2</span>  GHz AVX</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">E5-2680 (16 core)</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">AVX</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">YMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Sandy Bridge</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2011/2012</span>
</td>
<td align="center" valign="top">
<span class="scriptsize"><span class="formula">2.7</span>  GHz AVX </span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">Core i7-3770 (4 core)</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">AVX</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">YMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Sandy Bridge</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2011/2012</span>
</td>
<td align="center" valign="top">
<span class="scriptsize"><span class="formula">3.4</span>  GHz AVX</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">Core i3-4350 (2 core)</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">AVX2</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">YMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Haswell</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2013/2014</span>
</td>
<td align="center" valign="top">
<span class="scriptsize"><span class="formula">3.6</span>  GHz AVX2</span>
</td>

</tr>
<tr>
<td align="center" valign="top">
<span class="scriptsize">Xeon Phi SE10P</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">AVX-512</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">ZMM</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Phi/MIC</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">2013/2013</span>
</td>
<td align="center" valign="top">
<span class="scriptsize">Phi/MIC</span>
</td>

</tr>

</table>
</span><span class="default"><div class="caption">
Table 9.1 Machines used to run and time programs. The Xeon Phi, which is a coprocessor, uses a <span class="formula">1.09</span> GHz clock. The second column gives the highest level of instruction set pertinent to this book. For the interpretation and meaning of the instruction set, see table <a class="Reference" href="#tab:proc-sse2-avx-avx2">3.1↑</a> and the associated discussion. The second to last column gives the year of the instruction set as well as the computer. The last column gives the name with which the machine is referenced in the text. Much of the information in this table may be verified at <a class="FlexURL" href="ark.intel.com">ark.intel.com</a>. 
</div>
</span>
</div>

</div>

</div>

</div>
<div class="Indented">
For cache parameters of the machines in table <a class="Reference" href="#tab:appendix-machines-used">9.1↑</a>, see section <a class="Reference" href="#subsec:processor-cpuid-cache-parameters">3.1.4↑</a>.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-9.2">9.2</a> Plotting in C/C++ and other preliminaries
</h2>
<div class="Unindented">
In this section, we describe C++ classes for plotting, gathering statistics, and making tables. These classes are used throughout the book. However, in almost every instance, the code showing how these facilities are invoked is suppressed. 
</div>
<h? class="Subsubsection">
<b><u>Plotting in C/C++ programs</u></b>
</h?>
<div class="Unindented">
Plots and pictures that show output and display program data can make programs less cryptic and clarify what is going on. A picture can be worth a thousand lines of code.
</div>
<div class="Indented">
Plotting libraries are not part of the C/C++ languages for a good reason. Although plotting is very helpful, it is too high level of an activity and far removed from the view of the machine that the C language offers. There are many libraries, external to the language, that may be used to generate plots. Some of these libraries offer precise control and many graphics capabilities. No single plotting library appears to be dominant. Some of the libraries are extensive enough to overwhelm those who are not dedicated to computer graphics. In such a situation, it is difficult for the programmer to decide whether any of the libraries is worth learning.
</div>
<div class="Indented">
One solution is to use a simple C++ class that outsources all the plotting to Python. Such a class can be slow, but the programmer who is willing to wait a few hundred milliseconds is spared the trouble of learning how to use a C/C++ graphics library. The public interface to the <tt>PyPlot</tt> class follows.
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class PyPlot{
public:
	/*
	 * name must be less than 25 chars
	 */
	PyPlot(const char *namei);
	~PyPlot();
	/*
	 * functions for drawing lines 
	 */
	void plot(double *x, double *y, int n);
	void plot(double *y, int n);
	void semilogx(double *x, double *y, int n);
	void linestyle(const char* s);
	void linewidth(const char* s);
	void markersize(const char* s);
	/*
	 * functions for specifying axes, properties
	 */
	void axis(); //"tight"
	void axis(double x0, double x1, 
              double y0, double y1);
	void title(const char* s);
	void xticks(double *ticks, int n);
	void yticks(double *ticks, int n);
	void ticksize(const char *s);
    /*
	 * writes python command to pipe
	 */
	void pycmd(const char *s);
	/*
	 * functions for showing/output
	 */
	void show();
	/*
	 * eps output
	 */
	void output();
	/*
	 * save python script in FIGS/
	 */
	void savescript();
};
</pre>
</div>

</div>
<div class="Indented">
The name supplied through the constructor is used for naming intermediate data files as well as the output. This class can show the plot, save an <tt>eps</tt> file, or save a Python script for generating the plot.
</div>
<div class="Indented">
The Linux operating system provides a system call to open pipes to shell commands. For example, 
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">FILE *pypipe = popen("python", "w");
</pre>
</div>

</div>
<div class="Indented">
opens a writable pipe to Python. We can treat this pipe as a file and write commands to be executed by Python to it as if we were writing to a file. For example, the following program fragment plots the sine function:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">fprintf(pypipe,"import numpy as np\n");
fprintf(pypipe,"from matplotlib import pyplot as plt\n");
fprintf(pypipe, "x = np.linspace(-5.0, 5.0, 500)\n");
fprintf(pypipe, "plt.plot(x, np.sin(x))\n");
fprintf(pypipe, "plt.show()\n");
</pre>
</div>

</div>
<div class="Indented">
When the pipe is no longer needed, it may be closed using <tt>pclose()</tt>. The <tt>PyPlot</tt> class is implemented using pipes.
</div>
<div class="Indented">
Within the C/C++ framework, we can use pipes and rely on Python for plotting as we just showed. Conversely, it is easy to integrate C/C++ programs into Python.<a class="Label" name="proc-python-ctypes"> </a> The <tt>ctypes</tt> library in Python virtually erases the distinction between C and Python.<a class="Label" name="appndx-ctypes"> </a> Numpy supports the <tt>ctypes</tt> interface through the <tt>ndpointer</tt> facility, and the pointer corresponding to an array <tt>x</tt> can be accessed using the syntax <tt>x.ctypes.data</tt>.<span class="FootOuter"><span class="SupFootMarker"> [146] </span><span class="HoverFoot"><span class="SupFootMarker"> [146] </span>Passing an array to C using Numpy’s <tt>ndpointer</tt><span class="default"> facility, although safer and more convenient, can be slow. The cost is more than <span class="formula">10<sup>4</sup></span> cycles per array. A C function call from Python using the <tt>x.ctypes.data</tt> syntax takes around one or two thousand cycles. Although far more expensive than a typical function call in C, which may consume just a few cycles, an overhead of around a thousand cycles is often manageable.</span></span></span> Using Python’s <tt>ctypes</tt> library, the whole C standard library can be loaded in a single easy line and the <tt>printf</tt> routine of the C standard library used to print messages in the next line---the <tt>ctypes</tt> library is quite remarkable.
</div>
<h? class="Subsubsection">
<b><u>The <tt>StatVector</tt> class</u></b>
</h?>
<div class="Unindented">
If we want to find out the number of cycles required to multiply two square matrices of dimension <span class="formula">1000</span>, it is never enough to time just once. The computer system is so complicated with so many heavily designed parts that the first run is likely to be atypical. One must time the same program several times and look at the mean or median. System activities can make the occasional timing figure to be far in excess of the typical. Therefore, if our intention is to get an idea of whether the program is well optimized, it is often better to look at medians.
</div>
<div class="Indented">
The public interface to the <tt>StatVector</tt> class, which is used to gather data and calculate means and medians, is as follows:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class StatVector{
public:
	StatVector(int n);
	~StatVector();
	void insert(double x);
	double median();
	double mean();
	double max();
	double min();
	void flush();
	void print(const char* banner = "");
};
</pre>
</div>

</div>
<div class="Indented">
The class interface is self-explanatory for the most part. The argument to the constructor specifies the maximum size of the dataset. The member function <tt>flush()</tt> may be used to discard all the data items inserted and start over again.
</div>
<h? class="Subsubsection">
<b><u>The <tt>Table</tt> class</u></b>
</h?>
<div class="Unindented">
When programs such as the FFT are timed, it is natural to lay out the timing figures in a table. The rows may be indexed by the dimension of the FFT and the columns by the implementation. The following class is used to generate tables:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">class Table{
public:
	Table();
	void dim(int nrows, int ncols);
	void rows(const char* rowsi[]);
	void cols(const char* colsi[]);
	void data(double *datai);
	void print(const char *banner="");
};
</pre>
</div>

</div>
<div class="Indented">
This class interface too is almost self-explanatory. The member functions <tt>dim()</tt>, <tt>rows()</tt>, <tt>cols()</tt>, and <tt>data()</tt> must be called in that order. The class prints the data in a well-formatted table.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-9.3">9.3</a> C/C++ versus Python versus MATLAB 
</h2>
<div class="Unindented">
How much faster C/C++ can be relative to interpreted languages such as Python and MATLAB is often not understood. That point is worth going into because it provides necessary motivation for the considerable effort of mastering C/C++ as well as techniques of program optimization. 
</div>
<div class="Indented">
The following is a simple Python program for identifying prime numbers less than <span class="formula"><i>n</i></span>:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">import numpy as np
​
def primes(n):
    assert n &gt;= 3
    p = np.zeros((n+1), dtype = bool)
    p[0] = False
    p[1] = False
    p[2] = True
    ksqrt = 1
    for k in range(3,n+1):
        if ksqrt*ksqrt &lt; k:
            ksqrt += 1
        p[k] = True
        for j in range(2, ksqrt+1):
            if p[j] and k%j == 0:
                p[k] = False
                break
    return p
</pre>
</div>

</div>
<div class="Indented">
With <span class="formula"><i>n</i> = 10<sup>7</sup></span>, this program consumes <span class="formula">4.9 × 10<sup>11</sup></span> cycles on a <span class="formula">3.6</span>  GHz AVX2 machine (see table <a class="Reference" href="#tab:appendix-machines-used">9.1↑</a>). The same program in MATLAB consumes <span class="formula">4.7 × 10<sup>11</sup></span> cycles. In C/C++, the program consumes <span class="formula">1.1 × 10<sup>10</sup></span> cycles, which is less than <span class="formula">2.5</span>% of the time taken by Python or MATLAB.
</div>
<div class="Indented">
In fact, a C/C++ speedup of around <span class="formula">40</span> is at the low end. Much of the overhead in interpreted languages is due to symbol-table lookup. In a simple program such as this, the symbol-table lookup will be less expensive than in more complicated programs. In addition, the C/C++ compiler has no room to optimize this program for the instruction set architecture. So none of the vast range of optimizations possible in C/C++ can kick in.
</div>
<div class="Indented">
However, if one limits oneself to a narrow idiom, relying mostly on BLAS/LAPACK routines and not much more, Python and MATLAB can be as fast as C/C++. However, such a thing is too constraining and not possible to sustain in more complex programs. For even moderately complex programs, C/C++ can be faster than MATLAB by more than a factor of 1,000.<span class="FootOuter"><span class="SupFootMarker"> [147] </span><span class="HoverFoot"><span class="SupFootMarker"> [147] </span>For an example of a fairly simple program that is several hundred times faster in C/C++ than in MATLAB, see (B. Sadiq and D. Viswanath, Finite difference weights, spectral differentiation, and superconvergence, <i>Mathematics of Computation</i>, 83 (2014), pp. 2403-2427). For an example of a more complex program that is <span class="formula">10<sup>4</sup></span> times faster, see (D. Viswanath, Spectral integration of linear boundary value problems, <i>Journal of Computational and Applied Mathematics</i>, 290 (2015), pp. 159-173). </span></span>
</div>
<div class="Indented">
As a program becomes more complex, it becomes harder to write in C/C++ because one is forced to think about its memory layout precisely. In Python or MATLAB, one can think at a higher level and create objects in memory with much less care. That makes the interpreted languages easier to program in, but it also makes them much slower.
</div>
<div class="Indented">
On multicore machines, one should expect an even greater speedup from using C/C++. The advantage, as well as the difficulty, of C/C++ lies in giving the programmer a relatively faithful picture of the machine. As the hardware platform becomes more complex, the speedup from C/C++ should be expected to increase. 
</div>
<div class="Indented">
It would be folly to say that the choice between C/C++ and Python or Matlab comes down solely to the matter of speed. A Python program can be written in a small fraction of the time that it takes to write a C/C++ program. Sometimes that is all that matters. 
</div>
<div class="Indented">
Another important point is how well a program can be structured. In C/C++, source files are organized in a source tree, and the program is built using the <tt>make</tt> utility or some equivalent tool. Python’s facilities for structuring programs are far superior. These include consistent and uniform control of name spaces, modules, and packages, with special significance attached to <tt>__init__.py</tt> and <tt>__main__.py</tt> files. In addition, as mentioned on page <a class="Reference" href="#proc-python-ctypes">1↑</a>, the <tt>ctypes</tt> library in Python virtually erases the distinction between Python and C. With judicious use of the <tt>ctypes</tt> library, one may attempt to approach the speed of C/C++ within the greater convenience and superior structure of Python.
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-9.4">9.4</a> GIT
</h2>
<div class="Unindented">
The source code for this book may be obtained in its entirety using the command
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">git clone https://github.com/divakarvi/bk-spca
</pre>
</div>

</div>
<div class="Indented">
The <tt>git</tt> utility is a tool for managing sources.<span class="FootOuter"><span class="SupFootMarker"> [148] </span><span class="HoverFoot"><span class="SupFootMarker"> [148] </span>For GIT documentation and more, see <a class="FlexURL" href="https://git-scm.com/">https://git-scm.com/</a>.</span></span>
</div>
<div class="Indented">
GIT is a cleanly designed and well-thought-out program. Although there are many facilities in GIT, GIT can be grasped easily by paying attention to its internal design. With that in mind, we mention a few points about GIT internals:
</div>
<ul>
<li>
GIT thinks of a file as an atom. If there is a slightest change to a file, it becomes a new object as far as GIT is concerned.
</li>
<li>
As the source tree evolves, every file is stored in <tt>.git/</tt>, including older versions. The name and location of a file are obtained using its <span class="formula">160</span>-bit SHA2 hash. Alternatively, GIT may compress several files into a single pack file.
</li>
<li>
A commit is a tree-like hierarchy of files, with the working directory corresponding to one particular commit. Commit objects are also stored in <tt>.git/</tt>.
</li>
<li>
GIT thinks of commits as being organized in a directed acyclic graph. Those vertices of the directed acyclic graph, whose files may be modified to produce new commits and grow the graph, are labeled using branch names.
</li>
<li>
When a GIT repository is cloned, it is cloned in full, including all the ancestors of a branch. 
</li>

</ul>
<h2 class="Section">
<a class="toc" name="toc-Section-9.5">9.5</a> Cscope
</h2>
<div class="Unindented">
The <tt>cscope</tt> utility is invaluable for browsing source code. Using <tt>cscope</tt>, one can easily find the definition of a function, all the places where it is called, and search for patterns inside the entire source tree. It is a search utility that predates the era of Internet search by a few decades.
</div>
<div class="Indented">
Suppose the source for this book is saved in the GIT repository <tt>bk-spca</tt>. To prepare <tt>cscope</tt> database files, we may run the command
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">cscope.py bk-spca
</pre>
</div>

</div>
<div class="Indented">
in the parent directory of <tt>bk-spca</tt>; <tt>cscope.py</tt> is the following Python script, which must be on the user’s path:
</div>
<div class="Indented">
<div class="listing">
<pre class="listing">#!/usr/bin/env python
import os, sys
​
if __name__ == ’__main__’:
    if len(sys.argv) != 2:
        print(’Usage: cscope.py dirname’)
        sys.exit(0)
​
    ddir = sys.argv[1]
    assert os.path.isdir(ddir)
    cmd = ’rm -rf cscope.*’
    print(cmd)
    os.system(cmd)
​
    cmd = ’find ’ + ddir \
          + ’ -name *.h’  + ’ -o -name *.hh’\
          + ’ -o -name *.c’ +’ -o -name *.cpp’ \
          + ’ &gt; cscope.files’
    print(cmd)
    os.system(cmd)
​
    cmd = ’cscope -b -q’
    print(cmd)
    os.system(cmd)
</pre>
</div>

</div>
<div class="Indented">
Once the database files are generated, one only has to say <tt>cscope -d</tt> to obtain access to the search facilities of <tt>cscope</tt>. 
</div>
<h2 class="Section">
<a class="toc" name="toc-Section-9.6">9.6</a> Compiling with gcc/g++
</h2>
<div class="Unindented">
The Makefiles in the GIT repository <tt>bk-spca</tt> use Intel compilers for the most part. The switch to <tt>gcc/g++</tt> is not too complicated. The following points must be kept in mind:
</div>
<ul>
<li>
To enable the <tt>restrict</tt> qualifier, use the option <tt>-Drestrict="__restrict__"</tt>.
</li>
<li>
To generate code for a specific instruction set such as AVX2, use an option such as <tt>-mavx2</tt>.
</li>
<li>
<tt>-openmp</tt> becomes <tt>-fopenmp</tt>.
</li>

</ul>
<div class="Unindented">

</div>
<div class="Unindented">

</div>
<div class="Indented">

</div>
<div class="Indented">
<div class="left">
<b>Scientific and Engineering Computation</b>
</div>

</div>
<div class="Indented">
<div class="left">
William Gropp and Ewing Lusk, editors; Janusz Kowalik, founding editor
</div>

</div>
<div class="Indented">
<div class="vspace" style="height: 0.5cm;">

</div>

</div>
<div class="Indented">
<div class="left">
<i>Data-Parallel Programming on MIMD Computers</i>, Philip J. Hatcher and Michael J. Quinn, 1991
</div>

</div>
<div class="Indented">
<div class="left">
<i>Enterprise Integration Modeling: Proceedings of the First International Conference</i>, edited by Charles J. Petrie, Jr., 1992
</div>

</div>
<div class="Indented">
<div class="left">
<i>The High Performance Fortran Handbook</i>, Charles H. Koelbel, David B. Loveman, Robert S. Schreiber, Guy L. Steele Jr., and Mary E. Zosel, 1994
</div>

</div>
<div class="Indented">
<div class="left">
<i>PVM: A User’s Guide and Tutorial for Network Parallel Computing</i>, Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidyalingham S. Sunderam, 1994
</div>

</div>
<div class="Indented">
<div class="left">
<i>Practical Parallel Programming</i>, Gregory V. Wilson, 1995
</div>

</div>
<div class="Indented">
<div class="left">
<i>Enabling Technologies for Petaflops Computing</i>, Thomas Sterling, Paul Messina, and Paul H. Smith, 1995
</div>

</div>
<div class="Indented">
<div class="left">
<i>An Introduction to High-Performance Scientific Computing</i>, Lloyd D. Fosdick, Elizabeth R. Jessup, Carolyn J. C. Schauble, and Gitta Domik, 1995
</div>

</div>
<div class="Indented">
<div class="left">
<i>Parallel Programming Using C++</i>, edited by Gregory V. Wilson and Paul Lu, 1996
</div>

</div>
<div class="Indented">
<div class="left">
<i>Using PLAPACK: Parallel Linear Algebra Package</i>, Robert A. van de Geijn, 1997
</div>

</div>
<div class="Indented">
<div class="left">
<i>Fortran 95 Handbook</i>, Jeanne C. Adams, Walter S. Brainerd, Jeanne T. Martin, Brian T. Smith, and Jerrold L. Wagener, 1997
</div>

</div>
<div class="Indented">
<div class="left">
<i>MPI—The Complete Reference: Volume 1</i>, The MPI Core, Marc Snir, Steve Otto, Steven Huss-Lederman, David Walker, and Jack Dongarra, 1998
</div>

</div>
<div class="Indented">
<div class="left">
<i>MPI—The Complete Reference: Volume 2</i>, The MPI-2 Extensions, William Gropp, Steven Huss-Lederman, Andrew Lumsdaine, Ewing Lusk, Bill Nitzberg, William Saphir, and Marc Snir, 1998
</div>

</div>
<div class="Indented">
<div class="left">
<i>A Programmer’s Guide to ZPL</i>, Lawrence Snyder, 1999
</div>

</div>
<div class="Indented">
<div class="left">
<i>How to Build a Beowulf</i>, Thomas L. Sterling, John Salmon, Donald J. Becker, and Daniel F. Savarese, 1999
</div>

</div>
<div class="Indented">
<div class="left">
<i>Using MPI-2: Advanced Features of the Message-Passing Interface</i>, William Gropp, Ewing Lusk, and Rajeev Thakur, 1999
</div>

</div>
<div class="Indented">
<div class="left">
<i>Beowulf Cluster Computing with Windows</i>, edited by Thomas Sterling, William Gropp, and Ewing Lusk, 2001
</div>

</div>
<div class="Indented">
<div class="left">
<i>Beowulf Cluster Computing with Linux</i>, <i>second edition</i>, edited by Thomas Sterling, William Gropp, and Ewing Lusk, 2003
</div>

</div>
<div class="Indented">
<div class="left">
<i>Scalable Input/Output: Achieving System Balance</i>, edited by Daniel A. Reed, 2003
</div>

</div>
<div class="Indented">
<div class="left">
<i>Using OpenMP: Portable Shared Memory Parallel Programming</i>, Barbara Chapman, Gabriele Jost, and Ruud van der Pas, 2008
</div>

</div>
<div class="Indented">
<div class="left">
<i>Quantum Computing without Magic: Devices</i>, Zdzislaw Meglicki, 2008
</div>

</div>
<div class="Indented">
<div class="left">
<i>Quantum Computing: A Gentle Introduction</i>, Eleanor G. Rieffel and Wolfgang H. Polak, 2011
</div>

</div>
<div class="Indented">
<div class="left">
<i>Using MPI: Portable Parallel Programming with the Message-Passing Interface, third edition</i>, William Gropp, Ewing Lusk, and Anthony Skjellum, 2015
</div>

</div>
<div class="Indented">
<div class="left">
<i>Using Advanced MPI: Beyond the Basics</i>, Pavan Balaji, William Gropp, Torsten Hoefler, Rajeev Thakur, and Ewing Lusk, 2015
</div>

</div>
<div class="Indented">
<div class="left">
<i>Scientific Programming and Computer Architecture</i>, Divakar Viswanath, 2017
</div>

</div>

<hr class="footer"/>
<div class="footer">
Copyright (C) 2017 
</div>
</div>
</body>
</html>
